{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_tanh\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.454152 Test MSE 384.78088424904854 Test RE 0.9999293746051297\n",
      "1 Train Loss 3.8074567 Test MSE 386.173430316612 Test RE 1.001737143924164\n",
      "2 Train Loss 2.9927678 Test MSE 385.9196072530607 Test RE 1.0014078802242188\n",
      "3 Train Loss 2.4071212 Test MSE 384.6568828938748 Test RE 0.99976824057557\n",
      "4 Train Loss 2.3851192 Test MSE 384.0937319378361 Test RE 0.9990361249072037\n",
      "5 Train Loss 2.3845146 Test MSE 384.03091422225873 Test RE 0.9989544264465318\n",
      "6 Train Loss 2.3823962 Test MSE 384.0191560110529 Test RE 0.998939133397709\n",
      "7 Train Loss 2.3812997 Test MSE 383.87485325828084 Test RE 0.9987514302468927\n",
      "8 Train Loss 2.3800244 Test MSE 383.5727606438652 Test RE 0.9983583661913644\n",
      "9 Train Loss 2.3797944 Test MSE 383.43814204705086 Test RE 0.9981831590204283\n",
      "10 Train Loss 2.378382 Test MSE 383.02416251910864 Test RE 0.9976441685767566\n",
      "11 Train Loss 2.3775952 Test MSE 382.7422124760966 Test RE 0.997276910284006\n",
      "12 Train Loss 2.377263 Test MSE 382.28590131217794 Test RE 0.9966822485394925\n",
      "13 Train Loss 2.3753848 Test MSE 381.62215973967267 Test RE 0.9958166308974714\n",
      "14 Train Loss 2.3709111 Test MSE 381.3057751563401 Test RE 0.9954037534618059\n",
      "15 Train Loss 2.3645806 Test MSE 380.7840961021598 Test RE 0.9947225953778709\n",
      "16 Train Loss 2.3530874 Test MSE 378.407688058525 Test RE 0.9916137911025448\n",
      "17 Train Loss 2.214468 Test MSE 352.0706295154881 Test RE 0.9564834964419251\n",
      "18 Train Loss 2.1566954 Test MSE 337.98617175210916 Test RE 0.9371563420886433\n",
      "19 Train Loss 2.0744317 Test MSE 324.2927406397982 Test RE 0.9179757232839922\n",
      "20 Train Loss 2.0502498 Test MSE 321.71022332772844 Test RE 0.914313250226842\n",
      "21 Train Loss 2.0110173 Test MSE 320.2750765738872 Test RE 0.9122715986563498\n",
      "22 Train Loss 1.8361759 Test MSE 288.93233119243644 Test RE 0.8664842107796916\n",
      "23 Train Loss 1.7877713 Test MSE 279.5018940820351 Test RE 0.8522263542053282\n",
      "24 Train Loss 1.7712328 Test MSE 271.88449229932814 Test RE 0.840533062338369\n",
      "25 Train Loss 1.6907518 Test MSE 264.3965170842656 Test RE 0.8288776844591109\n",
      "26 Train Loss 1.5821228 Test MSE 244.0789443244688 Test RE 0.7963935621809589\n",
      "27 Train Loss 1.3190587 Test MSE 189.39439863831112 Test RE 0.701529838488989\n",
      "28 Train Loss 1.2587143 Test MSE 179.392988017837 Test RE 0.6827556682642597\n",
      "29 Train Loss 1.2051575 Test MSE 177.45429458041778 Test RE 0.679056388896536\n",
      "30 Train Loss 1.1899161 Test MSE 172.54970482240526 Test RE 0.6696065494732018\n",
      "31 Train Loss 1.1849701 Test MSE 170.90605330516982 Test RE 0.6664096927532625\n",
      "32 Train Loss 1.1697849 Test MSE 173.35169129897042 Test RE 0.6711608634923882\n",
      "33 Train Loss 1.1479207 Test MSE 171.7491905995585 Test RE 0.6680514824351357\n",
      "34 Train Loss 1.1187353 Test MSE 169.45989483414152 Test RE 0.6635842182134899\n",
      "35 Train Loss 1.0990574 Test MSE 168.18059179583372 Test RE 0.6610746758198315\n",
      "36 Train Loss 1.0647485 Test MSE 161.9318443014493 Test RE 0.6486773198390132\n",
      "37 Train Loss 0.9475267 Test MSE 147.88484028286783 Test RE 0.6199039602456555\n",
      "38 Train Loss 0.8887374 Test MSE 135.83484129924528 Test RE 0.594111791978254\n",
      "39 Train Loss 0.86991775 Test MSE 129.67460812990947 Test RE 0.5804837339657214\n",
      "40 Train Loss 0.8578366 Test MSE 126.94618196290001 Test RE 0.5743444176687617\n",
      "41 Train Loss 0.77843267 Test MSE 114.48300003114227 Test RE 0.5454225422680264\n",
      "42 Train Loss 0.7059909 Test MSE 104.36853285263935 Test RE 0.5207716954208367\n",
      "43 Train Loss 0.6692688 Test MSE 99.88938352291312 Test RE 0.5094742620796676\n",
      "44 Train Loss 0.6240612 Test MSE 87.86574962613402 Test RE 0.4778288765077664\n",
      "45 Train Loss 0.5710247 Test MSE 68.76923275969627 Test RE 0.42272669292049236\n",
      "46 Train Loss 0.4797745 Test MSE 63.314560043742055 Test RE 0.40561534745849304\n",
      "47 Train Loss 0.46668845 Test MSE 66.20888626195043 Test RE 0.4147827854363213\n",
      "48 Train Loss 0.4558192 Test MSE 67.36150350927325 Test RE 0.4183776426455924\n",
      "49 Train Loss 0.4449355 Test MSE 63.68650431758137 Test RE 0.4068050057806794\n",
      "50 Train Loss 0.4381067 Test MSE 60.95866034364684 Test RE 0.3979974500846954\n",
      "51 Train Loss 0.43487012 Test MSE 61.30055681043386 Test RE 0.39911200588589574\n",
      "52 Train Loss 0.42387348 Test MSE 59.171019216434686 Test RE 0.3921182967740605\n",
      "53 Train Loss 0.41703898 Test MSE 59.34418719281029 Test RE 0.3926916578993742\n",
      "54 Train Loss 0.40981382 Test MSE 59.72656068950536 Test RE 0.39395474525152796\n",
      "55 Train Loss 0.37804866 Test MSE 52.89814344122727 Test RE 0.37075139816012137\n",
      "56 Train Loss 0.35949627 Test MSE 46.92380504110623 Test RE 0.3491879057827156\n",
      "57 Train Loss 0.34922668 Test MSE 44.52274383260822 Test RE 0.3401367385111512\n",
      "58 Train Loss 0.32612044 Test MSE 40.344206242712495 Test RE 0.3237823495139776\n",
      "59 Train Loss 0.28288972 Test MSE 36.37506201102201 Test RE 0.3074428898702013\n",
      "60 Train Loss 0.27176046 Test MSE 34.85308954428928 Test RE 0.3009422915142036\n",
      "61 Train Loss 0.26240084 Test MSE 31.029374392868878 Test RE 0.2839547201375504\n",
      "62 Train Loss 0.23832783 Test MSE 27.074905863432658 Test RE 0.265244300203129\n",
      "63 Train Loss 0.22626072 Test MSE 25.695858847720405 Test RE 0.25840097707775367\n",
      "64 Train Loss 0.21694309 Test MSE 23.581838246123187 Test RE 0.24754343358109726\n",
      "65 Train Loss 0.20528755 Test MSE 21.531418173579713 Test RE 0.23653689980264903\n",
      "66 Train Loss 0.1986745 Test MSE 20.524859253097052 Test RE 0.23094186908163236\n",
      "67 Train Loss 0.19342402 Test MSE 21.39195810969828 Test RE 0.23576962474625732\n",
      "68 Train Loss 0.1712614 Test MSE 20.463754581330942 Test RE 0.23059784368425382\n",
      "69 Train Loss 0.16445373 Test MSE 19.22653922355314 Test RE 0.22351832880862513\n",
      "70 Train Loss 0.1588676 Test MSE 19.135037283640496 Test RE 0.22298581610619567\n",
      "71 Train Loss 0.15259838 Test MSE 18.816808240587214 Test RE 0.22112383716563133\n",
      "72 Train Loss 0.14739388 Test MSE 17.941571212197275 Test RE 0.21591997394031354\n",
      "73 Train Loss 0.14569981 Test MSE 17.63384691347502 Test RE 0.21406029304016772\n",
      "74 Train Loss 0.14199962 Test MSE 17.427259474410704 Test RE 0.21280269870281038\n",
      "75 Train Loss 0.13854954 Test MSE 16.02717344990612 Test RE 0.20407558542779788\n",
      "76 Train Loss 0.13815102 Test MSE 15.611558220307991 Test RE 0.20141217022869778\n",
      "77 Train Loss 0.13518152 Test MSE 15.141239653152905 Test RE 0.1983550670949081\n",
      "78 Train Loss 0.12819794 Test MSE 14.256667899888797 Test RE 0.19247379044086899\n",
      "79 Train Loss 0.12322733 Test MSE 12.426063529069468 Test RE 0.1796922585449934\n",
      "80 Train Loss 0.118351966 Test MSE 11.021200382438408 Test RE 0.1692298742887933\n",
      "81 Train Loss 0.11464211 Test MSE 10.10395915504812 Test RE 0.16203482838582262\n",
      "82 Train Loss 0.11387658 Test MSE 9.583563818224059 Test RE 0.15780694128516567\n",
      "83 Train Loss 0.11260082 Test MSE 9.105548286216186 Test RE 0.1538210010630931\n",
      "84 Train Loss 0.11116296 Test MSE 8.861647940105001 Test RE 0.15174690030520424\n",
      "85 Train Loss 0.10903729 Test MSE 7.874452412664258 Test RE 0.143045026963081\n",
      "86 Train Loss 0.09926024 Test MSE 6.206812460098831 Test RE 0.12699800367321373\n",
      "87 Train Loss 0.0878896 Test MSE 5.111640104376677 Test RE 0.11525047312886377\n",
      "88 Train Loss 0.07822381 Test MSE 4.764850225674466 Test RE 0.1112723370189892\n",
      "89 Train Loss 0.075362444 Test MSE 4.302901575022412 Test RE 0.10574096964791704\n",
      "90 Train Loss 0.074000195 Test MSE 4.044878207081888 Test RE 0.10252158391055566\n",
      "91 Train Loss 0.07208732 Test MSE 3.9032829722991607 Test RE 0.10071116064996492\n",
      "92 Train Loss 0.068862975 Test MSE 3.3550565960409573 Test RE 0.09337110701008874\n",
      "93 Train Loss 0.066513464 Test MSE 2.9602516583053857 Test RE 0.08770551419682403\n",
      "94 Train Loss 0.06567781 Test MSE 2.9540042955978847 Test RE 0.087612917752929\n",
      "95 Train Loss 0.05938398 Test MSE 2.5989752337738197 Test RE 0.08217952998528329\n",
      "96 Train Loss 0.05534843 Test MSE 2.1603335973981577 Test RE 0.07492435146480358\n",
      "97 Train Loss 0.053695615 Test MSE 2.133428825275603 Test RE 0.07445633618869942\n",
      "98 Train Loss 0.0525949 Test MSE 2.3056841247659277 Test RE 0.0774038365533755\n",
      "99 Train Loss 0.052226935 Test MSE 2.364552885052129 Test RE 0.07838574645684412\n",
      "100 Train Loss 0.048362248 Test MSE 2.2351154892572707 Test RE 0.07621010616656525\n",
      "101 Train Loss 0.040908042 Test MSE 2.1446022962410978 Test RE 0.07465105777102617\n",
      "102 Train Loss 0.03413827 Test MSE 2.58976499104409 Test RE 0.08203378693070101\n",
      "103 Train Loss 0.03113656 Test MSE 2.873172369291674 Test RE 0.08640590497591047\n",
      "104 Train Loss 0.02853376 Test MSE 2.4949403252735767 Test RE 0.08051794158824926\n",
      "105 Train Loss 0.02509066 Test MSE 2.395930905396206 Test RE 0.07890412848696418\n",
      "106 Train Loss 0.023396954 Test MSE 2.61542817770827 Test RE 0.08243924043803237\n",
      "107 Train Loss 0.023348063 Test MSE 2.6274056409845947 Test RE 0.08262779179830641\n",
      "108 Train Loss 0.023338102 Test MSE 2.630016939317169 Test RE 0.08266884222223816\n",
      "109 Train Loss 0.023318185 Test MSE 2.6347043509182635 Test RE 0.08274247870321903\n",
      "110 Train Loss 0.02331503 Test MSE 2.6369594841039334 Test RE 0.08277788218184498\n",
      "111 Train Loss 0.023306135 Test MSE 2.639821975322754 Test RE 0.08282279881651204\n",
      "112 Train Loss 0.023296827 Test MSE 2.6430861570712607 Test RE 0.08287398884936442\n",
      "113 Train Loss 0.022913676 Test MSE 2.591067535201841 Test RE 0.0820544141300886\n",
      "114 Train Loss 0.02210826 Test MSE 2.325453529423933 Test RE 0.07773496639392449\n",
      "115 Train Loss 0.021628613 Test MSE 2.164607140284874 Test RE 0.07499842202408419\n",
      "116 Train Loss 0.021469356 Test MSE 2.0780974046498626 Test RE 0.07348446429730758\n",
      "117 Train Loss 0.021208577 Test MSE 1.9122502695656194 Test RE 0.07049120716400116\n",
      "118 Train Loss 0.020849198 Test MSE 1.9046016408603745 Test RE 0.0703500903617409\n",
      "119 Train Loss 0.020465227 Test MSE 2.0510694601947077 Test RE 0.07300502711579615\n",
      "120 Train Loss 0.019149955 Test MSE 1.6805849778596964 Test RE 0.06608346721687187\n",
      "121 Train Loss 0.018400004 Test MSE 1.500904072019792 Test RE 0.06245095017840537\n",
      "122 Train Loss 0.018134039 Test MSE 1.347180902666799 Test RE 0.05916645396504215\n",
      "123 Train Loss 0.017127583 Test MSE 1.238894202105069 Test RE 0.056738740732944465\n",
      "124 Train Loss 0.016605305 Test MSE 1.2801512377402522 Test RE 0.05767574629201872\n",
      "125 Train Loss 0.016550299 Test MSE 1.2959894605736428 Test RE 0.05803143599781978\n",
      "126 Train Loss 0.01654134 Test MSE 1.298915456638416 Test RE 0.05809690876052602\n",
      "127 Train Loss 0.01653311 Test MSE 1.3018289044980194 Test RE 0.05816202751275179\n",
      "128 Train Loss 0.016532898 Test MSE 1.3029745015297889 Test RE 0.05818761290027885\n",
      "129 Train Loss 0.016523642 Test MSE 1.3038477146906806 Test RE 0.05820710740278954\n",
      "130 Train Loss 0.016521934 Test MSE 1.3044243201459635 Test RE 0.05821997655364411\n",
      "131 Train Loss 0.01651851 Test MSE 1.3049888560668803 Test RE 0.05823257357143273\n",
      "132 Train Loss 0.01651255 Test MSE 1.304252499420006 Test RE 0.058216142016093896\n",
      "133 Train Loss 0.016507838 Test MSE 1.3038996552724413 Test RE 0.05820826677166298\n",
      "134 Train Loss 0.016502101 Test MSE 1.3022903190083557 Test RE 0.05817233394618322\n",
      "135 Train Loss 0.01649873 Test MSE 1.2996882828219192 Test RE 0.058114189383208344\n",
      "136 Train Loss 0.01648152 Test MSE 1.2942948422060152 Test RE 0.057993483024915256\n",
      "137 Train Loss 0.016472202 Test MSE 1.2898130567347539 Test RE 0.057892988247964336\n",
      "138 Train Loss 0.015874596 Test MSE 1.1849564794532605 Test RE 0.05548987960829087\n",
      "139 Train Loss 0.014741085 Test MSE 1.191272611089738 Test RE 0.05563757093329957\n",
      "140 Train Loss 0.013836894 Test MSE 1.0175919301178389 Test RE 0.05142205276142057\n",
      "141 Train Loss 0.011792763 Test MSE 0.5668669071626191 Test RE 0.03837983777064174\n",
      "142 Train Loss 0.01114679 Test MSE 0.49050712415938946 Test RE 0.03570139961671352\n",
      "143 Train Loss 0.010504305 Test MSE 0.5367710617408633 Test RE 0.03734712107906569\n",
      "144 Train Loss 0.010302473 Test MSE 0.5457793138678021 Test RE 0.03765920243140904\n",
      "145 Train Loss 0.010293559 Test MSE 0.5458409153086257 Test RE 0.03766132764540563\n",
      "146 Train Loss 0.010286085 Test MSE 0.5445695367113621 Test RE 0.03761744148847461\n",
      "147 Train Loss 0.010284273 Test MSE 0.5433897982802823 Test RE 0.03757667277147975\n",
      "148 Train Loss 0.010276678 Test MSE 0.5414048618266406 Test RE 0.03750797848691996\n",
      "149 Train Loss 0.01027196 Test MSE 0.5397113366139883 Test RE 0.03744926968775918\n",
      "150 Train Loss 0.010264279 Test MSE 0.5371606336988184 Test RE 0.03736067131789355\n",
      "151 Train Loss 0.010260268 Test MSE 0.535057425488312 Test RE 0.03728745826728351\n",
      "152 Train Loss 0.010257056 Test MSE 0.5326051979368845 Test RE 0.03720191386050197\n",
      "153 Train Loss 0.010251394 Test MSE 0.5301586651715461 Test RE 0.037116371640004545\n",
      "154 Train Loss 0.010245321 Test MSE 0.5277108549800351 Test RE 0.03703058699430878\n",
      "155 Train Loss 0.010239396 Test MSE 0.5246127645088827 Test RE 0.03692172720434139\n",
      "156 Train Loss 0.010230399 Test MSE 0.5215328988667884 Test RE 0.036813188721507635\n",
      "157 Train Loss 0.010223551 Test MSE 0.5171947665378233 Test RE 0.036659762186271365\n",
      "158 Train Loss 0.010084183 Test MSE 0.45919517721768344 Test RE 0.0345430945719082\n",
      "159 Train Loss 0.01003908 Test MSE 0.44467446844627795 Test RE 0.033992544896367084\n",
      "160 Train Loss 0.010032434 Test MSE 0.44259334145071394 Test RE 0.03391290712401664\n",
      "161 Train Loss 0.010025271 Test MSE 0.44069586724851334 Test RE 0.0338401337932026\n",
      "162 Train Loss 0.010018952 Test MSE 0.4397748475864939 Test RE 0.03380475369060453\n",
      "163 Train Loss 0.01001105 Test MSE 0.4394760631012467 Test RE 0.03379326820875032\n",
      "164 Train Loss 0.010002084 Test MSE 0.43933424749487476 Test RE 0.033787815352604146\n",
      "165 Train Loss 0.009995588 Test MSE 0.4404811888894909 Test RE 0.03383189043324047\n",
      "166 Train Loss 0.009988626 Test MSE 0.44145081421099874 Test RE 0.033869106806396876\n",
      "167 Train Loss 0.009980575 Test MSE 0.442360626784296 Test RE 0.03390399028342549\n",
      "168 Train Loss 0.009973869 Test MSE 0.4432011308146792 Test RE 0.03393618451081194\n",
      "169 Train Loss 0.00996544 Test MSE 0.4442142703842943 Test RE 0.03397495073047493\n",
      "170 Train Loss 0.009958564 Test MSE 0.44467658825599293 Test RE 0.03399262591927951\n",
      "171 Train Loss 0.009937414 Test MSE 0.44598655678351984 Test RE 0.03404265837536677\n",
      "172 Train Loss 0.009497559 Test MSE 0.40672478419984165 Test RE 0.032509695462985766\n",
      "173 Train Loss 0.008601697 Test MSE 0.2921978825712823 Test RE 0.02755504437556069\n",
      "174 Train Loss 0.008114304 Test MSE 0.1992417090284609 Test RE 0.02275373583242899\n",
      "175 Train Loss 0.00735464 Test MSE 0.12009706140777622 Test RE 0.017665615477338564\n",
      "176 Train Loss 0.006634332 Test MSE 0.07043591535005757 Test RE 0.013528812068681291\n",
      "177 Train Loss 0.0061677443 Test MSE 0.05559893162408086 Test RE 0.012019760264798776\n",
      "178 Train Loss 0.0059023774 Test MSE 0.055665111854716065 Test RE 0.0120269117865005\n",
      "179 Train Loss 0.005694761 Test MSE 0.05789612283844282 Test RE 0.012265558356004198\n",
      "180 Train Loss 0.0055194115 Test MSE 0.08700804987990172 Test RE 0.015036345629584799\n",
      "181 Train Loss 0.0053165928 Test MSE 0.1046939611707238 Test RE 0.016493901569811384\n",
      "182 Train Loss 0.0051399427 Test MSE 0.10053585057110148 Test RE 0.01616304048746197\n",
      "183 Train Loss 0.0051305047 Test MSE 0.10119109761582973 Test RE 0.016215626624538525\n",
      "184 Train Loss 0.005111132 Test MSE 0.10261973689930252 Test RE 0.016329693410927642\n",
      "185 Train Loss 0.005104744 Test MSE 0.1032420782139641 Test RE 0.016379134589509233\n",
      "186 Train Loss 0.005095043 Test MSE 0.10376990669198487 Test RE 0.016420950639947498\n",
      "187 Train Loss 0.00508683 Test MSE 0.10435865546344486 Test RE 0.016467467692734005\n",
      "188 Train Loss 0.0050816345 Test MSE 0.10458513579893534 Test RE 0.016485326950798698\n",
      "189 Train Loss 0.005075423 Test MSE 0.10466389143261902 Test RE 0.016491532746851185\n",
      "190 Train Loss 0.005068422 Test MSE 0.10464433029193992 Test RE 0.01648999158369635\n",
      "191 Train Loss 0.0050631384 Test MSE 0.10421905469442848 Test RE 0.01645644972561306\n",
      "192 Train Loss 0.005057416 Test MSE 0.10394804519475842 Test RE 0.016435039258361636\n",
      "193 Train Loss 0.005053147 Test MSE 0.10319900748760868 Test RE 0.01637571769386061\n",
      "194 Train Loss 0.0050498866 Test MSE 0.10264005149557123 Test RE 0.01633130964345325\n",
      "195 Train Loss 0.0050467597 Test MSE 0.10188487471025849 Test RE 0.01627111971411633\n",
      "196 Train Loss 0.0050446875 Test MSE 0.10130842279355448 Test RE 0.01622502443808186\n",
      "197 Train Loss 0.0050404975 Test MSE 0.10072796999996743 Test RE 0.01617847653348481\n",
      "198 Train Loss 0.0050391993 Test MSE 0.09992878557732399 Test RE 0.01611416800632806\n",
      "199 Train Loss 0.0050391993 Test MSE 0.09992878557732399 Test RE 0.01611416800632806\n",
      "Training time: 38.45\n",
      "Training time: 38.45\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4.5274663 Test MSE 384.7090677714284 Test RE 0.9998360555765978\n",
      "1 Train Loss 3.8580248 Test MSE 385.3476928477394 Test RE 1.0006655858128077\n",
      "2 Train Loss 3.2382157 Test MSE 388.4477906300618 Test RE 1.0046826683330334\n",
      "3 Train Loss 2.836474 Test MSE 387.9647836167376 Test RE 1.004057848563452\n",
      "4 Train Loss 2.488151 Test MSE 385.7131404138093 Test RE 1.0011399679736577\n",
      "5 Train Loss 2.3970213 Test MSE 385.0827868873595 Test RE 1.000321574604108\n",
      "6 Train Loss 2.3944242 Test MSE 384.9101313131375 Test RE 1.0000972975586735\n",
      "7 Train Loss 2.3866293 Test MSE 384.2988628939787 Test RE 0.9993028643302668\n",
      "8 Train Loss 2.3846235 Test MSE 384.3236108180549 Test RE 0.9993350401648416\n",
      "9 Train Loss 2.3831058 Test MSE 384.0820018060565 Test RE 0.999020869627181\n",
      "10 Train Loss 2.3807507 Test MSE 383.5223527915586 Test RE 0.9982927635733824\n",
      "11 Train Loss 2.379957 Test MSE 383.21050648829987 Test RE 0.9978868195461724\n",
      "12 Train Loss 2.3788655 Test MSE 383.2917540629936 Test RE 0.9979925989880203\n",
      "13 Train Loss 2.3788278 Test MSE 383.31662342113646 Test RE 0.9980249751512489\n",
      "14 Train Loss 2.3778317 Test MSE 383.1125370556527 Test RE 0.9977592543397826\n",
      "15 Train Loss 2.372732 Test MSE 379.5641512712002 Test RE 0.9931278856589099\n",
      "16 Train Loss 2.356081 Test MSE 376.62854506063144 Test RE 0.9892799308634186\n",
      "17 Train Loss 2.3263855 Test MSE 372.08876929336816 Test RE 0.9832996023058317\n",
      "18 Train Loss 2.2246416 Test MSE 353.06472230014134 Test RE 0.9578328891841525\n",
      "19 Train Loss 2.0858104 Test MSE 330.67476929795936 Test RE 0.92696452107323\n",
      "20 Train Loss 1.9357702 Test MSE 306.8283320606036 Test RE 0.8929153977122214\n",
      "21 Train Loss 1.8763742 Test MSE 296.4237797295111 Test RE 0.8776454444125835\n",
      "22 Train Loss 1.8104254 Test MSE 280.8976916568344 Test RE 0.8543516601554298\n",
      "23 Train Loss 1.6681997 Test MSE 257.413656000523 Test RE 0.8178588809574481\n",
      "24 Train Loss 1.6228275 Test MSE 249.6374003190006 Test RE 0.8054107244529705\n",
      "25 Train Loss 1.5519556 Test MSE 217.8388676299827 Test RE 0.7523679172425329\n",
      "26 Train Loss 1.395252 Test MSE 202.04272316027826 Test RE 0.7245764076420476\n",
      "27 Train Loss 1.3358309 Test MSE 200.72299137744096 Test RE 0.7222060842833645\n",
      "28 Train Loss 1.2850066 Test MSE 195.748550598969 Test RE 0.71320086306317\n",
      "29 Train Loss 1.1435465 Test MSE 175.6728619730239 Test RE 0.6756393273202844\n",
      "30 Train Loss 1.1249183 Test MSE 166.77295577979916 Test RE 0.6583023349794559\n",
      "31 Train Loss 1.1016467 Test MSE 160.99046911941946 Test RE 0.6467890599706498\n",
      "32 Train Loss 1.0496908 Test MSE 157.33516140972998 Test RE 0.639404189510966\n",
      "33 Train Loss 0.9586878 Test MSE 143.74182780695398 Test RE 0.6111589330347592\n",
      "34 Train Loss 0.8861581 Test MSE 132.36554449781045 Test RE 0.5864757479206826\n",
      "35 Train Loss 0.8034149 Test MSE 116.16954085659427 Test RE 0.5494253822045161\n",
      "36 Train Loss 0.7829302 Test MSE 109.75410511009476 Test RE 0.534038993907075\n",
      "37 Train Loss 0.7544449 Test MSE 100.11351378902245 Test RE 0.5100455170820176\n",
      "38 Train Loss 0.7106324 Test MSE 90.38089187831407 Test RE 0.48461950966170675\n",
      "39 Train Loss 0.6455874 Test MSE 93.28384209613716 Test RE 0.49234076383567527\n",
      "40 Train Loss 0.61957717 Test MSE 93.00134957491304 Test RE 0.4915947180220051\n",
      "41 Train Loss 0.54245496 Test MSE 82.32304968354349 Test RE 0.46251232240390666\n",
      "42 Train Loss 0.50197434 Test MSE 73.8244052315512 Test RE 0.43798835200849195\n",
      "43 Train Loss 0.48514888 Test MSE 69.69499015043823 Test RE 0.42556251130036254\n",
      "44 Train Loss 0.48306298 Test MSE 70.09015003916632 Test RE 0.4267672430885198\n",
      "45 Train Loss 0.48076823 Test MSE 68.92110334888044 Test RE 0.4231932122024481\n",
      "46 Train Loss 0.46723425 Test MSE 62.2004336697398 Test RE 0.40203076551386274\n",
      "47 Train Loss 0.44624147 Test MSE 59.498328123564136 Test RE 0.3932013169436952\n",
      "48 Train Loss 0.41230142 Test MSE 58.087809232882435 Test RE 0.38851257596561706\n",
      "49 Train Loss 0.34725413 Test MSE 45.50197821020478 Test RE 0.3438568826330074\n",
      "50 Train Loss 0.3011981 Test MSE 37.80387382727463 Test RE 0.31342290747350127\n",
      "51 Train Loss 0.2860999 Test MSE 35.11775932316832 Test RE 0.3020827885073621\n",
      "52 Train Loss 0.28092468 Test MSE 34.39368374091762 Test RE 0.2989523208206872\n",
      "53 Train Loss 0.27311644 Test MSE 33.580360250065816 Test RE 0.29539644063568987\n",
      "54 Train Loss 0.25977483 Test MSE 32.1967434129869 Test RE 0.28924679524081354\n",
      "55 Train Loss 0.2573074 Test MSE 32.38936315757066 Test RE 0.29011072675785976\n",
      "56 Train Loss 0.2564872 Test MSE 32.11593596183056 Test RE 0.28888359111178413\n",
      "57 Train Loss 0.25455427 Test MSE 30.083339816926987 Test RE 0.27959255785763204\n",
      "58 Train Loss 0.25351426 Test MSE 29.393779824275445 Test RE 0.2763696196859271\n",
      "59 Train Loss 0.24950147 Test MSE 29.34180476464467 Test RE 0.27612516859729686\n",
      "60 Train Loss 0.2325932 Test MSE 29.570501312183154 Test RE 0.27719917037356134\n",
      "61 Train Loss 0.21597546 Test MSE 28.72264718828259 Test RE 0.273196300701615\n",
      "62 Train Loss 0.20772596 Test MSE 29.0109473546052 Test RE 0.27456396506992614\n",
      "63 Train Loss 0.18547465 Test MSE 26.726451562793535 Test RE 0.2635319244318362\n",
      "64 Train Loss 0.17931573 Test MSE 25.618911637685997 Test RE 0.2580137913063933\n",
      "65 Train Loss 0.17528486 Test MSE 24.854637146771264 Test RE 0.2541360621085022\n",
      "66 Train Loss 0.16377684 Test MSE 21.1491114786586 Test RE 0.23442754836238383\n",
      "67 Train Loss 0.15209302 Test MSE 18.237673093567714 Test RE 0.21769441934714548\n",
      "68 Train Loss 0.1371789 Test MSE 15.03355745073187 Test RE 0.19764847296310245\n",
      "69 Train Loss 0.12380055 Test MSE 11.729825987894207 Test RE 0.17458557887033446\n",
      "70 Train Loss 0.11814818 Test MSE 10.036870337727576 Test RE 0.16149598861026268\n",
      "71 Train Loss 0.11358438 Test MSE 9.476509702606315 Test RE 0.1569230672429744\n",
      "72 Train Loss 0.10981683 Test MSE 8.91912821258993 Test RE 0.152238251043546\n",
      "73 Train Loss 0.105504155 Test MSE 7.416985566006846 Test RE 0.13882775460439542\n",
      "74 Train Loss 0.09897199 Test MSE 5.681494317851296 Test RE 0.12150492196969431\n",
      "75 Train Loss 0.0849641 Test MSE 5.699747562047882 Test RE 0.12169994817096379\n",
      "76 Train Loss 0.07872712 Test MSE 6.191193926449867 Test RE 0.12683811709778733\n",
      "77 Train Loss 0.07728527 Test MSE 6.312478839259077 Test RE 0.1280744649348231\n",
      "78 Train Loss 0.07518837 Test MSE 6.123853899210969 Test RE 0.12614643833041395\n",
      "79 Train Loss 0.07445574 Test MSE 6.02100353441318 Test RE 0.12508263559178573\n",
      "80 Train Loss 0.0737135 Test MSE 5.6809047282459595 Test RE 0.12149861729933689\n",
      "81 Train Loss 0.073009335 Test MSE 5.337794002322504 Test RE 0.11777238966406968\n",
      "82 Train Loss 0.06898707 Test MSE 4.454196997363455 Test RE 0.10758390227798734\n",
      "83 Train Loss 0.0612066 Test MSE 3.954004903874902 Test RE 0.10136340345680236\n",
      "84 Train Loss 0.055309244 Test MSE 3.99369797661988 Test RE 0.10187091140403226\n",
      "85 Train Loss 0.05160673 Test MSE 4.12771716462618 Test RE 0.10356608233405669\n",
      "86 Train Loss 0.04803019 Test MSE 3.6276819841549908 Test RE 0.09709060037810899\n",
      "87 Train Loss 0.046154775 Test MSE 3.043027860396945 Test RE 0.08892329499103839\n",
      "88 Train Loss 0.04495351 Test MSE 2.823025754911732 Test RE 0.085648547588202\n",
      "89 Train Loss 0.043490022 Test MSE 2.7656546249229836 Test RE 0.08477378122851795\n",
      "90 Train Loss 0.03903766 Test MSE 2.91371828734764 Test RE 0.08701344479825597\n",
      "91 Train Loss 0.036482215 Test MSE 3.1379026842157236 Test RE 0.09029887047503982\n",
      "92 Train Loss 0.034819104 Test MSE 3.410941637257941 Test RE 0.09414553479094742\n",
      "93 Train Loss 0.03174664 Test MSE 3.5514699039054194 Test RE 0.09606532400839714\n",
      "94 Train Loss 0.030173197 Test MSE 3.536838219440132 Test RE 0.09586723022390273\n",
      "95 Train Loss 0.02921328 Test MSE 3.2811121740461857 Test RE 0.09233643897443823\n",
      "96 Train Loss 0.027120661 Test MSE 2.852910048885526 Test RE 0.08610068802770356\n",
      "97 Train Loss 0.026012352 Test MSE 2.640262774417197 Test RE 0.08282971342877472\n",
      "98 Train Loss 0.024193808 Test MSE 2.346982391070845 Test RE 0.07809396943465538\n",
      "99 Train Loss 0.02250676 Test MSE 2.0738519670268745 Test RE 0.07340936358067561\n",
      "100 Train Loss 0.02083033 Test MSE 1.9509216312672135 Test RE 0.07120041001492601\n",
      "101 Train Loss 0.019681707 Test MSE 1.579551561687014 Test RE 0.06406627685539919\n",
      "102 Train Loss 0.018812243 Test MSE 1.2602393992564471 Test RE 0.05722543583826447\n",
      "103 Train Loss 0.018199783 Test MSE 1.0544809483258542 Test RE 0.0523458132368129\n",
      "104 Train Loss 0.017776124 Test MSE 0.8979059662774582 Test RE 0.04830343448498572\n",
      "105 Train Loss 0.017558713 Test MSE 0.8079644822944776 Test RE 0.04582038325616495\n",
      "106 Train Loss 0.017409539 Test MSE 0.7801436096347746 Test RE 0.04502459969087509\n",
      "107 Train Loss 0.016889822 Test MSE 0.6849489012492476 Test RE 0.04218826537788132\n",
      "108 Train Loss 0.01627868 Test MSE 0.6009976389039429 Test RE 0.039518364753730384\n",
      "109 Train Loss 0.015866082 Test MSE 0.5376807329072557 Test RE 0.037378753948225374\n",
      "110 Train Loss 0.014007442 Test MSE 0.39386319396215486 Test RE 0.03199154995923493\n",
      "111 Train Loss 0.01244513 Test MSE 0.28953463023609854 Test RE 0.02742918100130969\n",
      "112 Train Loss 0.012155823 Test MSE 0.2721425527990143 Test RE 0.02659260052845251\n",
      "113 Train Loss 0.011557842 Test MSE 0.3175479600361525 Test RE 0.028725476802210187\n",
      "114 Train Loss 0.010691919 Test MSE 0.40889708927939267 Test RE 0.032596396512650394\n",
      "115 Train Loss 0.010011567 Test MSE 0.4015254758391851 Test RE 0.0323012355846301\n",
      "116 Train Loss 0.008910496 Test MSE 0.27862012583540696 Test RE 0.02690721968393509\n",
      "117 Train Loss 0.008704705 Test MSE 0.241204713769869 Test RE 0.025035454374917474\n",
      "118 Train Loss 0.0086948145 Test MSE 0.24019777833480255 Test RE 0.02498314310303234\n",
      "119 Train Loss 0.008690031 Test MSE 0.239314071944759 Test RE 0.024937143287273797\n",
      "120 Train Loss 0.008684264 Test MSE 0.2385105853419214 Test RE 0.024895245402356037\n",
      "121 Train Loss 0.00867614 Test MSE 0.23855792946605658 Test RE 0.024897716121009703\n",
      "122 Train Loss 0.008671764 Test MSE 0.23806610975536757 Test RE 0.024872037860466996\n",
      "123 Train Loss 0.008666799 Test MSE 0.23817981620106946 Test RE 0.024877976911202766\n",
      "124 Train Loss 0.008663209 Test MSE 0.23760559762886452 Test RE 0.024847970135162448\n",
      "125 Train Loss 0.008659309 Test MSE 0.23747339989704436 Test RE 0.024841056783050156\n",
      "126 Train Loss 0.008656865 Test MSE 0.23757761902138597 Test RE 0.024846507139179002\n",
      "127 Train Loss 0.0086544035 Test MSE 0.2375116695279056 Test RE 0.024843058312013735\n",
      "128 Train Loss 0.008650922 Test MSE 0.2376875844946907 Test RE 0.024852256716596806\n",
      "129 Train Loss 0.008648219 Test MSE 0.2377068012638963 Test RE 0.024853261334549428\n",
      "130 Train Loss 0.008642776 Test MSE 0.23684558539270867 Test RE 0.024808198582907184\n",
      "131 Train Loss 0.008634251 Test MSE 0.23625242407200106 Test RE 0.024777114007962453\n",
      "132 Train Loss 0.00839676 Test MSE 0.22701392177742283 Test RE 0.024287836364333938\n",
      "133 Train Loss 0.0077870404 Test MSE 0.16156468426650672 Test RE 0.02048970947949023\n",
      "134 Train Loss 0.0067089936 Test MSE 0.10587666598640162 Test RE 0.01658680393422203\n",
      "135 Train Loss 0.0060479348 Test MSE 0.07741880830828153 Test RE 0.014183578725037305\n",
      "136 Train Loss 0.0056288955 Test MSE 0.05781029079427623 Test RE 0.012256463028243278\n",
      "137 Train Loss 0.0053945407 Test MSE 0.06498750234106311 Test RE 0.012995036556937511\n",
      "138 Train Loss 0.00531419 Test MSE 0.06349315654493329 Test RE 0.012844761411320783\n",
      "139 Train Loss 0.005306916 Test MSE 0.062333562283745564 Test RE 0.01272692708668656\n",
      "140 Train Loss 0.0052978927 Test MSE 0.060683454661897576 Test RE 0.012557342246099917\n",
      "141 Train Loss 0.0052798297 Test MSE 0.0567315891832668 Test RE 0.01214157584719216\n",
      "142 Train Loss 0.005272494 Test MSE 0.054686974309362084 Test RE 0.011920776082361372\n",
      "143 Train Loss 0.0052648345 Test MSE 0.052409360555125524 Test RE 0.011669896733086294\n",
      "144 Train Loss 0.005248738 Test MSE 0.04714612018203746 Test RE 0.011068418255985918\n",
      "145 Train Loss 0.0052458565 Test MSE 0.046274049239605146 Test RE 0.010965573108307319\n",
      "146 Train Loss 0.005240095 Test MSE 0.045277473681632686 Test RE 0.010846851037686785\n",
      "147 Train Loss 0.00522098 Test MSE 0.042925346216293736 Test RE 0.01056135123083151\n",
      "148 Train Loss 0.0050444324 Test MSE 0.04358553595132204 Test RE 0.010642257866689455\n",
      "149 Train Loss 0.004855955 Test MSE 0.03798625706246083 Test RE 0.009935182153975937\n",
      "150 Train Loss 0.0045063184 Test MSE 0.031067682641338742 Test RE 0.00898497788712959\n",
      "151 Train Loss 0.0042837528 Test MSE 0.03908106316441055 Test RE 0.010077336656947277\n",
      "152 Train Loss 0.0036780124 Test MSE 0.026307563625167726 Test RE 0.008268046248275181\n",
      "153 Train Loss 0.003476696 Test MSE 0.03738107070345375 Test RE 0.009855722136375157\n",
      "154 Train Loss 0.0034714886 Test MSE 0.038671037650358134 Test RE 0.010024333234862568\n",
      "155 Train Loss 0.0034651945 Test MSE 0.03984927220756601 Test RE 0.010175898806949024\n",
      "156 Train Loss 0.003462934 Test MSE 0.040927709860838705 Test RE 0.010312674364140581\n",
      "157 Train Loss 0.0034580329 Test MSE 0.04179412729459238 Test RE 0.010421259564546027\n",
      "158 Train Loss 0.0034541544 Test MSE 0.04259411977647634 Test RE 0.010520524847827038\n",
      "159 Train Loss 0.0034506395 Test MSE 0.04317385180618679 Test RE 0.01059187826733806\n",
      "160 Train Loss 0.0034455585 Test MSE 0.043560928794436835 Test RE 0.01063925328414208\n",
      "161 Train Loss 0.0034409172 Test MSE 0.0438357122994506 Test RE 0.010672756879079622\n",
      "162 Train Loss 0.0034369102 Test MSE 0.04389307372884486 Test RE 0.010679737539518121\n",
      "163 Train Loss 0.0034333935 Test MSE 0.04374644561654849 Test RE 0.010661884384062409\n",
      "164 Train Loss 0.0034280005 Test MSE 0.043430133997997465 Test RE 0.010623268717914797\n",
      "165 Train Loss 0.0034237497 Test MSE 0.04288026420783287 Test RE 0.010555803785727869\n",
      "166 Train Loss 0.0034197576 Test MSE 0.042328677491633604 Test RE 0.010487692187280271\n",
      "167 Train Loss 0.0034165317 Test MSE 0.04178493154221783 Test RE 0.01042011303271857\n",
      "168 Train Loss 0.0034157848 Test MSE 0.04123953739162591 Test RE 0.01035188586390258\n",
      "169 Train Loss 0.0034136842 Test MSE 0.04073184674877431 Test RE 0.010287968671223936\n",
      "170 Train Loss 0.0034096045 Test MSE 0.04016045015629367 Test RE 0.010215552700625161\n",
      "171 Train Loss 0.0034057407 Test MSE 0.03961594882186766 Test RE 0.010146064374906507\n",
      "172 Train Loss 0.003402059 Test MSE 0.03917611323508005 Test RE 0.010089583891120697\n",
      "173 Train Loss 0.0033969071 Test MSE 0.03843071812155888 Test RE 0.009993136794515595\n",
      "174 Train Loss 0.0033913578 Test MSE 0.037948062731596496 Test RE 0.009930186096213527\n",
      "175 Train Loss 0.0033882675 Test MSE 0.03767720923315634 Test RE 0.00989468438754705\n",
      "176 Train Loss 0.0032508874 Test MSE 0.035573898635563955 Test RE 0.009614535651063074\n",
      "177 Train Loss 0.003198638 Test MSE 0.031626553057753026 Test RE 0.009065432178411017\n",
      "178 Train Loss 0.0031904595 Test MSE 0.03060097147401725 Test RE 0.008917234539890507\n",
      "179 Train Loss 0.00318174 Test MSE 0.029193414332906915 Test RE 0.008709736736903638\n",
      "180 Train Loss 0.0031752388 Test MSE 0.02824691596531116 Test RE 0.008567381407276859\n",
      "181 Train Loss 0.0031708826 Test MSE 0.027459738404721358 Test RE 0.008447161166689003\n",
      "182 Train Loss 0.0031644295 Test MSE 0.02662550396182425 Test RE 0.008317857980124282\n",
      "183 Train Loss 0.0031599703 Test MSE 0.025711317603549624 Test RE 0.008173813966430016\n",
      "184 Train Loss 0.0031540862 Test MSE 0.025048596138530478 Test RE 0.008067784289399261\n",
      "185 Train Loss 0.003146556 Test MSE 0.024221091590407175 Test RE 0.00793340158266011\n",
      "186 Train Loss 0.0031407413 Test MSE 0.02418264072798003 Test RE 0.007927101963737278\n",
      "187 Train Loss 0.0031334 Test MSE 0.02289284240879435 Test RE 0.0077128066066085976\n",
      "188 Train Loss 0.0031286078 Test MSE 0.022622761861231012 Test RE 0.007667175327252928\n",
      "189 Train Loss 0.0031219942 Test MSE 0.02254178087396761 Test RE 0.007653440221187054\n",
      "190 Train Loss 0.0031127734 Test MSE 0.022579438054323483 Test RE 0.0076598302822635965\n",
      "191 Train Loss 0.0031049114 Test MSE 0.022761624108076254 Test RE 0.007690670521988529\n",
      "192 Train Loss 0.0029310817 Test MSE 0.03460972272121024 Test RE 0.00948334701323356\n",
      "193 Train Loss 0.002908994 Test MSE 0.03528717358732308 Test RE 0.009575710759784781\n",
      "194 Train Loss 0.0028910139 Test MSE 0.03507130651949479 Test RE 0.009546376423689756\n",
      "195 Train Loss 0.0028825453 Test MSE 0.03491608110347992 Test RE 0.009525226890183449\n",
      "196 Train Loss 0.002876822 Test MSE 0.034519497688482295 Test RE 0.009470977748181144\n",
      "197 Train Loss 0.0028705616 Test MSE 0.034362664780155525 Test RE 0.009449438443683228\n",
      "198 Train Loss 0.002863194 Test MSE 0.034149461685567845 Test RE 0.00942007831931879\n",
      "199 Train Loss 0.0028577833 Test MSE 0.03384696397391393 Test RE 0.009378263742030282\n",
      "Training time: 35.98\n",
      "Training time: 35.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.605664 Test MSE 386.4293879107944 Test RE 1.0020690670151315\n",
      "1 Train Loss 3.7290797 Test MSE 384.4609492785389 Test RE 0.9995135809477886\n",
      "2 Train Loss 2.547555 Test MSE 385.2261859713607 Test RE 1.000507809682203\n",
      "3 Train Loss 2.3877091 Test MSE 383.77732887708584 Test RE 0.998624554519677\n",
      "4 Train Loss 2.3837688 Test MSE 383.75065231139973 Test RE 0.9985898464552947\n",
      "5 Train Loss 2.3820794 Test MSE 383.9043928500803 Test RE 0.9987898570177054\n",
      "6 Train Loss 2.3816469 Test MSE 383.91810142927267 Test RE 0.9988076894101956\n",
      "7 Train Loss 2.3804734 Test MSE 383.64647420181484 Test RE 0.9984542919445552\n",
      "8 Train Loss 2.380429 Test MSE 383.59857109367243 Test RE 0.998391955183699\n",
      "9 Train Loss 2.3804228 Test MSE 383.5921519904266 Test RE 0.9983836016491263\n",
      "10 Train Loss 2.3804145 Test MSE 383.5852005525952 Test RE 0.9983745552792441\n",
      "11 Train Loss 2.3791282 Test MSE 382.93165938953 Test RE 0.9975236921298495\n",
      "12 Train Loss 2.3761787 Test MSE 382.41159895693454 Test RE 0.99684609229392\n",
      "13 Train Loss 2.371175 Test MSE 380.5090474575238 Test RE 0.9943632755840739\n",
      "14 Train Loss 2.3569114 Test MSE 378.21008996458755 Test RE 0.9913548553208771\n",
      "15 Train Loss 2.343362 Test MSE 377.2073329073161 Test RE 0.9900397821765011\n",
      "16 Train Loss 2.3179483 Test MSE 370.1438840585569 Test RE 0.9807264119890781\n",
      "17 Train Loss 2.2880518 Test MSE 365.6688012586379 Test RE 0.9747798351834465\n",
      "18 Train Loss 2.2634232 Test MSE 362.89702441505426 Test RE 0.9710783822234816\n",
      "19 Train Loss 2.2320883 Test MSE 351.9980811669287 Test RE 0.9563849439536345\n",
      "20 Train Loss 2.1960313 Test MSE 341.15542067047653 Test RE 0.9415398818817179\n",
      "21 Train Loss 2.0670817 Test MSE 315.89988879952756 Test RE 0.9060190286671381\n",
      "22 Train Loss 1.9758247 Test MSE 314.35686503410955 Test RE 0.9038035795954594\n",
      "23 Train Loss 1.9190843 Test MSE 302.4940772052238 Test RE 0.886586308743835\n",
      "24 Train Loss 1.8885121 Test MSE 296.5428822169916 Test RE 0.8778217448055506\n",
      "25 Train Loss 1.8364037 Test MSE 291.5947190208302 Test RE 0.8704671971036477\n",
      "26 Train Loss 1.7834176 Test MSE 277.4624871021535 Test RE 0.8491114944125747\n",
      "27 Train Loss 1.7586559 Test MSE 271.9583808640341 Test RE 0.8406472681212637\n",
      "28 Train Loss 1.7396411 Test MSE 271.6813448397286 Test RE 0.8402189876690559\n",
      "29 Train Loss 1.6719599 Test MSE 263.34819366581064 Test RE 0.8272328161178585\n",
      "30 Train Loss 1.6354569 Test MSE 253.89057448574283 Test RE 0.8122428025709942\n",
      "31 Train Loss 1.5864327 Test MSE 238.51761438951732 Test RE 0.7872683841781594\n",
      "32 Train Loss 1.5596166 Test MSE 239.33539039243763 Test RE 0.7886168344817701\n",
      "33 Train Loss 1.5145177 Test MSE 233.7424234645837 Test RE 0.779347871802182\n",
      "34 Train Loss 1.4406155 Test MSE 224.2365907523181 Test RE 0.7633361382643102\n",
      "35 Train Loss 1.4121953 Test MSE 220.16434808451845 Test RE 0.7563731075741841\n",
      "36 Train Loss 1.3334957 Test MSE 207.75486945177613 Test RE 0.7347476207352066\n",
      "37 Train Loss 1.2420075 Test MSE 195.61729817665494 Test RE 0.7129617168792604\n",
      "38 Train Loss 1.195455 Test MSE 186.9038890794328 Test RE 0.6969020654181228\n",
      "39 Train Loss 1.1632416 Test MSE 181.8496124147735 Test RE 0.6874146324923001\n",
      "40 Train Loss 1.1338875 Test MSE 173.7854315535943 Test RE 0.6719999888972291\n",
      "41 Train Loss 1.1107777 Test MSE 170.76125104169896 Test RE 0.6661273210103984\n",
      "42 Train Loss 1.0784563 Test MSE 169.7470375251378 Test RE 0.6641461880816002\n",
      "43 Train Loss 1.0238577 Test MSE 155.3006659108689 Test RE 0.6352566813980599\n",
      "44 Train Loss 0.99487054 Test MSE 149.10886153434703 Test RE 0.6224641010118782\n",
      "45 Train Loss 0.96954024 Test MSE 144.73707607904777 Test RE 0.6132710727046293\n",
      "46 Train Loss 0.9243535 Test MSE 131.66034930139938 Test RE 0.5849113974522115\n",
      "47 Train Loss 0.8778362 Test MSE 123.30178481450248 Test RE 0.5660401848162238\n",
      "48 Train Loss 0.86420876 Test MSE 123.92645969162695 Test RE 0.5674722174955626\n",
      "49 Train Loss 0.82984656 Test MSE 125.7007838782929 Test RE 0.5715201877217669\n",
      "50 Train Loss 0.82526964 Test MSE 125.57816849989017 Test RE 0.5712413737815171\n",
      "51 Train Loss 0.8164151 Test MSE 123.66060136258973 Test RE 0.5668631941428212\n",
      "52 Train Loss 0.80234206 Test MSE 121.56808538083297 Test RE 0.5620466595375253\n",
      "53 Train Loss 0.8010458 Test MSE 121.64479610763517 Test RE 0.5622239602174498\n",
      "54 Train Loss 0.78854865 Test MSE 120.90267220845391 Test RE 0.5605063439773252\n",
      "55 Train Loss 0.7793849 Test MSE 118.82006905581775 Test RE 0.5556578865576709\n",
      "56 Train Loss 0.7761004 Test MSE 117.36618751124806 Test RE 0.5522479103090568\n",
      "57 Train Loss 0.77277493 Test MSE 115.49509125537317 Test RE 0.5478281513938781\n",
      "58 Train Loss 0.7557009 Test MSE 110.8132229935758 Test RE 0.5366095231362454\n",
      "59 Train Loss 0.7464628 Test MSE 109.48476295463325 Test RE 0.5333833119660246\n",
      "60 Train Loss 0.74386114 Test MSE 110.82169477568284 Test RE 0.5366300349125018\n",
      "61 Train Loss 0.7434511 Test MSE 111.39449862268515 Test RE 0.5380150867238579\n",
      "62 Train Loss 0.74207944 Test MSE 111.63604601473084 Test RE 0.5385980857492435\n",
      "63 Train Loss 0.7209891 Test MSE 110.55480889646026 Test RE 0.5359834769006238\n",
      "64 Train Loss 0.6967378 Test MSE 106.53577567116467 Test RE 0.5261509012688694\n",
      "65 Train Loss 0.6801427 Test MSE 101.70886784343458 Test RE 0.5140933576054005\n",
      "66 Train Loss 0.6617898 Test MSE 98.48408271188113 Test RE 0.505877780709013\n",
      "67 Train Loss 0.65127945 Test MSE 96.14550713417009 Test RE 0.49983547911333776\n",
      "68 Train Loss 0.6434208 Test MSE 94.7609514564966 Test RE 0.4962234557195911\n",
      "69 Train Loss 0.63902855 Test MSE 94.26884849577993 Test RE 0.49493330991869144\n",
      "70 Train Loss 0.6368098 Test MSE 92.7718738339303 Test RE 0.4909878519053918\n",
      "71 Train Loss 0.63555825 Test MSE 92.03449984010578 Test RE 0.4890327122120065\n",
      "72 Train Loss 0.63358533 Test MSE 91.14168553238196 Test RE 0.4866549111840858\n",
      "73 Train Loss 0.62692285 Test MSE 88.07394555852166 Test RE 0.47839464399106585\n",
      "74 Train Loss 0.6118438 Test MSE 88.14614414225807 Test RE 0.47859068573772423\n",
      "75 Train Loss 0.6049726 Test MSE 89.43472341673801 Test RE 0.4820761729871678\n",
      "76 Train Loss 0.6036024 Test MSE 89.74688113094709 Test RE 0.48291674535769347\n",
      "77 Train Loss 0.59937763 Test MSE 90.08743670998028 Test RE 0.4838321212970212\n",
      "78 Train Loss 0.579836 Test MSE 88.46972505114256 Test RE 0.47946832449935506\n",
      "79 Train Loss 0.5462317 Test MSE 84.05321337588585 Test RE 0.4673473055762802\n",
      "80 Train Loss 0.53283316 Test MSE 82.00317329734376 Test RE 0.46161287342155816\n",
      "81 Train Loss 0.5283827 Test MSE 80.58957783671433 Test RE 0.4576168664217851\n",
      "82 Train Loss 0.5122828 Test MSE 77.20242738192823 Test RE 0.44789690381622566\n",
      "83 Train Loss 0.5024018 Test MSE 74.1476229804799 Test RE 0.43894610437042614\n",
      "84 Train Loss 0.4892462 Test MSE 70.36132996205147 Test RE 0.4275920307586828\n",
      "85 Train Loss 0.47224098 Test MSE 67.60278353233352 Test RE 0.4191262596566027\n",
      "86 Train Loss 0.46146917 Test MSE 66.31981959954018 Test RE 0.4151301253753676\n",
      "87 Train Loss 0.44518104 Test MSE 64.3240264290481 Test RE 0.40883605914481513\n",
      "88 Train Loss 0.42816108 Test MSE 62.90566880157458 Test RE 0.4043034754019188\n",
      "89 Train Loss 0.42442906 Test MSE 62.9063262031641 Test RE 0.4043055880020869\n",
      "90 Train Loss 0.42136067 Test MSE 62.45219074178334 Test RE 0.40284355630033064\n",
      "91 Train Loss 0.41519296 Test MSE 59.962512540551224 Test RE 0.3947321441703935\n",
      "92 Train Loss 0.40780506 Test MSE 57.717212496918904 Test RE 0.38727124944406777\n",
      "93 Train Loss 0.40418065 Test MSE 56.01505460448496 Test RE 0.3815179399092445\n",
      "94 Train Loss 0.40300152 Test MSE 54.86985953070917 Test RE 0.3775978447074576\n",
      "95 Train Loss 0.39907527 Test MSE 52.61743696980632 Test RE 0.369766384852725\n",
      "96 Train Loss 0.38290584 Test MSE 48.977491909511315 Test RE 0.3567474301275409\n",
      "97 Train Loss 0.36887908 Test MSE 46.84128788807457 Test RE 0.34888074110812545\n",
      "98 Train Loss 0.36112648 Test MSE 46.1882785543871 Test RE 0.34644035159592407\n",
      "99 Train Loss 0.3548598 Test MSE 46.58142381577135 Test RE 0.3479116424099659\n",
      "100 Train Loss 0.3365701 Test MSE 47.19538103257312 Test RE 0.35019692707086747\n",
      "101 Train Loss 0.32854056 Test MSE 46.78515151129786 Test RE 0.34867162247628314\n",
      "102 Train Loss 0.32480097 Test MSE 45.40863944359763 Test RE 0.3435040226608861\n",
      "103 Train Loss 0.32047275 Test MSE 44.18761123123064 Test RE 0.3388541779583526\n",
      "104 Train Loss 0.31142643 Test MSE 41.519077716071045 Test RE 0.3284629820237413\n",
      "105 Train Loss 0.3058051 Test MSE 40.23073331902797 Test RE 0.32332669052420343\n",
      "106 Train Loss 0.30107147 Test MSE 38.807946734917344 Test RE 0.3175578956060722\n",
      "107 Train Loss 0.29943296 Test MSE 37.68253145826758 Test RE 0.3129194928555002\n",
      "108 Train Loss 0.29863644 Test MSE 36.78866691620953 Test RE 0.30918584852128694\n",
      "109 Train Loss 0.2982969 Test MSE 36.354172759808044 Test RE 0.3073545989683693\n",
      "110 Train Loss 0.29668054 Test MSE 35.31818607050563 Test RE 0.3029435969715972\n",
      "111 Train Loss 0.29348272 Test MSE 34.41387392465965 Test RE 0.29904005519622395\n",
      "112 Train Loss 0.2618598 Test MSE 32.06189565941325 Test RE 0.2886404418954526\n",
      "113 Train Loss 0.2259828 Test MSE 29.978511407671043 Test RE 0.27910499862070415\n",
      "114 Train Loss 0.2144947 Test MSE 28.98666126700597 Test RE 0.27444901741702143\n",
      "115 Train Loss 0.20976081 Test MSE 28.940145984026632 Test RE 0.2742287229999344\n",
      "116 Train Loss 0.20481093 Test MSE 28.256023424959444 Test RE 0.2709680609312166\n",
      "117 Train Loss 0.20060615 Test MSE 26.610003767901535 Test RE 0.26295719023627534\n",
      "118 Train Loss 0.19055088 Test MSE 24.671157159304613 Test RE 0.25319629269331617\n",
      "119 Train Loss 0.1853544 Test MSE 23.92958642853968 Test RE 0.24936194626705363\n",
      "120 Train Loss 0.18308869 Test MSE 23.659140534139034 Test RE 0.24794883067859938\n",
      "121 Train Loss 0.18158312 Test MSE 23.68456005501093 Test RE 0.24808199376731982\n",
      "122 Train Loss 0.18038957 Test MSE 23.32745827675042 Test RE 0.24620467387728573\n",
      "123 Train Loss 0.17939374 Test MSE 23.09670687505729 Test RE 0.2449839393665378\n",
      "124 Train Loss 0.17908287 Test MSE 23.099270449779272 Test RE 0.24499753475075728\n",
      "125 Train Loss 0.17840132 Test MSE 23.060936378013576 Test RE 0.24479415921938089\n",
      "126 Train Loss 0.1762541 Test MSE 22.950035209637306 Test RE 0.24420483631623716\n",
      "127 Train Loss 0.17302099 Test MSE 22.593339199175016 Test RE 0.2422996539024546\n",
      "128 Train Loss 0.17241842 Test MSE 22.429395466921942 Test RE 0.2414189555435002\n",
      "129 Train Loss 0.17240058 Test MSE 22.422341989012384 Test RE 0.24138099247916503\n",
      "130 Train Loss 0.17239526 Test MSE 22.42036207233675 Test RE 0.2413703351445526\n",
      "131 Train Loss 0.17239448 Test MSE 22.417946966361946 Test RE 0.24135733467123618\n",
      "132 Train Loss 0.17237924 Test MSE 22.414675860546158 Test RE 0.24133972524909547\n",
      "133 Train Loss 0.17237838 Test MSE 22.413060352218892 Test RE 0.2413310279704987\n",
      "134 Train Loss 0.17188719 Test MSE 22.457981770699817 Test RE 0.24157275099664263\n",
      "135 Train Loss 0.1693314 Test MSE 23.20706416191847 Test RE 0.24556851502413948\n",
      "136 Train Loss 0.1678088 Test MSE 23.514323495084874 Test RE 0.24718882143274268\n",
      "137 Train Loss 0.16520636 Test MSE 22.922818954793993 Test RE 0.2440599931247619\n",
      "138 Train Loss 0.16316065 Test MSE 22.51037673633453 Test RE 0.24185438410009366\n",
      "139 Train Loss 0.16062878 Test MSE 22.40826040117473 Test RE 0.24130518502162746\n",
      "140 Train Loss 0.15884893 Test MSE 22.028156196028633 Test RE 0.2392498400901402\n",
      "141 Train Loss 0.15643401 Test MSE 21.481933428799184 Test RE 0.2362649320896843\n",
      "142 Train Loss 0.15471548 Test MSE 21.025617872232132 Test RE 0.23374211321263316\n",
      "143 Train Loss 0.15335815 Test MSE 20.987107901593163 Test RE 0.23352795714279723\n",
      "144 Train Loss 0.15256238 Test MSE 20.908153567768345 Test RE 0.23308827249849098\n",
      "145 Train Loss 0.15078959 Test MSE 20.011729098312898 Test RE 0.22803677485098922\n",
      "146 Train Loss 0.14672074 Test MSE 18.63738969701707 Test RE 0.22006710261606408\n",
      "147 Train Loss 0.14205667 Test MSE 17.617984915400374 Test RE 0.21396399562193325\n",
      "148 Train Loss 0.14005202 Test MSE 17.099988253414033 Test RE 0.21079508894160698\n",
      "149 Train Loss 0.1363612 Test MSE 16.649571844876558 Test RE 0.20800037475466168\n",
      "150 Train Loss 0.13470912 Test MSE 16.603566644004342 Test RE 0.2077128082495154\n",
      "151 Train Loss 0.13256697 Test MSE 16.38368433575934 Test RE 0.20633284594965884\n",
      "152 Train Loss 0.13003571 Test MSE 15.647639909397231 Test RE 0.20164478945149708\n",
      "153 Train Loss 0.12850724 Test MSE 15.294804125177341 Test RE 0.19935840129847782\n",
      "154 Train Loss 0.12713653 Test MSE 14.982221342346033 Test RE 0.19731072255795148\n",
      "155 Train Loss 0.12660296 Test MSE 14.948311946269564 Test RE 0.19708730850766226\n",
      "156 Train Loss 0.12545756 Test MSE 14.860247459211385 Test RE 0.19650590402694404\n",
      "157 Train Loss 0.12449686 Test MSE 14.674977604526056 Test RE 0.19527709515181066\n",
      "158 Train Loss 0.12392247 Test MSE 14.363227411135071 Test RE 0.19319176083575765\n",
      "159 Train Loss 0.12271279 Test MSE 13.57905009607108 Test RE 0.18784397843677653\n",
      "160 Train Loss 0.122259066 Test MSE 13.307909851706068 Test RE 0.18595913084657173\n",
      "161 Train Loss 0.121454105 Test MSE 13.074753693190374 Test RE 0.18432291951347823\n",
      "162 Train Loss 0.12073213 Test MSE 12.56486460171879 Test RE 0.18069306682301178\n",
      "163 Train Loss 0.11989348 Test MSE 12.179404313153647 Test RE 0.17789986003832944\n",
      "164 Train Loss 0.11846304 Test MSE 11.945686633048362 Test RE 0.17618467978900879\n",
      "165 Train Loss 0.114336304 Test MSE 11.238711753667081 Test RE 0.17089165231370726\n",
      "166 Train Loss 0.11297833 Test MSE 11.089852105706603 Test RE 0.16975612760244124\n",
      "167 Train Loss 0.11133907 Test MSE 10.791319749270336 Test RE 0.16745567223917163\n",
      "168 Train Loss 0.11006854 Test MSE 10.536714631873089 Test RE 0.16546844727784366\n",
      "169 Train Loss 0.10931967 Test MSE 10.65725837774221 Test RE 0.16641226436425338\n",
      "170 Train Loss 0.10870494 Test MSE 10.732840082490645 Test RE 0.16700132303614873\n",
      "171 Train Loss 0.107308164 Test MSE 10.78259660442938 Test RE 0.16738797730317745\n",
      "172 Train Loss 0.102923185 Test MSE 11.524092258907553 Test RE 0.1730477455685759\n",
      "173 Train Loss 0.0984624 Test MSE 12.21440971526833 Test RE 0.1781553318060405\n",
      "174 Train Loss 0.0959274 Test MSE 12.617278866857108 Test RE 0.181069554680784\n",
      "175 Train Loss 0.095186636 Test MSE 12.691918802123277 Test RE 0.18160434079479665\n",
      "176 Train Loss 0.094644256 Test MSE 12.569917056799035 Test RE 0.1807293923973386\n",
      "177 Train Loss 0.093540676 Test MSE 12.587115648892935 Test RE 0.18085299021402557\n",
      "178 Train Loss 0.091796875 Test MSE 12.75663558931132 Test RE 0.18206675729892352\n",
      "179 Train Loss 0.08987194 Test MSE 13.015601502699626 Test RE 0.18390549427906222\n",
      "180 Train Loss 0.08777462 Test MSE 12.882123598482499 Test RE 0.18296006815731922\n",
      "181 Train Loss 0.083819464 Test MSE 12.382920938598385 Test RE 0.17938004662951515\n",
      "182 Train Loss 0.08113382 Test MSE 11.979810705845347 Test RE 0.1764361450907932\n",
      "183 Train Loss 0.07936034 Test MSE 11.725877669293041 Test RE 0.17455619320399424\n",
      "184 Train Loss 0.07786735 Test MSE 11.366732846116868 Test RE 0.17186221664820453\n",
      "185 Train Loss 0.07546008 Test MSE 10.758913773573155 Test RE 0.1672040512767732\n",
      "186 Train Loss 0.07130861 Test MSE 9.896192716845201 Test RE 0.1603602241276743\n",
      "187 Train Loss 0.06984993 Test MSE 9.518325097842634 Test RE 0.15726890013258268\n",
      "188 Train Loss 0.068385065 Test MSE 9.29124893028063 Test RE 0.15538161476529383\n",
      "189 Train Loss 0.06628112 Test MSE 9.220893979648553 Test RE 0.15479220858649204\n",
      "190 Train Loss 0.06345813 Test MSE 8.852396675605538 Test RE 0.15166767027339303\n",
      "191 Train Loss 0.06191145 Test MSE 8.550162690205891 Test RE 0.14905610613345113\n",
      "192 Train Loss 0.061281044 Test MSE 8.301952110405022 Test RE 0.14687662834700047\n",
      "193 Train Loss 0.06108476 Test MSE 8.129261229226044 Test RE 0.14534099275379464\n",
      "194 Train Loss 0.060679708 Test MSE 7.921767876503849 Test RE 0.14347414283949703\n",
      "195 Train Loss 0.060301032 Test MSE 7.9109238502537975 Test RE 0.14337590907384104\n",
      "196 Train Loss 0.060100548 Test MSE 7.923174692601184 Test RE 0.14348688196392226\n",
      "197 Train Loss 0.06009455 Test MSE 7.921383034943996 Test RE 0.14347065779143744\n",
      "198 Train Loss 0.060085446 Test MSE 7.9187991972884815 Test RE 0.14344725688272192\n",
      "199 Train Loss 0.06007712 Test MSE 7.914855609725732 Test RE 0.14341153383612668\n",
      "Training time: 65.08\n",
      "Training time: 65.08\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.5025535 Test MSE 385.809223122481 Test RE 1.0012646542237245\n",
      "1 Train Loss 3.590336 Test MSE 385.929181171519 Test RE 1.0014203016427579\n",
      "2 Train Loss 3.1897702 Test MSE 388.17916227204495 Test RE 1.0043352176103584\n",
      "3 Train Loss 2.7043066 Test MSE 387.0930898508239 Test RE 1.002929236917918\n",
      "4 Train Loss 2.4171746 Test MSE 385.22233145779023 Test RE 1.000502804206567\n",
      "5 Train Loss 2.3850157 Test MSE 384.38317548979944 Test RE 0.9994124784951312\n",
      "6 Train Loss 2.3829365 Test MSE 384.03462293878226 Test RE 0.9989592500553948\n",
      "7 Train Loss 2.3821273 Test MSE 383.77056987289114 Test RE 0.9986157606999749\n",
      "8 Train Loss 2.3812685 Test MSE 383.77089695357347 Test RE 0.9986161862509165\n",
      "9 Train Loss 2.3810468 Test MSE 383.78926819098626 Test RE 0.998640088005372\n",
      "10 Train Loss 2.3810372 Test MSE 383.7888421304703 Test RE 0.9986395336891566\n",
      "11 Train Loss 2.381029 Test MSE 383.7887633236342 Test RE 0.9986394311593052\n",
      "12 Train Loss 2.3810208 Test MSE 383.78726278988796 Test RE 0.9986374789218053\n",
      "13 Train Loss 2.3805418 Test MSE 383.56860881221183 Test RE 0.9983529630090896\n",
      "14 Train Loss 2.3801787 Test MSE 383.33637145268034 Test RE 0.9980506833695295\n",
      "15 Train Loss 2.3793147 Test MSE 383.1014587666482 Test RE 0.9977448283627974\n",
      "16 Train Loss 2.3789518 Test MSE 383.0942894526852 Test RE 0.9977354924816201\n",
      "17 Train Loss 2.378614 Test MSE 383.0758845671098 Test RE 0.997711525238256\n",
      "18 Train Loss 2.3782692 Test MSE 383.14857846055094 Test RE 0.9978061854580399\n",
      "19 Train Loss 2.377246 Test MSE 383.01050086899454 Test RE 0.9976263765045602\n",
      "20 Train Loss 2.3742335 Test MSE 382.016514029852 Test RE 0.9963310181668972\n",
      "21 Train Loss 2.3723428 Test MSE 381.6741742984504 Test RE 0.9958844927819489\n",
      "22 Train Loss 2.3714037 Test MSE 381.6116307443172 Test RE 0.9958028934601459\n",
      "23 Train Loss 2.3683736 Test MSE 380.03036991879503 Test RE 0.9937376279714967\n",
      "24 Train Loss 2.358821 Test MSE 377.3291612510701 Test RE 0.9901996480633485\n",
      "25 Train Loss 2.3268294 Test MSE 367.7527135403047 Test RE 0.9775534786477345\n",
      "26 Train Loss 2.2757976 Test MSE 362.3723592535132 Test RE 0.9703761509474399\n",
      "27 Train Loss 2.159016 Test MSE 341.2016202152701 Test RE 0.94160363177217\n",
      "28 Train Loss 2.1520374 Test MSE 339.9218507656512 Test RE 0.9398361025935398\n",
      "29 Train Loss 2.1304867 Test MSE 335.39584939247305 Test RE 0.9335582568797398\n",
      "30 Train Loss 1.9727055 Test MSE 312.005778238958 Test RE 0.9004174460068423\n",
      "31 Train Loss 1.934328 Test MSE 298.07559148384524 Test RE 0.8800873724213387\n",
      "32 Train Loss 1.855544 Test MSE 283.88542762881093 Test RE 0.8588832486559702\n",
      "33 Train Loss 1.7795035 Test MSE 276.2306011100658 Test RE 0.8472244426241512\n",
      "34 Train Loss 1.7480572 Test MSE 274.30399747069373 Test RE 0.8442647383676763\n",
      "35 Train Loss 1.6796219 Test MSE 262.30437100103313 Test RE 0.8255917535952687\n",
      "36 Train Loss 1.5944328 Test MSE 248.07225374831157 Test RE 0.8028819208410986\n",
      "37 Train Loss 1.5225711 Test MSE 235.10771657415822 Test RE 0.7816206492870338\n",
      "38 Train Loss 1.5117744 Test MSE 234.8864103907539 Test RE 0.7812526940998331\n",
      "39 Train Loss 1.4715798 Test MSE 231.79331757438536 Test RE 0.7760916989158014\n",
      "40 Train Loss 1.3936912 Test MSE 219.40680000177966 Test RE 0.7550707106563913\n",
      "41 Train Loss 1.3762509 Test MSE 214.31046866539356 Test RE 0.7462498821388167\n",
      "42 Train Loss 1.3197259 Test MSE 198.7343129294696 Test RE 0.7186195225216959\n",
      "43 Train Loss 1.1909072 Test MSE 183.57878613469518 Test RE 0.690675148069385\n",
      "44 Train Loss 1.16241 Test MSE 175.6783727207888 Test RE 0.6756499244286605\n",
      "45 Train Loss 1.1426538 Test MSE 168.52955241323733 Test RE 0.6617601576684049\n",
      "46 Train Loss 1.0993452 Test MSE 165.16717178103693 Test RE 0.6551254162662032\n",
      "47 Train Loss 1.0836716 Test MSE 165.5807294999779 Test RE 0.6559450791170721\n",
      "48 Train Loss 1.0744629 Test MSE 163.00833626601417 Test RE 0.6508298897442426\n",
      "49 Train Loss 1.0208441 Test MSE 152.80384625612703 Test RE 0.630129374714561\n",
      "50 Train Loss 0.958578 Test MSE 148.6679585754831 Test RE 0.6215431314581419\n",
      "51 Train Loss 0.9529441 Test MSE 148.09165553145718 Test RE 0.620337273082855\n",
      "52 Train Loss 0.9472439 Test MSE 146.3573288277192 Test RE 0.6166941373776255\n",
      "53 Train Loss 0.9436766 Test MSE 145.7859254935214 Test RE 0.6154891218832773\n",
      "54 Train Loss 0.93550503 Test MSE 143.19690721222491 Test RE 0.6099993912248225\n",
      "55 Train Loss 0.9294803 Test MSE 140.25464757956638 Test RE 0.6037000518600751\n",
      "56 Train Loss 0.9260881 Test MSE 138.65353022722974 Test RE 0.6002443050653583\n",
      "57 Train Loss 0.8988388 Test MSE 134.50236261776584 Test RE 0.5911906257964128\n",
      "58 Train Loss 0.87499267 Test MSE 132.29807692927213 Test RE 0.5863262636414383\n",
      "59 Train Loss 0.8702576 Test MSE 129.52221043496314 Test RE 0.580142532279806\n",
      "60 Train Loss 0.86476314 Test MSE 126.09283295401887 Test RE 0.5724107530694669\n",
      "61 Train Loss 0.8599363 Test MSE 123.83140922690181 Test RE 0.5672545527422966\n",
      "62 Train Loss 0.84722203 Test MSE 119.52602186987295 Test RE 0.5573061237810769\n",
      "63 Train Loss 0.83558935 Test MSE 118.717989315405 Test RE 0.5554191491077789\n",
      "64 Train Loss 0.82684225 Test MSE 116.64992861084109 Test RE 0.5505602103444112\n",
      "65 Train Loss 0.82018006 Test MSE 114.21476883622385 Test RE 0.5447832109489957\n",
      "66 Train Loss 0.81272036 Test MSE 111.57224140524839 Test RE 0.538444148228972\n",
      "67 Train Loss 0.79392076 Test MSE 108.97620545595781 Test RE 0.5321430854451937\n",
      "68 Train Loss 0.77793735 Test MSE 108.31285497316156 Test RE 0.5305210056705675\n",
      "69 Train Loss 0.7677383 Test MSE 108.84625218196476 Test RE 0.5318257025609159\n",
      "70 Train Loss 0.75811493 Test MSE 109.41049950742473 Test RE 0.5332023844942494\n",
      "71 Train Loss 0.7373875 Test MSE 109.22502152150733 Test RE 0.5327502375145281\n",
      "72 Train Loss 0.70964473 Test MSE 108.30446849678233 Test RE 0.5305004666126196\n",
      "73 Train Loss 0.6912296 Test MSE 105.54157005596133 Test RE 0.5236900925901864\n",
      "74 Train Loss 0.6856626 Test MSE 103.67321556052771 Test RE 0.5190340707512657\n",
      "75 Train Loss 0.67783105 Test MSE 102.81544617118412 Test RE 0.5168824240575861\n",
      "76 Train Loss 0.62074053 Test MSE 93.9647485752777 Test RE 0.49413436753268913\n",
      "77 Train Loss 0.5960317 Test MSE 87.28732127298244 Test RE 0.47625348365225995\n",
      "78 Train Loss 0.5738489 Test MSE 82.98333239889867 Test RE 0.46436343812397807\n",
      "79 Train Loss 0.5687162 Test MSE 82.96887153581756 Test RE 0.4643229758499473\n",
      "80 Train Loss 0.5671417 Test MSE 83.38848992138973 Test RE 0.46549566100335477\n",
      "81 Train Loss 0.5584679 Test MSE 80.31095421436224 Test RE 0.4568251184828005\n",
      "82 Train Loss 0.5250505 Test MSE 75.20764575480025 Test RE 0.442072581395045\n",
      "83 Train Loss 0.5047938 Test MSE 72.92559981300643 Test RE 0.43531395276535817\n",
      "84 Train Loss 0.4847075 Test MSE 70.43807670237261 Test RE 0.42782516557034356\n",
      "85 Train Loss 0.4817171 Test MSE 68.81650408214699 Test RE 0.42287195713373044\n",
      "86 Train Loss 0.47725514 Test MSE 67.24238929245895 Test RE 0.41800757389255244\n",
      "87 Train Loss 0.47309816 Test MSE 65.27657751469243 Test RE 0.41185208653330124\n",
      "88 Train Loss 0.47211605 Test MSE 64.96120888177934 Test RE 0.410855998103613\n",
      "89 Train Loss 0.471381 Test MSE 65.54425588382155 Test RE 0.41269565946124964\n",
      "90 Train Loss 0.4622597 Test MSE 64.52590772514333 Test RE 0.4094771236064173\n",
      "91 Train Loss 0.438279 Test MSE 60.660544072010396 Test RE 0.39702306076958904\n",
      "92 Train Loss 0.4163974 Test MSE 59.49035071241483 Test RE 0.3931749562557803\n",
      "93 Train Loss 0.4026808 Test MSE 59.40976723795483 Test RE 0.3929085757399738\n",
      "94 Train Loss 0.3922996 Test MSE 58.394329185658414 Test RE 0.38953628619530495\n",
      "95 Train Loss 0.3715941 Test MSE 55.90719393938616 Test RE 0.3811504439944113\n",
      "96 Train Loss 0.3618439 Test MSE 53.822918592044935 Test RE 0.3739781290258213\n",
      "97 Train Loss 0.35826343 Test MSE 52.49409960741877 Test RE 0.36933275704624824\n",
      "98 Train Loss 0.35588494 Test MSE 52.192475909603466 Test RE 0.36827016148216907\n",
      "99 Train Loss 0.354908 Test MSE 51.52862064078834 Test RE 0.36592058457379933\n",
      "100 Train Loss 0.3530036 Test MSE 50.01973179445534 Test RE 0.3605232370927493\n",
      "101 Train Loss 0.35058054 Test MSE 49.81248963896844 Test RE 0.3597756004961893\n",
      "102 Train Loss 0.34796718 Test MSE 50.10160012014733 Test RE 0.3608181543731813\n",
      "103 Train Loss 0.34677124 Test MSE 49.44091083173323 Test RE 0.3584312064376703\n",
      "104 Train Loss 0.34333655 Test MSE 48.04407907747174 Test RE 0.35333163177060106\n",
      "105 Train Loss 0.32548165 Test MSE 45.66210069679986 Test RE 0.3444613713566529\n",
      "106 Train Loss 0.30206332 Test MSE 41.49377464599407 Test RE 0.32836287878251136\n",
      "107 Train Loss 0.28593126 Test MSE 38.570536851807695 Test RE 0.3165850659916844\n",
      "108 Train Loss 0.28211045 Test MSE 37.79392320109115 Test RE 0.31338165562378023\n",
      "109 Train Loss 0.27841303 Test MSE 36.56901210169631 Test RE 0.3082614356003722\n",
      "110 Train Loss 0.26677507 Test MSE 36.600739223771804 Test RE 0.30839512977517497\n",
      "111 Train Loss 0.26161653 Test MSE 37.18708977671685 Test RE 0.31085558794207635\n",
      "112 Train Loss 0.25676572 Test MSE 36.95263890059597 Test RE 0.3098741236775743\n",
      "113 Train Loss 0.25131324 Test MSE 35.81281769258479 Test RE 0.3050575854635267\n",
      "114 Train Loss 0.24896935 Test MSE 35.073099843111464 Test RE 0.3018906470900027\n",
      "115 Train Loss 0.24464697 Test MSE 34.011626910532556 Test RE 0.29728725143146495\n",
      "116 Train Loss 0.24220209 Test MSE 33.19263591249856 Test RE 0.29368614161983925\n",
      "117 Train Loss 0.23990998 Test MSE 32.5731721587946 Test RE 0.29093274851313994\n",
      "118 Train Loss 0.23259744 Test MSE 30.879132176728778 Test RE 0.28326644081646957\n",
      "119 Train Loss 0.2286469 Test MSE 29.521749637436056 Test RE 0.27697057265364244\n",
      "120 Train Loss 0.22458336 Test MSE 28.487518013327858 Test RE 0.2720757837767625\n",
      "121 Train Loss 0.22145295 Test MSE 28.071309531140624 Test RE 0.2700809291504076\n",
      "122 Train Loss 0.21929818 Test MSE 27.36382348004663 Test RE 0.2666557620636217\n",
      "123 Train Loss 0.21823952 Test MSE 27.011067316695694 Test RE 0.2649314126204542\n",
      "124 Train Loss 0.2149248 Test MSE 27.013870816064355 Test RE 0.26494516098085874\n",
      "125 Train Loss 0.20913613 Test MSE 27.67137718072945 Test RE 0.26815010391602306\n",
      "126 Train Loss 0.20578472 Test MSE 27.722485250034634 Test RE 0.2683976216280496\n",
      "127 Train Loss 0.20437625 Test MSE 27.00603281633802 Test RE 0.2649067216404158\n",
      "128 Train Loss 0.20102699 Test MSE 25.474541579039006 Test RE 0.25728577262617286\n",
      "129 Train Loss 0.19890124 Test MSE 24.62064449186698 Test RE 0.2529369580126218\n",
      "130 Train Loss 0.19363686 Test MSE 22.788529045050304 Test RE 0.24334404851964048\n",
      "131 Train Loss 0.19123282 Test MSE 21.837585214489792 Test RE 0.23821268753153263\n",
      "132 Train Loss 0.190563 Test MSE 21.567654494018214 Test RE 0.2367358561282495\n",
      "133 Train Loss 0.1895932 Test MSE 21.451373474390156 Test RE 0.23609681837531582\n",
      "134 Train Loss 0.18770613 Test MSE 21.715428111362783 Test RE 0.23754548508079862\n",
      "135 Train Loss 0.18586588 Test MSE 22.231938780206352 Test RE 0.24035394319204356\n",
      "136 Train Loss 0.18071395 Test MSE 22.81754597599615 Test RE 0.24349892580726795\n",
      "137 Train Loss 0.17854771 Test MSE 22.861834511608095 Test RE 0.2437351252419156\n",
      "138 Train Loss 0.17757621 Test MSE 22.902529312161736 Test RE 0.24395195697307417\n",
      "139 Train Loss 0.17601497 Test MSE 22.503083045078597 Test RE 0.24181519874700752\n",
      "140 Train Loss 0.17506339 Test MSE 22.020093761034726 Test RE 0.23920605265739547\n",
      "141 Train Loss 0.17438094 Test MSE 21.893020784376674 Test RE 0.23851485203222872\n",
      "142 Train Loss 0.17225124 Test MSE 21.694791110917084 Test RE 0.2374325839787965\n",
      "143 Train Loss 0.16730869 Test MSE 20.910166586050842 Test RE 0.2330994929933854\n",
      "144 Train Loss 0.15428893 Test MSE 18.876795669280433 Test RE 0.22147602482770232\n",
      "145 Train Loss 0.14204004 Test MSE 16.863095449145067 Test RE 0.20932988345200648\n",
      "146 Train Loss 0.1329357 Test MSE 15.277130162387984 Test RE 0.19924318336529956\n",
      "147 Train Loss 0.12360734 Test MSE 13.682401226259557 Test RE 0.18855747043808105\n",
      "148 Train Loss 0.11880958 Test MSE 13.293423829629676 Test RE 0.18585789250129875\n",
      "149 Train Loss 0.11373226 Test MSE 12.617080861187615 Test RE 0.1810681338934875\n",
      "150 Train Loss 0.11157501 Test MSE 12.04926861166377 Test RE 0.17694688660278876\n",
      "151 Train Loss 0.10735065 Test MSE 11.196812338364726 Test RE 0.17057280142850914\n",
      "152 Train Loss 0.10491223 Test MSE 10.771106891181182 Test RE 0.16729877093919346\n",
      "153 Train Loss 0.10026524 Test MSE 9.726210797005171 Test RE 0.1589770454436891\n",
      "154 Train Loss 0.098936036 Test MSE 9.164097601723185 Test RE 0.15431474855157953\n",
      "155 Train Loss 0.09574278 Test MSE 8.46869911052273 Test RE 0.14834432403248474\n",
      "156 Train Loss 0.089762464 Test MSE 7.601570208577163 Test RE 0.14054462402139417\n",
      "157 Train Loss 0.0860945 Test MSE 7.005600622257256 Test RE 0.1349227766415616\n",
      "158 Train Loss 0.082885474 Test MSE 6.601272709753917 Test RE 0.13097138490420104\n",
      "159 Train Loss 0.07879781 Test MSE 6.362994191322886 Test RE 0.1285858991077257\n",
      "160 Train Loss 0.07614271 Test MSE 6.022826208292315 Test RE 0.12510156662214317\n",
      "161 Train Loss 0.0743828 Test MSE 5.710724018519383 Test RE 0.12181707543468702\n",
      "162 Train Loss 0.07278703 Test MSE 5.553678138730373 Test RE 0.12013040353056975\n",
      "163 Train Loss 0.07081481 Test MSE 5.526015388079408 Test RE 0.1198308465727881\n",
      "164 Train Loss 0.06547894 Test MSE 5.726891836461769 Test RE 0.1219893937143644\n",
      "165 Train Loss 0.061002247 Test MSE 5.814718364362679 Test RE 0.12292123777361\n",
      "166 Train Loss 0.05980518 Test MSE 5.7932765933752295 Test RE 0.12269439245683625\n",
      "167 Train Loss 0.05814237 Test MSE 5.918620913474782 Test RE 0.12401460796609493\n",
      "168 Train Loss 0.05716581 Test MSE 5.947487727568697 Test RE 0.12431666753703478\n",
      "169 Train Loss 0.056885213 Test MSE 5.955726665487457 Test RE 0.12440274445593955\n",
      "170 Train Loss 0.05605957 Test MSE 5.98304868941601 Test RE 0.12468776807235649\n",
      "171 Train Loss 0.05503299 Test MSE 5.970879849359809 Test RE 0.12456090316804977\n",
      "172 Train Loss 0.05434294 Test MSE 5.987882292404366 Test RE 0.1247381244644579\n",
      "173 Train Loss 0.053791314 Test MSE 5.986039247661515 Test RE 0.12471892605453383\n",
      "174 Train Loss 0.053097636 Test MSE 5.821991327474155 Test RE 0.12299808777696827\n",
      "175 Train Loss 0.05200044 Test MSE 5.600943918829383 Test RE 0.1206405183723568\n",
      "176 Train Loss 0.05179055 Test MSE 5.550251453865636 Test RE 0.12009333687465613\n",
      "177 Train Loss 0.051768545 Test MSE 5.544573599044754 Test RE 0.12003189398202826\n",
      "178 Train Loss 0.051762365 Test MSE 5.543788362673724 Test RE 0.12002339407265818\n",
      "179 Train Loss 0.05175665 Test MSE 5.543294357009591 Test RE 0.12001804632545361\n",
      "180 Train Loss 0.051726542 Test MSE 5.542589007351203 Test RE 0.12001041030834533\n",
      "181 Train Loss 0.051703822 Test MSE 5.542467040346326 Test RE 0.12000908986127573\n",
      "182 Train Loss 0.051491775 Test MSE 5.4857339592181855 Test RE 0.11939329926104755\n",
      "183 Train Loss 0.050929938 Test MSE 5.23891250398447 Test RE 0.11667643612938564\n",
      "184 Train Loss 0.050285958 Test MSE 5.185504960699089 Test RE 0.11608018980718962\n",
      "185 Train Loss 0.048773978 Test MSE 5.229261253513719 Test RE 0.11656891452004041\n",
      "186 Train Loss 0.047779724 Test MSE 5.072624547359539 Test RE 0.11480979514401658\n",
      "187 Train Loss 0.047402624 Test MSE 5.032303297317004 Test RE 0.11435258503058031\n",
      "188 Train Loss 0.047333814 Test MSE 5.033783631651277 Test RE 0.11436940313558502\n",
      "189 Train Loss 0.047267668 Test MSE 5.03624355938927 Test RE 0.11439734495077915\n",
      "190 Train Loss 0.047260456 Test MSE 5.036080456592774 Test RE 0.11439549251078858\n",
      "191 Train Loss 0.047231533 Test MSE 5.039767884761315 Test RE 0.11443736515142873\n",
      "192 Train Loss 0.047040474 Test MSE 5.047134113289157 Test RE 0.1145209666189736\n",
      "193 Train Loss 0.046790116 Test MSE 5.016008361694985 Test RE 0.1141672942426543\n",
      "194 Train Loss 0.046492044 Test MSE 4.906044503420613 Test RE 0.11290893844603724\n",
      "195 Train Loss 0.04597141 Test MSE 4.706577040241654 Test RE 0.11058982440178611\n",
      "196 Train Loss 0.044968426 Test MSE 4.440436002858687 Test RE 0.10741758647408758\n",
      "197 Train Loss 0.044030823 Test MSE 4.190674507039581 Test RE 0.10435290597239068\n",
      "198 Train Loss 0.043081567 Test MSE 4.052938597509787 Test RE 0.10262368250028574\n",
      "199 Train Loss 0.042035382 Test MSE 4.059800081570895 Test RE 0.10271051492707367\n",
      "Training time: 79.66\n",
      "Training time: 79.66\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4.5164413 Test MSE 386.5620577391544 Test RE 1.00224106858457\n",
      "1 Train Loss 3.6274495 Test MSE 385.6599495680545 Test RE 1.0010709356902823\n",
      "2 Train Loss 3.1515772 Test MSE 388.36409957630997 Test RE 1.0045744330918607\n",
      "3 Train Loss 2.807423 Test MSE 388.4022467926345 Test RE 1.00462376923964\n",
      "4 Train Loss 2.440324 Test MSE 384.8133824301423 Test RE 0.9999716001952176\n",
      "5 Train Loss 2.3952246 Test MSE 384.4602436492533 Test RE 0.9995126637071963\n",
      "6 Train Loss 2.38441 Test MSE 384.31981362259626 Test RE 0.9993301033359007\n",
      "7 Train Loss 2.3821547 Test MSE 384.0052660948857 Test RE 0.9989210674938732\n",
      "8 Train Loss 2.3816037 Test MSE 383.9271077388826 Test RE 0.9988194048235902\n",
      "9 Train Loss 2.380799 Test MSE 383.7576781086981 Test RE 0.9985989876224709\n",
      "10 Train Loss 2.3807569 Test MSE 383.7380525960572 Test RE 0.9985734529312558\n",
      "11 Train Loss 2.3807497 Test MSE 383.7331511196807 Test RE 0.9985670755342452\n",
      "12 Train Loss 2.3807445 Test MSE 383.72641483434063 Test RE 0.9985583107686086\n",
      "13 Train Loss 2.3807411 Test MSE 383.7228163331968 Test RE 0.9985536286285356\n",
      "14 Train Loss 2.3807323 Test MSE 383.7136156297209 Test RE 0.9985416571604289\n",
      "15 Train Loss 2.38034 Test MSE 383.5738359591048 Test RE 0.9983597655988871\n",
      "16 Train Loss 2.380334 Test MSE 383.57213438129804 Test RE 0.998357551176858\n",
      "17 Train Loss 2.3803265 Test MSE 383.5700662305863 Test RE 0.9983548596926778\n",
      "18 Train Loss 2.3803203 Test MSE 383.56752816822836 Test RE 0.998351556657191\n",
      "19 Train Loss 2.3803113 Test MSE 383.5643134463945 Test RE 0.9983473730010728\n",
      "20 Train Loss 2.3803015 Test MSE 383.5604098902399 Test RE 0.9983422928687311\n",
      "21 Train Loss 2.3799012 Test MSE 383.34551230488086 Test RE 0.9980625828135679\n",
      "22 Train Loss 2.3791177 Test MSE 383.2901195493929 Test RE 0.9979904710604062\n",
      "23 Train Loss 2.378649 Test MSE 383.0815278726492 Test RE 0.9977188741349997\n",
      "24 Train Loss 2.3779373 Test MSE 382.80167778991233 Test RE 0.9973543789812616\n",
      "25 Train Loss 2.3767347 Test MSE 382.5299446307316 Test RE 0.9970003283276971\n",
      "26 Train Loss 2.3752012 Test MSE 382.1418135712506 Test RE 0.9964944006124715\n",
      "27 Train Loss 2.374132 Test MSE 381.68470859365436 Test RE 0.9958982360079193\n",
      "28 Train Loss 2.3717198 Test MSE 380.3324118128228 Test RE 0.9941324527067154\n",
      "29 Train Loss 2.3194041 Test MSE 364.3037159853504 Test RE 0.9729586491915287\n",
      "30 Train Loss 2.1511917 Test MSE 335.82486197408866 Test RE 0.934155134040874\n",
      "31 Train Loss 2.0458477 Test MSE 318.5829862612313 Test RE 0.9098585320229203\n",
      "32 Train Loss 1.9820797 Test MSE 311.2108631181785 Test RE 0.8992696917546229\n",
      "33 Train Loss 1.9034362 Test MSE 298.9901436148872 Test RE 0.8814364754088019\n",
      "34 Train Loss 1.8214015 Test MSE 279.8585729542915 Test RE 0.8527699537784019\n",
      "35 Train Loss 1.6619071 Test MSE 255.2222029178956 Test RE 0.8143700794363291\n",
      "36 Train Loss 1.5968504 Test MSE 247.39579604264694 Test RE 0.8017865012419374\n",
      "37 Train Loss 1.5746096 Test MSE 245.4286274163756 Test RE 0.7985924349337421\n",
      "38 Train Loss 1.5514549 Test MSE 242.53416021896635 Test RE 0.7938693606610906\n",
      "39 Train Loss 1.4675554 Test MSE 231.08076636349455 Test RE 0.7748978967484689\n",
      "40 Train Loss 1.4578105 Test MSE 229.97603577737428 Test RE 0.7730433958085348\n",
      "41 Train Loss 1.429376 Test MSE 225.7322863399707 Test RE 0.7658776970480773\n",
      "42 Train Loss 1.335866 Test MSE 209.73856543152738 Test RE 0.7382470651388467\n",
      "43 Train Loss 1.2759038 Test MSE 196.89433106099204 Test RE 0.7152851168302381\n",
      "44 Train Loss 1.2283752 Test MSE 190.81237302386674 Test RE 0.7041510785801953\n",
      "45 Train Loss 1.1791422 Test MSE 182.96582367750446 Test RE 0.689521115160793\n",
      "46 Train Loss 1.1219621 Test MSE 172.59013123038173 Test RE 0.6696849854294715\n",
      "47 Train Loss 1.1164201 Test MSE 170.56942686675305 Test RE 0.6657530697435314\n",
      "48 Train Loss 1.1052128 Test MSE 169.96663845071075 Test RE 0.6645756505918242\n",
      "49 Train Loss 1.0607684 Test MSE 164.87398880678538 Test RE 0.6545437119034078\n",
      "50 Train Loss 1.0390583 Test MSE 159.73609708343164 Test RE 0.6442643743216654\n",
      "51 Train Loss 1.0173728 Test MSE 155.5053837587774 Test RE 0.6356752422284221\n",
      "52 Train Loss 1.0128826 Test MSE 153.4319521723174 Test RE 0.6314231317317203\n",
      "53 Train Loss 0.99231553 Test MSE 147.35851939599553 Test RE 0.6187998605426128\n",
      "54 Train Loss 0.97696036 Test MSE 144.61480275268406 Test RE 0.6130119734473244\n",
      "55 Train Loss 0.9572054 Test MSE 140.39325362875294 Test RE 0.6039982801808647\n",
      "56 Train Loss 0.9493338 Test MSE 137.40638290976491 Test RE 0.5975386976778079\n",
      "57 Train Loss 0.9333595 Test MSE 137.59232274745366 Test RE 0.597942858983732\n",
      "58 Train Loss 0.9172512 Test MSE 140.94378998119163 Test RE 0.60518137712879\n",
      "59 Train Loss 0.90687114 Test MSE 139.46390398052736 Test RE 0.6019958420214113\n",
      "60 Train Loss 0.8466092 Test MSE 126.15432710943738 Test RE 0.5725503154235534\n",
      "61 Train Loss 0.82624745 Test MSE 122.54312987518988 Test RE 0.5642961233904699\n",
      "62 Train Loss 0.8132939 Test MSE 119.29758406203766 Test RE 0.5567733081065274\n",
      "63 Train Loss 0.7943585 Test MSE 116.70708237864332 Test RE 0.5506950699941393\n",
      "64 Train Loss 0.7260856 Test MSE 105.70343106418271 Test RE 0.5240915104004278\n",
      "65 Train Loss 0.6904736 Test MSE 97.71660260894049 Test RE 0.5039027889832225\n",
      "66 Train Loss 0.61850023 Test MSE 84.63730654742379 Test RE 0.46896831308371045\n",
      "67 Train Loss 0.6056173 Test MSE 82.74800580578561 Test RE 0.46370454286435775\n",
      "68 Train Loss 0.5960006 Test MSE 80.59376490632108 Test RE 0.4576287541176721\n",
      "69 Train Loss 0.58767796 Test MSE 79.3366402288654 Test RE 0.4540456141606898\n",
      "70 Train Loss 0.541593 Test MSE 74.3230347769166 Test RE 0.43946500732138066\n",
      "71 Train Loss 0.5109995 Test MSE 72.42304935909269 Test RE 0.43381142535569234\n",
      "72 Train Loss 0.50432765 Test MSE 71.58920702758999 Test RE 0.4313068528064769\n",
      "73 Train Loss 0.49610826 Test MSE 70.1284669342951 Test RE 0.4268838797444319\n",
      "74 Train Loss 0.4690534 Test MSE 68.25642982434131 Test RE 0.42114763561189633\n",
      "75 Train Loss 0.45382577 Test MSE 65.89072167048445 Test RE 0.41378497284339844\n",
      "76 Train Loss 0.44120684 Test MSE 62.13955095476411 Test RE 0.40183396047937225\n",
      "77 Train Loss 0.4366031 Test MSE 60.544850443386174 Test RE 0.3966442728656711\n",
      "78 Train Loss 0.43165335 Test MSE 59.72245583864008 Test RE 0.3939412072774305\n",
      "79 Train Loss 0.42154402 Test MSE 60.435274453948104 Test RE 0.3962851806291801\n",
      "80 Train Loss 0.41264027 Test MSE 59.40070795023318 Test RE 0.39287861763959625\n",
      "81 Train Loss 0.4044864 Test MSE 56.609249204720484 Test RE 0.38353612845460217\n",
      "82 Train Loss 0.40057153 Test MSE 55.82173662402923 Test RE 0.38085902755951334\n",
      "83 Train Loss 0.39448184 Test MSE 55.30483354763055 Test RE 0.3790915702981484\n",
      "84 Train Loss 0.37759364 Test MSE 53.50768041504193 Test RE 0.37288133498088377\n",
      "85 Train Loss 0.37086317 Test MSE 50.99033952025592 Test RE 0.364004317078913\n",
      "86 Train Loss 0.3648101 Test MSE 49.48975011623514 Test RE 0.35860819754057066\n",
      "87 Train Loss 0.34601578 Test MSE 47.18613762837182 Test RE 0.3501626316569192\n",
      "88 Train Loss 0.328584 Test MSE 45.88685165884464 Test RE 0.3453080581425969\n",
      "89 Train Loss 0.30461997 Test MSE 41.369937981921375 Test RE 0.3278725190984107\n",
      "90 Train Loss 0.29432327 Test MSE 39.8578155821996 Test RE 0.3218246675026292\n",
      "91 Train Loss 0.28834108 Test MSE 39.60354872683461 Test RE 0.320796509444044\n",
      "92 Train Loss 0.2859926 Test MSE 38.683580205328944 Test RE 0.31704865370486285\n",
      "93 Train Loss 0.28375873 Test MSE 37.444418266340925 Test RE 0.31192926826109213\n",
      "94 Train Loss 0.2748462 Test MSE 35.519481718879284 Test RE 0.30380568235608063\n",
      "95 Train Loss 0.2587594 Test MSE 34.976290717046034 Test RE 0.3014737183748251\n",
      "96 Train Loss 0.24371013 Test MSE 32.873120416471814 Test RE 0.2922691979853368\n",
      "97 Train Loss 0.21361437 Test MSE 27.789838631608536 Test RE 0.26872346760655563\n",
      "98 Train Loss 0.20813078 Test MSE 25.613388285750574 Test RE 0.25798597635051673\n",
      "99 Train Loss 0.19612056 Test MSE 23.181046347810888 Test RE 0.24543082100819308\n",
      "100 Train Loss 0.18144302 Test MSE 22.20253122224127 Test RE 0.24019492508914175\n",
      "101 Train Loss 0.17560463 Test MSE 21.140564212241415 Test RE 0.23438017244304168\n",
      "102 Train Loss 0.17270151 Test MSE 19.876198381381517 Test RE 0.22726326613575304\n",
      "103 Train Loss 0.15586108 Test MSE 17.09345922717645 Test RE 0.2107548427717632\n",
      "104 Train Loss 0.13795143 Test MSE 16.68003340555671 Test RE 0.20819056344832898\n",
      "105 Train Loss 0.13697456 Test MSE 16.570534707221505 Test RE 0.20750608844117963\n",
      "106 Train Loss 0.13456495 Test MSE 16.268585513914 Test RE 0.20560680277189874\n",
      "107 Train Loss 0.12835936 Test MSE 15.729946516196533 Test RE 0.20217441983158724\n",
      "108 Train Loss 0.12467218 Test MSE 14.386103855633595 Test RE 0.193345548773076\n",
      "109 Train Loss 0.12203683 Test MSE 12.67366030669887 Test RE 0.18147366648681176\n",
      "110 Train Loss 0.11948489 Test MSE 11.957914514985454 Test RE 0.1762748300874855\n",
      "111 Train Loss 0.11730865 Test MSE 12.006873242275104 Test RE 0.17663531835716034\n",
      "112 Train Loss 0.11289862 Test MSE 11.140057368571807 Test RE 0.17013994818460604\n",
      "113 Train Loss 0.101937994 Test MSE 10.246165707064797 Test RE 0.16317111084643782\n",
      "114 Train Loss 0.08767329 Test MSE 10.218010490281467 Test RE 0.16294676944434336\n",
      "115 Train Loss 0.082215905 Test MSE 10.098900140890677 Test RE 0.16199425819379892\n",
      "116 Train Loss 0.07514573 Test MSE 8.927805589640395 Test RE 0.15231228897219273\n",
      "117 Train Loss 0.06703517 Test MSE 7.378969564902141 Test RE 0.13847151444877337\n",
      "118 Train Loss 0.06489985 Test MSE 6.8174771925635955 Test RE 0.13309888870221953\n",
      "119 Train Loss 0.06149661 Test MSE 6.48994232002173 Test RE 0.1298622732669935\n",
      "120 Train Loss 0.056602463 Test MSE 5.931195100863372 Test RE 0.1241462733986813\n",
      "121 Train Loss 0.05177407 Test MSE 4.852927284212136 Test RE 0.1122960485065645\n",
      "122 Train Loss 0.050549135 Test MSE 4.48623439015005 Test RE 0.10797011467523698\n",
      "123 Train Loss 0.050175536 Test MSE 4.413886560259952 Test RE 0.10709597923874063\n",
      "124 Train Loss 0.049891442 Test MSE 4.288678308530923 Test RE 0.1055660612892381\n",
      "125 Train Loss 0.049572535 Test MSE 4.1698582830201865 Test RE 0.10409340864042242\n",
      "126 Train Loss 0.0488896 Test MSE 4.310741242728127 Test RE 0.10583725314486829\n",
      "127 Train Loss 0.04864024 Test MSE 4.425521553633714 Test RE 0.10723703870174586\n",
      "128 Train Loss 0.04848431 Test MSE 4.493057784110505 Test RE 0.10805219272169106\n",
      "129 Train Loss 0.04845178 Test MSE 4.481902571427455 Test RE 0.1079179751880609\n",
      "130 Train Loss 0.048278943 Test MSE 4.290857855319255 Test RE 0.10559288271622852\n",
      "131 Train Loss 0.048103504 Test MSE 4.2228671435233744 Test RE 0.10475295710157362\n",
      "132 Train Loss 0.04781046 Test MSE 4.286673193412444 Test RE 0.10554138039256411\n",
      "133 Train Loss 0.047763474 Test MSE 4.255387920961706 Test RE 0.10515554068728034\n",
      "134 Train Loss 0.047739007 Test MSE 4.229366566390726 Test RE 0.10483353884555775\n",
      "135 Train Loss 0.047718927 Test MSE 4.201080526676472 Test RE 0.10448238693230354\n",
      "136 Train Loss 0.047516193 Test MSE 4.032617250835916 Test RE 0.1023660827297671\n",
      "137 Train Loss 0.04657857 Test MSE 3.9442126781092544 Test RE 0.10123781071569636\n",
      "138 Train Loss 0.04519034 Test MSE 3.8522380270298937 Test RE 0.10005047147701841\n",
      "139 Train Loss 0.044060573 Test MSE 3.3005868799126286 Test RE 0.09261006030903951\n",
      "140 Train Loss 0.043430805 Test MSE 3.068926876364185 Test RE 0.0893009034879475\n",
      "141 Train Loss 0.04276959 Test MSE 2.7623995804027413 Test RE 0.08472387917817341\n",
      "142 Train Loss 0.041969493 Test MSE 2.434431370580202 Test RE 0.07953556131720502\n",
      "143 Train Loss 0.041509755 Test MSE 2.1521674977613388 Test RE 0.07478260969708102\n",
      "144 Train Loss 0.041295893 Test MSE 1.9719139934711973 Test RE 0.07158245138609363\n",
      "145 Train Loss 0.041027606 Test MSE 1.8211558318247614 Test RE 0.06879171504575023\n",
      "146 Train Loss 0.040135417 Test MSE 1.6058477202745596 Test RE 0.06459735894538758\n",
      "147 Train Loss 0.03806781 Test MSE 1.5821204763891348 Test RE 0.0641183530085591\n",
      "148 Train Loss 0.035485856 Test MSE 1.5919430639750445 Test RE 0.06431708427784524\n",
      "149 Train Loss 0.03319261 Test MSE 1.5998641973903012 Test RE 0.06447689904927305\n",
      "150 Train Loss 0.031931344 Test MSE 1.789690368318011 Test RE 0.06819484285419351\n",
      "151 Train Loss 0.028951801 Test MSE 2.26392083421836 Test RE 0.0766996179391272\n",
      "152 Train Loss 0.026969481 Test MSE 2.3182164995560726 Test RE 0.0776139128786142\n",
      "153 Train Loss 0.026484527 Test MSE 2.3605335172916897 Test RE 0.07831909640501321\n",
      "154 Train Loss 0.026266128 Test MSE 2.3624620276561523 Test RE 0.07835108246909893\n",
      "155 Train Loss 0.025771933 Test MSE 2.4163890338537395 Test RE 0.07924028166427871\n",
      "156 Train Loss 0.025470626 Test MSE 2.432604071393492 Test RE 0.07950570577251952\n",
      "157 Train Loss 0.025317937 Test MSE 2.356781868456275 Test RE 0.07825683450881629\n",
      "158 Train Loss 0.025295237 Test MSE 2.3419316394448235 Test RE 0.0780098943090286\n",
      "159 Train Loss 0.025264189 Test MSE 2.320033930642186 Test RE 0.07764433072402553\n",
      "160 Train Loss 0.025115158 Test MSE 2.272032998035183 Test RE 0.07683691149640648\n",
      "161 Train Loss 0.025105232 Test MSE 2.2720159726782696 Test RE 0.076836623609238\n",
      "162 Train Loss 0.024760261 Test MSE 2.253973464088551 Test RE 0.07653092842289237\n",
      "163 Train Loss 0.024143018 Test MSE 2.1072249141746093 Test RE 0.07399766720972183\n",
      "164 Train Loss 0.023344126 Test MSE 1.8776106195602404 Test RE 0.06984982927952796\n",
      "165 Train Loss 0.021470446 Test MSE 1.8572243209993575 Test RE 0.0694695944674295\n",
      "166 Train Loss 0.020919155 Test MSE 2.0019121048065514 Test RE 0.07212487696624657\n",
      "167 Train Loss 0.020634158 Test MSE 1.9847933972504757 Test RE 0.07181583854201395\n",
      "168 Train Loss 0.020057011 Test MSE 1.8427252020550084 Test RE 0.0691978929077913\n",
      "169 Train Loss 0.019302066 Test MSE 1.7070342600428305 Test RE 0.06660145258109734\n",
      "170 Train Loss 0.01847981 Test MSE 1.5519487637050124 Test RE 0.0635040278717278\n",
      "171 Train Loss 0.017005855 Test MSE 1.186066420150222 Test RE 0.055515862022252976\n",
      "172 Train Loss 0.016478973 Test MSE 0.9865337823028567 Test RE 0.05063123995076383\n",
      "173 Train Loss 0.01584174 Test MSE 0.8279899643543576 Test RE 0.04638473918971645\n",
      "174 Train Loss 0.015356872 Test MSE 0.7478028470260379 Test RE 0.04408147720733035\n",
      "175 Train Loss 0.015097214 Test MSE 0.7362585591266818 Test RE 0.043739897483499\n",
      "176 Train Loss 0.015087226 Test MSE 0.7383317960672743 Test RE 0.04380143797152257\n",
      "177 Train Loss 0.015081276 Test MSE 0.7392031383561847 Test RE 0.04382727648453776\n",
      "178 Train Loss 0.015077206 Test MSE 0.7394596603962047 Test RE 0.04383488040732077\n",
      "179 Train Loss 0.015072178 Test MSE 0.7400386915575045 Test RE 0.04385203941966283\n",
      "180 Train Loss 0.015067888 Test MSE 0.7399113114856664 Test RE 0.04384826521408083\n",
      "181 Train Loss 0.015065836 Test MSE 0.7396295817608596 Test RE 0.043839916554150225\n",
      "182 Train Loss 0.01506133 Test MSE 0.7389453638063687 Test RE 0.043819634105217026\n",
      "183 Train Loss 0.015057685 Test MSE 0.7378447653456719 Test RE 0.04378698907120041\n",
      "184 Train Loss 0.01505113 Test MSE 0.7362761394178609 Test RE 0.043740419688323734\n",
      "185 Train Loss 0.01504894 Test MSE 0.7345275591900227 Test RE 0.043688449311230566\n",
      "186 Train Loss 0.015042931 Test MSE 0.7326156424331621 Test RE 0.04363155348563889\n",
      "187 Train Loss 0.015035711 Test MSE 0.7296881862676802 Test RE 0.04354429264458537\n",
      "188 Train Loss 0.014875034 Test MSE 0.691265789990653 Test RE 0.042382357925888126\n",
      "189 Train Loss 0.014663215 Test MSE 0.679374624733632 Test RE 0.04201624562664523\n",
      "190 Train Loss 0.013475986 Test MSE 0.5292431592768476 Test RE 0.03708431054050393\n",
      "191 Train Loss 0.0125350775 Test MSE 0.4145654370806908 Test RE 0.03282155313596981\n",
      "192 Train Loss 0.012068842 Test MSE 0.3647227733723317 Test RE 0.030785344886558638\n",
      "193 Train Loss 0.011580971 Test MSE 0.2957369489102134 Test RE 0.027721413855916508\n",
      "194 Train Loss 0.011130949 Test MSE 0.26914348754330375 Test RE 0.026445666713942242\n",
      "195 Train Loss 0.010913533 Test MSE 0.2658336979904839 Test RE 0.026282556028559185\n",
      "196 Train Loss 0.010904805 Test MSE 0.2639167967593291 Test RE 0.02618762409068864\n",
      "197 Train Loss 0.01087267 Test MSE 0.25349563861490354 Test RE 0.025665387659122515\n",
      "198 Train Loss 0.010732183 Test MSE 0.19286498084297288 Test RE 0.02238665836827723\n",
      "199 Train Loss 0.010548487 Test MSE 0.15340620758014414 Test RE 0.019965677388625702\n",
      "Training time: 73.63\n",
      "Training time: 73.63\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4.369238 Test MSE 386.46744182550384 Test RE 1.0021184055401806\n",
      "1 Train Loss 3.66893 Test MSE 386.7808460843841 Test RE 1.0025246551940072\n",
      "2 Train Loss 3.5695083 Test MSE 386.01767703876794 Test RE 1.0015351108909532\n",
      "3 Train Loss 3.1475182 Test MSE 389.678268664936 Test RE 1.006272666435459\n",
      "4 Train Loss 2.9080777 Test MSE 388.8660224252625 Test RE 1.005223380888162\n",
      "5 Train Loss 2.633297 Test MSE 386.9405576119115 Test RE 1.0027316176383054\n",
      "6 Train Loss 2.566001 Test MSE 387.91232687216615 Test RE 1.0039899669040762\n",
      "7 Train Loss 2.5223823 Test MSE 388.27020740822894 Test RE 1.0044529911599307\n",
      "8 Train Loss 2.4094958 Test MSE 385.89907828694066 Test RE 1.0013812449594384\n",
      "9 Train Loss 2.3899822 Test MSE 384.11498358773395 Test RE 0.9990637625260035\n",
      "10 Train Loss 2.389869 Test MSE 383.9897556150866 Test RE 0.9989008934196318\n",
      "11 Train Loss 2.3898618 Test MSE 383.9738305470998 Test RE 0.9988801796774776\n",
      "12 Train Loss 2.3898392 Test MSE 383.9387949013617 Test RE 0.998834607287287\n",
      "13 Train Loss 2.3889 Test MSE 383.3382318296784 Test RE 0.9980531051959196\n",
      "14 Train Loss 2.3838615 Test MSE 382.85789272348717 Test RE 0.9974276076989236\n",
      "15 Train Loss 2.3806546 Test MSE 383.0701941283237 Test RE 0.9977041149082493\n",
      "16 Train Loss 2.3790872 Test MSE 383.0038897831785 Test RE 0.9976177665290724\n",
      "17 Train Loss 2.3787026 Test MSE 382.9471331084116 Test RE 0.997543846176771\n",
      "18 Train Loss 2.3785186 Test MSE 383.0281878367132 Test RE 0.9976494108362236\n",
      "19 Train Loss 2.378501 Test MSE 383.04928786621895 Test RE 0.9976768894171454\n",
      "20 Train Loss 2.3784964 Test MSE 383.05897770509216 Test RE 0.997689508246465\n",
      "21 Train Loss 2.3784893 Test MSE 383.0683190517046 Test RE 0.9977016730921322\n",
      "22 Train Loss 2.3784857 Test MSE 383.07684264644325 Test RE 0.9977127728842629\n",
      "23 Train Loss 2.3784795 Test MSE 383.0850940314526 Test RE 0.997723518075528\n",
      "24 Train Loss 2.3784745 Test MSE 383.09296858689794 Test RE 0.9977337724405637\n",
      "25 Train Loss 2.3784673 Test MSE 383.09884173633975 Test RE 0.9977414204758585\n",
      "26 Train Loss 2.378104 Test MSE 383.0788838575231 Test RE 0.9977154310189073\n",
      "27 Train Loss 2.376458 Test MSE 382.79597987088874 Test RE 0.997346956253037\n",
      "28 Train Loss 2.375049 Test MSE 382.75189340735443 Test RE 0.9972895225695729\n",
      "29 Train Loss 2.3738773 Test MSE 382.3166308145958 Test RE 0.996722306169133\n",
      "30 Train Loss 2.3687122 Test MSE 380.77359934124996 Test RE 0.9947088849349711\n",
      "31 Train Loss 2.3671105 Test MSE 379.9614544453444 Test RE 0.9936475206937995\n",
      "32 Train Loss 2.3664079 Test MSE 379.25312043024803 Test RE 0.9927208968713508\n",
      "33 Train Loss 2.3636298 Test MSE 376.57174344236955 Test RE 0.9892053284194806\n",
      "34 Train Loss 2.3038516 Test MSE 362.3815950095133 Test RE 0.9703885168192093\n",
      "35 Train Loss 2.2089937 Test MSE 342.8713210911938 Test RE 0.9439047311608889\n",
      "36 Train Loss 2.071996 Test MSE 328.5821278253974 Test RE 0.9240267657715977\n",
      "37 Train Loss 2.0144656 Test MSE 316.39689238926223 Test RE 0.9067314659822548\n",
      "38 Train Loss 2.006025 Test MSE 313.43774158618453 Test RE 0.9024813320895927\n",
      "39 Train Loss 2.0003028 Test MSE 308.48926351626045 Test RE 0.8953289128282125\n",
      "40 Train Loss 1.9825834 Test MSE 306.68307639473625 Test RE 0.8927040150575161\n",
      "41 Train Loss 1.9626617 Test MSE 308.8375244564119 Test RE 0.8958341494336969\n",
      "42 Train Loss 1.9607079 Test MSE 309.288820870221 Test RE 0.8964884403169856\n",
      "43 Train Loss 1.933624 Test MSE 304.2956658152961 Test RE 0.8892225464972804\n",
      "44 Train Loss 1.9144394 Test MSE 298.5468296505242 Test RE 0.8807827781935308\n",
      "45 Train Loss 1.8753682 Test MSE 296.9312598279749 Test RE 0.8783963914740727\n",
      "46 Train Loss 1.8413932 Test MSE 292.59356262260854 Test RE 0.8719567941924308\n",
      "47 Train Loss 1.8246082 Test MSE 286.6044282678343 Test RE 0.862986557103327\n",
      "48 Train Loss 1.8125188 Test MSE 282.6001807634548 Test RE 0.856936813334928\n",
      "49 Train Loss 1.7907244 Test MSE 282.0074405297504 Test RE 0.856037649805175\n",
      "50 Train Loss 1.7541085 Test MSE 278.78396954799564 Test RE 0.8511311423056485\n",
      "51 Train Loss 1.726874 Test MSE 274.2193974630486 Test RE 0.8441345355416443\n",
      "52 Train Loss 1.7173135 Test MSE 271.6354265233737 Test RE 0.8401479797173367\n",
      "53 Train Loss 1.6942766 Test MSE 262.9368342036815 Test RE 0.8265864797374901\n",
      "54 Train Loss 1.669938 Test MSE 259.65563208996696 Test RE 0.8214127810137801\n",
      "55 Train Loss 1.6672517 Test MSE 259.20069108961326 Test RE 0.8206928694529865\n",
      "56 Train Loss 1.6576254 Test MSE 260.06518071119115 Test RE 0.8220603231476922\n",
      "57 Train Loss 1.646917 Test MSE 260.3648134141882 Test RE 0.8225337530179655\n",
      "58 Train Loss 1.6420836 Test MSE 258.0195623154614 Test RE 0.8188208629365955\n",
      "59 Train Loss 1.6400162 Test MSE 256.3577922405272 Test RE 0.8161798035612012\n",
      "60 Train Loss 1.6387421 Test MSE 255.86433096678357 Test RE 0.8153938958599807\n",
      "61 Train Loss 1.6312019 Test MSE 252.48435287825342 Test RE 0.8099902980399079\n",
      "62 Train Loss 1.6018758 Test MSE 244.10890991245927 Test RE 0.7964424473255073\n",
      "63 Train Loss 1.5728918 Test MSE 241.66415954693076 Test RE 0.7924442266358797\n",
      "64 Train Loss 1.56159 Test MSE 241.60591248817755 Test RE 0.7923487215201693\n",
      "65 Train Loss 1.5576717 Test MSE 242.2344242242413 Test RE 0.793378657030331\n",
      "66 Train Loss 1.5540876 Test MSE 242.62189512486452 Test RE 0.7940129358091926\n",
      "67 Train Loss 1.5412441 Test MSE 241.01158419898678 Test RE 0.7913735690207189\n",
      "68 Train Loss 1.5304664 Test MSE 238.67661142089415 Test RE 0.7875307389794157\n",
      "69 Train Loss 1.5270011 Test MSE 235.96452062184403 Test RE 0.783043585784632\n",
      "70 Train Loss 1.5223134 Test MSE 233.79082141982465 Test RE 0.7794285522592724\n",
      "71 Train Loss 1.5220062 Test MSE 233.77432362708868 Test RE 0.7794010510137391\n",
      "72 Train Loss 1.5220062 Test MSE 233.77432362708868 Test RE 0.7794010510137391\n",
      "73 Train Loss 1.5220062 Test MSE 233.77432362708868 Test RE 0.7794010510137391\n",
      "74 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "75 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "76 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "77 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "78 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "79 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "80 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "81 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "82 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "83 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "84 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "85 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "86 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "87 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "88 Train Loss 1.522006 Test MSE 233.7743393684763 Test RE 0.7794010772545463\n",
      "89 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "90 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "91 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "92 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "93 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "94 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "95 Train Loss 1.5219723 Test MSE 233.77428634690136 Test RE 0.7794009888678672\n",
      "96 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "97 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "98 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "99 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "100 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "101 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "102 Train Loss 1.5219723 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "103 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "104 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "105 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "106 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "107 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "108 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "109 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "110 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "111 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "112 Train Loss 1.5219723 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "113 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "114 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "115 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "116 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "117 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "118 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "119 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "120 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "121 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "122 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "123 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "124 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "125 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "126 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "127 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "128 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "129 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "130 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "131 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "132 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "133 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "134 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "135 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "136 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "137 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "138 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "139 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "140 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "141 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "142 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "143 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "144 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "145 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "146 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "147 Train Loss 1.5219723 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "148 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "149 Train Loss 1.5219723 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "150 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "151 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "152 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "153 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "154 Train Loss 1.5219722 Test MSE 233.77428634188072 Test RE 0.7794009888594978\n",
      "155 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "156 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "157 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "158 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "159 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "160 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "161 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "162 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "163 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "164 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "165 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "166 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "167 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "168 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "169 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "170 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "171 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "172 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "173 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "174 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "175 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "176 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "177 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "178 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "179 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "180 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "181 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "182 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "183 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "184 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "185 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "186 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "187 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "188 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "189 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "190 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "191 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "192 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "193 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "194 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "195 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "196 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "197 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "198 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "199 Train Loss 1.5219722 Test MSE 233.7742861996278 Test RE 0.779400988622363\n",
      "Training time: 46.19\n",
      "Training time: 46.19\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 4.5472407 Test MSE 385.89467121288146 Test RE 1.0013755269179134\n",
      "1 Train Loss 3.990301 Test MSE 387.2518423624916 Test RE 1.0031348737770467\n",
      "2 Train Loss 3.3353806 Test MSE 387.0608247711636 Test RE 1.002887437843082\n",
      "3 Train Loss 3.0226853 Test MSE 390.6549231312993 Test RE 1.0075328928264085\n",
      "4 Train Loss 2.8137963 Test MSE 389.92403632167793 Test RE 1.0065899413688386\n",
      "5 Train Loss 2.6920574 Test MSE 387.0889822067544 Test RE 1.0029239156045575\n",
      "6 Train Loss 2.4241028 Test MSE 384.69951994940004 Test RE 0.9998236483879622\n",
      "7 Train Loss 2.3848283 Test MSE 384.28625174242404 Test RE 0.9992864676342584\n",
      "8 Train Loss 2.3822799 Test MSE 384.0058661791708 Test RE 0.9989218479995766\n",
      "9 Train Loss 2.3819978 Test MSE 383.8897708112181 Test RE 0.9987708360277916\n",
      "10 Train Loss 2.3819907 Test MSE 383.8852561314569 Test RE 0.9987649630610186\n",
      "11 Train Loss 2.3819838 Test MSE 383.8834273979886 Test RE 0.998762584124786\n",
      "12 Train Loss 2.381978 Test MSE 383.8818016333607 Test RE 0.9987604692189519\n",
      "13 Train Loss 2.381972 Test MSE 383.8814315754426 Test RE 0.9987599878216739\n",
      "14 Train Loss 2.3819666 Test MSE 383.8814635546743 Test RE 0.9987600294225094\n",
      "15 Train Loss 2.3819604 Test MSE 383.88162444172355 Test RE 0.9987602387156796\n",
      "16 Train Loss 2.3819542 Test MSE 383.8823998423408 Test RE 0.9987612474106331\n",
      "17 Train Loss 2.3819466 Test MSE 383.882973352045 Test RE 0.9987619934712152\n",
      "18 Train Loss 2.3812575 Test MSE 383.7625506567082 Test RE 0.9986053271777979\n",
      "19 Train Loss 2.380724 Test MSE 383.7117850534647 Test RE 0.9985392752944304\n",
      "20 Train Loss 2.3803968 Test MSE 383.60838820243373 Test RE 0.9984047305958624\n",
      "21 Train Loss 2.380303 Test MSE 383.5973587298667 Test RE 0.9983903774728864\n",
      "22 Train Loss 2.380295 Test MSE 383.5983624883694 Test RE 0.9983916837151634\n",
      "23 Train Loss 2.3802876 Test MSE 383.5970548385236 Test RE 0.998389982003202\n",
      "24 Train Loss 2.3802814 Test MSE 383.5967272533791 Test RE 0.9983895556993598\n",
      "25 Train Loss 2.3802772 Test MSE 383.5960151603909 Test RE 0.9983886290145348\n",
      "26 Train Loss 2.380272 Test MSE 383.5948067803485 Test RE 0.9983870564827618\n",
      "27 Train Loss 2.380268 Test MSE 383.594553234395 Test RE 0.9983867265290769\n",
      "28 Train Loss 2.3802652 Test MSE 383.5933791419236 Test RE 0.9983851986147384\n",
      "29 Train Loss 2.380263 Test MSE 383.59340313738875 Test RE 0.9983852298414463\n",
      "30 Train Loss 2.380261 Test MSE 383.59218530119887 Test RE 0.9983836449984596\n",
      "31 Train Loss 2.380258 Test MSE 383.5922020777474 Test RE 0.9983836668308017\n",
      "32 Train Loss 2.380258 Test MSE 383.5922020777474 Test RE 0.9983836668308017\n",
      "33 Train Loss 2.380258 Test MSE 383.5922020777474 Test RE 0.9983836668308017\n",
      "34 Train Loss 2.3802578 Test MSE 383.5922000210698 Test RE 0.9983836641543219\n",
      "35 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "36 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "37 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "38 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "39 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "40 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "41 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "42 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "43 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "44 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "45 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "46 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "47 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "48 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "49 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "50 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "51 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "52 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "53 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "54 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "55 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "56 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "57 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "58 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "59 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "60 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "61 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "62 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "63 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "64 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "65 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "66 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "67 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "68 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "69 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "70 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "71 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "72 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "73 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "74 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "75 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "76 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "77 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "78 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "79 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "80 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "81 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "82 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "83 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "84 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "85 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "86 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "87 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "88 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "89 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "90 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "91 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "92 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "93 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "94 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "95 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "96 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "97 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "98 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "99 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "100 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "101 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "102 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "103 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "104 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "105 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "106 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "107 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "108 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "109 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "110 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "111 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "112 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "113 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "114 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "115 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "116 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "117 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "118 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "119 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "120 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "121 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "122 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "123 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "124 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "125 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "126 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "127 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "128 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "129 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "130 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "131 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "132 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "133 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "134 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "135 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "136 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "137 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "138 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "139 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "140 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "141 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "142 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "143 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "144 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "145 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "146 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "147 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "148 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "149 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "150 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "151 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "152 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "153 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "154 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "155 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "156 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "157 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "158 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "159 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "160 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "161 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "162 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "163 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "164 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "165 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "166 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "167 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "168 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "169 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "170 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "171 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "172 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "173 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "174 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "175 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "176 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "177 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "178 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "179 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "180 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "181 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "182 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "183 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "184 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "185 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "186 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "187 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "188 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "189 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "190 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "191 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "192 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "193 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "194 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "195 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "196 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "197 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "198 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "199 Train Loss 2.3802576 Test MSE 383.5921989649851 Test RE 0.9983836627799749\n",
      "Training time: 30.77\n",
      "Training time: 30.77\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4.2598248 Test MSE 386.6643176399248 Test RE 1.002373624655494\n",
      "1 Train Loss 3.5261257 Test MSE 384.99559263012156 Test RE 1.0002083168350517\n",
      "2 Train Loss 3.2387357 Test MSE 387.7622057265157 Test RE 1.0037956772430938\n",
      "3 Train Loss 2.946827 Test MSE 387.22840993393527 Test RE 1.0031045237060023\n",
      "4 Train Loss 2.848699 Test MSE 388.4659401618586 Test RE 1.0047061390638312\n",
      "5 Train Loss 2.5930023 Test MSE 386.64635122991405 Test RE 1.0023503366716975\n",
      "6 Train Loss 2.4019775 Test MSE 385.4536678773645 Test RE 1.0008031736031795\n",
      "7 Train Loss 2.3932397 Test MSE 385.25704878528427 Test RE 1.0005478872639493\n",
      "8 Train Loss 2.3900378 Test MSE 384.9251425256095 Test RE 1.0001167988986448\n",
      "9 Train Loss 2.384806 Test MSE 384.1416769284148 Test RE 0.9990984759415354\n",
      "10 Train Loss 2.381022 Test MSE 383.5000723089488 Test RE 0.998263765566557\n",
      "11 Train Loss 2.3800685 Test MSE 383.59419114843627 Test RE 0.9983862553259572\n",
      "12 Train Loss 2.3793068 Test MSE 383.45014579192934 Test RE 0.9981987832383858\n",
      "13 Train Loss 2.3764718 Test MSE 382.3119375792827 Test RE 0.9967161883778788\n",
      "14 Train Loss 2.347894 Test MSE 374.2723444465202 Test RE 0.9861805919098717\n",
      "15 Train Loss 2.275223 Test MSE 359.5857038517039 Test RE 0.9666378378265378\n",
      "16 Train Loss 2.220542 Test MSE 349.61891990959737 Test RE 0.9531473525665487\n",
      "17 Train Loss 2.127198 Test MSE 338.4171885590864 Test RE 0.9377537059337387\n",
      "18 Train Loss 1.9826312 Test MSE 310.65139803330413 Test RE 0.8984610176878032\n",
      "19 Train Loss 1.9231738 Test MSE 300.0332736560616 Test RE 0.8829727339200809\n",
      "20 Train Loss 1.8810878 Test MSE 293.9360279428905 Test RE 0.8739548424852154\n",
      "21 Train Loss 1.7355919 Test MSE 274.0638766295205 Test RE 0.8438951302940868\n",
      "22 Train Loss 1.6410488 Test MSE 255.59470983389875 Test RE 0.8149641654388048\n",
      "23 Train Loss 1.5252157 Test MSE 241.02719308651612 Test RE 0.7913991948865485\n",
      "24 Train Loss 1.4732385 Test MSE 221.80792865217128 Test RE 0.7591911129615587\n",
      "25 Train Loss 1.3314824 Test MSE 200.0942096350621 Test RE 0.7210740111971499\n",
      "26 Train Loss 1.3168976 Test MSE 195.38330551052044 Test RE 0.7125351755323404\n",
      "27 Train Loss 1.2931201 Test MSE 189.35872935811295 Test RE 0.7014637746485678\n",
      "28 Train Loss 1.2664118 Test MSE 189.36709009049994 Test RE 0.7014792602989623\n",
      "29 Train Loss 1.1996043 Test MSE 184.1040437836889 Test RE 0.6916625259779383\n",
      "30 Train Loss 1.1685612 Test MSE 179.607205353706 Test RE 0.6831631938465618\n",
      "31 Train Loss 1.1539255 Test MSE 175.48070459879628 Test RE 0.6752697067654119\n",
      "32 Train Loss 1.1419827 Test MSE 166.53218176131722 Test RE 0.6578269600979035\n",
      "33 Train Loss 1.1325169 Test MSE 163.35354669279388 Test RE 0.6515186718171218\n",
      "34 Train Loss 1.0697327 Test MSE 158.56337955010721 Test RE 0.6418950540093176\n",
      "35 Train Loss 1.0362684 Test MSE 154.01344594918174 Test RE 0.6326185196214194\n",
      "36 Train Loss 0.94644505 Test MSE 145.83196803261427 Test RE 0.6155863070014959\n",
      "37 Train Loss 0.9023682 Test MSE 137.3749132964715 Test RE 0.5974702678562187\n",
      "38 Train Loss 0.88551104 Test MSE 134.3406668663639 Test RE 0.5908351608148693\n",
      "39 Train Loss 0.8633658 Test MSE 133.9003215430117 Test RE 0.5898660385998086\n",
      "40 Train Loss 0.8529872 Test MSE 132.36139035377525 Test RE 0.5864665449069575\n",
      "41 Train Loss 0.8055445 Test MSE 125.52062279106275 Test RE 0.5711104742113753\n",
      "42 Train Loss 0.76592416 Test MSE 118.53510759364049 Test RE 0.5549911804411013\n",
      "43 Train Loss 0.7389435 Test MSE 112.3686407274446 Test RE 0.5403624302689239\n",
      "44 Train Loss 0.71617746 Test MSE 111.53739436370519 Test RE 0.5383600562959893\n",
      "45 Train Loss 0.7100996 Test MSE 111.40419015870938 Test RE 0.5380384903894463\n",
      "46 Train Loss 0.6889573 Test MSE 106.72621717225569 Test RE 0.5266209603988133\n",
      "47 Train Loss 0.6659152 Test MSE 103.58785459576022 Test RE 0.5188203493363447\n",
      "48 Train Loss 0.64686483 Test MSE 100.57784560995604 Test RE 0.511226957937346\n",
      "49 Train Loss 0.614046 Test MSE 93.53232058635234 Test RE 0.49299604737209507\n",
      "50 Train Loss 0.5865815 Test MSE 87.17685694792677 Test RE 0.4759520328005611\n",
      "51 Train Loss 0.58463746 Test MSE 86.86644212579242 Test RE 0.4751039045015464\n",
      "52 Train Loss 0.5788077 Test MSE 85.14826570188845 Test RE 0.4703817742121631\n",
      "53 Train Loss 0.564666 Test MSE 80.23379917043698 Test RE 0.45660562892608064\n",
      "54 Train Loss 0.5617658 Test MSE 78.91008519264352 Test RE 0.45282337642795534\n",
      "55 Train Loss 0.55756885 Test MSE 76.71548405957024 Test RE 0.4464821464567893\n",
      "56 Train Loss 0.5410227 Test MSE 72.44210949117016 Test RE 0.4338685063484654\n",
      "57 Train Loss 0.53471565 Test MSE 71.49746142303323 Test RE 0.4310303921799861\n",
      "58 Train Loss 0.52708715 Test MSE 73.29431228787296 Test RE 0.43641304085654564\n",
      "59 Train Loss 0.51971656 Test MSE 74.72314294315947 Test RE 0.44064632028483375\n",
      "60 Train Loss 0.49516672 Test MSE 71.81277715974379 Test RE 0.4319798045457618\n",
      "61 Train Loss 0.46038425 Test MSE 68.02014415913985 Test RE 0.42041805292514034\n",
      "62 Train Loss 0.45593867 Test MSE 66.05187476174734 Test RE 0.41429067382108353\n",
      "63 Train Loss 0.45371974 Test MSE 65.3917305926529 Test RE 0.41221519639472964\n",
      "64 Train Loss 0.4491717 Test MSE 64.34413036876765 Test RE 0.4088999433093563\n",
      "65 Train Loss 0.44042075 Test MSE 62.63006222136106 Test RE 0.4034168222735816\n",
      "66 Train Loss 0.43305835 Test MSE 59.82740788283838 Test RE 0.39428719762720554\n",
      "67 Train Loss 0.4095876 Test MSE 55.68128941657197 Test RE 0.3803796061190773\n",
      "68 Train Loss 0.37923467 Test MSE 51.553118836251215 Test RE 0.36600755884812997\n",
      "69 Train Loss 0.3561404 Test MSE 48.792096195359825 Test RE 0.35607158750513107\n",
      "70 Train Loss 0.34098896 Test MSE 46.58910085914272 Test RE 0.3479403107342672\n",
      "71 Train Loss 0.32449248 Test MSE 44.49838119342171 Test RE 0.34004366515321693\n",
      "72 Train Loss 0.29644597 Test MSE 42.395970726524695 Test RE 0.33191346744295075\n",
      "73 Train Loss 0.2801964 Test MSE 40.26517198848215 Test RE 0.3234650494125939\n",
      "74 Train Loss 0.25810376 Test MSE 37.15063788771951 Test RE 0.3107031956517906\n",
      "75 Train Loss 0.247554 Test MSE 35.59457436823041 Test RE 0.30412665450306803\n",
      "76 Train Loss 0.24567896 Test MSE 34.95684464421956 Test RE 0.30138990023912987\n",
      "77 Train Loss 0.2436162 Test MSE 34.27463773558793 Test RE 0.29843449421685475\n",
      "78 Train Loss 0.23471977 Test MSE 33.756609166664994 Test RE 0.2961706307641356\n",
      "79 Train Loss 0.23220217 Test MSE 34.24797369054941 Test RE 0.2983183876792395\n",
      "80 Train Loss 0.22874394 Test MSE 34.26695743131782 Test RE 0.2984010555536264\n",
      "81 Train Loss 0.2242625 Test MSE 32.96861895258474 Test RE 0.29269342055043435\n",
      "82 Train Loss 0.21776965 Test MSE 30.66316850824256 Test RE 0.28227414288653835\n",
      "83 Train Loss 0.2070644 Test MSE 29.350272797536885 Test RE 0.27616501052845244\n",
      "84 Train Loss 0.19730303 Test MSE 29.11793988870456 Test RE 0.27506979583240376\n",
      "85 Train Loss 0.195387 Test MSE 28.574322467076623 Test RE 0.27248999020167347\n",
      "86 Train Loss 0.19219077 Test MSE 26.873109737845528 Test RE 0.26425398503548797\n",
      "87 Train Loss 0.18529034 Test MSE 25.164677781453648 Test RE 0.2557162161201805\n",
      "88 Train Loss 0.16907904 Test MSE 23.960167135006092 Test RE 0.24952123087828368\n",
      "89 Train Loss 0.16499944 Test MSE 23.809735060446904 Test RE 0.24873669756103836\n",
      "90 Train Loss 0.15675151 Test MSE 22.15679442173264 Test RE 0.239947399014176\n",
      "91 Train Loss 0.15049753 Test MSE 20.53931644974274 Test RE 0.23102318959750057\n",
      "92 Train Loss 0.14826596 Test MSE 20.247304191947897 Test RE 0.2293750553350548\n",
      "93 Train Loss 0.14525159 Test MSE 20.56702542510949 Test RE 0.23117897030108303\n",
      "94 Train Loss 0.1425989 Test MSE 19.48726312798962 Test RE 0.22502874969995892\n",
      "95 Train Loss 0.13579214 Test MSE 17.920411647133506 Test RE 0.21579261272610958\n",
      "96 Train Loss 0.13192734 Test MSE 17.912432082823692 Test RE 0.21574456352969848\n",
      "97 Train Loss 0.12309586 Test MSE 16.52085347622427 Test RE 0.20719478597432456\n",
      "98 Train Loss 0.11786325 Test MSE 15.353599394494456 Test RE 0.19974121391275784\n",
      "99 Train Loss 0.11680746 Test MSE 14.947394483936073 Test RE 0.19708126023421205\n",
      "100 Train Loss 0.11630832 Test MSE 14.798898360992496 Test RE 0.19609985667675695\n",
      "101 Train Loss 0.11449441 Test MSE 14.3393166690422 Test RE 0.19303088882833308\n",
      "102 Train Loss 0.1127223 Test MSE 13.658587776199171 Test RE 0.18839331214194574\n",
      "103 Train Loss 0.11081975 Test MSE 12.67262933691599 Test RE 0.18146628512787266\n",
      "104 Train Loss 0.10737834 Test MSE 11.755893866117958 Test RE 0.17477946707846725\n",
      "105 Train Loss 0.10500485 Test MSE 11.24504209429694 Test RE 0.17093977393573853\n",
      "106 Train Loss 0.10348065 Test MSE 10.768614877171574 Test RE 0.16727941661460072\n",
      "107 Train Loss 0.100751944 Test MSE 9.958023785589349 Test RE 0.16086040662182038\n",
      "108 Train Loss 0.09821984 Test MSE 9.820190410068024 Test RE 0.15974325768798517\n",
      "109 Train Loss 0.09539749 Test MSE 9.678207002361583 Test RE 0.15858424390741224\n",
      "110 Train Loss 0.09340734 Test MSE 9.54807223838782 Test RE 0.1575144606967815\n",
      "111 Train Loss 0.09257691 Test MSE 9.75346832577399 Test RE 0.15919965472665165\n",
      "112 Train Loss 0.090685755 Test MSE 10.321509548272509 Test RE 0.1637699407338163\n",
      "113 Train Loss 0.08654948 Test MSE 10.690234142330851 Test RE 0.16666952252080003\n",
      "114 Train Loss 0.083684675 Test MSE 10.632734981102075 Test RE 0.16622068862230788\n",
      "115 Train Loss 0.08148343 Test MSE 10.478157009017002 Test RE 0.1650080125487466\n",
      "116 Train Loss 0.08086111 Test MSE 10.377602693432301 Test RE 0.16421434878195065\n",
      "117 Train Loss 0.08013984 Test MSE 10.365730403299505 Test RE 0.16412038881981397\n",
      "118 Train Loss 0.07818425 Test MSE 10.511660803364144 Test RE 0.16527160767902396\n",
      "119 Train Loss 0.076047406 Test MSE 10.255516287588259 Test RE 0.16324554828564247\n",
      "120 Train Loss 0.074616306 Test MSE 9.791172005914477 Test RE 0.15950706451205862\n",
      "121 Train Loss 0.07396387 Test MSE 9.357200841361013 Test RE 0.1559321109085539\n",
      "122 Train Loss 0.07263016 Test MSE 9.157852048861317 Test RE 0.1542621549833262\n",
      "123 Train Loss 0.07094777 Test MSE 8.706781804301977 Test RE 0.15041509177544615\n",
      "124 Train Loss 0.07049071 Test MSE 8.42614874093631 Test RE 0.14797118204261833\n",
      "125 Train Loss 0.070083626 Test MSE 8.11148560495663 Test RE 0.14518200286216468\n",
      "126 Train Loss 0.06954506 Test MSE 8.056042500236064 Test RE 0.14468498276163191\n",
      "127 Train Loss 0.06881542 Test MSE 7.998556209282009 Test RE 0.1441678371520276\n",
      "128 Train Loss 0.067639485 Test MSE 7.3540446832787385 Test RE 0.13823745022378398\n",
      "129 Train Loss 0.066327885 Test MSE 6.738923301689555 Test RE 0.13232985581368842\n",
      "130 Train Loss 0.06517143 Test MSE 6.574976525686874 Test RE 0.13071026220197463\n",
      "131 Train Loss 0.0642807 Test MSE 6.420745009892453 Test RE 0.12916810672863238\n",
      "132 Train Loss 0.063487105 Test MSE 6.262327632519202 Test RE 0.12756468917490146\n",
      "133 Train Loss 0.061004058 Test MSE 5.787913057515782 Test RE 0.12263758279578418\n",
      "134 Train Loss 0.0564904 Test MSE 4.75065061106116 Test RE 0.11110641331381503\n",
      "135 Train Loss 0.05386121 Test MSE 4.251111386200398 Test RE 0.1051026883534535\n",
      "136 Train Loss 0.051826395 Test MSE 4.126569518629246 Test RE 0.10355168388355675\n",
      "137 Train Loss 0.04969875 Test MSE 3.8523139288413284 Test RE 0.10005145713451934\n",
      "138 Train Loss 0.04641109 Test MSE 3.7753078916549465 Test RE 0.0990464172046988\n",
      "139 Train Loss 0.03721596 Test MSE 3.7544035732592143 Test RE 0.0987718207995658\n",
      "140 Train Loss 0.036515728 Test MSE 3.7508485589970717 Test RE 0.09872504660681598\n",
      "141 Train Loss 0.036256254 Test MSE 3.7300982390266535 Test RE 0.09845158616118029\n",
      "142 Train Loss 0.034852073 Test MSE 3.3883335263827172 Test RE 0.09383301239103446\n",
      "143 Train Loss 0.033004094 Test MSE 2.9792431946336917 Test RE 0.0879864023891366\n",
      "144 Train Loss 0.032055534 Test MSE 2.823093472207842 Test RE 0.08564957482879083\n",
      "145 Train Loss 0.029423896 Test MSE 2.806733952842898 Test RE 0.08540104927964862\n",
      "146 Train Loss 0.02606438 Test MSE 2.563786754737949 Test RE 0.08162130464089712\n",
      "147 Train Loss 0.023979317 Test MSE 2.324985552497667 Test RE 0.0777271442636268\n",
      "148 Train Loss 0.023709262 Test MSE 2.3065813576448733 Test RE 0.07741889553550503\n",
      "149 Train Loss 0.023192191 Test MSE 2.1623149465967377 Test RE 0.07495870201174978\n",
      "150 Train Loss 0.022794323 Test MSE 1.9846930160635905 Test RE 0.07181402247131591\n",
      "151 Train Loss 0.022554252 Test MSE 1.8905028588594832 Test RE 0.07008922401279437\n",
      "152 Train Loss 0.022158947 Test MSE 1.8699280905631215 Test RE 0.06970678221265154\n",
      "153 Train Loss 0.020510197 Test MSE 1.8221592245174096 Test RE 0.06881066334166312\n",
      "154 Train Loss 0.019334562 Test MSE 1.698374395867833 Test RE 0.0664323016477989\n",
      "155 Train Loss 0.0182907 Test MSE 1.5470553183654514 Test RE 0.06340383164023838\n",
      "156 Train Loss 0.017481267 Test MSE 1.4512027816750421 Test RE 0.06140823762709136\n",
      "157 Train Loss 0.016757227 Test MSE 1.3084894184429834 Test RE 0.058310624137233966\n",
      "158 Train Loss 0.015868397 Test MSE 1.169051715012248 Test RE 0.05511622243926426\n",
      "159 Train Loss 0.01434975 Test MSE 1.0046705025695548 Test RE 0.05109452995937926\n",
      "160 Train Loss 0.013164941 Test MSE 0.8931163042289956 Test RE 0.04817443073016341\n",
      "161 Train Loss 0.01250509 Test MSE 0.8267524657028589 Test RE 0.04635006333713811\n",
      "162 Train Loss 0.012174152 Test MSE 0.8394277492081994 Test RE 0.046704017823847734\n",
      "163 Train Loss 0.011901554 Test MSE 0.8641475385912365 Test RE 0.047386707155788126\n",
      "164 Train Loss 0.011630116 Test MSE 0.8948891003942365 Test RE 0.0482222190793154\n",
      "165 Train Loss 0.011450987 Test MSE 0.881696557758722 Test RE 0.047865450984885885\n",
      "166 Train Loss 0.011196829 Test MSE 0.8233728648967873 Test RE 0.046255231371637964\n",
      "167 Train Loss 0.011136794 Test MSE 0.8078388812153569 Test RE 0.04581682164337831\n",
      "168 Train Loss 0.011116584 Test MSE 0.80034733905737 Test RE 0.045603884304499465\n",
      "169 Train Loss 0.011039838 Test MSE 0.7629324631920388 Test RE 0.044525174470419336\n",
      "170 Train Loss 0.011030186 Test MSE 0.7587616057161708 Test RE 0.04440330086245474\n",
      "171 Train Loss 0.011024871 Test MSE 0.7553363297202528 Test RE 0.04430296262915174\n",
      "172 Train Loss 0.010994269 Test MSE 0.746807850096563 Test RE 0.04405214090949218\n",
      "173 Train Loss 0.010702085 Test MSE 0.7659333380211506 Test RE 0.044612654917020635\n",
      "174 Train Loss 0.009935469 Test MSE 0.8191778061230582 Test RE 0.04613724642496318\n",
      "175 Train Loss 0.009225268 Test MSE 0.766018765539698 Test RE 0.044615142758947644\n",
      "176 Train Loss 0.008971585 Test MSE 0.7362851130979639 Test RE 0.043740686240040454\n",
      "177 Train Loss 0.00893368 Test MSE 0.7404632558245102 Test RE 0.043864616694092855\n",
      "178 Train Loss 0.00892723 Test MSE 0.7423524328582453 Test RE 0.0439205379277899\n",
      "179 Train Loss 0.008922566 Test MSE 0.7441053081150106 Test RE 0.043972360908332404\n",
      "180 Train Loss 0.008914914 Test MSE 0.7467993416965719 Test RE 0.04405188996522128\n",
      "181 Train Loss 0.008909289 Test MSE 0.748753671346432 Test RE 0.04410949289756575\n",
      "182 Train Loss 0.008904945 Test MSE 0.7509828534316522 Test RE 0.04417510527350412\n",
      "183 Train Loss 0.0089011565 Test MSE 0.7536535574545034 Test RE 0.044253585045519746\n",
      "184 Train Loss 0.008900033 Test MSE 0.7561016870675762 Test RE 0.04432540231039659\n",
      "185 Train Loss 0.008898541 Test MSE 0.758405292338895 Test RE 0.044392873774521655\n",
      "186 Train Loss 0.00889346 Test MSE 0.7607574101881605 Test RE 0.0444616605018963\n",
      "187 Train Loss 0.00888909 Test MSE 0.7632572544214082 Test RE 0.04453465096281853\n",
      "188 Train Loss 0.008884336 Test MSE 0.76513735467437 Test RE 0.04458946741529048\n",
      "189 Train Loss 0.008881519 Test MSE 0.7667686606580623 Test RE 0.0446369754444869\n",
      "190 Train Loss 0.00887933 Test MSE 0.7682957616704601 Test RE 0.04468140296748246\n",
      "191 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "192 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "193 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "194 Train Loss 0.008876794 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "195 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "196 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "197 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "198 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "199 Train Loss 0.008876795 Test MSE 0.7695250313667549 Test RE 0.044717133700891726\n",
      "Training time: 74.42\n",
      "Training time: 74.42\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.472434 Test MSE 385.13679785240225 Test RE 1.0003917237267028\n",
      "1 Train Loss 3.7323124 Test MSE 387.6247013747899 Test RE 1.0036176834747867\n",
      "2 Train Loss 2.4194627 Test MSE 384.0560424407744 Test RE 0.99898710810519\n",
      "3 Train Loss 2.3857405 Test MSE 383.80923468809567 Test RE 0.9986660646112279\n",
      "4 Train Loss 2.3822808 Test MSE 383.9651467124237 Test RE 0.9988688844293448\n",
      "5 Train Loss 2.381849 Test MSE 383.9885847317721 Test RE 0.9988993704658766\n",
      "6 Train Loss 2.3809557 Test MSE 383.76929987662334 Test RE 0.998614108359439\n",
      "7 Train Loss 2.3808582 Test MSE 383.7662762132713 Test RE 0.9986101743829187\n",
      "8 Train Loss 2.3808546 Test MSE 383.76725628544307 Test RE 0.9986114495202592\n",
      "9 Train Loss 2.3808508 Test MSE 383.76723324254783 Test RE 0.9986114195399793\n",
      "10 Train Loss 2.3808453 Test MSE 383.76618938618725 Test RE 0.998610061415183\n",
      "11 Train Loss 2.3808372 Test MSE 383.76400288303853 Test RE 0.9986072166268536\n",
      "12 Train Loss 2.3808289 Test MSE 383.76058645473296 Test RE 0.9986027716065696\n",
      "13 Train Loss 2.3807986 Test MSE 383.7431133798096 Test RE 0.9985800375631559\n",
      "14 Train Loss 2.3807797 Test MSE 383.72320087682897 Test RE 0.9985541289732202\n",
      "15 Train Loss 2.3807762 Test MSE 383.72094063437805 Test RE 0.9985511880802144\n",
      "16 Train Loss 2.3807697 Test MSE 383.71602523481783 Test RE 0.9985447924248438\n",
      "17 Train Loss 2.3807645 Test MSE 383.7126024272674 Test RE 0.998540338826281\n",
      "18 Train Loss 2.3807578 Test MSE 383.70753848961925 Test RE 0.998533749828857\n",
      "19 Train Loss 2.3807495 Test MSE 383.70347977150755 Test RE 0.9985284687523517\n",
      "20 Train Loss 2.38074 Test MSE 383.6976228534515 Test RE 0.9985208478653335\n",
      "21 Train Loss 2.380717 Test MSE 383.68728179681057 Test RE 0.9985073921783505\n",
      "22 Train Loss 2.3806932 Test MSE 383.6755397947061 Test RE 0.998492113374284\n",
      "23 Train Loss 2.3806715 Test MSE 383.6629390711937 Test RE 0.998475716933738\n",
      "24 Train Loss 2.3803878 Test MSE 383.45236889626375 Test RE 0.9982016768307465\n",
      "25 Train Loss 2.3799016 Test MSE 383.1622938019183 Test RE 0.9978240442349646\n",
      "26 Train Loss 2.379153 Test MSE 382.8402724410223 Test RE 0.9974046551116349\n",
      "27 Train Loss 2.3731577 Test MSE 382.2905167819831 Test RE 0.9966882651659532\n",
      "28 Train Loss 2.369013 Test MSE 380.81826048243863 Test RE 0.9947672181958009\n",
      "29 Train Loss 2.3638983 Test MSE 379.17195093976244 Test RE 0.9926146578492218\n",
      "30 Train Loss 2.352359 Test MSE 376.01719638824653 Test RE 0.98847669833156\n",
      "31 Train Loss 2.288734 Test MSE 360.0992099742557 Test RE 0.9673277948483558\n",
      "32 Train Loss 2.2473803 Test MSE 353.18710178219675 Test RE 0.9579988770747498\n",
      "33 Train Loss 2.19349 Test MSE 348.11143009143854 Test RE 0.951090238289087\n",
      "34 Train Loss 2.1375716 Test MSE 338.4858406387814 Test RE 0.9378488185338918\n",
      "35 Train Loss 2.0785103 Test MSE 331.1569729130183 Test RE 0.9276401438452838\n",
      "36 Train Loss 2.0327156 Test MSE 322.24698908848694 Test RE 0.9150756871530276\n",
      "37 Train Loss 2.0013185 Test MSE 315.2994647077323 Test RE 0.90515759375077\n",
      "38 Train Loss 1.934978 Test MSE 297.84954464392837 Test RE 0.8797536002066665\n",
      "39 Train Loss 1.8915777 Test MSE 288.8322991027255 Test RE 0.8663342038051293\n",
      "40 Train Loss 1.8009462 Test MSE 280.72289202247487 Test RE 0.8540857918291578\n",
      "41 Train Loss 1.672678 Test MSE 255.02080504668058 Test RE 0.814048703068699\n",
      "42 Train Loss 1.5870478 Test MSE 245.30499485844547 Test RE 0.7983912675648439\n",
      "43 Train Loss 1.5340716 Test MSE 240.79033226296994 Test RE 0.7910102397273903\n",
      "44 Train Loss 1.461024 Test MSE 226.74507474458196 Test RE 0.7675938980441391\n",
      "45 Train Loss 1.405568 Test MSE 213.83958497060502 Test RE 0.7454295999125403\n",
      "46 Train Loss 1.3942461 Test MSE 208.2085887894296 Test RE 0.7355494969813196\n",
      "47 Train Loss 1.300414 Test MSE 188.45943702210684 Test RE 0.6997961152072503\n",
      "48 Train Loss 1.2403765 Test MSE 183.56782201711377 Test RE 0.6906545227107459\n",
      "49 Train Loss 1.2234207 Test MSE 185.3817756609399 Test RE 0.6940585384597722\n",
      "50 Train Loss 1.2102363 Test MSE 186.79401277149125 Test RE 0.6966971893090915\n",
      "51 Train Loss 1.1901671 Test MSE 179.81610890153286 Test RE 0.6835603765216096\n",
      "52 Train Loss 1.1728232 Test MSE 171.72718385958936 Test RE 0.6680086813419253\n",
      "53 Train Loss 1.116708 Test MSE 163.5279057462223 Test RE 0.6518662855802975\n",
      "54 Train Loss 1.0385514 Test MSE 159.27720766850666 Test RE 0.6433382882786255\n",
      "55 Train Loss 1.0012597 Test MSE 156.06956858393892 Test RE 0.6368273361753287\n",
      "56 Train Loss 0.995782 Test MSE 153.38921286413373 Test RE 0.6313351824258018\n",
      "57 Train Loss 0.9552706 Test MSE 144.40328310781922 Test RE 0.6125635009528771\n",
      "58 Train Loss 0.924366 Test MSE 138.30746812972157 Test RE 0.5994947692371554\n",
      "59 Train Loss 0.91152024 Test MSE 136.2209937151173 Test RE 0.594955665631304\n",
      "60 Train Loss 0.9047229 Test MSE 136.35645571995119 Test RE 0.5952514124864793\n",
      "61 Train Loss 0.8846767 Test MSE 130.03912414631944 Test RE 0.5812990328545031\n",
      "62 Train Loss 0.85589993 Test MSE 121.69844267916618 Test RE 0.5623479197464551\n",
      "63 Train Loss 0.810981 Test MSE 116.0705103731134 Test RE 0.549191149283358\n",
      "64 Train Loss 0.801976 Test MSE 116.1128024802519 Test RE 0.5492911933664999\n",
      "65 Train Loss 0.776725 Test MSE 111.89964962929686 Test RE 0.5392336004179967\n",
      "66 Train Loss 0.7521761 Test MSE 111.71366915849312 Test RE 0.5387853031095632\n",
      "67 Train Loss 0.7375178 Test MSE 110.75983041850861 Test RE 0.5364802316334323\n",
      "68 Train Loss 0.7267642 Test MSE 108.12471823433616 Test RE 0.5300600545134805\n",
      "69 Train Loss 0.7132222 Test MSE 104.9717649743667 Test RE 0.5222745123405814\n",
      "70 Train Loss 0.6881465 Test MSE 102.13795685608997 Test RE 0.5151766438880493\n",
      "71 Train Loss 0.66601986 Test MSE 100.79415445810922 Test RE 0.5117764006153327\n",
      "72 Train Loss 0.6561135 Test MSE 99.65277259541035 Test RE 0.5088705009790321\n",
      "73 Train Loss 0.6477906 Test MSE 97.79376783417909 Test RE 0.5041017116725599\n",
      "74 Train Loss 0.63425833 Test MSE 93.53133721154052 Test RE 0.49299345574833053\n",
      "75 Train Loss 0.6171941 Test MSE 90.24147040606485 Test RE 0.4842455786173131\n",
      "76 Train Loss 0.5986919 Test MSE 86.19588069618858 Test RE 0.47326658068415534\n",
      "77 Train Loss 0.58493084 Test MSE 81.57344949256257 Test RE 0.4604017824879986\n",
      "78 Train Loss 0.57272166 Test MSE 77.69881508175048 Test RE 0.44933451586162676\n",
      "79 Train Loss 0.55664366 Test MSE 74.29736184250059 Test RE 0.43938909996095443\n",
      "80 Train Loss 0.5003993 Test MSE 69.53009838405795 Test RE 0.4250587928254094\n",
      "81 Train Loss 0.4694926 Test MSE 66.21584526726159 Test RE 0.41480458311278673\n",
      "82 Train Loss 0.46129978 Test MSE 66.07836586219747 Test RE 0.41437374425392154\n",
      "83 Train Loss 0.45465392 Test MSE 66.68262928194268 Test RE 0.4162640835564526\n",
      "84 Train Loss 0.44620228 Test MSE 64.25783445968966 Test RE 0.4086256507572543\n",
      "85 Train Loss 0.43674302 Test MSE 60.92381324601374 Test RE 0.3978836759420605\n",
      "86 Train Loss 0.43159336 Test MSE 60.23274517042803 Test RE 0.39562061255902836\n",
      "87 Train Loss 0.42783445 Test MSE 58.52003124525108 Test RE 0.38995532680393846\n",
      "88 Train Loss 0.4024409 Test MSE 52.70500441819578 Test RE 0.3700739448571769\n",
      "89 Train Loss 0.3763607 Test MSE 50.34471858200382 Test RE 0.36169253158368436\n",
      "90 Train Loss 0.35796022 Test MSE 45.85193342328813 Test RE 0.3451766496629282\n",
      "91 Train Loss 0.3442184 Test MSE 43.23107104624928 Test RE 0.33516648141200195\n",
      "92 Train Loss 0.33148253 Test MSE 42.445060573459685 Test RE 0.33210557140024316\n",
      "93 Train Loss 0.3254199 Test MSE 42.376468745987566 Test RE 0.33183711921424397\n",
      "94 Train Loss 0.31953076 Test MSE 42.87276266914244 Test RE 0.33377462561556137\n",
      "95 Train Loss 0.31727356 Test MSE 42.203848365522 Test RE 0.3311605609151228\n",
      "96 Train Loss 0.3159432 Test MSE 41.44291658429233 Test RE 0.32816158328684925\n",
      "97 Train Loss 0.30174765 Test MSE 39.0236047521054 Test RE 0.31843901703919697\n",
      "98 Train Loss 0.28247967 Test MSE 37.683794539590004 Test RE 0.31292473718808045\n",
      "99 Train Loss 0.28108528 Test MSE 37.02705572338573 Test RE 0.31018598570394407\n",
      "100 Train Loss 0.28075537 Test MSE 36.63639049071442 Test RE 0.30854529069086395\n",
      "101 Train Loss 0.27969602 Test MSE 36.34396712002358 Test RE 0.3073114543956908\n",
      "102 Train Loss 0.27584058 Test MSE 35.61700628942702 Test RE 0.3042224706599921\n",
      "103 Train Loss 0.2738075 Test MSE 35.93903275962468 Test RE 0.3055946696952124\n",
      "104 Train Loss 0.27254623 Test MSE 36.224265861495894 Test RE 0.30680496171374594\n",
      "105 Train Loss 0.27161902 Test MSE 36.02632622365516 Test RE 0.3059655789281894\n",
      "106 Train Loss 0.26843512 Test MSE 35.20472872330053 Test RE 0.30245661237662086\n",
      "107 Train Loss 0.26376712 Test MSE 35.37369485319676 Test RE 0.30318156825505116\n",
      "108 Train Loss 0.2611748 Test MSE 35.255106758596476 Test RE 0.3026729430122773\n",
      "109 Train Loss 0.2565115 Test MSE 33.73653372722453 Test RE 0.29608254964500325\n",
      "110 Train Loss 0.25414068 Test MSE 32.990426252370845 Test RE 0.29279020649756354\n",
      "111 Train Loss 0.25245944 Test MSE 32.88693137932809 Test RE 0.29233058697290903\n",
      "112 Train Loss 0.2520999 Test MSE 32.85058873286659 Test RE 0.29216901817556434\n",
      "113 Train Loss 0.2501315 Test MSE 32.68579595057886 Test RE 0.29143527364958444\n",
      "114 Train Loss 0.24536233 Test MSE 32.257380593059686 Test RE 0.2895190410943127\n",
      "115 Train Loss 0.24411087 Test MSE 31.961777971738815 Test RE 0.2881894297544663\n",
      "116 Train Loss 0.24341035 Test MSE 32.003595241177656 Test RE 0.28837789479832937\n",
      "117 Train Loss 0.24203514 Test MSE 31.46321840484828 Test RE 0.2859329171281755\n",
      "118 Train Loss 0.24089509 Test MSE 30.946265496270275 Test RE 0.2835741938282101\n",
      "119 Train Loss 0.23925525 Test MSE 30.648327705415458 Test RE 0.2822058250624414\n",
      "120 Train Loss 0.23339784 Test MSE 28.605141292239544 Test RE 0.27263689759081355\n",
      "121 Train Loss 0.2306098 Test MSE 26.90132965330818 Test RE 0.2643926974598819\n",
      "122 Train Loss 0.22900918 Test MSE 26.338189625693886 Test RE 0.26161072392739304\n",
      "123 Train Loss 0.22744027 Test MSE 26.497972937952678 Test RE 0.2624030680978375\n",
      "124 Train Loss 0.22503215 Test MSE 25.378062262579924 Test RE 0.2567981033443478\n",
      "125 Train Loss 0.21754989 Test MSE 25.135579556146514 Test RE 0.2555683294597984\n",
      "126 Train Loss 0.20662886 Test MSE 26.288918491090357 Test RE 0.2613659103825092\n",
      "127 Train Loss 0.20051192 Test MSE 27.046029487930596 Test RE 0.2651028161039775\n",
      "128 Train Loss 0.19825782 Test MSE 27.68689329801041 Test RE 0.26822527302216265\n",
      "129 Train Loss 0.19703117 Test MSE 27.442404797731104 Test RE 0.26703836830087363\n",
      "130 Train Loss 0.1957742 Test MSE 26.945966465042478 Test RE 0.26461195717145547\n",
      "131 Train Loss 0.19491568 Test MSE 26.84422288461035 Test RE 0.26411191888403074\n",
      "132 Train Loss 0.19357823 Test MSE 26.331346829892503 Test RE 0.2615767378205976\n",
      "133 Train Loss 0.1918157 Test MSE 25.323922002912603 Test RE 0.2565240371287655\n",
      "134 Train Loss 0.1911275 Test MSE 24.824321862229123 Test RE 0.2539810295156845\n",
      "135 Train Loss 0.18999274 Test MSE 24.395089310495372 Test RE 0.2517756864585879\n",
      "136 Train Loss 0.18885109 Test MSE 23.844739340737934 Test RE 0.24891947261666666\n",
      "137 Train Loss 0.18679968 Test MSE 22.84644783383812 Test RE 0.2436530910321364\n",
      "138 Train Loss 0.18470003 Test MSE 22.34298646293272 Test RE 0.24095347483555551\n",
      "139 Train Loss 0.1796954 Test MSE 21.660671142385635 Test RE 0.23724580227624337\n",
      "140 Train Loss 0.17166433 Test MSE 21.31848465561879 Test RE 0.235364385811015\n",
      "141 Train Loss 0.16331154 Test MSE 21.427032100971978 Test RE 0.23596282807295482\n",
      "142 Train Loss 0.16081123 Test MSE 21.40597531304448 Test RE 0.23584685679663683\n",
      "143 Train Loss 0.15999818 Test MSE 21.097850305634875 Test RE 0.23414327349622213\n",
      "144 Train Loss 0.15932208 Test MSE 21.089403670135994 Test RE 0.2340963985576701\n",
      "145 Train Loss 0.15792 Test MSE 20.978156909610416 Test RE 0.23347815204796926\n",
      "146 Train Loss 0.15608315 Test MSE 20.11145805675359 Test RE 0.22860428220232532\n",
      "147 Train Loss 0.15418752 Test MSE 19.235379172255367 Test RE 0.2235697073607379\n",
      "148 Train Loss 0.15391342 Test MSE 18.936806135681937 Test RE 0.2218277882681322\n",
      "149 Train Loss 0.1538885 Test MSE 18.870114136470843 Test RE 0.22143682510543336\n",
      "150 Train Loss 0.15333463 Test MSE 18.22500030190304 Test RE 0.2176187716637553\n",
      "151 Train Loss 0.1527941 Test MSE 17.922499791532733 Test RE 0.21580518478691868\n",
      "152 Train Loss 0.15279329 Test MSE 17.92252145897048 Test RE 0.21580531523590865\n",
      "153 Train Loss 0.1527883 Test MSE 17.924120998429174 Test RE 0.21581494505882273\n",
      "154 Train Loss 0.15277934 Test MSE 17.937626210560065 Test RE 0.21589623433942853\n",
      "155 Train Loss 0.15277033 Test MSE 17.946672685305515 Test RE 0.21595066890114967\n",
      "156 Train Loss 0.15275276 Test MSE 17.9842298724469 Test RE 0.21617651190363552\n",
      "157 Train Loss 0.15273952 Test MSE 17.989457716801777 Test RE 0.21620792984722645\n",
      "158 Train Loss 0.15273461 Test MSE 18.02859119112632 Test RE 0.2164429667004374\n",
      "159 Train Loss 0.15273461 Test MSE 18.02859119112632 Test RE 0.2164429667004374\n",
      "160 Train Loss 0.15273461 Test MSE 18.02859119112632 Test RE 0.2164429667004374\n",
      "161 Train Loss 0.15273461 Test MSE 18.02859119112632 Test RE 0.2164429667004374\n",
      "162 Train Loss 0.15272672 Test MSE 18.052240292142493 Test RE 0.21658488028689188\n",
      "163 Train Loss 0.15271875 Test MSE 18.08447113301757 Test RE 0.21677814161072193\n",
      "164 Train Loss 0.15271536 Test MSE 18.084506154372125 Test RE 0.2167783515107081\n",
      "165 Train Loss 0.15271531 Test MSE 18.084506154372125 Test RE 0.2167783515107081\n",
      "166 Train Loss 0.15270641 Test MSE 18.089103060603733 Test RE 0.2168059012383229\n",
      "167 Train Loss 0.15270543 Test MSE 18.08910304920656 Test RE 0.21680590117002282\n",
      "168 Train Loss 0.15270543 Test MSE 18.08910304920656 Test RE 0.21680590117002282\n",
      "169 Train Loss 0.15270543 Test MSE 18.08910304920656 Test RE 0.21680590117002282\n",
      "170 Train Loss 0.15270495 Test MSE 18.089099952625713 Test RE 0.21680588261307654\n",
      "171 Train Loss 0.15270495 Test MSE 18.089099952625713 Test RE 0.21680588261307654\n",
      "172 Train Loss 0.15270492 Test MSE 18.089099952625713 Test RE 0.21680588261307654\n",
      "173 Train Loss 0.15270492 Test MSE 18.089099952625713 Test RE 0.21680588261307654\n",
      "174 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "175 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "176 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "177 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "178 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "179 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "180 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "181 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "182 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "183 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "184 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "185 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "186 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "187 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "188 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "189 Train Loss 0.15270431 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "190 Train Loss 0.1527043 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "191 Train Loss 0.1527042 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "192 Train Loss 0.1527042 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "193 Train Loss 0.1527042 Test MSE 18.089100405541547 Test RE 0.21680588532727507\n",
      "194 Train Loss 0.15270373 Test MSE 18.0891001589864 Test RE 0.21680588384973873\n",
      "195 Train Loss 0.15270373 Test MSE 18.0891001589864 Test RE 0.21680588384973873\n",
      "196 Train Loss 0.15270373 Test MSE 18.0891001589864 Test RE 0.21680588384973873\n",
      "197 Train Loss 0.15270373 Test MSE 18.0891001589864 Test RE 0.21680588384973873\n",
      "198 Train Loss 0.15270373 Test MSE 18.0891001589864 Test RE 0.21680588384973873\n",
      "199 Train Loss 0.15270373 Test MSE 18.0891001589864 Test RE 0.21680588384973873\n",
      "Training time: 66.35\n",
      "Training time: 66.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4.445677 Test MSE 386.45715724609187 Test RE 1.002105071383438\n",
      "1 Train Loss 3.6194022 Test MSE 387.6333884808141 Test RE 1.0036289295129595\n",
      "2 Train Loss 3.4317527 Test MSE 385.5046405023123 Test RE 1.0008693448225259\n",
      "3 Train Loss 2.9717023 Test MSE 389.23132282650533 Test RE 1.0056954230374522\n",
      "4 Train Loss 2.7443604 Test MSE 387.52187551967273 Test RE 1.0034845589719852\n",
      "5 Train Loss 2.5534203 Test MSE 385.82373890140457 Test RE 1.001283489957537\n",
      "6 Train Loss 2.4008067 Test MSE 384.3902312514112 Test RE 0.9994216510920214\n",
      "7 Train Loss 2.3857584 Test MSE 384.22330478678055 Test RE 0.9992046215927294\n",
      "8 Train Loss 2.3839512 Test MSE 384.20775570549 Test RE 0.9991844030493914\n",
      "9 Train Loss 2.3834867 Test MSE 383.9935354207593 Test RE 0.9989058097512744\n",
      "10 Train Loss 2.380601 Test MSE 383.6371594805301 Test RE 0.998442170915792\n",
      "11 Train Loss 2.3803046 Test MSE 383.65750576975984 Test RE 0.9984686468753627\n",
      "12 Train Loss 2.3803012 Test MSE 383.6548725603305 Test RE 0.998465220405754\n",
      "13 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "14 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "15 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "16 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "17 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "18 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "19 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "20 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "21 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "22 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "23 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "24 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "25 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "26 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "27 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "28 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "29 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "30 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "31 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "32 Train Loss 2.3802981 Test MSE 383.65283659882647 Test RE 0.9984625710982723\n",
      "33 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "34 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "35 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "36 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "37 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "38 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "39 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "40 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "41 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "42 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "43 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "44 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "45 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "46 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "47 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "48 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "49 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "50 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "51 Train Loss 2.3802927 Test MSE 383.64291909664064 Test RE 0.9984496657867752\n",
      "52 Train Loss 2.38029 Test MSE 383.6492283342211 Test RE 0.998457875804399\n",
      "53 Train Loss 2.379633 Test MSE 383.3277307930823 Test RE 0.9980394349393567\n",
      "54 Train Loss 2.3789277 Test MSE 383.08595430264074 Test RE 0.9977246383387903\n",
      "55 Train Loss 2.3786309 Test MSE 382.78528156805424 Test RE 0.9973330193324823\n",
      "56 Train Loss 2.3755534 Test MSE 381.37549987930424 Test RE 0.9954947579560766\n",
      "57 Train Loss 2.3578045 Test MSE 376.4419784037094 Test RE 0.989034875736462\n",
      "58 Train Loss 2.3032324 Test MSE 366.24650257146425 Test RE 0.9755495335394878\n",
      "59 Train Loss 2.1709065 Test MSE 347.3462778279285 Test RE 0.9500444105533097\n",
      "60 Train Loss 2.065392 Test MSE 327.7852934072369 Test RE 0.9229056712226769\n",
      "61 Train Loss 2.0163796 Test MSE 317.9590641332799 Test RE 0.9089671487584634\n",
      "62 Train Loss 1.9558519 Test MSE 304.33381591595213 Test RE 0.8892782864736518\n",
      "63 Train Loss 1.8447536 Test MSE 294.2608230261726 Test RE 0.8744375629377992\n",
      "64 Train Loss 1.7831401 Test MSE 278.79542272544336 Test RE 0.8511486254771199\n",
      "65 Train Loss 1.7405347 Test MSE 274.19322113112196 Test RE 0.8440942450418859\n",
      "66 Train Loss 1.690444 Test MSE 263.82979009532426 Test RE 0.8279888691441091\n",
      "67 Train Loss 1.6545502 Test MSE 249.65932865564446 Test RE 0.8054460976175104\n",
      "68 Train Loss 1.5003325 Test MSE 238.26193271866873 Test RE 0.7868463103847362\n",
      "69 Train Loss 1.4624813 Test MSE 225.74608077591776 Test RE 0.7659010979749179\n",
      "70 Train Loss 1.3828279 Test MSE 211.77933056178557 Test RE 0.7418299584259747\n",
      "71 Train Loss 1.3382895 Test MSE 204.71486485506884 Test RE 0.7293521575884239\n",
      "72 Train Loss 1.3234726 Test MSE 203.73896482908884 Test RE 0.7276116266877783\n",
      "73 Train Loss 1.3130776 Test MSE 202.1129399173008 Test RE 0.7247023042472858\n",
      "74 Train Loss 1.2812515 Test MSE 189.02527292436724 Test RE 0.7008458715902846\n",
      "75 Train Loss 1.2558385 Test MSE 179.36758777852657 Test RE 0.6827073308920241\n",
      "76 Train Loss 1.0700316 Test MSE 163.79907805119393 Test RE 0.6524065446299208\n",
      "77 Train Loss 0.9748677 Test MSE 151.01485455987242 Test RE 0.6264298107496391\n",
      "78 Train Loss 0.9643653 Test MSE 148.66951671647456 Test RE 0.6215463885462962\n",
      "79 Train Loss 0.89048886 Test MSE 132.75240389640993 Test RE 0.5873321570485094\n",
      "80 Train Loss 0.82344663 Test MSE 120.61411372960663 Test RE 0.5598370639856588\n",
      "81 Train Loss 0.81317776 Test MSE 120.35964962817278 Test RE 0.55924619759422\n",
      "82 Train Loss 0.7997381 Test MSE 121.44912059225388 Test RE 0.5617715868167414\n",
      "83 Train Loss 0.77028626 Test MSE 115.47316829261395 Test RE 0.5477761553046412\n",
      "84 Train Loss 0.7321762 Test MSE 103.5327898217467 Test RE 0.5186824348915484\n",
      "85 Train Loss 0.69972396 Test MSE 95.13620988583962 Test RE 0.4972050206123983\n",
      "86 Train Loss 0.669907 Test MSE 94.14152629469145 Test RE 0.49459896144833143\n",
      "87 Train Loss 0.64155805 Test MSE 94.5070653884988 Test RE 0.4955582623124545\n",
      "88 Train Loss 0.6268996 Test MSE 94.23968269963456 Test RE 0.4948567403999965\n",
      "89 Train Loss 0.6170798 Test MSE 92.34949922289745 Test RE 0.48986888461680134\n",
      "90 Train Loss 0.6069051 Test MSE 90.67370454660359 Test RE 0.4854039010401244\n",
      "91 Train Loss 0.5841271 Test MSE 87.16931062120938 Test RE 0.4759314323373322\n",
      "92 Train Loss 0.5780967 Test MSE 85.19242822605662 Test RE 0.4705037411921418\n",
      "93 Train Loss 0.56079954 Test MSE 82.01545180170467 Test RE 0.46164743125206753\n",
      "94 Train Loss 0.53837794 Test MSE 80.78969785419909 Test RE 0.4581846911731042\n",
      "95 Train Loss 0.51924443 Test MSE 78.00942859572731 Test RE 0.4502317635425475\n",
      "96 Train Loss 0.5050624 Test MSE 75.90842413871265 Test RE 0.4441274030039587\n",
      "97 Train Loss 0.48708144 Test MSE 74.29996060375632 Test RE 0.4393967843346387\n",
      "98 Train Loss 0.46857166 Test MSE 70.28770966250605 Test RE 0.4273682736744145\n",
      "99 Train Loss 0.4527224 Test MSE 65.85829360344194 Test RE 0.4136831383373281\n",
      "100 Train Loss 0.39346018 Test MSE 59.101173981990485 Test RE 0.39188680100504986\n",
      "101 Train Loss 0.37605414 Test MSE 56.493409304279396 Test RE 0.3831435111740435\n",
      "102 Train Loss 0.3630778 Test MSE 53.125657072729354 Test RE 0.37154783909474165\n",
      "103 Train Loss 0.35281664 Test MSE 49.54176095584769 Test RE 0.35879658620249033\n",
      "104 Train Loss 0.34863618 Test MSE 47.25143152141062 Test RE 0.35040481695422465\n",
      "105 Train Loss 0.34607032 Test MSE 46.46318072655963 Test RE 0.3474697893746564\n",
      "106 Train Loss 0.3443941 Test MSE 47.52181543208062 Test RE 0.3514059366178971\n",
      "107 Train Loss 0.33846557 Test MSE 48.11432703231874 Test RE 0.35358985045890345\n",
      "108 Train Loss 0.3279028 Test MSE 45.87447593968247 Test RE 0.3452614900781777\n",
      "109 Train Loss 0.32679555 Test MSE 44.8852848623966 Test RE 0.3415187682283099\n",
      "110 Train Loss 0.32018402 Test MSE 41.82867492823427 Test RE 0.32968534004840877\n",
      "111 Train Loss 0.31088823 Test MSE 41.24334218453007 Test RE 0.3273704747671409\n",
      "112 Train Loss 0.3067158 Test MSE 41.4490579990727 Test RE 0.32818589747495497\n",
      "113 Train Loss 0.30500245 Test MSE 41.62395759052521 Test RE 0.3288775797273337\n",
      "114 Train Loss 0.30026194 Test MSE 41.15137360356396 Test RE 0.3270052691422689\n",
      "115 Train Loss 0.28153837 Test MSE 40.24935762516433 Test RE 0.3234015218535067\n",
      "116 Train Loss 0.26658314 Test MSE 37.202756830713454 Test RE 0.3109210633323455\n",
      "117 Train Loss 0.25521785 Test MSE 34.53385792853061 Test RE 0.2995609033240006\n",
      "118 Train Loss 0.2519051 Test MSE 34.235259659760786 Test RE 0.29826300949434753\n",
      "119 Train Loss 0.25129434 Test MSE 33.83798363145526 Test RE 0.296527393783248\n",
      "120 Train Loss 0.24919155 Test MSE 32.93247467404068 Test RE 0.29253293318393264\n",
      "121 Train Loss 0.24332584 Test MSE 32.43033383392391 Test RE 0.2902941554381919\n",
      "122 Train Loss 0.23928964 Test MSE 32.39861164945734 Test RE 0.29015214305839204\n",
      "123 Train Loss 0.2373772 Test MSE 32.5193761178185 Test RE 0.2906924050578176\n",
      "124 Train Loss 0.2354472 Test MSE 32.36396268244803 Test RE 0.28999694873213355\n",
      "125 Train Loss 0.23223762 Test MSE 30.732088189297038 Test RE 0.2825911897800979\n",
      "126 Train Loss 0.23061965 Test MSE 29.240319411407732 Test RE 0.27564723390327167\n",
      "127 Train Loss 0.22987081 Test MSE 28.464898740624854 Test RE 0.2719677473651416\n",
      "128 Train Loss 0.2283083 Test MSE 27.388623191221097 Test RE 0.26677656916200987\n",
      "129 Train Loss 0.2159753 Test MSE 24.80837948572699 Test RE 0.2538994621014154\n",
      "130 Train Loss 0.19505636 Test MSE 24.173694499003965 Test RE 0.2506306019556998\n",
      "131 Train Loss 0.18060711 Test MSE 24.52842761763701 Test RE 0.2524628246545352\n",
      "132 Train Loss 0.1746049 Test MSE 24.433945391886752 Test RE 0.2519761186846159\n",
      "133 Train Loss 0.16973186 Test MSE 24.381791699061065 Test RE 0.25170705642459884\n",
      "134 Train Loss 0.16501644 Test MSE 23.899657752871207 Test RE 0.24920595928816577\n",
      "135 Train Loss 0.15785018 Test MSE 22.49056493915164 Test RE 0.24174793042290343\n",
      "136 Train Loss 0.15287726 Test MSE 21.396019085784218 Test RE 0.23579200253344498\n",
      "137 Train Loss 0.14919578 Test MSE 20.743147470331092 Test RE 0.2321666901588823\n",
      "138 Train Loss 0.14643997 Test MSE 20.296836149609675 Test RE 0.22965544958965606\n",
      "139 Train Loss 0.13881822 Test MSE 19.89327393834766 Test RE 0.22736086562809082\n",
      "140 Train Loss 0.1358257 Test MSE 19.63927195334076 Test RE 0.22590470412755478\n",
      "141 Train Loss 0.13279074 Test MSE 18.68865301209313 Test RE 0.220369549002703\n",
      "142 Train Loss 0.13013951 Test MSE 18.034379868796087 Test RE 0.2164777120113218\n",
      "143 Train Loss 0.12632753 Test MSE 17.422079176671808 Test RE 0.21277106827709302\n",
      "144 Train Loss 0.11841259 Test MSE 16.45238137156043 Test RE 0.2067649718152002\n",
      "145 Train Loss 0.11446175 Test MSE 15.887522229054605 Test RE 0.20318454377304154\n",
      "146 Train Loss 0.105138876 Test MSE 14.251641806347648 Test RE 0.19243985977212358\n",
      "147 Train Loss 0.1012258 Test MSE 13.62588548770112 Test RE 0.18816764515973408\n",
      "148 Train Loss 0.09778769 Test MSE 12.812698064576525 Test RE 0.18246638943952934\n",
      "149 Train Loss 0.093351774 Test MSE 11.691030357605563 Test RE 0.17429662457114242\n",
      "150 Train Loss 0.08823971 Test MSE 11.008864312966784 Test RE 0.16913513796634333\n",
      "151 Train Loss 0.08201216 Test MSE 9.297815960666913 Test RE 0.15543651672485345\n",
      "152 Train Loss 0.073041126 Test MSE 7.930495042338691 Test RE 0.143553151590495\n",
      "153 Train Loss 0.07076359 Test MSE 7.704376379720185 Test RE 0.14149181842183806\n",
      "154 Train Loss 0.06768823 Test MSE 7.362003907733752 Test RE 0.13831223664413667\n",
      "155 Train Loss 0.066095084 Test MSE 7.2091240482681505 Test RE 0.1368686021525359\n",
      "156 Train Loss 0.06543991 Test MSE 7.18549945126793 Test RE 0.1366441560932347\n",
      "157 Train Loss 0.063322626 Test MSE 7.34767942576114 Test RE 0.13817761189728742\n",
      "158 Train Loss 0.061441272 Test MSE 7.433935636159386 Test RE 0.13898629592029146\n",
      "159 Train Loss 0.05981238 Test MSE 7.134516799274608 Test RE 0.1361585334370613\n",
      "160 Train Loss 0.057562806 Test MSE 6.992264673426418 Test RE 0.13479429512641192\n",
      "161 Train Loss 0.052666605 Test MSE 7.000138003266622 Test RE 0.13487016334736218\n",
      "162 Train Loss 0.04998693 Test MSE 6.719651214906985 Test RE 0.13214050075718664\n",
      "163 Train Loss 0.04920736 Test MSE 6.4394222455956704 Test RE 0.12935583815948531\n",
      "164 Train Loss 0.048844267 Test MSE 6.346626001469543 Test RE 0.1284204051880528\n",
      "165 Train Loss 0.046754643 Test MSE 6.211764608730923 Test RE 0.12704865669346974\n",
      "166 Train Loss 0.045295935 Test MSE 6.023473239347458 Test RE 0.12510828626023357\n",
      "167 Train Loss 0.04336052 Test MSE 5.593418182776981 Test RE 0.12055944151319443\n",
      "168 Train Loss 0.041760933 Test MSE 5.167174531257746 Test RE 0.1158748401368142\n",
      "169 Train Loss 0.040380206 Test MSE 4.928567706807916 Test RE 0.11316781898737625\n",
      "170 Train Loss 0.039428048 Test MSE 4.764846486063806 Test RE 0.11127229335389169\n",
      "171 Train Loss 0.038033858 Test MSE 4.386492117113921 Test RE 0.10676312054269131\n",
      "172 Train Loss 0.03677044 Test MSE 3.9988940299838776 Test RE 0.1019371601082455\n",
      "173 Train Loss 0.035616465 Test MSE 3.7777764934520106 Test RE 0.09907879419408665\n",
      "174 Train Loss 0.033828564 Test MSE 3.3713437980779 Test RE 0.09359746875286125\n",
      "175 Train Loss 0.033110544 Test MSE 3.184111234683664 Test RE 0.09096130827525362\n",
      "176 Train Loss 0.032799013 Test MSE 3.102776648416933 Test RE 0.08979204024529516\n",
      "177 Train Loss 0.032254342 Test MSE 3.0169951868251736 Test RE 0.08854211488959907\n",
      "178 Train Loss 0.03150107 Test MSE 2.9354999063507354 Test RE 0.08733807549483479\n",
      "179 Train Loss 0.030681197 Test MSE 2.911987125095281 Test RE 0.08698759178890045\n",
      "180 Train Loss 0.030344572 Test MSE 3.0161497066130885 Test RE 0.0885297075361078\n",
      "181 Train Loss 0.0298392 Test MSE 3.203917508983086 Test RE 0.0912437751111328\n",
      "182 Train Loss 0.029580288 Test MSE 3.2963235497852734 Test RE 0.09255022930694037\n",
      "183 Train Loss 0.029553292 Test MSE 3.306312254684126 Test RE 0.0926903486912593\n",
      "184 Train Loss 0.029536836 Test MSE 3.3115914163625906 Test RE 0.09276431813646625\n",
      "185 Train Loss 0.029536221 Test MSE 3.313550940423003 Test RE 0.09279175917746005\n",
      "186 Train Loss 0.029529616 Test MSE 3.314326379580614 Test RE 0.09280261613358584\n",
      "187 Train Loss 0.029529614 Test MSE 3.314326379580614 Test RE 0.09280261613358584\n",
      "188 Train Loss 0.029529616 Test MSE 3.314326379580614 Test RE 0.09280261613358584\n",
      "189 Train Loss 0.029522192 Test MSE 3.3143637950983806 Test RE 0.092803139957696\n",
      "190 Train Loss 0.029522058 Test MSE 3.314365767842197 Test RE 0.09280316757638915\n",
      "191 Train Loss 0.029522024 Test MSE 3.3143673079693756 Test RE 0.09280318913838234\n",
      "192 Train Loss 0.02952199 Test MSE 3.3143673151289024 Test RE 0.09280318923861666\n",
      "193 Train Loss 0.02952199 Test MSE 3.3143673151289024 Test RE 0.09280318923861666\n",
      "194 Train Loss 0.02952199 Test MSE 3.3143673151289024 Test RE 0.09280318923861666\n",
      "195 Train Loss 0.02952199 Test MSE 3.3143673151289024 Test RE 0.09280318923861666\n",
      "196 Train Loss 0.02952199 Test MSE 3.3143673151289024 Test RE 0.09280318923861666\n",
      "197 Train Loss 0.02952199 Test MSE 3.3143673151289024 Test RE 0.09280318923861666\n",
      "198 Train Loss 0.029521853 Test MSE 3.3143670482262464 Test RE 0.092803185501943\n",
      "199 Train Loss 0.029521853 Test MSE 3.3143670482262464 Test RE 0.092803185501943\n",
      "Training time: 63.82\n",
      "Training time: 63.82\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd510223d90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24236910123550953\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
