{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-4.0*x) + np.exp(3.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 12.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 3.9611592 Test MSE 14252.737475923419 Test RE 0.999803624155561\n",
      "1 Train Loss 2.9104 Test MSE 14251.455817264296 Test RE 0.9997586701324132\n",
      "2 Train Loss 2.8357592 Test MSE 14254.131750942997 Test RE 0.999852525888635\n",
      "3 Train Loss 2.8302712 Test MSE 14255.334246619219 Test RE 0.9998946993792435\n",
      "4 Train Loss 2.827626 Test MSE 14256.352062542654 Test RE 0.9999303944588533\n",
      "5 Train Loss 2.8273711 Test MSE 14255.70902619723 Test RE 0.9999078431485865\n",
      "6 Train Loss 2.8266091 Test MSE 14250.133850678503 Test RE 0.9997123001957844\n",
      "7 Train Loss 2.8261306 Test MSE 14246.703544988213 Test RE 0.9995919671087407\n",
      "8 Train Loss 2.824499 Test MSE 14236.124967331627 Test RE 0.9992207856327529\n",
      "9 Train Loss 2.8224049 Test MSE 14229.41647035826 Test RE 0.9989853262112459\n",
      "10 Train Loss 2.819777 Test MSE 14214.56878854172 Test RE 0.9984639946588759\n",
      "11 Train Loss 2.817098 Test MSE 14191.828347747469 Test RE 0.9976650045514875\n",
      "12 Train Loss 2.813035 Test MSE 14174.838587563176 Test RE 0.9970676479336996\n",
      "13 Train Loss 2.8083644 Test MSE 14146.974795069182 Test RE 0.9960871869473922\n",
      "14 Train Loss 2.8034656 Test MSE 14096.763855085195 Test RE 0.9943179418649826\n",
      "15 Train Loss 2.7936113 Test MSE 14034.805566860645 Test RE 0.9921304156859554\n",
      "16 Train Loss 2.7800982 Test MSE 13978.774676146711 Test RE 0.9901480033063257\n",
      "17 Train Loss 2.7750974 Test MSE 13938.05225126867 Test RE 0.9887047210076773\n",
      "18 Train Loss 2.7488456 Test MSE 13782.444350149504 Test RE 0.9831701569506714\n",
      "19 Train Loss 2.708626 Test MSE 13596.00118785405 Test RE 0.9764975560366673\n",
      "20 Train Loss 2.6955805 Test MSE 13538.384957121096 Test RE 0.9744262910996564\n",
      "21 Train Loss 2.6782327 Test MSE 13443.871156176261 Test RE 0.9710190149688165\n",
      "22 Train Loss 2.6762366 Test MSE 13444.88625525194 Test RE 0.9710556733794597\n",
      "23 Train Loss 2.670832 Test MSE 13442.909054347263 Test RE 0.9709842691774355\n",
      "24 Train Loss 2.6676443 Test MSE 13418.661115578492 Test RE 0.9701081569549546\n",
      "25 Train Loss 2.665734 Test MSE 13386.423536304876 Test RE 0.9689421410457899\n",
      "26 Train Loss 2.6577299 Test MSE 13309.870854202889 Test RE 0.9661676333603186\n",
      "27 Train Loss 2.6313393 Test MSE 13175.90476574746 Test RE 0.9612930165251014\n",
      "28 Train Loss 2.630238 Test MSE 13162.742666924341 Test RE 0.9608127536767527\n",
      "29 Train Loss 2.6210673 Test MSE 13094.33263134342 Test RE 0.9583127101157489\n",
      "30 Train Loss 2.5918646 Test MSE 13025.910340395068 Test RE 0.9558056777336879\n",
      "31 Train Loss 2.586801 Test MSE 13017.465653997846 Test RE 0.9554958035060276\n",
      "32 Train Loss 2.5823972 Test MSE 12972.046890888532 Test RE 0.9538274542669821\n",
      "33 Train Loss 2.578219 Test MSE 12932.251558225405 Test RE 0.952363266021562\n",
      "34 Train Loss 2.5657425 Test MSE 12882.16485437198 Test RE 0.9505172219420385\n",
      "35 Train Loss 2.5626183 Test MSE 12891.468503187638 Test RE 0.9508603972709871\n",
      "36 Train Loss 2.555076 Test MSE 12847.676827227599 Test RE 0.9492440107190687\n",
      "37 Train Loss 2.5521834 Test MSE 12804.372895546709 Test RE 0.9476429160445149\n",
      "38 Train Loss 2.528939 Test MSE 12683.434020812036 Test RE 0.9431569965289969\n",
      "39 Train Loss 2.5252419 Test MSE 12637.554890858766 Test RE 0.9414496346283909\n",
      "40 Train Loss 2.5152423 Test MSE 12609.42563879278 Test RE 0.9404012899486435\n",
      "41 Train Loss 2.497003 Test MSE 12497.800653342025 Test RE 0.9362295840275581\n",
      "42 Train Loss 2.4863014 Test MSE 12413.85380866091 Test RE 0.9330799922093952\n",
      "43 Train Loss 2.4778948 Test MSE 12324.658959210059 Test RE 0.9297218100128742\n",
      "44 Train Loss 2.4475331 Test MSE 12150.001661841461 Test RE 0.9231105882206135\n",
      "45 Train Loss 2.405839 Test MSE 11894.409083979888 Test RE 0.9133495082809617\n",
      "46 Train Loss 2.3836637 Test MSE 11811.868207174242 Test RE 0.9101749111313414\n",
      "47 Train Loss 2.3728957 Test MSE 11765.197327037682 Test RE 0.9083749965536865\n",
      "48 Train Loss 2.3539412 Test MSE 11676.650472437332 Test RE 0.9049502490471134\n",
      "49 Train Loss 2.3448973 Test MSE 11667.472069140276 Test RE 0.9045945121327258\n",
      "50 Train Loss 2.3300505 Test MSE 11591.187525317455 Test RE 0.9016324418289572\n",
      "51 Train Loss 2.295371 Test MSE 11391.525989282347 Test RE 0.893833271245659\n",
      "52 Train Loss 2.2837422 Test MSE 11345.115210533559 Test RE 0.8920106077582243\n",
      "53 Train Loss 2.274785 Test MSE 11282.732257455216 Test RE 0.8895547947991722\n",
      "54 Train Loss 2.2648437 Test MSE 11285.3995934685 Test RE 0.8896599378296106\n",
      "55 Train Loss 2.2613175 Test MSE 11285.10882222058 Test RE 0.8896484765962482\n",
      "56 Train Loss 2.2588584 Test MSE 11271.552891881038 Test RE 0.8891139830068203\n",
      "57 Train Loss 2.2475333 Test MSE 11166.8541444525 Test RE 0.8849749658854864\n",
      "58 Train Loss 2.2303278 Test MSE 11049.591791862873 Test RE 0.8803161731535241\n",
      "59 Train Loss 2.2239354 Test MSE 10980.5588312377 Test RE 0.8775619516471466\n",
      "60 Train Loss 2.2221093 Test MSE 10951.749067341823 Test RE 0.8764099630198422\n",
      "61 Train Loss 2.2018533 Test MSE 10797.707677070897 Test RE 0.8702245812231042\n",
      "62 Train Loss 2.1530838 Test MSE 10697.283545311577 Test RE 0.8661683640404844\n",
      "63 Train Loss 2.1322372 Test MSE 10629.099706314439 Test RE 0.8634034988168968\n",
      "64 Train Loss 2.1206903 Test MSE 10502.93063306519 Test RE 0.8582638339530031\n",
      "65 Train Loss 2.1060152 Test MSE 10386.252849220586 Test RE 0.8534832637641285\n",
      "66 Train Loss 2.077448 Test MSE 10254.56589620516 Test RE 0.8480553609312405\n",
      "67 Train Loss 2.0617442 Test MSE 10153.235582267032 Test RE 0.84385493644013\n",
      "68 Train Loss 2.045196 Test MSE 10090.683190701775 Test RE 0.8412514956204374\n",
      "69 Train Loss 2.0233018 Test MSE 10051.231471831838 Test RE 0.8396053572881547\n",
      "70 Train Loss 2.0132775 Test MSE 9985.834406893347 Test RE 0.8368695069345639\n",
      "71 Train Loss 2.0090063 Test MSE 9928.984558108588 Test RE 0.8344839370657763\n",
      "72 Train Loss 2.0050156 Test MSE 9886.145146132449 Test RE 0.8326817666299802\n",
      "73 Train Loss 1.9914447 Test MSE 9733.854659139213 Test RE 0.8262433791112028\n",
      "74 Train Loss 1.9807419 Test MSE 9682.691886286111 Test RE 0.8240690813307959\n",
      "75 Train Loss 1.9678779 Test MSE 9653.161129868566 Test RE 0.8228114782405047\n",
      "76 Train Loss 1.9523247 Test MSE 9483.45169381605 Test RE 0.8155466008227888\n",
      "77 Train Loss 1.9204643 Test MSE 9304.064174970526 Test RE 0.8077963981772286\n",
      "78 Train Loss 1.8999482 Test MSE 9256.629167177778 Test RE 0.8057345684400357\n",
      "79 Train Loss 1.8896755 Test MSE 9136.171692470796 Test RE 0.8004748473605088\n",
      "80 Train Loss 1.8606304 Test MSE 8961.208636253976 Test RE 0.7927730136128528\n",
      "81 Train Loss 1.812275 Test MSE 8770.049438656519 Test RE 0.7842717731769066\n",
      "82 Train Loss 1.7661184 Test MSE 8592.824278059197 Test RE 0.7763070479580748\n",
      "83 Train Loss 1.758795 Test MSE 8570.079176648269 Test RE 0.7752789294933499\n",
      "84 Train Loss 1.7392384 Test MSE 8464.875387108752 Test RE 0.7705056866456619\n",
      "85 Train Loss 1.6876845 Test MSE 8229.003602672477 Test RE 0.759694863410379\n",
      "86 Train Loss 1.6706827 Test MSE 8158.19782241722 Test RE 0.7564194366237801\n",
      "87 Train Loss 1.663325 Test MSE 8139.746892325864 Test RE 0.7555635771177845\n",
      "88 Train Loss 1.6614529 Test MSE 8131.303529651645 Test RE 0.7551716022436388\n",
      "89 Train Loss 1.6526326 Test MSE 8064.863730899678 Test RE 0.7520800708487027\n",
      "90 Train Loss 1.6398534 Test MSE 8017.884689199629 Test RE 0.7498863818941693\n",
      "91 Train Loss 1.636631 Test MSE 7977.563776900968 Test RE 0.7479984642285269\n",
      "92 Train Loss 1.6222295 Test MSE 7849.036750698875 Test RE 0.7419484724421801\n",
      "93 Train Loss 1.596127 Test MSE 7733.54278439628 Test RE 0.7364695750362045\n",
      "94 Train Loss 1.5865599 Test MSE 7620.359690543297 Test RE 0.7310604665942266\n",
      "95 Train Loss 1.5803832 Test MSE 7560.727062137115 Test RE 0.7281944155259037\n",
      "96 Train Loss 1.5659674 Test MSE 7429.798378396368 Test RE 0.721861829748302\n",
      "97 Train Loss 1.5463443 Test MSE 7205.173153583301 Test RE 0.7108660523425536\n",
      "98 Train Loss 1.521517 Test MSE 6941.748189734612 Test RE 0.6977502352710604\n",
      "99 Train Loss 1.4912409 Test MSE 6787.942999227443 Test RE 0.6899770684853244\n",
      "100 Train Loss 1.4621035 Test MSE 6656.262635955696 Test RE 0.6832518063511529\n",
      "101 Train Loss 1.4513611 Test MSE 6575.602142783713 Test RE 0.679099370525786\n",
      "102 Train Loss 1.4469945 Test MSE 6509.85021298629 Test RE 0.6756955545673697\n",
      "103 Train Loss 1.4439057 Test MSE 6516.471855188035 Test RE 0.6760391167745945\n",
      "104 Train Loss 1.4376198 Test MSE 6517.732140528462 Test RE 0.6761044865802996\n",
      "105 Train Loss 1.423604 Test MSE 6476.587585383977 Test RE 0.6739670821409486\n",
      "106 Train Loss 1.3787825 Test MSE 6421.001581413822 Test RE 0.6710686525235173\n",
      "107 Train Loss 1.3234887 Test MSE 6304.462519508045 Test RE 0.6649509293431866\n",
      "108 Train Loss 1.3118789 Test MSE 6268.139537092233 Test RE 0.6630326142903366\n",
      "109 Train Loss 1.2999609 Test MSE 6244.061326342402 Test RE 0.6617579138832327\n",
      "110 Train Loss 1.2924815 Test MSE 6163.69819804352 Test RE 0.6574856015628365\n",
      "111 Train Loss 1.2869328 Test MSE 6063.25439796004 Test RE 0.6521063946441875\n",
      "112 Train Loss 1.2777942 Test MSE 5999.071040244355 Test RE 0.6486457339887177\n",
      "113 Train Loss 1.2686394 Test MSE 5991.365787685602 Test RE 0.6482290373818499\n",
      "114 Train Loss 1.248932 Test MSE 5864.475015327672 Test RE 0.641327900428975\n",
      "115 Train Loss 1.21634 Test MSE 5660.756878928785 Test RE 0.630090331013046\n",
      "116 Train Loss 1.1914752 Test MSE 5559.224131468134 Test RE 0.6244140343003901\n",
      "117 Train Loss 1.1897826 Test MSE 5562.141593958035 Test RE 0.6245778580226152\n",
      "118 Train Loss 1.188062 Test MSE 5541.982198965919 Test RE 0.6234449723309213\n",
      "119 Train Loss 1.184626 Test MSE 5505.14953116888 Test RE 0.6213697741451482\n",
      "120 Train Loss 1.1802454 Test MSE 5477.756733283844 Test RE 0.6198219249053589\n",
      "121 Train Loss 1.1756208 Test MSE 5397.691043048576 Test RE 0.6152754333718078\n",
      "122 Train Loss 1.154878 Test MSE 5191.827503034733 Test RE 0.6034283235552248\n",
      "123 Train Loss 1.1343454 Test MSE 5117.466127829451 Test RE 0.599091354249474\n",
      "124 Train Loss 1.1071987 Test MSE 5056.580099233883 Test RE 0.5955167883812842\n",
      "125 Train Loss 1.0919602 Test MSE 4910.92589819707 Test RE 0.5868772224829895\n",
      "126 Train Loss 1.0775199 Test MSE 4751.271333935233 Test RE 0.5772586908191325\n",
      "127 Train Loss 1.0712999 Test MSE 4731.5076319159225 Test RE 0.5760568380035872\n",
      "128 Train Loss 1.0630367 Test MSE 4701.994239371317 Test RE 0.574257412955689\n",
      "129 Train Loss 1.0573088 Test MSE 4621.049519525318 Test RE 0.5692930409128614\n",
      "130 Train Loss 1.0489914 Test MSE 4576.899613833777 Test RE 0.5665669766142495\n",
      "131 Train Loss 1.0391233 Test MSE 4529.064677144563 Test RE 0.5635984954602685\n",
      "132 Train Loss 1.0140743 Test MSE 4380.3424935343855 Test RE 0.5542677349195843\n",
      "133 Train Loss 0.9735209 Test MSE 4271.181418197933 Test RE 0.5473178004763621\n",
      "134 Train Loss 0.93333566 Test MSE 4136.816867326486 Test RE 0.5386401365104435\n",
      "135 Train Loss 0.8978792 Test MSE 4002.6072491489067 Test RE 0.5298306188908491\n",
      "136 Train Loss 0.8573194 Test MSE 3807.0134061466174 Test RE 0.5167229688905509\n",
      "137 Train Loss 0.831517 Test MSE 3667.3960335575393 Test RE 0.5071593881474042\n",
      "138 Train Loss 0.8148461 Test MSE 3633.997132025646 Test RE 0.504844761131902\n",
      "139 Train Loss 0.7922263 Test MSE 3495.2322753930307 Test RE 0.49511215088112026\n",
      "140 Train Loss 0.768335 Test MSE 3347.275612029268 Test RE 0.4845195446483501\n",
      "141 Train Loss 0.75616497 Test MSE 3272.289264040334 Test RE 0.47906165168864473\n",
      "142 Train Loss 0.7473357 Test MSE 3195.905174840516 Test RE 0.4734373383463754\n",
      "143 Train Loss 0.74020135 Test MSE 3099.855503655724 Test RE 0.46626872792115825\n",
      "144 Train Loss 0.7327056 Test MSE 3005.4668897425554 Test RE 0.45911505878176895\n",
      "145 Train Loss 0.7189989 Test MSE 2892.722616296498 Test RE 0.450421341048623\n",
      "146 Train Loss 0.711882 Test MSE 2812.9782038392527 Test RE 0.4441695142801594\n",
      "147 Train Loss 0.7024666 Test MSE 2738.5398985456704 Test RE 0.4382532042524859\n",
      "148 Train Loss 0.6561029 Test MSE 2611.9395075137663 Test RE 0.42800330340223947\n",
      "149 Train Loss 0.6224251 Test MSE 2546.1927832673337 Test RE 0.42258220528678936\n",
      "150 Train Loss 0.59731686 Test MSE 2425.5258517568172 Test RE 0.4124473500629843\n",
      "151 Train Loss 0.5822552 Test MSE 2300.5573647706033 Test RE 0.40168174701085096\n",
      "152 Train Loss 0.5660597 Test MSE 2236.8178581150714 Test RE 0.3960781405207598\n",
      "153 Train Loss 0.55386543 Test MSE 2177.895811084466 Test RE 0.39082659913025647\n",
      "154 Train Loss 0.5416145 Test MSE 2084.1873621772697 Test RE 0.3823260965718316\n",
      "155 Train Loss 0.5250721 Test MSE 2024.0583123257225 Test RE 0.37677065820263694\n",
      "156 Train Loss 0.5180109 Test MSE 2025.6752022246521 Test RE 0.37692111707388587\n",
      "157 Train Loss 0.50790477 Test MSE 1989.541991910738 Test RE 0.3735443042375474\n",
      "158 Train Loss 0.4900718 Test MSE 1947.8168291493369 Test RE 0.3696065173845598\n",
      "159 Train Loss 0.4793465 Test MSE 1904.6793335275363 Test RE 0.3654908412405576\n",
      "160 Train Loss 0.45974278 Test MSE 1782.3323717107821 Test RE 0.35355738353325133\n",
      "161 Train Loss 0.44469962 Test MSE 1725.001575730709 Test RE 0.3478246139774681\n",
      "162 Train Loss 0.4328604 Test MSE 1634.9082497495785 Test RE 0.3386197278988794\n",
      "163 Train Loss 0.41044405 Test MSE 1514.7288980403325 Test RE 0.3259365161980612\n",
      "164 Train Loss 0.383559 Test MSE 1373.6358051811164 Test RE 0.31038545750839625\n",
      "165 Train Loss 0.35207084 Test MSE 1255.6944898489885 Test RE 0.29676150029142945\n",
      "166 Train Loss 0.3135426 Test MSE 1144.0322702769415 Test RE 0.2832596415544498\n",
      "167 Train Loss 0.3079121 Test MSE 1100.1877297406081 Test RE 0.27777871353929295\n",
      "168 Train Loss 0.3007032 Test MSE 1047.5854217073163 Test RE 0.27105678746348705\n",
      "169 Train Loss 0.28319436 Test MSE 977.4329213500794 Test RE 0.2618237537699971\n",
      "170 Train Loss 0.27373195 Test MSE 914.9204636653895 Test RE 0.25331285775988605\n",
      "171 Train Loss 0.26712975 Test MSE 870.5621258663609 Test RE 0.247095848209315\n",
      "172 Train Loss 0.25672904 Test MSE 814.0038067393481 Test RE 0.2389344555432805\n",
      "173 Train Loss 0.25236598 Test MSE 759.9200604134961 Test RE 0.23086043962645117\n",
      "174 Train Loss 0.24543343 Test MSE 698.184418511869 Test RE 0.22128431929726733\n",
      "175 Train Loss 0.24065629 Test MSE 674.3009006184642 Test RE 0.21746653445815772\n",
      "176 Train Loss 0.2393189 Test MSE 658.6856166635954 Test RE 0.21493376871364386\n",
      "177 Train Loss 0.23542589 Test MSE 617.7480572981674 Test RE 0.20814752819858187\n",
      "178 Train Loss 0.22410333 Test MSE 578.9960579670017 Test RE 0.2015131385623327\n",
      "179 Train Loss 0.21822733 Test MSE 556.6016335668357 Test RE 0.197577643837335\n",
      "180 Train Loss 0.21104899 Test MSE 509.30807881859755 Test RE 0.18899740723304626\n",
      "181 Train Loss 0.20053935 Test MSE 477.56284950491056 Test RE 0.18301253267743517\n",
      "182 Train Loss 0.19592248 Test MSE 456.7341350962256 Test RE 0.17897703089897718\n",
      "183 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "184 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "185 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "186 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "187 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "188 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "189 Train Loss 0.19396761 Test MSE 434.7397069854708 Test RE 0.17461446510220333\n",
      "190 Train Loss 0.19396274 Test MSE 434.7348940477105 Test RE 0.1746134985343179\n",
      "191 Train Loss 0.19396272 Test MSE 434.7348940477105 Test RE 0.1746134985343179\n",
      "192 Train Loss 0.19396272 Test MSE 434.7348940477105 Test RE 0.1746134985343179\n",
      "193 Train Loss 0.19396268 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "194 Train Loss 0.19396228 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "195 Train Loss 0.19396228 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "196 Train Loss 0.19396228 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "197 Train Loss 0.19396228 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "198 Train Loss 0.19396228 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "199 Train Loss 0.19396228 Test MSE 434.7348942643132 Test RE 0.17461349857781766\n",
      "Training time: 86.32\n",
      "Training time: 86.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 3.0881438 Test MSE 14265.201780687794 Test RE 1.0002407027782378\n",
      "1 Train Loss 2.8618135 Test MSE 14260.321049880375 Test RE 1.0000695757470128\n",
      "2 Train Loss 2.8344147 Test MSE 14259.035073446641 Test RE 1.000024482270603\n",
      "3 Train Loss 2.828939 Test MSE 14259.526514590943 Test RE 1.0000417151667604\n",
      "4 Train Loss 2.8283617 Test MSE 14259.402038992157 Test RE 1.0000373503281788\n",
      "5 Train Loss 2.8274841 Test MSE 14257.617950896056 Test RE 0.9999747877278036\n",
      "6 Train Loss 2.827437 Test MSE 14257.337863121596 Test RE 0.9999649655369816\n",
      "7 Train Loss 2.8274317 Test MSE 14257.279153768663 Test RE 0.9999629066899134\n",
      "8 Train Loss 2.8274279 Test MSE 14257.230971903138 Test RE 0.999961217022534\n",
      "9 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "10 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "11 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "12 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "13 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "14 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "15 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "16 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "17 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "18 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "19 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "20 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "21 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "22 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "23 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "24 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "25 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "26 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "27 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "28 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "29 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "30 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "31 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "32 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "33 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "34 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "35 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "36 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "37 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "38 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "39 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "40 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "41 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "42 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "43 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "44 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "45 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "46 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "47 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "48 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "49 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "50 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "51 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "52 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "53 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "54 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "55 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "56 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "57 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "58 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "59 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "60 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "61 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "62 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "63 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "64 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "65 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "66 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "67 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "68 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "69 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "70 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "71 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "72 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "73 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "74 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "75 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "76 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "77 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "78 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "79 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "80 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "81 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "82 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "83 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "84 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "85 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "86 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "87 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "88 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "89 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "90 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "91 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "92 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "93 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "94 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "95 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "96 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "97 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "98 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "99 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "100 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "101 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "102 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "103 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "104 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "105 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "106 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "107 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "108 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "109 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "110 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "111 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "112 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "113 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "114 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "115 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "116 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "117 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "118 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "119 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "120 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "121 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "122 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "123 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "124 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "125 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "126 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "127 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "128 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "129 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "130 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "131 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "132 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "133 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "134 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "135 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "136 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "137 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "138 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "139 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "140 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "141 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "142 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "143 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "144 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "145 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "146 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "147 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "148 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "149 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "150 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "151 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "152 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "153 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "154 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "155 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "156 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "157 Train Loss 2.8274257 Test MSE 14257.19181317336 Test RE 0.9999598437811926\n",
      "158 Train Loss 2.827425 Test MSE 14256.967682884739 Test RE 0.9999519838122674\n",
      "159 Train Loss 2.8274212 Test MSE 14256.99852202193 Test RE 0.9999530653059071\n",
      "160 Train Loss 2.8274195 Test MSE 14257.026554903605 Test RE 0.999954048386327\n",
      "161 Train Loss 2.8274162 Test MSE 14257.052173039685 Test RE 0.9999549467835933\n",
      "162 Train Loss 2.8274126 Test MSE 14257.078801097705 Test RE 0.9999558805967544\n",
      "163 Train Loss 2.8274066 Test MSE 14257.100638502916 Test RE 0.9999566464069773\n",
      "164 Train Loss 2.8274004 Test MSE 14257.111394349406 Test RE 0.999957023600715\n",
      "165 Train Loss 2.827396 Test MSE 14257.109948189003 Test RE 0.9999569728857333\n",
      "166 Train Loss 2.8273895 Test MSE 14257.09624727816 Test RE 0.9999564924123178\n",
      "167 Train Loss 2.827386 Test MSE 14257.071130132896 Test RE 0.9999556115855667\n",
      "168 Train Loss 2.8273802 Test MSE 14257.035306471467 Test RE 0.9999543552933925\n",
      "169 Train Loss 2.8273754 Test MSE 14256.989649803932 Test RE 0.9999527541673828\n",
      "170 Train Loss 2.827368 Test MSE 14256.935556032551 Test RE 0.9999508571586764\n",
      "171 Train Loss 2.8273585 Test MSE 14256.870508920878 Test RE 0.9999485760236804\n",
      "172 Train Loss 2.827302 Test MSE 14256.354720143043 Test RE 0.9999304876599499\n",
      "173 Train Loss 2.827295 Test MSE 14256.279705815963 Test RE 0.9999278569309588\n",
      "174 Train Loss 2.8272889 Test MSE 14256.210962597434 Test RE 0.9999254461217063\n",
      "175 Train Loss 2.827283 Test MSE 14256.145299435595 Test RE 0.9999231433239232\n",
      "176 Train Loss 2.8272772 Test MSE 14256.078441471529 Test RE 0.9999207986191464\n",
      "177 Train Loss 2.8272693 Test MSE 14256.006719290845 Test RE 0.9999182833204727\n",
      "178 Train Loss 2.827261 Test MSE 14255.924340801412 Test RE 0.999915394296977\n",
      "179 Train Loss 2.8269699 Test MSE 14254.458908842693 Test RE 0.9998640000275435\n",
      "180 Train Loss 2.826524 Test MSE 14252.738643842291 Test RE 0.9998036651192532\n",
      "181 Train Loss 2.826246 Test MSE 14250.621300715597 Test RE 0.9997293984783574\n",
      "182 Train Loss 2.8261752 Test MSE 14249.847491662527 Test RE 0.999702255445617\n",
      "183 Train Loss 2.826168 Test MSE 14249.76054368599 Test RE 0.99969920551049\n",
      "184 Train Loss 2.8261607 Test MSE 14249.659081285037 Test RE 0.9996956464309578\n",
      "185 Train Loss 2.8259094 Test MSE 14247.303492740659 Test RE 0.9996130139649024\n",
      "186 Train Loss 2.8253558 Test MSE 14242.960166041124 Test RE 0.9994606350661214\n",
      "187 Train Loss 2.8247306 Test MSE 14238.088019820272 Test RE 0.9992896757007793\n",
      "188 Train Loss 2.822319 Test MSE 14223.916232286174 Test RE 0.9987922336759718\n",
      "189 Train Loss 2.8199174 Test MSE 14208.777776283263 Test RE 0.9982605869316686\n",
      "190 Train Loss 2.8183267 Test MSE 14196.803451682585 Test RE 0.9978398605335089\n",
      "191 Train Loss 2.8162005 Test MSE 14186.01819691308 Test RE 0.997460761045291\n",
      "192 Train Loss 2.813954 Test MSE 14183.169514699453 Test RE 0.9973606063947439\n",
      "193 Train Loss 2.8116863 Test MSE 14175.72366389092 Test RE 0.9970987758788897\n",
      "194 Train Loss 2.8104389 Test MSE 14164.097875194468 Test RE 0.9966898218936583\n",
      "195 Train Loss 2.8081038 Test MSE 14145.046555703575 Test RE 0.996019301056393\n",
      "196 Train Loss 2.8033314 Test MSE 14111.476723497399 Test RE 0.9948366940416473\n",
      "197 Train Loss 2.7989862 Test MSE 14100.64283258155 Test RE 0.9944547346661714\n",
      "198 Train Loss 2.7927532 Test MSE 14065.301854224943 Test RE 0.9932077329291972\n",
      "199 Train Loss 2.7874033 Test MSE 14029.504959034855 Test RE 0.991943046119141\n",
      "Training time: 18.79\n",
      "Training time: 18.79\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 2.8833845 Test MSE 14254.268250258165 Test RE 0.9998573132324278\n",
      "1 Train Loss 2.8381062 Test MSE 14253.624937359884 Test RE 0.9998347505742733\n",
      "2 Train Loss 2.82854 Test MSE 14255.42144815878 Test RE 0.9998977576107214\n",
      "3 Train Loss 2.828259 Test MSE 14255.852042897073 Test RE 0.999912858793931\n",
      "4 Train Loss 2.8273163 Test MSE 14255.748292403912 Test RE 0.9999092202306007\n",
      "5 Train Loss 2.827091 Test MSE 14254.759330134073 Test RE 0.9998745363394919\n",
      "6 Train Loss 2.8261726 Test MSE 14248.282158138807 Test RE 0.9996473457200111\n",
      "7 Train Loss 2.82484 Test MSE 14239.897739312853 Test RE 0.9993531805948751\n",
      "8 Train Loss 2.8246489 Test MSE 14237.919506465032 Test RE 0.9992837621906883\n",
      "9 Train Loss 2.8228734 Test MSE 14218.470556661685 Test RE 0.9986010198384413\n",
      "10 Train Loss 2.8205981 Test MSE 14200.130306799228 Test RE 0.9979567697447367\n",
      "11 Train Loss 2.817058 Test MSE 14190.915729026434 Test RE 0.9976329261477725\n",
      "12 Train Loss 2.81347 Test MSE 14174.088063736097 Test RE 0.9970412514097426\n",
      "13 Train Loss 2.8102624 Test MSE 14157.82339000618 Test RE 0.9964690380389739\n",
      "14 Train Loss 2.8065405 Test MSE 14140.395144661166 Test RE 0.9958555237197013\n",
      "15 Train Loss 2.8029206 Test MSE 14125.327311967665 Test RE 0.9953247965446597\n",
      "16 Train Loss 2.7942684 Test MSE 14077.27249264843 Test RE 0.9936302905880704\n",
      "17 Train Loss 2.7933748 Test MSE 14066.573179220417 Test RE 0.9932526186095102\n",
      "18 Train Loss 2.7855418 Test MSE 14000.805260845005 Test RE 0.9909279340289102\n",
      "19 Train Loss 2.7753484 Test MSE 13924.605872972626 Test RE 0.9882276921735552\n",
      "20 Train Loss 2.758465 Test MSE 13876.800303585751 Test RE 0.9865298557733889\n",
      "21 Train Loss 2.7513306 Test MSE 13811.175783432152 Test RE 0.9841944012976446\n",
      "22 Train Loss 2.7341833 Test MSE 13692.95296699808 Test RE 0.9799730263917369\n",
      "23 Train Loss 2.6823795 Test MSE 13414.559493465431 Test RE 0.9699598813131849\n",
      "24 Train Loss 2.6548975 Test MSE 13301.897436599113 Test RE 0.9658781935834261\n",
      "25 Train Loss 2.640436 Test MSE 13233.094466350001 Test RE 0.9633769923252475\n",
      "26 Train Loss 2.6391203 Test MSE 13223.681353193564 Test RE 0.9630342913080064\n",
      "27 Train Loss 2.6350605 Test MSE 13195.642663348988 Test RE 0.9620127698517961\n",
      "28 Train Loss 2.618492 Test MSE 13120.315561806732 Test RE 0.9592630232538081\n",
      "29 Train Loss 2.6025846 Test MSE 13038.750764053306 Test RE 0.9562766592796964\n",
      "30 Train Loss 2.5811205 Test MSE 12975.44938161206 Test RE 0.9539525376986342\n",
      "31 Train Loss 2.5667906 Test MSE 12907.584689980715 Test RE 0.9514545676476139\n",
      "32 Train Loss 2.5582366 Test MSE 12837.497103362819 Test RE 0.9488678743599083\n",
      "33 Train Loss 2.551924 Test MSE 12781.207617647498 Test RE 0.9467853047590071\n",
      "34 Train Loss 2.5256674 Test MSE 12624.973111162313 Test RE 0.9409808706278562\n",
      "35 Train Loss 2.504706 Test MSE 12510.426920169317 Test RE 0.9367023912328369\n",
      "36 Train Loss 2.4921257 Test MSE 12414.648517375943 Test RE 0.9331098586370853\n",
      "37 Train Loss 2.4671373 Test MSE 12331.690023766816 Test RE 0.9299869695618574\n",
      "38 Train Loss 2.451361 Test MSE 12283.775631924433 Test RE 0.928178493669233\n",
      "39 Train Loss 2.4332085 Test MSE 12211.561357921124 Test RE 0.9254461685299874\n",
      "40 Train Loss 2.423537 Test MSE 12112.402185109931 Test RE 0.9216811493633876\n",
      "41 Train Loss 2.3792045 Test MSE 11884.098951773696 Test RE 0.9129535745434628\n",
      "42 Train Loss 2.3462002 Test MSE 11697.838751246629 Test RE 0.9057709316454904\n",
      "43 Train Loss 2.3349948 Test MSE 11598.030323856503 Test RE 0.9018985396230684\n",
      "44 Train Loss 2.3212032 Test MSE 11529.373523886274 Test RE 0.8992250952023465\n",
      "45 Train Loss 2.309067 Test MSE 11449.857177943035 Test RE 0.8961188202996029\n",
      "46 Train Loss 2.3025448 Test MSE 11383.974652144336 Test RE 0.8935369652318084\n",
      "47 Train Loss 2.2932405 Test MSE 11367.733879488205 Test RE 0.8928993623476904\n",
      "48 Train Loss 2.266344 Test MSE 11254.450090360144 Test RE 0.888439181903014\n",
      "49 Train Loss 2.2384312 Test MSE 11117.033606244067 Test RE 0.8829986162813455\n",
      "50 Train Loss 2.2027884 Test MSE 10917.167662176635 Test RE 0.8750251864637754\n",
      "51 Train Loss 2.1880958 Test MSE 10829.74965226118 Test RE 0.8715148115119619\n",
      "52 Train Loss 2.1849952 Test MSE 10817.138577518796 Test RE 0.8710072309916214\n",
      "53 Train Loss 2.1632323 Test MSE 10739.287575076385 Test RE 0.8678672493944993\n",
      "54 Train Loss 2.1279352 Test MSE 10497.67348889961 Test RE 0.8580490090808702\n",
      "55 Train Loss 2.0392911 Test MSE 9981.884530687614 Test RE 0.8367039795606512\n",
      "56 Train Loss 2.0014234 Test MSE 9666.871347270542 Test RE 0.8233955833544921\n",
      "57 Train Loss 1.9660163 Test MSE 9397.747719290106 Test RE 0.8118531029117863\n",
      "58 Train Loss 1.9492239 Test MSE 9396.511416723335 Test RE 0.8117997002712307\n",
      "59 Train Loss 1.9370738 Test MSE 9319.53389132098 Test RE 0.8084676742370858\n",
      "60 Train Loss 1.9285413 Test MSE 9312.10821951265 Test RE 0.8081455222913868\n",
      "61 Train Loss 1.9153355 Test MSE 9327.7058540248 Test RE 0.8088220546215438\n",
      "62 Train Loss 1.8641173 Test MSE 9152.908383169568 Test RE 0.8012077128493494\n",
      "63 Train Loss 1.8520602 Test MSE 9044.925826854243 Test RE 0.7964675176094914\n",
      "64 Train Loss 1.851295 Test MSE 9016.710376631854 Test RE 0.7952242656921432\n",
      "65 Train Loss 1.8432416 Test MSE 8870.292471793442 Test RE 0.7887412122223421\n",
      "66 Train Loss 1.8207023 Test MSE 8746.96546308092 Test RE 0.7832389375347312\n",
      "67 Train Loss 1.7891467 Test MSE 8714.136062095062 Test RE 0.7817677165947247\n",
      "68 Train Loss 1.7845598 Test MSE 8682.320776140112 Test RE 0.7803392957585528\n",
      "69 Train Loss 1.7800274 Test MSE 8594.09250518197 Test RE 0.7763643339735606\n",
      "70 Train Loss 1.7679454 Test MSE 8476.011065466972 Test RE 0.7710123263270668\n",
      "71 Train Loss 1.7573553 Test MSE 8362.559468461015 Test RE 0.7658349345008378\n",
      "72 Train Loss 1.7496253 Test MSE 8243.705066578807 Test RE 0.7603731741329129\n",
      "73 Train Loss 1.7369293 Test MSE 8098.870157196743 Test RE 0.7536640189769437\n",
      "74 Train Loss 1.7193284 Test MSE 7930.514012190157 Test RE 0.7457894442497645\n",
      "75 Train Loss 1.6851678 Test MSE 7674.980914198288 Test RE 0.733675836409932\n",
      "76 Train Loss 1.674988 Test MSE 7578.860410691041 Test RE 0.7290671289021448\n",
      "77 Train Loss 1.670504 Test MSE 7552.310933470128 Test RE 0.7277890124289138\n",
      "78 Train Loss 1.665588 Test MSE 7560.515271868577 Test RE 0.7281842164028834\n",
      "79 Train Loss 1.6617011 Test MSE 7556.231610504733 Test RE 0.7279778986823565\n",
      "80 Train Loss 1.6572055 Test MSE 7499.889881117004 Test RE 0.7252588001577964\n",
      "81 Train Loss 1.6481446 Test MSE 7363.092463187105 Test RE 0.7186140281543932\n",
      "82 Train Loss 1.6268983 Test MSE 7257.558916871952 Test RE 0.713445575220292\n",
      "83 Train Loss 1.5922533 Test MSE 7097.165919801512 Test RE 0.7055179098445034\n",
      "84 Train Loss 1.5666875 Test MSE 6996.7305401730155 Test RE 0.700508062300761\n",
      "85 Train Loss 1.5490837 Test MSE 6964.085647824901 Test RE 0.6988719590822079\n",
      "86 Train Loss 1.5211277 Test MSE 6965.902254016861 Test RE 0.6989631047408033\n",
      "87 Train Loss 1.5078758 Test MSE 6954.660401164908 Test RE 0.6983988696359726\n",
      "88 Train Loss 1.491002 Test MSE 6908.281421395629 Test RE 0.6960662460418888\n",
      "89 Train Loss 1.4319885 Test MSE 6662.663944733236 Test RE 0.6835802680559152\n",
      "90 Train Loss 1.4016743 Test MSE 6511.280094499853 Test RE 0.6757697583894423\n",
      "91 Train Loss 1.3497484 Test MSE 6161.964133749907 Test RE 0.6573931081871901\n",
      "92 Train Loss 1.270442 Test MSE 5791.9864904800415 Test RE 0.6373519719786929\n",
      "93 Train Loss 1.243635 Test MSE 5647.346385385894 Test RE 0.6293435375276399\n",
      "94 Train Loss 1.2317224 Test MSE 5533.184913337348 Test RE 0.6229499507315819\n",
      "95 Train Loss 1.2172031 Test MSE 5381.667083010152 Test RE 0.6143614798963377\n",
      "96 Train Loss 1.1442366 Test MSE 5032.404968498996 Test RE 0.5940915222360261\n",
      "97 Train Loss 1.0916814 Test MSE 4544.829583008497 Test RE 0.5645785387390249\n",
      "98 Train Loss 1.0746847 Test MSE 4350.627351061206 Test RE 0.5523845291870615\n",
      "99 Train Loss 1.0454819 Test MSE 4110.28005252261 Test RE 0.5369097250914092\n",
      "100 Train Loss 0.94933033 Test MSE 3374.5814668104103 Test RE 0.4864917975372693\n",
      "101 Train Loss 0.87081635 Test MSE 3022.8214139209576 Test RE 0.4604386891402402\n",
      "102 Train Loss 0.8474988 Test MSE 2892.9513561700155 Test RE 0.45043914906162447\n",
      "103 Train Loss 0.8159238 Test MSE 2627.5458933308996 Test RE 0.42928006281844666\n",
      "104 Train Loss 0.78580546 Test MSE 2374.7494488205884 Test RE 0.4081073923358305\n",
      "105 Train Loss 0.7795204 Test MSE 2369.03474655709 Test RE 0.4076160527093773\n",
      "106 Train Loss 0.7492434 Test MSE 2400.0939495878756 Test RE 0.41027937457766434\n",
      "107 Train Loss 0.6957452 Test MSE 2496.0998640569624 Test RE 0.41840468813796666\n",
      "108 Train Loss 0.66249794 Test MSE 2418.842864328686 Test RE 0.4118787554798767\n",
      "109 Train Loss 0.6431516 Test MSE 2381.347606427723 Test RE 0.4086739550648051\n",
      "110 Train Loss 0.5810078 Test MSE 2289.1231918347053 Test RE 0.4006822892849204\n",
      "111 Train Loss 0.50986767 Test MSE 1963.0208062661802 Test RE 0.3710462230071516\n",
      "112 Train Loss 0.5009544 Test MSE 1828.95639800187 Test RE 0.35815188380526414\n",
      "113 Train Loss 0.4797896 Test MSE 1748.6306121063303 Test RE 0.3501987586156181\n",
      "114 Train Loss 0.44923186 Test MSE 1661.6754399150561 Test RE 0.34138046388725063\n",
      "115 Train Loss 0.42411843 Test MSE 1579.696771937761 Test RE 0.3328529643586918\n",
      "116 Train Loss 0.40476698 Test MSE 1518.7828453145353 Test RE 0.3263723851342833\n",
      "117 Train Loss 0.38846225 Test MSE 1455.4834954667738 Test RE 0.3194987815806298\n",
      "118 Train Loss 0.3687714 Test MSE 1350.1653690508351 Test RE 0.3077223539482665\n",
      "119 Train Loss 0.3519104 Test MSE 1247.9934993470824 Test RE 0.2958501033552051\n",
      "120 Train Loss 0.34423548 Test MSE 1221.1846764137526 Test RE 0.292655194323028\n",
      "121 Train Loss 0.3322462 Test MSE 1248.641080160777 Test RE 0.2959268513525786\n",
      "122 Train Loss 0.3245992 Test MSE 1246.843845935866 Test RE 0.2957138031870801\n",
      "123 Train Loss 0.3102849 Test MSE 1135.0838507141298 Test RE 0.2821496631869705\n",
      "124 Train Loss 0.30177617 Test MSE 1100.160705221496 Test RE 0.277775301902329\n",
      "125 Train Loss 0.2988617 Test MSE 1069.8952245076746 Test RE 0.2739278496488768\n",
      "126 Train Loss 0.2948922 Test MSE 1037.2038615794993 Test RE 0.269710358476889\n",
      "127 Train Loss 0.29367453 Test MSE 1042.7694211282524 Test RE 0.2704330133077244\n",
      "128 Train Loss 0.2857548 Test MSE 1046.1648749718051 Test RE 0.270872945912888\n",
      "129 Train Loss 0.24538593 Test MSE 975.5141775451784 Test RE 0.2615666417566007\n",
      "130 Train Loss 0.22289807 Test MSE 975.6241133895337 Test RE 0.2615813800044758\n",
      "131 Train Loss 0.20998201 Test MSE 915.6936886514414 Test RE 0.25341987605973365\n",
      "132 Train Loss 0.19147766 Test MSE 797.9677515464131 Test RE 0.2365692178387339\n",
      "133 Train Loss 0.18532233 Test MSE 779.5407645539467 Test RE 0.23382178903740203\n",
      "134 Train Loss 0.17033468 Test MSE 704.3027230883414 Test RE 0.22225177980929672\n",
      "135 Train Loss 0.13246194 Test MSE 508.4134721048942 Test RE 0.18883134599347925\n",
      "136 Train Loss 0.101803906 Test MSE 361.55164360387704 Test RE 0.15923947381026174\n",
      "137 Train Loss 0.082407795 Test MSE 221.30172662521701 Test RE 0.1245827648628489\n",
      "138 Train Loss 0.0726609 Test MSE 124.09702520544538 Test RE 0.09329239646039533\n",
      "139 Train Loss 0.069717415 Test MSE 113.77592605300322 Test RE 0.08932864642244763\n",
      "140 Train Loss 0.068649665 Test MSE 108.49713069885827 Test RE 0.08723177111212115\n",
      "141 Train Loss 0.067042805 Test MSE 93.97355462946469 Test RE 0.08118361596046064\n",
      "142 Train Loss 0.061447524 Test MSE 73.80122793869897 Test RE 0.0719444600785081\n",
      "143 Train Loss 0.0553599 Test MSE 65.49182945393342 Test RE 0.06777337680194023\n",
      "144 Train Loss 0.046379108 Test MSE 65.75665915256313 Test RE 0.06791026635885396\n",
      "145 Train Loss 0.04412409 Test MSE 70.40170023993737 Test RE 0.07026792637995974\n",
      "146 Train Loss 0.043271974 Test MSE 72.52369689283516 Test RE 0.07131904692318962\n",
      "147 Train Loss 0.042692356 Test MSE 75.9187642282037 Test RE 0.07296929139946777\n",
      "148 Train Loss 0.042626612 Test MSE 76.8592477780846 Test RE 0.07341987294413098\n",
      "149 Train Loss 0.041813284 Test MSE 83.82244606209476 Test RE 0.07667357774089492\n",
      "150 Train Loss 0.041581586 Test MSE 83.05765776602195 Test RE 0.07632299489789986\n",
      "151 Train Loss 0.041580938 Test MSE 83.05765776602195 Test RE 0.07632299489789986\n",
      "152 Train Loss 0.041578956 Test MSE 83.0519610520472 Test RE 0.07632037745274867\n",
      "153 Train Loss 0.041578956 Test MSE 83.0519610520472 Test RE 0.07632037745274867\n",
      "154 Train Loss 0.041578956 Test MSE 83.0519610520472 Test RE 0.07632037745274867\n",
      "155 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "156 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "157 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "158 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "159 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "160 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "161 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "162 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "163 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "164 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "165 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "166 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "167 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "168 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "169 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "170 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "171 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "172 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "173 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "174 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "175 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "176 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "177 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "178 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "179 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "180 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "181 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "182 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "183 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "184 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "185 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "186 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "187 Train Loss 0.04157091 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "188 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "189 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "190 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "191 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "192 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "193 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "194 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "195 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "196 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "197 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "198 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "199 Train Loss 0.041570917 Test MSE 82.86845292904871 Test RE 0.07623601367593248\n",
      "Training time: 73.35\n",
      "Training time: 73.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.6997237 Test MSE 14252.920166929816 Test RE 0.9998100318560218\n",
      "1 Train Loss 3.6467059 Test MSE 14246.756702973193 Test RE 0.9995938319697958\n",
      "2 Train Loss 2.9073322 Test MSE 14237.16577551861 Test RE 0.9992573116596289\n",
      "3 Train Loss 2.8347597 Test MSE 14240.345987017216 Test RE 0.9993689094383005\n",
      "4 Train Loss 2.8278677 Test MSE 14242.047881820821 Test RE 0.9994286260330113\n",
      "5 Train Loss 2.825328 Test MSE 14237.591474736726 Test RE 0.9992722507241603\n",
      "6 Train Loss 2.8226466 Test MSE 14215.838589370649 Test RE 0.9985085905278618\n",
      "7 Train Loss 2.8173206 Test MSE 14192.870540836044 Test RE 0.9977016362122819\n",
      "8 Train Loss 2.8141122 Test MSE 14186.317165394912 Test RE 0.9974712716675852\n",
      "9 Train Loss 2.8115351 Test MSE 14166.128832651337 Test RE 0.9967612758643389\n",
      "10 Train Loss 2.808868 Test MSE 14143.982238751069 Test RE 0.9959818285685992\n",
      "11 Train Loss 2.8044627 Test MSE 14121.912271076893 Test RE 0.9952044708249165\n",
      "12 Train Loss 2.7980745 Test MSE 14097.10925266907 Test RE 0.9943301231327748\n",
      "13 Train Loss 2.7944725 Test MSE 14082.302203901138 Test RE 0.9938077833206813\n",
      "14 Train Loss 2.7869236 Test MSE 14045.876647906036 Test RE 0.9925216498545256\n",
      "15 Train Loss 2.7766426 Test MSE 13978.303720079593 Test RE 0.990131323727893\n",
      "16 Train Loss 2.7741122 Test MSE 13968.501346662964 Test RE 0.9897840949334453\n",
      "17 Train Loss 2.7718453 Test MSE 13942.556076612163 Test RE 0.9888644489823986\n",
      "18 Train Loss 2.7643433 Test MSE 13898.487302550948 Test RE 0.9873004411717943\n",
      "19 Train Loss 2.7621436 Test MSE 13891.901103435543 Test RE 0.9870664830550084\n",
      "20 Train Loss 2.7457979 Test MSE 13752.670928058295 Test RE 0.9821076398360311\n",
      "21 Train Loss 2.7227938 Test MSE 13665.484937601057 Test RE 0.9789896212473922\n",
      "22 Train Loss 2.7197897 Test MSE 13627.555976744508 Test RE 0.9776300697826821\n",
      "23 Train Loss 2.7137914 Test MSE 13590.955039362503 Test RE 0.976316325949206\n",
      "24 Train Loss 2.6996958 Test MSE 13533.31789192916 Test RE 0.9742439228320762\n",
      "25 Train Loss 2.68308 Test MSE 13489.076766717677 Test RE 0.9726501920558692\n",
      "26 Train Loss 2.67817 Test MSE 13456.842449620031 Test RE 0.9714873449515221\n",
      "27 Train Loss 2.6696293 Test MSE 13425.67259072856 Test RE 0.9703615727347996\n",
      "28 Train Loss 2.6574936 Test MSE 13344.172746817923 Test RE 0.9674118247368501\n",
      "29 Train Loss 2.6188786 Test MSE 13152.044771973806 Test RE 0.9604222285601771\n",
      "30 Train Loss 2.6147842 Test MSE 13124.378428756532 Test RE 0.95941153554932\n",
      "31 Train Loss 2.6124544 Test MSE 13102.066858583868 Test RE 0.9585956842215929\n",
      "32 Train Loss 2.6055791 Test MSE 13069.69853588961 Test RE 0.9574108588792422\n",
      "33 Train Loss 2.6038916 Test MSE 13078.19748674966 Test RE 0.9577221003903583\n",
      "34 Train Loss 2.5964158 Test MSE 13018.428727207196 Test RE 0.9555311481504859\n",
      "35 Train Loss 2.5699286 Test MSE 12880.706098070375 Test RE 0.9504634028728631\n",
      "36 Train Loss 2.5581458 Test MSE 12827.391649462783 Test RE 0.9484943347107936\n",
      "37 Train Loss 2.545786 Test MSE 12771.712587431524 Test RE 0.946433560793736\n",
      "38 Train Loss 2.5191867 Test MSE 12605.237678809563 Test RE 0.9402451095583123\n",
      "39 Train Loss 2.4753811 Test MSE 12356.890825317 Test RE 0.9309367362072456\n",
      "40 Train Loss 2.4385056 Test MSE 12188.013481592277 Test RE 0.9245534568164341\n",
      "41 Train Loss 2.4156003 Test MSE 12004.92013705425 Test RE 0.9175826673490508\n",
      "42 Train Loss 2.3529294 Test MSE 11758.502690505678 Test RE 0.9081165178558286\n",
      "43 Train Loss 2.3153124 Test MSE 11526.327106801396 Test RE 0.8991062858120994\n",
      "44 Train Loss 2.3103569 Test MSE 11487.677998822654 Test RE 0.8975976163787697\n",
      "45 Train Loss 2.280754 Test MSE 11357.561063793104 Test RE 0.8924997517577027\n",
      "46 Train Loss 2.2523892 Test MSE 11194.299009861672 Test RE 0.8860618034859306\n",
      "47 Train Loss 2.2456648 Test MSE 11135.631472721374 Test RE 0.8837368990181627\n",
      "48 Train Loss 2.2434978 Test MSE 11159.368719749662 Test RE 0.8846783056136891\n",
      "49 Train Loss 2.2336133 Test MSE 11138.378354773855 Test RE 0.8838458902033163\n",
      "50 Train Loss 2.2116323 Test MSE 10987.660708579715 Test RE 0.8778456954002004\n",
      "51 Train Loss 2.1990373 Test MSE 10853.201669824473 Test RE 0.8724579416927193\n",
      "52 Train Loss 2.1946523 Test MSE 10720.920745610856 Test RE 0.8671247983328908\n",
      "53 Train Loss 2.1905262 Test MSE 10616.192157291789 Test RE 0.8628790984120943\n",
      "54 Train Loss 2.1634579 Test MSE 10592.648561999546 Test RE 0.8619217611760448\n",
      "55 Train Loss 2.1320813 Test MSE 10513.427263329133 Test RE 0.8586926013467968\n",
      "56 Train Loss 2.1105025 Test MSE 10444.79373807172 Test RE 0.8558851628772106\n",
      "57 Train Loss 2.0899866 Test MSE 10398.742577120394 Test RE 0.8539962770303499\n",
      "58 Train Loss 2.0843127 Test MSE 10311.314084344982 Test RE 0.8503986686909263\n",
      "59 Train Loss 2.07735 Test MSE 10172.955119605867 Test RE 0.844674003273595\n",
      "60 Train Loss 2.046676 Test MSE 9938.016128655769 Test RE 0.834863381076682\n",
      "61 Train Loss 2.0309067 Test MSE 9947.23363269272 Test RE 0.8352504589880645\n",
      "62 Train Loss 1.9969046 Test MSE 9786.838675244318 Test RE 0.8284890609173852\n",
      "63 Train Loss 1.971756 Test MSE 9554.93075931054 Test RE 0.8186143169797873\n",
      "64 Train Loss 1.9193609 Test MSE 9247.189135008888 Test RE 0.8053236142841939\n",
      "65 Train Loss 1.8847853 Test MSE 8912.522344601031 Test RE 0.790616510662907\n",
      "66 Train Loss 1.8739533 Test MSE 8748.43529761256 Test RE 0.7833047422576821\n",
      "67 Train Loss 1.8307191 Test MSE 8506.095426672415 Test RE 0.7723794120590503\n",
      "68 Train Loss 1.8009441 Test MSE 8518.090091103966 Test RE 0.7729237962737873\n",
      "69 Train Loss 1.7423074 Test MSE 8202.920991345502 Test RE 0.7584899452341053\n",
      "70 Train Loss 1.6861074 Test MSE 7986.771870518028 Test RE 0.7484300279057502\n",
      "71 Train Loss 1.6441798 Test MSE 7868.703862484352 Test RE 0.7428774306546592\n",
      "72 Train Loss 1.6175146 Test MSE 7776.346396121873 Test RE 0.7385048685468831\n",
      "73 Train Loss 1.6091453 Test MSE 7710.016284579853 Test RE 0.735348500948637\n",
      "74 Train Loss 1.5872159 Test MSE 7435.95631956441 Test RE 0.722160913368998\n",
      "75 Train Loss 1.5406733 Test MSE 7076.435394408386 Test RE 0.7044867621269492\n",
      "76 Train Loss 1.5124762 Test MSE 6874.538224339631 Test RE 0.6943642126239001\n",
      "77 Train Loss 1.4635364 Test MSE 6416.536263182849 Test RE 0.6708352733382659\n",
      "78 Train Loss 1.4510943 Test MSE 6247.800507964176 Test RE 0.6619560271435815\n",
      "79 Train Loss 1.4348483 Test MSE 6118.0618868003485 Test RE 0.6550470522404851\n",
      "80 Train Loss 1.3804364 Test MSE 5758.562242994119 Test RE 0.6355103039431839\n",
      "81 Train Loss 1.3504059 Test MSE 5603.977959156886 Test RE 0.6269223790743668\n",
      "82 Train Loss 1.2939851 Test MSE 5387.109216731042 Test RE 0.6146720335269632\n",
      "83 Train Loss 1.2522264 Test MSE 5223.752046477025 Test RE 0.6052807204781088\n",
      "84 Train Loss 1.192402 Test MSE 4935.595778658082 Test RE 0.5883494554251758\n",
      "85 Train Loss 1.1127539 Test MSE 4513.254798580472 Test RE 0.562613941910277\n",
      "86 Train Loss 1.0907409 Test MSE 4410.864426382253 Test RE 0.5561954329413628\n",
      "87 Train Loss 1.0699022 Test MSE 4427.147552122138 Test RE 0.5572211113983294\n",
      "88 Train Loss 1.0261856 Test MSE 4271.814972600173 Test RE 0.5473583914405651\n",
      "89 Train Loss 0.9534067 Test MSE 3937.4052315559006 Test RE 0.5254974594553679\n",
      "90 Train Loss 0.9153596 Test MSE 3722.4270560190794 Test RE 0.5109503039406694\n",
      "91 Train Loss 0.7993262 Test MSE 3442.9683718860997 Test RE 0.49139652429102015\n",
      "92 Train Loss 0.7276662 Test MSE 3288.9505357702483 Test RE 0.4802797044707103\n",
      "93 Train Loss 0.67646575 Test MSE 2991.4979853967366 Test RE 0.4580468713805359\n",
      "94 Train Loss 0.6494654 Test MSE 2874.9512537580636 Test RE 0.4490356338910861\n",
      "95 Train Loss 0.63668317 Test MSE 2873.531284338449 Test RE 0.44892472842468567\n",
      "96 Train Loss 0.6193592 Test MSE 2700.9656249415893 Test RE 0.43523628255324764\n",
      "97 Train Loss 0.6068998 Test MSE 2595.4835652585675 Test RE 0.4266529033960728\n",
      "98 Train Loss 0.6034322 Test MSE 2542.469507759583 Test RE 0.42227312311617743\n",
      "99 Train Loss 0.6012816 Test MSE 2539.7095370689894 Test RE 0.4220438621747355\n",
      "100 Train Loss 0.5989673 Test MSE 2557.288908824625 Test RE 0.4235019957542021\n",
      "101 Train Loss 0.5949091 Test MSE 2512.0049507370686 Test RE 0.4197356035337217\n",
      "102 Train Loss 0.58894175 Test MSE 2426.814250171514 Test RE 0.4125568780573165\n",
      "103 Train Loss 0.58030665 Test MSE 2422.782621233127 Test RE 0.41221404843688764\n",
      "104 Train Loss 0.56595373 Test MSE 2389.541274028521 Test RE 0.4093764277096027\n",
      "105 Train Loss 0.5597461 Test MSE 2303.808099051771 Test RE 0.40196543906394794\n",
      "106 Train Loss 0.554664 Test MSE 2196.6881729567317 Test RE 0.3925091358660621\n",
      "107 Train Loss 0.54453856 Test MSE 2132.699378694432 Test RE 0.38675005574222654\n",
      "108 Train Loss 0.49886015 Test MSE 1996.7774621434137 Test RE 0.37422293174036475\n",
      "109 Train Loss 0.4486159 Test MSE 1796.9813860823147 Test RE 0.35500735680084117\n",
      "110 Train Loss 0.39891797 Test MSE 1604.3530906270298 Test RE 0.33544053446968186\n",
      "111 Train Loss 0.3842985 Test MSE 1538.494029444708 Test RE 0.3284834333459943\n",
      "112 Train Loss 0.36572662 Test MSE 1411.774491630029 Test RE 0.3146648477022962\n",
      "113 Train Loss 0.34428617 Test MSE 1255.7699742991351 Test RE 0.296770419874264\n",
      "114 Train Loss 0.31760746 Test MSE 1048.296071656887 Test RE 0.27114871019906767\n",
      "115 Train Loss 0.28657925 Test MSE 808.8018965334373 Test RE 0.2381697738163073\n",
      "116 Train Loss 0.2699047 Test MSE 690.6159489369602 Test RE 0.22008166630483372\n",
      "117 Train Loss 0.26339373 Test MSE 630.407903129419 Test RE 0.2102695514609779\n",
      "118 Train Loss 0.25817525 Test MSE 591.0759037809091 Test RE 0.20360441506154717\n",
      "119 Train Loss 0.24985188 Test MSE 591.2004548798535 Test RE 0.2036258656212034\n",
      "120 Train Loss 0.23831096 Test MSE 687.0795000896601 Test RE 0.2195174551185958\n",
      "121 Train Loss 0.22606787 Test MSE 670.6752002355304 Test RE 0.2168810901484358\n",
      "122 Train Loss 0.21775156 Test MSE 617.1593091434925 Test RE 0.20804831648597735\n",
      "123 Train Loss 0.2145356 Test MSE 636.1064540274386 Test RE 0.21121777566753963\n",
      "124 Train Loss 0.21001853 Test MSE 650.7153559227246 Test RE 0.21362943482830518\n",
      "125 Train Loss 0.20074283 Test MSE 582.146059357989 Test RE 0.20206055649714538\n",
      "126 Train Loss 0.19760802 Test MSE 519.9846960261219 Test RE 0.19096810768905603\n",
      "127 Train Loss 0.19533777 Test MSE 489.80228789588074 Test RE 0.18534290616861998\n",
      "128 Train Loss 0.17891467 Test MSE 426.6955551164748 Test RE 0.17299144373091774\n",
      "129 Train Loss 0.15303504 Test MSE 364.58814136904346 Test RE 0.15990676342358623\n",
      "130 Train Loss 0.14103296 Test MSE 323.5111105802069 Test RE 0.15062954492662528\n",
      "131 Train Loss 0.13174255 Test MSE 281.14008704942 Test RE 0.14041934769736425\n",
      "132 Train Loss 0.1256997 Test MSE 272.14782578018475 Test RE 0.13815544252272335\n",
      "133 Train Loss 0.11715335 Test MSE 251.9100641081156 Test RE 0.13291938493173286\n",
      "134 Train Loss 0.1088082 Test MSE 236.30391172435898 Test RE 0.12873629889103358\n",
      "135 Train Loss 0.10482739 Test MSE 223.76891641511182 Test RE 0.12527529761700196\n",
      "136 Train Loss 0.10013166 Test MSE 180.40214010062297 Test RE 0.11248287454214607\n",
      "137 Train Loss 0.09896568 Test MSE 162.65634361726043 Test RE 0.10680733194541478\n",
      "138 Train Loss 0.09653992 Test MSE 140.10015521058213 Test RE 0.09912538154148548\n",
      "139 Train Loss 0.0949662 Test MSE 133.95576771731737 Test RE 0.09692733512433656\n",
      "140 Train Loss 0.093197204 Test MSE 137.47697470736077 Test RE 0.09819300411667346\n",
      "141 Train Loss 0.09068528 Test MSE 125.71661688750609 Test RE 0.09389920307999529\n",
      "142 Train Loss 0.088886514 Test MSE 104.23095678733306 Test RE 0.08549956909335157\n",
      "143 Train Loss 0.08691939 Test MSE 88.11048207894423 Test RE 0.07861028167856994\n",
      "144 Train Loss 0.08442123 Test MSE 80.85383776779615 Test RE 0.07530362502637249\n",
      "145 Train Loss 0.0814828 Test MSE 55.987344232414834 Test RE 0.06266290107422189\n",
      "146 Train Loss 0.079272285 Test MSE 33.9517574626027 Test RE 0.04879740122607861\n",
      "147 Train Loss 0.07810791 Test MSE 23.79859189006329 Test RE 0.04085462180409009\n",
      "148 Train Loss 0.0778297 Test MSE 21.355910281468773 Test RE 0.03870121570898999\n",
      "149 Train Loss 0.07750602 Test MSE 18.747706002428846 Test RE 0.036260987819742095\n",
      "150 Train Loss 0.07713617 Test MSE 17.508118679896764 Test RE 0.0350417110916665\n",
      "151 Train Loss 0.07558194 Test MSE 19.879253134552766 Test RE 0.037339250453644335\n",
      "152 Train Loss 0.07194136 Test MSE 32.45590197918397 Test RE 0.04771032833218252\n",
      "153 Train Loss 0.068582244 Test MSE 23.622669374429115 Test RE 0.04070334017613553\n",
      "154 Train Loss 0.06623001 Test MSE 17.468795368934302 Test RE 0.03500233705265375\n",
      "155 Train Loss 0.065037504 Test MSE 18.286712456477037 Test RE 0.03581239630101258\n",
      "156 Train Loss 0.06355437 Test MSE 22.472384960532008 Test RE 0.03969996708534132\n",
      "157 Train Loss 0.060241457 Test MSE 22.467665493376312 Test RE 0.03969579813466698\n",
      "158 Train Loss 0.049777247 Test MSE 20.752534191620644 Test RE 0.03815057893753338\n",
      "159 Train Loss 0.041058514 Test MSE 23.618088356397333 Test RE 0.04069939329415636\n",
      "160 Train Loss 0.038927507 Test MSE 22.28308437766577 Test RE 0.039532403196718344\n",
      "161 Train Loss 0.036581118 Test MSE 17.026148820888285 Test RE 0.0345560247441443\n",
      "162 Train Loss 0.033192966 Test MSE 5.838639387850601 Test RE 0.020235847139755212\n",
      "163 Train Loss 0.03196552 Test MSE 3.0017508420651193 Test RE 0.014509514616313744\n",
      "164 Train Loss 0.03128987 Test MSE 3.6229500300039956 Test RE 0.015940308996257514\n",
      "165 Train Loss 0.030045686 Test MSE 5.661753602065322 Test RE 0.019926959874049164\n",
      "166 Train Loss 0.029333923 Test MSE 5.0324903996360995 Test RE 0.018786982952428366\n",
      "167 Train Loss 0.02875754 Test MSE 5.763032133542161 Test RE 0.020104398512256115\n",
      "168 Train Loss 0.02814297 Test MSE 8.580662447021494 Test RE 0.024531605530519966\n",
      "169 Train Loss 0.027452385 Test MSE 7.435063315672292 Test RE 0.022835361930562466\n",
      "170 Train Loss 0.027022878 Test MSE 5.121898976342947 Test RE 0.01895313551729635\n",
      "171 Train Loss 0.026935074 Test MSE 5.121070137068712 Test RE 0.018951601931927645\n",
      "172 Train Loss 0.026934663 Test MSE 5.145245081916808 Test RE 0.0189962815098225\n",
      "173 Train Loss 0.026926475 Test MSE 5.167486580830606 Test RE 0.019037295121185044\n",
      "174 Train Loss 0.026922656 Test MSE 5.1884425185527085 Test RE 0.019075857458848294\n",
      "175 Train Loss 0.0269185 Test MSE 5.210589795953318 Test RE 0.019116527507657948\n",
      "176 Train Loss 0.026912114 Test MSE 5.259181659188549 Test RE 0.01920545717888549\n",
      "177 Train Loss 0.026905382 Test MSE 5.287289116897764 Test RE 0.019256710137522996\n",
      "178 Train Loss 0.026899938 Test MSE 5.324200655828898 Test RE 0.01932381055082791\n",
      "179 Train Loss 0.026897818 Test MSE 5.379936608967923 Test RE 0.01942469207504043\n",
      "180 Train Loss 0.026877927 Test MSE 5.490574848244243 Test RE 0.01962340973126941\n",
      "181 Train Loss 0.02687383 Test MSE 5.562902955861356 Test RE 0.019752237807334625\n",
      "182 Train Loss 0.026849704 Test MSE 5.793232356937942 Test RE 0.020157006580591122\n",
      "183 Train Loss 0.026828859 Test MSE 5.9521445824309716 Test RE 0.020431596338612284\n",
      "184 Train Loss 0.026820049 Test MSE 6.048491989000038 Test RE 0.02059629571407958\n",
      "185 Train Loss 0.026790882 Test MSE 6.1996076124605315 Test RE 0.02085199754790663\n",
      "186 Train Loss 0.026788166 Test MSE 6.281537298918438 Test RE 0.02098932804283185\n",
      "187 Train Loss 0.026781082 Test MSE 6.362569624541806 Test RE 0.021124276218880873\n",
      "188 Train Loss 0.026750019 Test MSE 6.576703211493216 Test RE 0.021476805578325826\n",
      "189 Train Loss 0.02675002 Test MSE 6.576703211493216 Test RE 0.021476805578325826\n",
      "190 Train Loss 0.02675002 Test MSE 6.576703211493216 Test RE 0.021476805578325826\n",
      "191 Train Loss 0.026749132 Test MSE 6.683999958882929 Test RE 0.021651290298025025\n",
      "192 Train Loss 0.026734995 Test MSE 6.860226447584709 Test RE 0.021934856092555726\n",
      "193 Train Loss 0.026699848 Test MSE 7.070413618768421 Test RE 0.022268346690993025\n",
      "194 Train Loss 0.026678981 Test MSE 7.129072886208465 Test RE 0.02236052989690183\n",
      "195 Train Loss 0.02667898 Test MSE 7.129072886208465 Test RE 0.02236052989690183\n",
      "196 Train Loss 0.026677333 Test MSE 7.1290896082552395 Test RE 0.02236055612146241\n",
      "197 Train Loss 0.026677333 Test MSE 7.1290896082552395 Test RE 0.02236055612146241\n",
      "198 Train Loss 0.026677333 Test MSE 7.1290896082552395 Test RE 0.02236055612146241\n",
      "199 Train Loss 0.026677333 Test MSE 7.1290896082552395 Test RE 0.02236055612146241\n",
      "Training time: 96.24\n",
      "Training time: 96.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4.509636 Test MSE 14251.095914148587 Test RE 0.9997460462104555\n",
      "1 Train Loss 3.0387125 Test MSE 14260.914483660918 Test RE 1.0000903841458184\n",
      "2 Train Loss 2.844851 Test MSE 14251.064070840102 Test RE 0.9997449292704061\n",
      "3 Train Loss 2.829441 Test MSE 14248.650200362063 Test RE 0.9996602564011614\n",
      "4 Train Loss 2.8267205 Test MSE 14248.506611673221 Test RE 0.999655219423444\n",
      "5 Train Loss 2.8214405 Test MSE 14218.326439277454 Test RE 0.9985959589519355\n",
      "6 Train Loss 2.8201075 Test MSE 14211.324498187016 Test RE 0.9983500449496339\n",
      "7 Train Loss 2.8154151 Test MSE 14181.230412164972 Test RE 0.9972924252105223\n",
      "8 Train Loss 2.812769 Test MSE 14149.945217465367 Test RE 0.9961917550417717\n",
      "9 Train Loss 2.8003387 Test MSE 14094.494784032695 Test RE 0.9942379139635361\n",
      "10 Train Loss 2.7883484 Test MSE 14032.640438316315 Test RE 0.992053885493766\n",
      "11 Train Loss 2.780071 Test MSE 14003.044341603761 Test RE 0.9910071680060628\n",
      "12 Train Loss 2.7684948 Test MSE 13926.811051332681 Test RE 0.9883059397042213\n",
      "13 Train Loss 2.7615793 Test MSE 13887.664533886307 Test RE 0.9869159602918373\n",
      "14 Train Loss 2.7554293 Test MSE 13866.287754420666 Test RE 0.9861561057629545\n",
      "15 Train Loss 2.748627 Test MSE 13834.802935186652 Test RE 0.9850358855474199\n",
      "16 Train Loss 2.7456489 Test MSE 13796.167795462943 Test RE 0.9836595158963747\n",
      "17 Train Loss 2.7345197 Test MSE 13700.442585179393 Test RE 0.9802409970935791\n",
      "18 Train Loss 2.7221432 Test MSE 13659.460830862337 Test RE 0.9787738152188334\n",
      "19 Train Loss 2.7099419 Test MSE 13535.54942032823 Test RE 0.9743242417673693\n",
      "20 Train Loss 2.6870956 Test MSE 13473.248216337384 Test RE 0.9720793538165031\n",
      "21 Train Loss 2.6825628 Test MSE 13462.2863007396 Test RE 0.9716838285519879\n",
      "22 Train Loss 2.661993 Test MSE 13396.698750810878 Test RE 0.9693139423156988\n",
      "23 Train Loss 2.6455486 Test MSE 13280.212566191203 Test RE 0.9650905808883067\n",
      "24 Train Loss 2.6181707 Test MSE 13127.842840693145 Test RE 0.9595381540204014\n",
      "25 Train Loss 2.578964 Test MSE 12898.664271913665 Test RE 0.9511257362035609\n",
      "26 Train Loss 2.5247064 Test MSE 12631.914480417407 Test RE 0.9412395166492012\n",
      "27 Train Loss 2.4932718 Test MSE 12442.66576571109 Test RE 0.9341621815163399\n",
      "28 Train Loss 2.475617 Test MSE 12344.793235749954 Test RE 0.9304809237987022\n",
      "29 Train Loss 2.4433327 Test MSE 12203.462411870589 Test RE 0.9251392306407715\n",
      "30 Train Loss 2.4174654 Test MSE 12044.392304788862 Test RE 0.9190899349275047\n",
      "31 Train Loss 2.3756924 Test MSE 11863.699171418832 Test RE 0.9121696677589499\n",
      "32 Train Loss 2.3580954 Test MSE 11761.563363098883 Test RE 0.9082346989952843\n",
      "33 Train Loss 2.3329883 Test MSE 11487.393441885246 Test RE 0.8975864992839788\n",
      "34 Train Loss 2.288107 Test MSE 11303.840269169079 Test RE 0.8903865064277353\n",
      "35 Train Loss 2.2676847 Test MSE 10996.05748144809 Test RE 0.8781810562839919\n",
      "36 Train Loss 2.16257 Test MSE 10336.724431759403 Test RE 0.8514458499232458\n",
      "37 Train Loss 2.150319 Test MSE 10301.248397863279 Test RE 0.8499834967584792\n",
      "38 Train Loss 2.1223674 Test MSE 10245.856458011838 Test RE 0.8476951479887956\n",
      "39 Train Loss 2.0711982 Test MSE 10227.707416424708 Test RE 0.8469440310077568\n",
      "40 Train Loss 2.0139096 Test MSE 9820.153804389014 Test RE 0.8298979822103545\n",
      "41 Train Loss 1.9439695 Test MSE 9468.110188928853 Test RE 0.8148866735751022\n",
      "42 Train Loss 1.9250485 Test MSE 9416.223217882793 Test RE 0.8126507422256959\n",
      "43 Train Loss 1.8377447 Test MSE 8742.319586207399 Test RE 0.7830309045321009\n",
      "44 Train Loss 1.7763621 Test MSE 8466.597300329007 Test RE 0.7705840502601986\n",
      "45 Train Loss 1.7483346 Test MSE 8497.005083008298 Test RE 0.7719665862671823\n",
      "46 Train Loss 1.7094843 Test MSE 8283.18032527482 Test RE 0.7621915361569885\n",
      "47 Train Loss 1.6770167 Test MSE 8006.615502550004 Test RE 0.7493592116138603\n",
      "48 Train Loss 1.65937 Test MSE 7924.893774934795 Test RE 0.7455251324730077\n",
      "49 Train Loss 1.6290303 Test MSE 7845.84372706177 Test RE 0.7417975430943965\n",
      "50 Train Loss 1.6119535 Test MSE 7671.697171868291 Test RE 0.7335188679316361\n",
      "51 Train Loss 1.5809265 Test MSE 7511.8343014490965 Test RE 0.7258360986061654\n",
      "52 Train Loss 1.5702264 Test MSE 7399.169419555485 Test RE 0.7203723741170953\n",
      "53 Train Loss 1.4487314 Test MSE 6741.393053623528 Test RE 0.6876071568986646\n",
      "54 Train Loss 1.3350669 Test MSE 6413.9280439524555 Test RE 0.6706989175826449\n",
      "55 Train Loss 1.2715499 Test MSE 5841.034931193065 Test RE 0.6400449355120406\n",
      "56 Train Loss 1.2231387 Test MSE 5447.910272454906 Test RE 0.6181310174799014\n",
      "57 Train Loss 1.1600283 Test MSE 5213.9861531206 Test RE 0.6047146644872663\n",
      "58 Train Loss 1.0594814 Test MSE 4961.9896152874735 Test RE 0.5899205012303924\n",
      "59 Train Loss 0.98063624 Test MSE 4697.721190611536 Test RE 0.573996418625542\n",
      "60 Train Loss 0.9693276 Test MSE 4639.179162166737 Test RE 0.5704086939750675\n",
      "61 Train Loss 0.9622022 Test MSE 4542.153573953646 Test RE 0.5644123015112965\n",
      "62 Train Loss 0.9152843 Test MSE 4221.126990930852 Test RE 0.5441013114840577\n",
      "63 Train Loss 0.84773797 Test MSE 3911.346900365108 Test RE 0.5237556627819061\n",
      "64 Train Loss 0.8182546 Test MSE 3678.324038977585 Test RE 0.5079144358676111\n",
      "65 Train Loss 0.7942259 Test MSE 3435.1367446161394 Test RE 0.49083732290152765\n",
      "66 Train Loss 0.7761327 Test MSE 3243.4031421040654 Test RE 0.4769425071164079\n",
      "67 Train Loss 0.7687526 Test MSE 3047.1058485995395 Test RE 0.46228450209331623\n",
      "68 Train Loss 0.760152 Test MSE 2824.454092871811 Test RE 0.4450746140644812\n",
      "69 Train Loss 0.7443048 Test MSE 2668.3523204829316 Test RE 0.4326006320981202\n",
      "70 Train Loss 0.729421 Test MSE 2766.4717551704175 Test RE 0.44048252545392874\n",
      "71 Train Loss 0.6857631 Test MSE 2791.6347213696135 Test RE 0.44248123681770757\n",
      "72 Train Loss 0.66785526 Test MSE 2701.166319286137 Test RE 0.4352524522958549\n",
      "73 Train Loss 0.6586298 Test MSE 2606.7146914393516 Test RE 0.4275750090307885\n",
      "74 Train Loss 0.6207752 Test MSE 2503.5886290681756 Test RE 0.41903186411881016\n",
      "75 Train Loss 0.59068495 Test MSE 2449.0709888401743 Test RE 0.41444437599542466\n",
      "76 Train Loss 0.5589843 Test MSE 2318.327034157451 Test RE 0.4032300721691994\n",
      "77 Train Loss 0.53090495 Test MSE 2207.430107465813 Test RE 0.39346766187584004\n",
      "78 Train Loss 0.4999168 Test MSE 2070.3689132626178 Test RE 0.38105655145078504\n",
      "79 Train Loss 0.4825366 Test MSE 1880.6438652271397 Test RE 0.3631774245757476\n",
      "80 Train Loss 0.47079247 Test MSE 1795.805915767284 Test RE 0.35489122625047986\n",
      "81 Train Loss 0.45251453 Test MSE 1755.2830315365102 Test RE 0.3508642672399758\n",
      "82 Train Loss 0.440817 Test MSE 1693.179488845996 Test RE 0.3446014203462094\n",
      "83 Train Loss 0.42931724 Test MSE 1665.7762306947895 Test RE 0.3418014447778929\n",
      "84 Train Loss 0.42452127 Test MSE 1668.8695732374506 Test RE 0.3421186598188367\n",
      "85 Train Loss 0.40294096 Test MSE 1601.6169643682913 Test RE 0.3351543757337268\n",
      "86 Train Loss 0.39588064 Test MSE 1573.5782887038083 Test RE 0.33220773574796103\n",
      "87 Train Loss 0.37825945 Test MSE 1439.5967158023857 Test RE 0.31775031324170927\n",
      "88 Train Loss 0.3371563 Test MSE 1171.0016346907944 Test RE 0.2865789680107049\n",
      "89 Train Loss 0.3080334 Test MSE 1097.1180426022113 Test RE 0.2773909209208301\n",
      "90 Train Loss 0.27482545 Test MSE 1048.1166021304666 Test RE 0.27112549871574204\n",
      "91 Train Loss 0.23612294 Test MSE 917.0858655310893 Test RE 0.2536124466220476\n",
      "92 Train Loss 0.22629601 Test MSE 857.5013144798094 Test RE 0.24523528690734353\n",
      "93 Train Loss 0.20693457 Test MSE 813.092621116345 Test RE 0.23880068798621876\n",
      "94 Train Loss 0.18627764 Test MSE 743.5975460799522 Test RE 0.22836762626456353\n",
      "95 Train Loss 0.16764341 Test MSE 661.0974618380415 Test RE 0.21532691021755163\n",
      "96 Train Loss 0.1637771 Test MSE 618.4173293099072 Test RE 0.2082602518260524\n",
      "97 Train Loss 0.15705146 Test MSE 551.8881738175395 Test RE 0.196739293587331\n",
      "98 Train Loss 0.1550608 Test MSE 528.2902851442554 Test RE 0.19248720923614765\n",
      "99 Train Loss 0.15126896 Test MSE 497.13812402317313 Test RE 0.18672570091324614\n",
      "100 Train Loss 0.134313 Test MSE 417.15895568460263 Test RE 0.17104734976291538\n",
      "101 Train Loss 0.12374236 Test MSE 370.5485902030074 Test RE 0.16120857772248104\n",
      "102 Train Loss 0.11850712 Test MSE 356.4574031790074 Test RE 0.15811365687229573\n",
      "103 Train Loss 0.10756217 Test MSE 289.82540516713124 Test RE 0.14257185150313162\n",
      "104 Train Loss 0.10021277 Test MSE 222.09111205096974 Test RE 0.12480476104449299\n",
      "105 Train Loss 0.09665065 Test MSE 187.886767239463 Test RE 0.11479253873443225\n",
      "106 Train Loss 0.09386511 Test MSE 152.98713444777934 Test RE 0.10358408134861558\n",
      "107 Train Loss 0.0913074 Test MSE 125.42424825208153 Test RE 0.09378995275759516\n",
      "108 Train Loss 0.09075621 Test MSE 125.86947417766324 Test RE 0.09395627118194264\n",
      "109 Train Loss 0.08959619 Test MSE 132.32513837285424 Test RE 0.09633558570261963\n",
      "110 Train Loss 0.08556506 Test MSE 121.62352064107067 Test RE 0.09235796371837902\n",
      "111 Train Loss 0.073227294 Test MSE 114.10348146971617 Test RE 0.08945714044436388\n",
      "112 Train Loss 0.07196683 Test MSE 109.04378932771645 Test RE 0.08745125196339755\n",
      "113 Train Loss 0.06990023 Test MSE 92.22707294642254 Test RE 0.08042568646735085\n",
      "114 Train Loss 0.067550786 Test MSE 94.7438870738904 Test RE 0.08151568137098049\n",
      "115 Train Loss 0.063184 Test MSE 119.47342673270232 Test RE 0.09153795880163654\n",
      "116 Train Loss 0.058300797 Test MSE 113.29574843106653 Test RE 0.08913994670761519\n",
      "117 Train Loss 0.04979831 Test MSE 66.41938996463686 Test RE 0.06825162642196932\n",
      "118 Train Loss 0.0465198 Test MSE 53.858913172555894 Test RE 0.0614602548109775\n",
      "119 Train Loss 0.04533275 Test MSE 42.62247481511171 Test RE 0.054674509450855384\n",
      "120 Train Loss 0.044859283 Test MSE 33.82678749674313 Test RE 0.04870751145364238\n",
      "121 Train Loss 0.044736538 Test MSE 32.08399717733594 Test RE 0.04743618984419232\n",
      "122 Train Loss 0.04452171 Test MSE 30.137591548877477 Test RE 0.04597479856360781\n",
      "123 Train Loss 0.044465777 Test MSE 30.089727748291953 Test RE 0.04593827602019192\n",
      "124 Train Loss 0.044461507 Test MSE 30.162300362806 Test RE 0.04599364131018236\n",
      "125 Train Loss 0.04439812 Test MSE 30.594046517376764 Test RE 0.04632165046034175\n",
      "126 Train Loss 0.0443672 Test MSE 31.03100948554948 Test RE 0.04665127479391467\n",
      "127 Train Loss 0.043749373 Test MSE 36.6145486215931 Test RE 0.050674842323866105\n",
      "128 Train Loss 0.04307209 Test MSE 35.28430963537225 Test RE 0.04974579502296548\n",
      "129 Train Loss 0.042397317 Test MSE 34.04592784985832 Test RE 0.048865027889675575\n",
      "130 Train Loss 0.04128353 Test MSE 38.12653189346511 Test RE 0.05171055689390374\n",
      "131 Train Loss 0.039258238 Test MSE 40.88707795699745 Test RE 0.05354989220008329\n",
      "132 Train Loss 0.036402825 Test MSE 39.97722317737024 Test RE 0.05295072076448688\n",
      "133 Train Loss 0.032012567 Test MSE 41.71357778236413 Test RE 0.05408841862671691\n",
      "134 Train Loss 0.029000465 Test MSE 41.544212356454956 Test RE 0.05397850207143176\n",
      "135 Train Loss 0.027117852 Test MSE 47.186897454679745 Test RE 0.05752760257036457\n",
      "136 Train Loss 0.025633436 Test MSE 44.30446244861582 Test RE 0.05574286670473547\n",
      "137 Train Loss 0.024129845 Test MSE 34.020311719341905 Test RE 0.04884664142541207\n",
      "138 Train Loss 0.022837885 Test MSE 29.629429196165034 Test RE 0.045585550757276244\n",
      "139 Train Loss 0.021257583 Test MSE 24.03489499128501 Test RE 0.041056949479512876\n",
      "140 Train Loss 0.019598156 Test MSE 20.520778676064182 Test RE 0.0379369560940717\n",
      "141 Train Loss 0.017528959 Test MSE 23.2142372766534 Test RE 0.040349928855391785\n",
      "142 Train Loss 0.014364474 Test MSE 16.10649604183592 Test RE 0.03360981321101622\n",
      "143 Train Loss 0.012212476 Test MSE 10.232296111050562 Test RE 0.026788729291155798\n",
      "144 Train Loss 0.010925662 Test MSE 8.91069675118699 Test RE 0.02499892857747711\n",
      "145 Train Loss 0.0099394815 Test MSE 6.777784913759542 Test RE 0.021802658639211783\n",
      "146 Train Loss 0.009131307 Test MSE 3.084845394127657 Test RE 0.014708970103674975\n",
      "147 Train Loss 0.0088888155 Test MSE 2.644549246169022 Test RE 0.013618879997885315\n",
      "148 Train Loss 0.008847103 Test MSE 2.6030548384503076 Test RE 0.013511613772219974\n",
      "149 Train Loss 0.008835625 Test MSE 2.4721350188556133 Test RE 0.013167449367881511\n",
      "150 Train Loss 0.008816504 Test MSE 2.456735559714083 Test RE 0.013126373868101166\n",
      "151 Train Loss 0.008812673 Test MSE 2.439518753527301 Test RE 0.01308029817847473\n",
      "152 Train Loss 0.008800428 Test MSE 2.4312298805119896 Test RE 0.013058057483660486\n",
      "153 Train Loss 0.008798083 Test MSE 2.4127969820763164 Test RE 0.013008462047868707\n",
      "154 Train Loss 0.008789111 Test MSE 2.4125942486803456 Test RE 0.013007915523418567\n",
      "155 Train Loss 0.00878755 Test MSE 2.412371210018556 Test RE 0.013007314233929142\n",
      "156 Train Loss 0.008787552 Test MSE 2.412371210018556 Test RE 0.013007314233929142\n",
      "157 Train Loss 0.008785394 Test MSE 2.3734067326178008 Test RE 0.01290183991045031\n",
      "158 Train Loss 0.00877397 Test MSE 2.3665914702486748 Test RE 0.012883302709012672\n",
      "159 Train Loss 0.008770286 Test MSE 2.3706263343557707 Test RE 0.0128942805728882\n",
      "160 Train Loss 0.008356516 Test MSE 4.1690666946275945 Test RE 0.01709956195825697\n",
      "161 Train Loss 0.007918459 Test MSE 5.834187927575829 Test RE 0.02022813162252442\n",
      "162 Train Loss 0.0077213617 Test MSE 5.828709171779635 Test RE 0.0202186314979747\n",
      "163 Train Loss 0.007646233 Test MSE 5.745296531789845 Test RE 0.020073439257711996\n",
      "164 Train Loss 0.007619211 Test MSE 5.724584281915984 Test RE 0.02003722341734278\n",
      "165 Train Loss 0.0076084444 Test MSE 5.737625963631772 Test RE 0.02006003471796981\n",
      "166 Train Loss 0.0076084444 Test MSE 5.737625963631772 Test RE 0.02006003471796981\n",
      "167 Train Loss 0.0076073525 Test MSE 5.737588641624976 Test RE 0.020059969474786107\n",
      "168 Train Loss 0.007602565 Test MSE 5.737608940491505 Test RE 0.020060004959578426\n",
      "169 Train Loss 0.0076022036 Test MSE 5.749427984078337 Test RE 0.020080655382436086\n",
      "170 Train Loss 0.007595096 Test MSE 5.7714186344893506 Test RE 0.020119021392898662\n",
      "171 Train Loss 0.007588855 Test MSE 5.787102279074845 Test RE 0.02014633924354438\n",
      "172 Train Loss 0.007588855 Test MSE 5.787102279074845 Test RE 0.02014633924354438\n",
      "173 Train Loss 0.007586426 Test MSE 5.7872903212587525 Test RE 0.02014666655164028\n",
      "174 Train Loss 0.007586426 Test MSE 5.7872903212587525 Test RE 0.02014666655164028\n",
      "175 Train Loss 0.0075861085 Test MSE 5.787288490671123 Test RE 0.020146663365326868\n",
      "176 Train Loss 0.0075861085 Test MSE 5.787288490671123 Test RE 0.020146663365326868\n",
      "177 Train Loss 0.0075861085 Test MSE 5.787288490671123 Test RE 0.020146663365326868\n",
      "178 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "179 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "180 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "181 Train Loss 0.0075856945 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "182 Train Loss 0.0075856945 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "183 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "184 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "185 Train Loss 0.0075856945 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "186 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "187 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "188 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "189 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "190 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "191 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "192 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "193 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "194 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "195 Train Loss 0.0075856945 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "196 Train Loss 0.0075856945 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "197 Train Loss 0.0075856945 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "198 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "199 Train Loss 0.007585695 Test MSE 5.787288477814691 Test RE 0.020146663342949015\n",
      "Training time: 100.75\n",
      "Training time: 100.75\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 3.2330444 Test MSE 14208.769853745624 Test RE 0.9982603086263032\n",
      "1 Train Loss 2.8977761 Test MSE 14211.05540936412 Test RE 0.9983405931173938\n",
      "2 Train Loss 2.823295 Test MSE 14211.095077301086 Test RE 0.9983419864721781\n",
      "3 Train Loss 2.8184605 Test MSE 14205.087917265675 Test RE 0.9981309600188322\n",
      "4 Train Loss 2.8158352 Test MSE 14186.13196622977 Test RE 0.9974647607653011\n",
      "5 Train Loss 2.8129818 Test MSE 14173.432277182746 Test RE 0.9970181862994564\n",
      "6 Train Loss 2.8051083 Test MSE 14123.187736536423 Test RE 0.9952494123400354\n",
      "7 Train Loss 2.7973516 Test MSE 14062.596281337828 Test RE 0.9931122026242079\n",
      "8 Train Loss 2.7817886 Test MSE 13984.490259796512 Test RE 0.9903504064318783\n",
      "9 Train Loss 2.7643254 Test MSE 13919.902434853502 Test RE 0.9880607768435534\n",
      "10 Train Loss 2.7521045 Test MSE 13853.293746269908 Test RE 0.9856939372244866\n",
      "11 Train Loss 2.7420526 Test MSE 13754.936076571226 Test RE 0.9821885160555303\n",
      "12 Train Loss 2.7182658 Test MSE 13681.544225491556 Test RE 0.979564692676551\n",
      "13 Train Loss 2.693861 Test MSE 13484.623280407379 Test RE 0.9724896161328169\n",
      "14 Train Loss 2.6728806 Test MSE 13307.899615599239 Test RE 0.9660960843017253\n",
      "15 Train Loss 2.6588871 Test MSE 13222.616098821223 Test RE 0.9629955011586058\n",
      "16 Train Loss 2.608674 Test MSE 12966.009272639289 Test RE 0.9536054570626036\n",
      "17 Train Loss 2.5708096 Test MSE 12735.113919408346 Test RE 0.9450765361220804\n",
      "18 Train Loss 2.5581255 Test MSE 12675.301221419597 Test RE 0.9428545651590778\n",
      "19 Train Loss 2.5473404 Test MSE 12695.80064378731 Test RE 0.9436166837582841\n",
      "20 Train Loss 2.5373564 Test MSE 12697.795758027842 Test RE 0.943690824380825\n",
      "21 Train Loss 2.5151434 Test MSE 12577.732704400842 Test RE 0.9392187290437615\n",
      "22 Train Loss 2.5057015 Test MSE 12465.880096356725 Test RE 0.9350332104702918\n",
      "23 Train Loss 2.5012717 Test MSE 12404.961839513502 Test RE 0.9327457525396955\n",
      "24 Train Loss 2.493004 Test MSE 12305.422337354283 Test RE 0.9289959606761732\n",
      "25 Train Loss 2.4669394 Test MSE 12249.073089735208 Test RE 0.9268664811623614\n",
      "26 Train Loss 2.4579763 Test MSE 12226.372685423705 Test RE 0.9260072324306232\n",
      "27 Train Loss 2.453772 Test MSE 12186.276321464162 Test RE 0.9244875660690473\n",
      "28 Train Loss 2.4346893 Test MSE 12138.807408790062 Test RE 0.9226852419913735\n",
      "29 Train Loss 2.411031 Test MSE 12093.18870743779 Test RE 0.9209498440384437\n",
      "30 Train Loss 2.3879766 Test MSE 11973.850652642775 Test RE 0.9163945174232543\n",
      "31 Train Loss 2.3741817 Test MSE 11843.781254503017 Test RE 0.911403627101447\n",
      "32 Train Loss 2.294765 Test MSE 11275.898571969357 Test RE 0.8892853627902577\n",
      "33 Train Loss 2.2574987 Test MSE 11224.436248534872 Test RE 0.8872537274934255\n",
      "34 Train Loss 2.2505574 Test MSE 11254.928112196874 Test RE 0.8884580494983486\n",
      "35 Train Loss 2.240535 Test MSE 11187.116605933781 Test RE 0.8857775036421405\n",
      "36 Train Loss 2.2224302 Test MSE 10933.43714379848 Test RE 0.8756769537831863\n",
      "37 Train Loss 2.1819363 Test MSE 10800.950587435844 Test RE 0.8703552500900926\n",
      "38 Train Loss 2.125099 Test MSE 10474.919519858324 Test RE 0.8571185833160094\n",
      "39 Train Loss 2.109706 Test MSE 10444.37774523487 Test RE 0.855868118709034\n",
      "40 Train Loss 2.1051948 Test MSE 10411.93982271811 Test RE 0.8545380168090279\n",
      "41 Train Loss 2.0736442 Test MSE 10317.218654979733 Test RE 0.850642115856985\n",
      "42 Train Loss 2.0592132 Test MSE 10295.6987962284 Test RE 0.8497545096930228\n",
      "43 Train Loss 2.0504718 Test MSE 10232.597481114759 Test RE 0.8471464769716517\n",
      "44 Train Loss 2.0221043 Test MSE 9998.783819974566 Test RE 0.8374119482313354\n",
      "45 Train Loss 1.9454231 Test MSE 9675.533375192483 Test RE 0.8237644037447233\n",
      "46 Train Loss 1.8831786 Test MSE 9253.248691432551 Test RE 0.8055874298413921\n",
      "47 Train Loss 1.8144639 Test MSE 8937.543892979866 Test RE 0.7917255448596556\n",
      "48 Train Loss 1.7659974 Test MSE 8763.620992411788 Test RE 0.7839842848798722\n",
      "49 Train Loss 1.7492218 Test MSE 8651.059699801093 Test RE 0.7789332060342432\n",
      "50 Train Loss 1.7280917 Test MSE 8544.465588487054 Test RE 0.7741195157770797\n",
      "51 Train Loss 1.7049866 Test MSE 8395.999897891554 Test RE 0.7673646275273301\n",
      "52 Train Loss 1.6799995 Test MSE 8192.59792440789 Test RE 0.7580125294783285\n",
      "53 Train Loss 1.6527998 Test MSE 8082.185964642507 Test RE 0.7528873206225772\n",
      "54 Train Loss 1.624777 Test MSE 7917.264328277392 Test RE 0.7451661804148448\n",
      "55 Train Loss 1.6095641 Test MSE 7853.742456382327 Test RE 0.7421708480097742\n",
      "56 Train Loss 1.5992076 Test MSE 7820.181157828488 Test RE 0.7405833956233691\n",
      "57 Train Loss 1.566693 Test MSE 7563.275699083021 Test RE 0.7283171383011894\n",
      "58 Train Loss 1.5436174 Test MSE 7348.74355378281 Test RE 0.7179134830968631\n",
      "59 Train Loss 1.5283747 Test MSE 7192.417019827736 Test RE 0.7102365102557173\n",
      "60 Train Loss 1.5106401 Test MSE 6932.412780978206 Test RE 0.6972809028406624\n",
      "61 Train Loss 1.4694909 Test MSE 6679.517822455279 Test RE 0.6844443144553133\n",
      "62 Train Loss 1.4465815 Test MSE 6552.493673928507 Test RE 0.677905049616368\n",
      "63 Train Loss 1.4133857 Test MSE 6335.209490067721 Test RE 0.666570445619003\n",
      "64 Train Loss 1.3625412 Test MSE 6206.604131733208 Test RE 0.6597700345318298\n",
      "65 Train Loss 1.3459904 Test MSE 6212.979699038521 Test RE 0.6601088130729151\n",
      "66 Train Loss 1.3106332 Test MSE 5945.565056421156 Test RE 0.6457466049394762\n",
      "67 Train Loss 1.2347629 Test MSE 5477.2971137969835 Test RE 0.6197959208105204\n",
      "68 Train Loss 1.1034898 Test MSE 5120.751360179527 Test RE 0.5992836211329721\n",
      "69 Train Loss 1.0478771 Test MSE 4913.444391268826 Test RE 0.587027688688311\n",
      "70 Train Loss 0.9367671 Test MSE 4089.5964278828556 Test RE 0.5355571110115152\n",
      "71 Train Loss 0.9152605 Test MSE 3960.420703264251 Test RE 0.5270310772122209\n",
      "72 Train Loss 0.9078551 Test MSE 3949.505663659507 Test RE 0.5263043193064003\n",
      "73 Train Loss 0.89468884 Test MSE 3937.392030648475 Test RE 0.5254965785390384\n",
      "74 Train Loss 0.8843067 Test MSE 3946.7931879546836 Test RE 0.5261235583456599\n",
      "75 Train Loss 0.8800002 Test MSE 3952.1298555531075 Test RE 0.5264791379172832\n",
      "76 Train Loss 0.87633735 Test MSE 3938.542321063853 Test RE 0.5255733336006149\n",
      "77 Train Loss 0.84530544 Test MSE 3774.48353831716 Test RE 0.5145106059144697\n",
      "78 Train Loss 0.81949574 Test MSE 3578.5965345819386 Test RE 0.5009817809154388\n",
      "79 Train Loss 0.80119026 Test MSE 3390.539371509248 Test RE 0.48764071533193304\n",
      "80 Train Loss 0.7340783 Test MSE 3044.0526268574336 Test RE 0.4620528378662195\n",
      "81 Train Loss 0.67178994 Test MSE 2777.388064542075 Test RE 0.441350726873221\n",
      "82 Train Loss 0.65480006 Test MSE 2621.970932135307 Test RE 0.42882441131251203\n",
      "83 Train Loss 0.64179015 Test MSE 2439.126260472012 Test RE 0.41360207104200847\n",
      "84 Train Loss 0.6306395 Test MSE 2319.5304130768586 Test RE 0.4033347113355586\n",
      "85 Train Loss 0.622673 Test MSE 2257.677409132835 Test RE 0.39792067756433713\n",
      "86 Train Loss 0.603281 Test MSE 2176.478084728577 Test RE 0.39069937188441195\n",
      "87 Train Loss 0.58125716 Test MSE 2154.352854576829 Test RE 0.3887084502565485\n",
      "88 Train Loss 0.55103207 Test MSE 1895.18215992959 Test RE 0.3645784914048164\n",
      "89 Train Loss 0.47365052 Test MSE 1508.8930124523042 Test RE 0.32530803282117426\n",
      "90 Train Loss 0.3953042 Test MSE 1261.6872136613922 Test RE 0.2974687953120175\n",
      "91 Train Loss 0.3429796 Test MSE 1175.1234464454374 Test RE 0.2870828899953199\n",
      "92 Train Loss 0.3247003 Test MSE 1149.28237847483 Test RE 0.2839088546649793\n",
      "93 Train Loss 0.32238722 Test MSE 1142.0589127027822 Test RE 0.28301523681749724\n",
      "94 Train Loss 0.3115428 Test MSE 1158.6652752058778 Test RE 0.28506543398426687\n",
      "95 Train Loss 0.3061825 Test MSE 1169.1987981040356 Test RE 0.2863582791393891\n",
      "96 Train Loss 0.2942019 Test MSE 1065.32120337787 Test RE 0.27334167365229445\n",
      "97 Train Loss 0.284943 Test MSE 988.86204542593 Test RE 0.26335005766943115\n",
      "98 Train Loss 0.28383735 Test MSE 986.8438564156789 Test RE 0.26308118211994763\n",
      "99 Train Loss 0.28228122 Test MSE 971.098670134004 Test RE 0.2609740007878511\n",
      "100 Train Loss 0.274651 Test MSE 929.0345424498344 Test RE 0.2552592534038536\n",
      "101 Train Loss 0.25881708 Test MSE 836.5316152267892 Test RE 0.24221818372805667\n",
      "102 Train Loss 0.2463288 Test MSE 703.5193391563798 Test RE 0.2221281419836429\n",
      "103 Train Loss 0.24126282 Test MSE 639.4226149847486 Test RE 0.21176762202763813\n",
      "104 Train Loss 0.23925021 Test MSE 607.156979296184 Test RE 0.20635550519209694\n",
      "105 Train Loss 0.23686418 Test MSE 590.9812080163668 Test RE 0.20358810476373765\n",
      "106 Train Loss 0.22561347 Test MSE 633.8390719668104 Test RE 0.21084099995573646\n",
      "107 Train Loss 0.21786098 Test MSE 632.0657219715063 Test RE 0.2105458487183353\n",
      "108 Train Loss 0.21423736 Test MSE 596.4561885551934 Test RE 0.204528973285285\n",
      "109 Train Loss 0.1987453 Test MSE 518.3178260698768 Test RE 0.19066177702647713\n",
      "110 Train Loss 0.18825603 Test MSE 480.5648778601499 Test RE 0.18358685300051344\n",
      "111 Train Loss 0.17741588 Test MSE 477.58342946861603 Test RE 0.18301647598113638\n",
      "112 Train Loss 0.16361642 Test MSE 455.91741947046967 Test RE 0.17881693914042318\n",
      "113 Train Loss 0.14818251 Test MSE 405.53628357163575 Test RE 0.1686476998001554\n",
      "114 Train Loss 0.13212745 Test MSE 366.3645543523846 Test RE 0.16029585353110748\n",
      "115 Train Loss 0.11916015 Test MSE 342.0931134220633 Test RE 0.15489511790114266\n",
      "116 Train Loss 0.11052456 Test MSE 299.6335794901952 Test RE 0.14496421395672934\n",
      "117 Train Loss 0.10843109 Test MSE 277.39810117979266 Test RE 0.13948172370912193\n",
      "118 Train Loss 0.108043306 Test MSE 273.60230764813673 Test RE 0.13852413335419042\n",
      "119 Train Loss 0.10793431 Test MSE 273.05902674227775 Test RE 0.1383865341625404\n",
      "120 Train Loss 0.1079341 Test MSE 272.89816348894306 Test RE 0.13834576534424825\n",
      "121 Train Loss 0.10788165 Test MSE 272.7142314283799 Test RE 0.1382991352928252\n",
      "122 Train Loss 0.107827306 Test MSE 272.59346059747554 Test RE 0.13826850918473718\n",
      "123 Train Loss 0.10780271 Test MSE 272.4350180631836 Test RE 0.13822831967086005\n",
      "124 Train Loss 0.10777476 Test MSE 272.4059614237997 Test RE 0.13822094808264512\n",
      "125 Train Loss 0.10772395 Test MSE 272.28547261063477 Test RE 0.13819037621136543\n",
      "126 Train Loss 0.10768699 Test MSE 272.20799121050607 Test RE 0.13817071312477758\n",
      "127 Train Loss 0.105974525 Test MSE 266.3048749589389 Test RE 0.13666431285951258\n",
      "128 Train Loss 0.09790753 Test MSE 252.15219035582348 Test RE 0.1329832480854721\n",
      "129 Train Loss 0.086794704 Test MSE 223.8505694029643 Test RE 0.12529815192925867\n",
      "130 Train Loss 0.07900864 Test MSE 161.861889319166 Test RE 0.10654617579454717\n",
      "131 Train Loss 0.07045341 Test MSE 121.52152015170046 Test RE 0.09231922724112922\n",
      "132 Train Loss 0.06381285 Test MSE 100.90021768953608 Test RE 0.0841223924090209\n",
      "133 Train Loss 0.060261887 Test MSE 76.21280651337179 Test RE 0.07311046415225735\n",
      "134 Train Loss 0.059214383 Test MSE 68.81570529054666 Test RE 0.06947192715707584\n",
      "135 Train Loss 0.058134995 Test MSE 69.48289874019602 Test RE 0.06980789267461346\n",
      "136 Train Loss 0.05691144 Test MSE 77.56462387136008 Test RE 0.07375600909039204\n",
      "137 Train Loss 0.05549995 Test MSE 84.87739873571945 Test RE 0.07715455918778484\n",
      "138 Train Loss 0.054723717 Test MSE 88.06753656226563 Test RE 0.07859112180934294\n",
      "139 Train Loss 0.053750213 Test MSE 81.62090702899383 Test RE 0.07565998869714748\n",
      "140 Train Loss 0.052437924 Test MSE 66.4154939192545 Test RE 0.06824962463202747\n",
      "141 Train Loss 0.05231201 Test MSE 65.48352889107363 Test RE 0.0677690818005249\n",
      "142 Train Loss 0.05231201 Test MSE 65.48352889107363 Test RE 0.0677690818005249\n",
      "143 Train Loss 0.052312013 Test MSE 65.48352889107363 Test RE 0.0677690818005249\n",
      "144 Train Loss 0.05231201 Test MSE 65.48352889107363 Test RE 0.0677690818005249\n",
      "145 Train Loss 0.05231201 Test MSE 65.48352889107363 Test RE 0.0677690818005249\n",
      "146 Train Loss 0.05231201 Test MSE 65.48352889107363 Test RE 0.0677690818005249\n",
      "147 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "148 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "149 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "150 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "151 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "152 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "153 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "154 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "155 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "156 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "157 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "158 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "159 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "160 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "161 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "162 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "163 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "164 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "165 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "166 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "167 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "168 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "169 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "170 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "171 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "172 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "173 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "174 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "175 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "176 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "177 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "178 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "179 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "180 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "181 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "182 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "183 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "184 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "185 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "186 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "187 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "188 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "189 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "190 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "191 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "192 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "193 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "194 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "195 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "196 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "197 Train Loss 0.05230885 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "198 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "199 Train Loss 0.052308846 Test MSE 65.48354497831014 Test RE 0.06776909012488676\n",
      "Training time: 97.72\n",
      "Training time: 97.72\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 3.1180499 Test MSE 14249.022238684645 Test RE 0.9996733070934043\n",
      "1 Train Loss 2.8456354 Test MSE 14252.145529702915 Test RE 0.999782861964344\n",
      "2 Train Loss 2.828823 Test MSE 14250.29190240159 Test RE 0.9997178442073411\n",
      "3 Train Loss 2.8273842 Test MSE 14250.014251973702 Test RE 0.9997081049879214\n",
      "4 Train Loss 2.8268745 Test MSE 14249.757712434544 Test RE 0.9996991061965418\n",
      "5 Train Loss 2.8260834 Test MSE 14245.7390410566 Test RE 0.9995581302742268\n",
      "6 Train Loss 2.8257024 Test MSE 14243.232043279075 Test RE 0.999470174140057\n",
      "7 Train Loss 2.8233454 Test MSE 14223.542335739914 Test RE 0.9987791062286967\n",
      "8 Train Loss 2.8213632 Test MSE 14217.98302553826 Test RE 0.9985838993876683\n",
      "9 Train Loss 2.8197033 Test MSE 14207.843466795906 Test RE 0.998227765682552\n",
      "10 Train Loss 2.8169277 Test MSE 14170.363636208947 Test RE 0.9969102499712564\n",
      "11 Train Loss 2.802223 Test MSE 14107.196003413543 Test RE 0.9946857906134677\n",
      "12 Train Loss 2.7985797 Test MSE 14100.953295828689 Test RE 0.9944656823921117\n",
      "13 Train Loss 2.7958105 Test MSE 14077.285529996496 Test RE 0.9936307507020926\n",
      "14 Train Loss 2.7897627 Test MSE 14012.00625496386 Test RE 0.9913242383377389\n",
      "15 Train Loss 2.7656603 Test MSE 13921.052590630045 Test RE 0.9881015961072518\n",
      "16 Train Loss 2.7565918 Test MSE 13857.224970309493 Test RE 0.9858337851518242\n",
      "17 Train Loss 2.7477026 Test MSE 13801.14567084003 Test RE 0.9838369598412845\n",
      "18 Train Loss 2.7330396 Test MSE 13700.71606812548 Test RE 0.9802507806409458\n",
      "19 Train Loss 2.7157867 Test MSE 13624.10515416643 Test RE 0.977506282306725\n",
      "20 Train Loss 2.6969075 Test MSE 13512.972932954412 Test RE 0.9735113453617211\n",
      "21 Train Loss 2.6666791 Test MSE 13275.344461231425 Test RE 0.9649136788677891\n",
      "22 Train Loss 2.6436822 Test MSE 13215.452787276838 Test RE 0.9627346158970634\n",
      "23 Train Loss 2.6346807 Test MSE 13180.454627622117 Test RE 0.9614589775352469\n",
      "24 Train Loss 2.617161 Test MSE 13127.00155989611 Test RE 0.9595074081482134\n",
      "25 Train Loss 2.609341 Test MSE 13106.304043174652 Test RE 0.9587506757419585\n",
      "26 Train Loss 2.5965564 Test MSE 13036.18662358418 Test RE 0.9561826261821598\n",
      "27 Train Loss 2.5722215 Test MSE 12902.939920096107 Test RE 0.9512833626829307\n",
      "28 Train Loss 2.5684638 Test MSE 12826.88743995243 Test RE 0.9484756931748987\n",
      "29 Train Loss 2.561176 Test MSE 12735.972500910277 Test RE 0.9451083933775851\n",
      "30 Train Loss 2.5409422 Test MSE 12625.255762414734 Test RE 0.9409914040337736\n",
      "31 Train Loss 2.48351 Test MSE 12479.16235449506 Test RE 0.9355312116528709\n",
      "32 Train Loss 2.4642978 Test MSE 12366.088551250286 Test RE 0.9312831384018583\n",
      "33 Train Loss 2.456336 Test MSE 12267.60192615982 Test RE 0.9275672389923949\n",
      "34 Train Loss 2.4307804 Test MSE 12103.04619416963 Test RE 0.9213251132027329\n",
      "35 Train Loss 2.3980763 Test MSE 11926.833582569356 Test RE 0.9145935694331843\n",
      "36 Train Loss 2.3722274 Test MSE 11676.140006603231 Test RE 0.9049304680648187\n",
      "37 Train Loss 2.343288 Test MSE 11439.769638919912 Test RE 0.8957239845681236\n",
      "38 Train Loss 2.2931664 Test MSE 11275.657911820635 Test RE 0.8892758727842844\n",
      "39 Train Loss 2.2337887 Test MSE 10912.682838488185 Test RE 0.8748454357755855\n",
      "40 Train Loss 2.2011042 Test MSE 10720.872472113268 Test RE 0.8671228461127887\n",
      "41 Train Loss 2.1749947 Test MSE 10521.7768341237 Test RE 0.8590335126185086\n",
      "42 Train Loss 2.1500158 Test MSE 10255.391406662427 Test RE 0.8480894952128404\n",
      "43 Train Loss 2.0459 Test MSE 9829.785535563446 Test RE 0.8303048696884369\n",
      "44 Train Loss 1.9949197 Test MSE 9644.92746251806 Test RE 0.822460494704545\n",
      "45 Train Loss 1.9761369 Test MSE 9427.045936352422 Test RE 0.8131176260411754\n",
      "46 Train Loss 1.9322829 Test MSE 8814.271906179943 Test RE 0.7862466093568095\n",
      "47 Train Loss 1.8381037 Test MSE 8676.956418616011 Test RE 0.7800981928402463\n",
      "48 Train Loss 1.798931 Test MSE 8620.306197628785 Test RE 0.7775474653252106\n",
      "49 Train Loss 1.7699436 Test MSE 8568.254432016487 Test RE 0.7751963887520416\n",
      "50 Train Loss 1.760735 Test MSE 8606.47953039828 Test RE 0.7769236357137453\n",
      "51 Train Loss 1.7480177 Test MSE 8566.871198124565 Test RE 0.7751338135095478\n",
      "52 Train Loss 1.7431285 Test MSE 8495.974867916324 Test RE 0.771919786499055\n",
      "53 Train Loss 1.7415507 Test MSE 8486.697680709332 Test RE 0.7714982221418544\n",
      "54 Train Loss 1.7368218 Test MSE 8447.75725867622 Test RE 0.7697262132548773\n",
      "55 Train Loss 1.7266221 Test MSE 8378.268534785624 Test RE 0.766553907398922\n",
      "56 Train Loss 1.7081506 Test MSE 8324.041869919121 Test RE 0.7640691971151913\n",
      "57 Train Loss 1.6842403 Test MSE 8236.89582791224 Test RE 0.7600590779846861\n",
      "58 Train Loss 1.6579872 Test MSE 7983.061114446289 Test RE 0.7482561426379761\n",
      "59 Train Loss 1.6207935 Test MSE 7761.782285753259 Test RE 0.7378129814712397\n",
      "60 Train Loss 1.5603498 Test MSE 7512.056030190936 Test RE 0.725846810872362\n",
      "61 Train Loss 1.543853 Test MSE 7421.992701943165 Test RE 0.7214825394703235\n",
      "62 Train Loss 1.540021 Test MSE 7381.984242520262 Test RE 0.7195353258843654\n",
      "63 Train Loss 1.5280018 Test MSE 7234.085765271028 Test RE 0.7122908908867617\n",
      "64 Train Loss 1.441659 Test MSE 6834.813151696409 Test RE 0.6923550861887593\n",
      "65 Train Loss 1.4188263 Test MSE 6687.571657130812 Test RE 0.6848568248232012\n",
      "66 Train Loss 1.3972641 Test MSE 6521.823376810477 Test RE 0.6763166515792453\n",
      "67 Train Loss 1.3187693 Test MSE 6080.748906082452 Test RE 0.6530464890677372\n",
      "68 Train Loss 1.220517 Test MSE 5558.121636731619 Test RE 0.6243521149295818\n",
      "69 Train Loss 1.1873282 Test MSE 5300.882337082366 Test RE 0.6097329232979906\n",
      "70 Train Loss 1.1626112 Test MSE 4979.738244073292 Test RE 0.5909746080177521\n",
      "71 Train Loss 1.1273806 Test MSE 4836.649432363152 Test RE 0.5824221307655925\n",
      "72 Train Loss 1.1106437 Test MSE 4745.99318880025 Test RE 0.5769379659449055\n",
      "73 Train Loss 1.0949272 Test MSE 4500.8170096394215 Test RE 0.5618381712589929\n",
      "74 Train Loss 1.0858086 Test MSE 4387.374692449668 Test RE 0.5547124670238449\n",
      "75 Train Loss 1.0790995 Test MSE 4365.942140264313 Test RE 0.5533559087421658\n",
      "76 Train Loss 1.0640486 Test MSE 4092.147372585062 Test RE 0.5357241157053144\n",
      "77 Train Loss 1.0266922 Test MSE 3645.2623506820237 Test RE 0.5056266530447703\n",
      "78 Train Loss 0.9966483 Test MSE 3480.9779371843483 Test RE 0.49410153045382504\n",
      "79 Train Loss 0.9732783 Test MSE 3181.388996190239 Test RE 0.47236091049446427\n",
      "80 Train Loss 0.8593715 Test MSE 2648.296753994322 Test RE 0.43097183392278804\n",
      "81 Train Loss 0.81521404 Test MSE 2407.8216897309335 Test RE 0.4109393454927209\n",
      "82 Train Loss 0.7823338 Test MSE 2380.5182576080606 Test RE 0.4086027846954635\n",
      "83 Train Loss 0.7543756 Test MSE 2451.6120944510476 Test RE 0.41465932972579345\n",
      "84 Train Loss 0.7245667 Test MSE 2402.4945836086063 Test RE 0.4104845091432957\n",
      "85 Train Loss 0.7017047 Test MSE 2255.539731689591 Test RE 0.39773224774217236\n",
      "86 Train Loss 0.68689 Test MSE 2267.760874793651 Test RE 0.39880830422017094\n",
      "87 Train Loss 0.631977 Test MSE 2198.1887570520585 Test RE 0.3926431768330729\n",
      "88 Train Loss 0.5387477 Test MSE 2006.6361553222782 Test RE 0.37514562004292995\n",
      "89 Train Loss 0.4977217 Test MSE 1856.3963517659492 Test RE 0.3608285697539304\n",
      "90 Train Loss 0.4793636 Test MSE 1854.6814456957377 Test RE 0.3606618677249766\n",
      "91 Train Loss 0.4775018 Test MSE 1878.5570682676944 Test RE 0.3629758744803803\n",
      "92 Train Loss 0.4761737 Test MSE 1867.1103114402356 Test RE 0.36186831023998695\n",
      "93 Train Loss 0.46787432 Test MSE 1807.7749207873771 Test RE 0.35607193309157154\n",
      "94 Train Loss 0.45679846 Test MSE 1851.5577237759896 Test RE 0.3603580198182058\n",
      "95 Train Loss 0.44735056 Test MSE 1853.6099073717714 Test RE 0.360557666852682\n",
      "96 Train Loss 0.4411289 Test MSE 1824.3009949512082 Test RE 0.35769577568417404\n",
      "97 Train Loss 0.4354941 Test MSE 1851.0487004659503 Test RE 0.36030848227518447\n",
      "98 Train Loss 0.4265781 Test MSE 1826.9198605725096 Test RE 0.35795242774551406\n",
      "99 Train Loss 0.42064643 Test MSE 1777.273808956176 Test RE 0.35305529900183674\n",
      "100 Train Loss 0.412839 Test MSE 1796.76084445746 Test RE 0.3549855712936874\n",
      "101 Train Loss 0.40029073 Test MSE 1793.7962506292376 Test RE 0.35469259337830217\n",
      "102 Train Loss 0.3851909 Test MSE 1688.0077080280328 Test RE 0.3440747289838651\n",
      "103 Train Loss 0.37724602 Test MSE 1584.3639483481352 Test RE 0.33334430478249283\n",
      "104 Train Loss 0.35691798 Test MSE 1500.2102704855822 Test RE 0.3243707096233039\n",
      "105 Train Loss 0.34593844 Test MSE 1432.7398678431493 Test RE 0.31699268225257055\n",
      "106 Train Loss 0.32535163 Test MSE 1259.0938563554332 Test RE 0.2971629193024277\n",
      "107 Train Loss 0.31222895 Test MSE 1146.6619763769872 Test RE 0.28358500914312296\n",
      "108 Train Loss 0.305831 Test MSE 1102.6519859151072 Test RE 0.2780896309471568\n",
      "109 Train Loss 0.29598516 Test MSE 1039.7378684312253 Test RE 0.2700396240126219\n",
      "110 Train Loss 0.28572088 Test MSE 990.2458076831188 Test RE 0.2635342524595789\n",
      "111 Train Loss 0.28190023 Test MSE 981.8081026701052 Test RE 0.26240908670307705\n",
      "112 Train Loss 0.27550232 Test MSE 1025.7775637857258 Test RE 0.26822061973880396\n",
      "113 Train Loss 0.2721229 Test MSE 1023.83402536048 Test RE 0.2679664007645256\n",
      "114 Train Loss 0.2695886 Test MSE 971.8339199264328 Test RE 0.26107277796659173\n",
      "115 Train Loss 0.26710325 Test MSE 944.5803107942224 Test RE 0.2573860518520486\n",
      "116 Train Loss 0.2610524 Test MSE 891.1580488107616 Test RE 0.250001682071625\n",
      "117 Train Loss 0.25418591 Test MSE 826.5505662053921 Test RE 0.2407688384676356\n",
      "118 Train Loss 0.24190484 Test MSE 728.5350236814469 Test RE 0.22604285357809564\n",
      "119 Train Loss 0.23365837 Test MSE 707.3216429493772 Test RE 0.222727599955103\n",
      "120 Train Loss 0.22299409 Test MSE 687.4006190865472 Test RE 0.21956874684725705\n",
      "121 Train Loss 0.21837275 Test MSE 665.6465876706873 Test RE 0.21606649093545635\n",
      "122 Train Loss 0.21434462 Test MSE 641.6079918566604 Test RE 0.2121291961272446\n",
      "123 Train Loss 0.21064943 Test MSE 643.0026615821489 Test RE 0.2123596246543708\n",
      "124 Train Loss 0.20218816 Test MSE 698.896862817137 Test RE 0.22139719245175454\n",
      "125 Train Loss 0.19059664 Test MSE 729.489257605428 Test RE 0.2261908404066129\n",
      "126 Train Loss 0.18200065 Test MSE 715.571095549254 Test RE 0.2240226645800359\n",
      "127 Train Loss 0.17716908 Test MSE 698.1578144222066 Test RE 0.22128010327367992\n",
      "128 Train Loss 0.17475244 Test MSE 668.5332950668686 Test RE 0.21653449151518483\n",
      "129 Train Loss 0.16949692 Test MSE 670.7992109367692 Test RE 0.21690114034031976\n",
      "130 Train Loss 0.16361646 Test MSE 642.7095096009398 Test RE 0.21231121059206262\n",
      "131 Train Loss 0.15662938 Test MSE 553.1187260899449 Test RE 0.19695850754611602\n",
      "132 Train Loss 0.14680982 Test MSE 470.7224373353263 Test RE 0.1816971074834162\n",
      "133 Train Loss 0.13872667 Test MSE 416.0642086674639 Test RE 0.17082276320318665\n",
      "134 Train Loss 0.13500217 Test MSE 394.8400759335285 Test RE 0.16640875719484302\n",
      "135 Train Loss 0.13259883 Test MSE 383.692967316333 Test RE 0.16404291701176116\n",
      "136 Train Loss 0.13026392 Test MSE 372.28555923519394 Test RE 0.1615859734749652\n",
      "137 Train Loss 0.1297006 Test MSE 358.7146414493298 Test RE 0.1586134879397821\n",
      "138 Train Loss 0.1273769 Test MSE 323.92585196898165 Test RE 0.1507260675730719\n",
      "139 Train Loss 0.124991484 Test MSE 297.06344989262476 Test RE 0.14434115426169233\n",
      "140 Train Loss 0.12329267 Test MSE 266.59608931070363 Test RE 0.13673901621391513\n",
      "141 Train Loss 0.12187335 Test MSE 244.76356479650272 Test RE 0.13102040833913284\n",
      "142 Train Loss 0.11982222 Test MSE 233.49097220267092 Test RE 0.12796777253549188\n",
      "143 Train Loss 0.11902957 Test MSE 223.2844026447318 Test RE 0.12513959847785885\n",
      "144 Train Loss 0.118614264 Test MSE 211.82044026161958 Test RE 0.12188478537909689\n",
      "145 Train Loss 0.11861427 Test MSE 211.82044026161958 Test RE 0.12188478537909689\n",
      "146 Train Loss 0.11861426 Test MSE 211.82044023552407 Test RE 0.121884785371589\n",
      "147 Train Loss 0.11861098 Test MSE 211.80260970410708 Test RE 0.12187965528067454\n",
      "148 Train Loss 0.11844219 Test MSE 204.860799892544 Test RE 0.11986571928080546\n",
      "149 Train Loss 0.11814346 Test MSE 203.22172666358045 Test RE 0.11938523874514978\n",
      "150 Train Loss 0.11814346 Test MSE 203.22172666358045 Test RE 0.11938523874514978\n",
      "151 Train Loss 0.118143074 Test MSE 203.2208474680487 Test RE 0.11938498049746207\n",
      "152 Train Loss 0.11813659 Test MSE 202.65008265373856 Test RE 0.11921721065666874\n",
      "153 Train Loss 0.118020006 Test MSE 201.05530848540147 Test RE 0.11874718849986704\n",
      "154 Train Loss 0.11783093 Test MSE 196.2726754933358 Test RE 0.11732632973552994\n",
      "155 Train Loss 0.11782747 Test MSE 195.9437199652163 Test RE 0.11722796828577856\n",
      "156 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "157 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "158 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "159 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "160 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "161 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "162 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "163 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "164 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "165 Train Loss 0.117728904 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "166 Train Loss 0.117724165 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "167 Train Loss 0.117724165 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "168 Train Loss 0.117724165 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "169 Train Loss 0.117724165 Test MSE 196.00138015806195 Test RE 0.11724521530462358\n",
      "170 Train Loss 0.11772199 Test MSE 196.00138015220037 Test RE 0.11724521530287041\n",
      "171 Train Loss 0.117717475 Test MSE 195.80642947515818 Test RE 0.1171868924468273\n",
      "172 Train Loss 0.1175716 Test MSE 194.64519978254478 Test RE 0.11683888738685508\n",
      "173 Train Loss 0.11757159 Test MSE 194.64519978254478 Test RE 0.11683888738685508\n",
      "174 Train Loss 0.11756848 Test MSE 194.31162069113375 Test RE 0.11673872636846375\n",
      "175 Train Loss 0.11756848 Test MSE 194.31162069113375 Test RE 0.11673872636846375\n",
      "176 Train Loss 0.11756499 Test MSE 194.3115302628257 Test RE 0.11673869920465672\n",
      "177 Train Loss 0.11753761 Test MSE 193.6192281476206 Test RE 0.11653055261982147\n",
      "178 Train Loss 0.11681615 Test MSE 190.82233666777276 Test RE 0.11568583046038161\n",
      "179 Train Loss 0.116250806 Test MSE 183.1129170543437 Test RE 0.11332482428240474\n",
      "180 Train Loss 0.11618045 Test MSE 182.1200188419471 Test RE 0.11301716449985615\n",
      "181 Train Loss 0.116179064 Test MSE 182.11948164739843 Test RE 0.11301699781787952\n",
      "182 Train Loss 0.116179064 Test MSE 182.11948164739843 Test RE 0.11301699781787952\n",
      "183 Train Loss 0.116179064 Test MSE 182.11948164739843 Test RE 0.11301699781787952\n",
      "184 Train Loss 0.116179064 Test MSE 182.11948164739843 Test RE 0.11301699781787952\n",
      "185 Train Loss 0.11616891 Test MSE 181.909422077936 Test RE 0.11295180119187329\n",
      "186 Train Loss 0.11616645 Test MSE 180.02336204084858 Test RE 0.11236472618560923\n",
      "187 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "188 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "189 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "190 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "191 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "192 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "193 Train Loss 0.115916654 Test MSE 177.10768281031153 Test RE 0.11145107562412779\n",
      "194 Train Loss 0.11591647 Test MSE 177.10775360695135 Test RE 0.11145109789973082\n",
      "195 Train Loss 0.11590907 Test MSE 177.10778898568122 Test RE 0.11145110903136682\n",
      "196 Train Loss 0.115904175 Test MSE 177.10774174838338 Test RE 0.11145109416852647\n",
      "197 Train Loss 0.11590341 Test MSE 177.10774174838338 Test RE 0.11145109416852647\n",
      "198 Train Loss 0.11590341 Test MSE 177.10774174838338 Test RE 0.11145109416852647\n",
      "199 Train Loss 0.11590341 Test MSE 177.10774174838338 Test RE 0.11145109416852647\n",
      "Training time: 103.48\n",
      "Training time: 103.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 2.8948452 Test MSE 14252.493419553866 Test RE 0.9997950640635869\n",
      "1 Train Loss 2.8337412 Test MSE 14250.486671081904 Test RE 0.9997246761045828\n",
      "2 Train Loss 2.8273032 Test MSE 14250.712075469079 Test RE 0.9997325825516432\n",
      "3 Train Loss 2.8263986 Test MSE 14250.72632165267 Test RE 0.9997330822589656\n",
      "4 Train Loss 2.8249068 Test MSE 14240.046309546362 Test RE 0.9993583938959413\n",
      "5 Train Loss 2.8243864 Test MSE 14238.173537135026 Test RE 0.9992926766810125\n",
      "6 Train Loss 2.8226974 Test MSE 14227.13203413902 Test RE 0.9989051328361892\n",
      "7 Train Loss 2.819981 Test MSE 14205.052537885473 Test RE 0.9981297170389605\n",
      "8 Train Loss 2.8182232 Test MSE 14186.391165528688 Test RE 0.9974738732205614\n",
      "9 Train Loss 2.811625 Test MSE 14164.477503968628 Test RE 0.9967031785367492\n",
      "10 Train Loss 2.808747 Test MSE 14150.620197260194 Test RE 0.9962155148965792\n",
      "11 Train Loss 2.8020444 Test MSE 14093.5420715722 Test RE 0.9942043108131153\n",
      "12 Train Loss 2.7971659 Test MSE 14076.326250194024 Test RE 0.9935968952351796\n",
      "13 Train Loss 2.7914605 Test MSE 14051.824258950173 Test RE 0.9927317651822546\n",
      "14 Train Loss 2.7804804 Test MSE 13987.52889742157 Test RE 0.9904579953571117\n",
      "15 Train Loss 2.775429 Test MSE 13978.10686891145 Test RE 0.9901243518807431\n",
      "16 Train Loss 2.7715344 Test MSE 13933.593318149919 Test RE 0.9885465597097708\n",
      "17 Train Loss 2.7600014 Test MSE 13878.4351862327 Test RE 0.9865879676221216\n",
      "18 Train Loss 2.7457662 Test MSE 13808.41483542976 Test RE 0.9840960228024106\n",
      "19 Train Loss 2.7400506 Test MSE 13744.38813217476 Test RE 0.9818118492099712\n",
      "20 Train Loss 2.730707 Test MSE 13697.99572075095 Test RE 0.9801534589022395\n",
      "21 Train Loss 2.717025 Test MSE 13624.648073415878 Test RE 0.9775257588772618\n",
      "22 Train Loss 2.7041588 Test MSE 13554.274130745674 Test RE 0.9749979356652569\n",
      "23 Train Loss 2.6918414 Test MSE 13500.479839555994 Test RE 0.9730612230761799\n",
      "24 Train Loss 2.6847315 Test MSE 13458.041405403295 Test RE 0.9715306219844483\n",
      "25 Train Loss 2.6690247 Test MSE 13374.255194728255 Test RE 0.9685016537297562\n",
      "26 Train Loss 2.6631455 Test MSE 13362.37606116096 Test RE 0.9680714422460236\n",
      "27 Train Loss 2.6536517 Test MSE 13256.760338935186 Test RE 0.9642380521814561\n",
      "28 Train Loss 2.633119 Test MSE 13130.79058853137 Test RE 0.9596458761265249\n",
      "29 Train Loss 2.5763414 Test MSE 12895.227562170736 Test RE 0.9509990191739995\n",
      "30 Train Loss 2.5455816 Test MSE 12724.80503921438 Test RE 0.9446939461710635\n",
      "31 Train Loss 2.4970505 Test MSE 12487.593684228612 Test RE 0.9358471960129665\n",
      "32 Train Loss 2.4523807 Test MSE 12202.90699143811 Test RE 0.9251181773093059\n",
      "33 Train Loss 2.4386067 Test MSE 12122.153814598558 Test RE 0.9220520949710277\n",
      "34 Train Loss 2.4069417 Test MSE 11956.503436194414 Test RE 0.9157304597040082\n",
      "35 Train Loss 2.4022224 Test MSE 11939.561379372852 Test RE 0.9150814464986592\n",
      "36 Train Loss 2.3792434 Test MSE 11815.409823225646 Test RE 0.9103113522270865\n",
      "37 Train Loss 2.3615165 Test MSE 11731.311392939799 Test RE 0.9070659096670557\n",
      "38 Train Loss 2.34098 Test MSE 11569.189574433349 Test RE 0.9007764689002171\n",
      "39 Train Loss 2.3175793 Test MSE 11451.77033283963 Test RE 0.8961936833512998\n",
      "40 Train Loss 2.2953439 Test MSE 11316.479027933992 Test RE 0.8908841353824426\n",
      "41 Train Loss 2.2821653 Test MSE 11154.14733269884 Test RE 0.8844713141422352\n",
      "42 Train Loss 2.246219 Test MSE 10973.734165160293 Test RE 0.8772891969427253\n",
      "43 Train Loss 2.2140653 Test MSE 10930.686997958454 Test RE 0.8755668150062239\n",
      "44 Train Loss 2.1910226 Test MSE 10845.414072169604 Test RE 0.8721448741358697\n",
      "45 Train Loss 2.1615722 Test MSE 10628.148254436297 Test RE 0.8633648546566348\n",
      "46 Train Loss 2.1352808 Test MSE 10455.110314132797 Test RE 0.8563077478189001\n",
      "47 Train Loss 2.1145637 Test MSE 10259.76125210729 Test RE 0.8482701623940703\n",
      "48 Train Loss 2.0934103 Test MSE 9985.59973509299 Test RE 0.8368596734634847\n",
      "49 Train Loss 1.9751892 Test MSE 9534.102535539563 Test RE 0.8177216059560076\n",
      "50 Train Loss 1.9304515 Test MSE 9296.835684246333 Test RE 0.8074825416067829\n",
      "51 Train Loss 1.9214053 Test MSE 9230.62203127081 Test RE 0.804601888968809\n",
      "52 Train Loss 1.9132123 Test MSE 9242.35568392553 Test RE 0.8051131178285829\n",
      "53 Train Loss 1.8942494 Test MSE 9196.743119630039 Test RE 0.8031239769576376\n",
      "54 Train Loss 1.8786505 Test MSE 8997.627210566932 Test RE 0.7943823048227387\n",
      "55 Train Loss 1.869526 Test MSE 8873.79213263899 Test RE 0.7888967907749783\n",
      "56 Train Loss 1.8632447 Test MSE 8925.371320668955 Test RE 0.7911862121183305\n",
      "57 Train Loss 1.8478492 Test MSE 8838.364956035035 Test RE 0.7873204445983307\n",
      "58 Train Loss 1.7934953 Test MSE 8475.992411763043 Test RE 0.7710114779183368\n",
      "59 Train Loss 1.7180299 Test MSE 8182.17448776881 Test RE 0.7575301665950785\n",
      "60 Train Loss 1.6840551 Test MSE 7988.770326599409 Test RE 0.7485236584120816\n",
      "61 Train Loss 1.6705906 Test MSE 7802.343167171123 Test RE 0.7397382705939836\n",
      "62 Train Loss 1.6654917 Test MSE 7681.372700135931 Test RE 0.7339812784031828\n",
      "63 Train Loss 1.6523244 Test MSE 7442.794470608408 Test RE 0.7224928888677128\n",
      "64 Train Loss 1.6427569 Test MSE 7310.913370553325 Test RE 0.7160632456394616\n",
      "65 Train Loss 1.6204431 Test MSE 7075.512834678583 Test RE 0.7044408384192443\n",
      "66 Train Loss 1.6001487 Test MSE 6854.5976934680975 Test RE 0.6933564324866692\n",
      "67 Train Loss 1.5796642 Test MSE 6600.532496097697 Test RE 0.6803855014528736\n",
      "68 Train Loss 1.4876466 Test MSE 6357.466630749323 Test RE 0.6677403315133007\n",
      "69 Train Loss 1.4572412 Test MSE 6365.273191147005 Test RE 0.6681501768400827\n",
      "70 Train Loss 1.4183316 Test MSE 6307.41932852357 Test RE 0.6651068429060308\n",
      "71 Train Loss 1.3787433 Test MSE 6407.512604938103 Test RE 0.6703634052045744\n",
      "72 Train Loss 1.3548032 Test MSE 6367.881583705632 Test RE 0.6682870617302128\n",
      "73 Train Loss 1.2799937 Test MSE 5820.512591521756 Test RE 0.6389195546838845\n",
      "74 Train Loss 1.2502608 Test MSE 5743.110618249006 Test RE 0.634657116783343\n",
      "75 Train Loss 1.2416623 Test MSE 5775.609653735692 Test RE 0.6364502780706979\n",
      "76 Train Loss 1.2240933 Test MSE 5653.2659697211 Test RE 0.6296732920833841\n",
      "77 Train Loss 1.2042402 Test MSE 5406.21420590291 Test RE 0.615761013609137\n",
      "78 Train Loss 1.1826731 Test MSE 5199.263589506245 Test RE 0.6038603043598715\n",
      "79 Train Loss 1.1751859 Test MSE 5165.390619991886 Test RE 0.6018900286274759\n",
      "80 Train Loss 1.162127 Test MSE 4899.106546556348 Test RE 0.5861705648139658\n",
      "81 Train Loss 1.1330943 Test MSE 4700.131850891193 Test RE 0.5741436743737952\n",
      "82 Train Loss 1.1194658 Test MSE 4594.404027591047 Test RE 0.5676493641883985\n",
      "83 Train Loss 1.0919948 Test MSE 4278.009642166229 Test RE 0.5477551169593816\n",
      "84 Train Loss 1.0715423 Test MSE 4070.1389219442285 Test RE 0.5342815536025257\n",
      "85 Train Loss 1.0486361 Test MSE 3802.0935499244324 Test RE 0.516388976820595\n",
      "86 Train Loss 1.0299239 Test MSE 3643.6111926266626 Test RE 0.5055121257483924\n",
      "87 Train Loss 1.0174193 Test MSE 3465.8462301941368 Test RE 0.4930264385451722\n",
      "88 Train Loss 0.9904176 Test MSE 3335.8620807074576 Test RE 0.4836927825578106\n",
      "89 Train Loss 0.95938647 Test MSE 3388.330111548876 Test RE 0.4874818172125493\n",
      "90 Train Loss 0.93998635 Test MSE 3441.3648457768245 Test RE 0.4912820795968201\n",
      "91 Train Loss 0.9199025 Test MSE 3532.994154153552 Test RE 0.49777951837462414\n",
      "92 Train Loss 0.8392528 Test MSE 3651.1303017902387 Test RE 0.5060334549600821\n",
      "93 Train Loss 0.78664136 Test MSE 3443.387094203123 Test RE 0.4914264043848031\n",
      "94 Train Loss 0.750857 Test MSE 3149.6759693579415 Test RE 0.4700006972774418\n",
      "95 Train Loss 0.7381873 Test MSE 2981.566802480749 Test RE 0.45728592670204804\n",
      "96 Train Loss 0.70533717 Test MSE 2780.3938346130544 Test RE 0.44158948358419603\n",
      "97 Train Loss 0.68040466 Test MSE 2803.626617752152 Test RE 0.44343059149527736\n",
      "98 Train Loss 0.6602139 Test MSE 2645.385030967218 Test RE 0.4307348484482767\n",
      "99 Train Loss 0.64502925 Test MSE 2533.6082823701927 Test RE 0.42153661017861466\n",
      "100 Train Loss 0.6177162 Test MSE 2434.563656160491 Test RE 0.41321505009352605\n",
      "101 Train Loss 0.60483587 Test MSE 2283.172818074696 Test RE 0.4001611812922895\n",
      "102 Train Loss 0.5972354 Test MSE 2187.0095613393387 Test RE 0.391643483355405\n",
      "103 Train Loss 0.59263843 Test MSE 2104.2721876024225 Test RE 0.38416387315172\n",
      "104 Train Loss 0.5893143 Test MSE 2047.3743995110888 Test RE 0.3789345442474262\n",
      "105 Train Loss 0.58428395 Test MSE 2005.4257671513465 Test RE 0.37503246043660876\n",
      "106 Train Loss 0.5640683 Test MSE 1962.7560786834147 Test RE 0.3710212030274224\n",
      "107 Train Loss 0.5530319 Test MSE 1954.7320202760998 Test RE 0.37026202954138865\n",
      "108 Train Loss 0.52739745 Test MSE 1815.121891595729 Test RE 0.3567947546900485\n",
      "109 Train Loss 0.51717186 Test MSE 1721.4327240106752 Test RE 0.3474646209192416\n",
      "110 Train Loss 0.49252552 Test MSE 1652.3032181925646 Test RE 0.34041637139510206\n",
      "111 Train Loss 0.469539 Test MSE 1631.8479117984148 Test RE 0.33830265316100105\n",
      "112 Train Loss 0.4564396 Test MSE 1553.175102281898 Test RE 0.33004698804273686\n",
      "113 Train Loss 0.43651444 Test MSE 1490.7765502019029 Test RE 0.3233492367183768\n",
      "114 Train Loss 0.41903025 Test MSE 1483.994231411039 Test RE 0.32261285619429064\n",
      "115 Train Loss 0.39693537 Test MSE 1449.7185503982953 Test RE 0.31886541115243355\n",
      "116 Train Loss 0.39068142 Test MSE 1466.3684227739996 Test RE 0.32069125237568646\n",
      "117 Train Loss 0.38360494 Test MSE 1479.2071888767796 Test RE 0.3220920964472736\n",
      "118 Train Loss 0.38023767 Test MSE 1477.5226455585832 Test RE 0.32190864254878276\n",
      "119 Train Loss 0.37754852 Test MSE 1461.7704657815025 Test RE 0.320188076549607\n",
      "120 Train Loss 0.36657923 Test MSE 1457.3893286431082 Test RE 0.3197078915363497\n",
      "121 Train Loss 0.35581917 Test MSE 1503.6411038219821 Test RE 0.32474139976617816\n",
      "122 Train Loss 0.34712407 Test MSE 1513.8036594106563 Test RE 0.3258369554377942\n",
      "123 Train Loss 0.33660161 Test MSE 1466.6863214562293 Test RE 0.32072601233102144\n",
      "124 Train Loss 0.32860112 Test MSE 1437.7201488001303 Test RE 0.31754314612639445\n",
      "125 Train Loss 0.3166334 Test MSE 1417.6461587664205 Test RE 0.3153185250988621\n",
      "126 Train Loss 0.29751503 Test MSE 1311.9527669188972 Test RE 0.30333649457339257\n",
      "127 Train Loss 0.28454542 Test MSE 1221.34271332052 Test RE 0.2926741303718929\n",
      "128 Train Loss 0.26123542 Test MSE 1114.918711513102 Test RE 0.27963219121271043\n",
      "129 Train Loss 0.2413897 Test MSE 1031.3702941301553 Test RE 0.2689508201840376\n",
      "130 Train Loss 0.21416278 Test MSE 846.6994115967065 Test RE 0.24368578311931963\n",
      "131 Train Loss 0.19765493 Test MSE 701.7399049418865 Test RE 0.22184704617864884\n",
      "132 Train Loss 0.18315625 Test MSE 584.0835785435664 Test RE 0.20239652973928876\n",
      "133 Train Loss 0.17678589 Test MSE 508.23438681632956 Test RE 0.18879808576689816\n",
      "134 Train Loss 0.16888303 Test MSE 420.89528108110807 Test RE 0.17181164345258573\n",
      "135 Train Loss 0.16268477 Test MSE 359.23403070207513 Test RE 0.15872827603649114\n",
      "136 Train Loss 0.16093308 Test MSE 331.47658684232994 Test RE 0.1524726656047336\n",
      "137 Train Loss 0.15707009 Test MSE 283.2518162939065 Test RE 0.14094572744659045\n",
      "138 Train Loss 0.15308154 Test MSE 246.46873287491945 Test RE 0.13147599915145947\n",
      "139 Train Loss 0.1462289 Test MSE 265.9833295732159 Test RE 0.13658178141503552\n",
      "140 Train Loss 0.13863403 Test MSE 304.58550746797664 Test RE 0.14615718887173257\n",
      "141 Train Loss 0.13319361 Test MSE 268.32097780508 Test RE 0.13718065674993765\n",
      "142 Train Loss 0.12738302 Test MSE 244.43021605570704 Test RE 0.13093115819082068\n",
      "143 Train Loss 0.115370184 Test MSE 271.22207786572267 Test RE 0.1379202650733576\n",
      "144 Train Loss 0.10208206 Test MSE 276.5275095032906 Test RE 0.13926267561033445\n",
      "145 Train Loss 0.09417184 Test MSE 230.91471772997969 Test RE 0.12725983941808933\n",
      "146 Train Loss 0.09128392 Test MSE 200.3940115744159 Test RE 0.1185517402238849\n",
      "147 Train Loss 0.089846015 Test MSE 185.90444392428242 Test RE 0.11418536631056327\n",
      "148 Train Loss 0.08802724 Test MSE 167.11740454190524 Test RE 0.10826208947987025\n",
      "149 Train Loss 0.0868272 Test MSE 171.75419870011146 Test RE 0.10975371896146395\n",
      "150 Train Loss 0.08459164 Test MSE 179.9854851025652 Test RE 0.11235290478742606\n",
      "151 Train Loss 0.083114445 Test MSE 181.7583193712328 Test RE 0.11290487984937363\n",
      "152 Train Loss 0.082865655 Test MSE 183.08514057999008 Test RE 0.11331622881162524\n",
      "153 Train Loss 0.08286272 Test MSE 183.0852128167236 Test RE 0.11331625116623413\n",
      "154 Train Loss 0.082838535 Test MSE 183.0940583869801 Test RE 0.11331898851115382\n",
      "155 Train Loss 0.082704246 Test MSE 184.95922902967695 Test RE 0.11389471363338347\n",
      "156 Train Loss 0.082503796 Test MSE 187.26673893957872 Test RE 0.11460297392362911\n",
      "157 Train Loss 0.08241771 Test MSE 189.52029795758645 Test RE 0.11529047513500462\n",
      "158 Train Loss 0.082323894 Test MSE 193.22073295980374 Test RE 0.11641057284588222\n",
      "159 Train Loss 0.08137281 Test MSE 190.11166960986233 Test RE 0.11547020895812439\n",
      "160 Train Loss 0.07918997 Test MSE 167.52929526088855 Test RE 0.10839542301108893\n",
      "161 Train Loss 0.075659886 Test MSE 130.94628342784983 Test RE 0.09583235309204835\n",
      "162 Train Loss 0.07507909 Test MSE 126.77248102419613 Test RE 0.09429269719258146\n",
      "163 Train Loss 0.07226805 Test MSE 141.02310241682687 Test RE 0.0994513530349395\n",
      "164 Train Loss 0.065558344 Test MSE 112.62692492855959 Test RE 0.08887644547063131\n",
      "165 Train Loss 0.06286928 Test MSE 97.5774039437066 Test RE 0.08272565100344637\n",
      "166 Train Loss 0.058861427 Test MSE 84.97187795173463 Test RE 0.07719748860696889\n",
      "167 Train Loss 0.050868932 Test MSE 70.1416452694902 Test RE 0.07013802589662049\n",
      "168 Train Loss 0.04027591 Test MSE 70.62742680945979 Test RE 0.07038048505638261\n",
      "169 Train Loss 0.03697369 Test MSE 65.42861461808252 Test RE 0.0677406603756567\n",
      "170 Train Loss 0.033778265 Test MSE 55.019383996669184 Test RE 0.06211885264703728\n",
      "171 Train Loss 0.032593627 Test MSE 48.709761808149445 Test RE 0.05844852646068166\n",
      "172 Train Loss 0.029462814 Test MSE 43.438794897293164 Test RE 0.05519559863196837\n",
      "173 Train Loss 0.026892575 Test MSE 36.3555405545474 Test RE 0.05049528955688049\n",
      "174 Train Loss 0.025399063 Test MSE 31.6193037997838 Test RE 0.047091412302003956\n",
      "175 Train Loss 0.024716964 Test MSE 28.557479434549403 Test RE 0.04475334487264411\n",
      "176 Train Loss 0.023655377 Test MSE 26.39932869752218 Test RE 0.043029074539856854\n",
      "177 Train Loss 0.020711072 Test MSE 25.464932391273898 Test RE 0.04226071373255944\n",
      "178 Train Loss 0.016002987 Test MSE 22.27917672439448 Test RE 0.039528936762378196\n",
      "179 Train Loss 0.014271765 Test MSE 18.79578966729429 Test RE 0.03630745869613143\n",
      "180 Train Loss 0.012429437 Test MSE 15.649815684195202 Test RE 0.03312990398627686\n",
      "181 Train Loss 0.011756846 Test MSE 16.689684503243203 Test RE 0.03421287945818968\n",
      "182 Train Loss 0.0111956885 Test MSE 17.568380061638333 Test RE 0.03510196451037782\n",
      "183 Train Loss 0.010784711 Test MSE 16.867107761249084 Test RE 0.03439425236514826\n",
      "184 Train Loss 0.010682413 Test MSE 16.744177762074916 Test RE 0.03426868794101259\n",
      "185 Train Loss 0.010656072 Test MSE 16.68802582307136 Test RE 0.034211179317159515\n",
      "186 Train Loss 0.01059734 Test MSE 16.545889847071336 Test RE 0.034065175295627956\n",
      "187 Train Loss 0.010594787 Test MSE 16.49102129185734 Test RE 0.034008645876524767\n",
      "188 Train Loss 0.0105369175 Test MSE 16.016993987845872 Test RE 0.033516300073597564\n",
      "189 Train Loss 0.009781656 Test MSE 9.735781137648754 Test RE 0.026130695244336807\n",
      "190 Train Loss 0.009702507 Test MSE 9.331504506802192 Test RE 0.025582406694856858\n",
      "191 Train Loss 0.009702508 Test MSE 9.331504506802192 Test RE 0.025582406694856858\n",
      "192 Train Loss 0.009700894 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "193 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "194 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "195 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "196 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "197 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "198 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "199 Train Loss 0.009700841 Test MSE 9.331487666008845 Test RE 0.02558238361025038\n",
      "Training time: 115.24\n",
      "Training time: 115.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4.102753 Test MSE 14265.427098987622 Test RE 1.0002486021282821\n",
      "1 Train Loss 2.9271173 Test MSE 14258.409501316815 Test RE 1.0000025455019637\n",
      "2 Train Loss 2.8378136 Test MSE 14258.060998572824 Test RE 0.999990324441802\n",
      "3 Train Loss 2.8290555 Test MSE 14257.641676202173 Test RE 0.9999756197285682\n",
      "4 Train Loss 2.8280828 Test MSE 14256.668006413922 Test RE 0.9999414744366808\n",
      "5 Train Loss 2.827655 Test MSE 14256.459201855094 Test RE 0.999934151789382\n",
      "6 Train Loss 2.8273964 Test MSE 14256.705695094235 Test RE 0.9999427961497781\n",
      "7 Train Loss 2.8273916 Test MSE 14256.717235072132 Test RE 0.9999432008475981\n",
      "8 Train Loss 2.8273873 Test MSE 14256.723733491728 Test RE 0.9999434287419178\n",
      "9 Train Loss 2.8273842 Test MSE 14256.72835396759 Test RE 0.9999435907782381\n",
      "10 Train Loss 2.8273826 Test MSE 14256.728651821153 Test RE 0.9999436012237186\n",
      "11 Train Loss 2.8273792 Test MSE 14256.726491936477 Test RE 0.9999435254783288\n",
      "12 Train Loss 2.827376 Test MSE 14256.718145816521 Test RE 0.9999432327866704\n",
      "13 Train Loss 2.8273711 Test MSE 14256.704047010862 Test RE 0.9999427383527956\n",
      "14 Train Loss 2.827365 Test MSE 14256.677816928473 Test RE 0.9999418184840662\n",
      "15 Train Loss 2.8273573 Test MSE 14256.639120684013 Test RE 0.9999404614351165\n",
      "16 Train Loss 2.8270242 Test MSE 14254.761792947782 Test RE 0.9998746227143143\n",
      "17 Train Loss 2.8268485 Test MSE 14253.665569908282 Test RE 0.9998361756785928\n",
      "18 Train Loss 2.8268416 Test MSE 14253.592138332118 Test RE 0.9998336002133387\n",
      "19 Train Loss 2.8268375 Test MSE 14253.533433382812 Test RE 0.9998315412502128\n",
      "20 Train Loss 2.8268332 Test MSE 14253.480313269847 Test RE 0.99982967816082\n",
      "21 Train Loss 2.8268294 Test MSE 14253.434114413429 Test RE 0.9998280578188345\n",
      "22 Train Loss 2.8268263 Test MSE 14253.392004775624 Test RE 0.9998265808965665\n",
      "23 Train Loss 2.8268223 Test MSE 14253.355306823203 Test RE 0.9998252937779668\n",
      "24 Train Loss 2.8268185 Test MSE 14253.326357807147 Test RE 0.9998242784390328\n",
      "25 Train Loss 2.8268156 Test MSE 14253.299114842215 Test RE 0.9998233229360823\n",
      "26 Train Loss 2.8268125 Test MSE 14253.283252553425 Test RE 0.9998227665915277\n",
      "27 Train Loss 2.8268096 Test MSE 14253.269513197623 Test RE 0.9998222847052214\n",
      "28 Train Loss 2.8268058 Test MSE 14253.263792904158 Test RE 0.999822084074852\n",
      "29 Train Loss 2.826803 Test MSE 14253.255955339202 Test RE 0.9998218091844228\n",
      "30 Train Loss 2.8268003 Test MSE 14253.255883759688 Test RE 0.999821806673882\n",
      "31 Train Loss 2.8267975 Test MSE 14253.254446858598 Test RE 0.9998217562767976\n",
      "32 Train Loss 2.8267941 Test MSE 14253.252850098737 Test RE 0.9998217002729108\n",
      "33 Train Loss 2.82679 Test MSE 14253.255586895039 Test RE 0.9998217962618133\n",
      "34 Train Loss 2.826786 Test MSE 14253.246961150113 Test RE 0.9998214937271036\n",
      "35 Train Loss 2.826781 Test MSE 14253.248055401124 Test RE 0.9998215321062766\n",
      "36 Train Loss 2.8267758 Test MSE 14253.242129438826 Test RE 0.9998213242622364\n",
      "37 Train Loss 2.8267665 Test MSE 14253.221265776987 Test RE 0.9998205925009481\n",
      "38 Train Loss 2.8262665 Test MSE 14249.972702411129 Test RE 0.9997066475344758\n",
      "39 Train Loss 2.8255255 Test MSE 14242.514664931998 Test RE 0.9994450040351384\n",
      "40 Train Loss 2.8241794 Test MSE 14224.473146560875 Test RE 0.998811786526211\n",
      "41 Train Loss 2.8224726 Test MSE 14215.261293382051 Test RE 0.9984883159287747\n",
      "42 Train Loss 2.8210723 Test MSE 14204.044447264037 Test RE 0.9980942992516442\n",
      "43 Train Loss 2.814391 Test MSE 14160.48670690306 Test RE 0.9965627596497688\n",
      "44 Train Loss 2.8055797 Test MSE 14138.006747698313 Test RE 0.9957714172032965\n",
      "45 Train Loss 2.8012295 Test MSE 14095.76130225393 Test RE 0.9942825836082003\n",
      "46 Train Loss 2.798746 Test MSE 14081.325982061464 Test RE 0.9937733360526553\n",
      "47 Train Loss 2.7924614 Test MSE 14068.430345032348 Test RE 0.9933181844691215\n",
      "48 Train Loss 2.7861316 Test MSE 14028.631855976571 Test RE 0.9919121796707259\n",
      "49 Train Loss 2.7803802 Test MSE 13985.518704275515 Test RE 0.9903868218343559\n",
      "50 Train Loss 2.772381 Test MSE 13928.87170574499 Test RE 0.988379053414876\n",
      "51 Train Loss 2.765415 Test MSE 13890.398237135516 Test RE 0.9870130897481885\n",
      "52 Train Loss 2.7576165 Test MSE 13892.114738362137 Test RE 0.9870740727672939\n",
      "53 Train Loss 2.7496073 Test MSE 13822.650136563476 Test RE 0.9846031517798839\n",
      "54 Train Loss 2.7469866 Test MSE 13796.533762340241 Test RE 0.9836725624328988\n",
      "55 Train Loss 2.7229218 Test MSE 13679.7806378108 Test RE 0.9795015563861214\n",
      "56 Train Loss 2.713023 Test MSE 13597.000906465128 Test RE 0.9765334564761463\n",
      "57 Train Loss 2.685854 Test MSE 13446.870336798807 Test RE 0.9711273207899109\n",
      "58 Train Loss 2.6802857 Test MSE 13421.486216584355 Test RE 0.9702102725561514\n",
      "59 Train Loss 2.6779697 Test MSE 13408.614096223786 Test RE 0.9697449120510442\n",
      "60 Train Loss 2.6701765 Test MSE 13395.462340061822 Test RE 0.9692692112273382\n",
      "61 Train Loss 2.6642597 Test MSE 13388.18504275253 Test RE 0.9690058900254545\n",
      "62 Train Loss 2.653991 Test MSE 13325.87148688714 Test RE 0.9667482043600502\n",
      "63 Train Loss 2.639372 Test MSE 13226.001343872513 Test RE 0.9631187659914827\n",
      "64 Train Loss 2.6317086 Test MSE 13153.796482609263 Test RE 0.9604861853762623\n",
      "65 Train Loss 2.6290114 Test MSE 13091.75940826762 Test RE 0.958218544433177\n",
      "66 Train Loss 2.6147647 Test MSE 12985.446400627437 Test RE 0.9543199563978436\n",
      "67 Train Loss 2.5835333 Test MSE 12842.412538704199 Test RE 0.9490495161653484\n",
      "68 Train Loss 2.551635 Test MSE 12658.755613654646 Test RE 0.9422389902020901\n",
      "69 Train Loss 2.5289783 Test MSE 12452.0896347378 Test RE 0.9345158740430648\n",
      "70 Train Loss 2.4764657 Test MSE 12322.210923371305 Test RE 0.9296294705270798\n",
      "71 Train Loss 2.45344 Test MSE 12169.529574850516 Test RE 0.9238521184032602\n",
      "72 Train Loss 2.443846 Test MSE 12078.441289842194 Test RE 0.920388132176233\n",
      "73 Train Loss 2.3963654 Test MSE 11853.374299639047 Test RE 0.9117726547900173\n",
      "74 Train Loss 2.3384418 Test MSE 11665.783403386935 Test RE 0.904529047521361\n",
      "75 Train Loss 2.3282762 Test MSE 11564.422663841608 Test RE 0.9005908740810948\n",
      "76 Train Loss 2.3145812 Test MSE 11494.601829301635 Test RE 0.8978680747591241\n",
      "77 Train Loss 2.2966166 Test MSE 11418.13217372533 Test RE 0.8948764863077221\n",
      "78 Train Loss 2.2773132 Test MSE 11256.74064074786 Test RE 0.8885295866390605\n",
      "79 Train Loss 2.2098753 Test MSE 10922.956790232342 Test RE 0.8752571587867893\n",
      "80 Train Loss 2.1617694 Test MSE 10707.567929282795 Test RE 0.8665846317774005\n",
      "81 Train Loss 2.1183236 Test MSE 10502.07561959529 Test RE 0.8582288988429991\n",
      "82 Train Loss 2.1039057 Test MSE 10339.104115170952 Test RE 0.8515438526792064\n",
      "83 Train Loss 2.099946 Test MSE 10245.037443836838 Test RE 0.8476612665743876\n",
      "84 Train Loss 2.0609117 Test MSE 9845.506874008164 Test RE 0.8309685814595204\n",
      "85 Train Loss 2.0211065 Test MSE 9605.472086994776 Test RE 0.8207765140376393\n",
      "86 Train Loss 1.9512081 Test MSE 9299.138600653663 Test RE 0.8075825460444782\n",
      "87 Train Loss 1.9020957 Test MSE 9176.389304308255 Test RE 0.8022347658898633\n",
      "88 Train Loss 1.8746854 Test MSE 9025.094727089301 Test RE 0.7955939065821169\n",
      "89 Train Loss 1.8293316 Test MSE 8723.811036776207 Test RE 0.7822015797215774\n",
      "90 Train Loss 1.7931653 Test MSE 8623.092165936236 Test RE 0.7776731016744869\n",
      "91 Train Loss 1.7690247 Test MSE 8387.116444931573 Test RE 0.7669585620340053\n",
      "92 Train Loss 1.7462872 Test MSE 8166.647122379279 Test RE 0.7568110403146846\n",
      "93 Train Loss 1.7315677 Test MSE 8122.88023818318 Test RE 0.7547803563193294\n",
      "94 Train Loss 1.724468 Test MSE 8050.617057059338 Test RE 0.7514154982101439\n",
      "95 Train Loss 1.6988797 Test MSE 7862.059064420245 Test RE 0.7425636996286115\n",
      "96 Train Loss 1.6095014 Test MSE 7773.763824938831 Test RE 0.7383822274200126\n",
      "97 Train Loss 1.5854896 Test MSE 7725.964649178365 Test RE 0.7361086515838047\n",
      "98 Train Loss 1.5405306 Test MSE 7495.835995103797 Test RE 0.7250627630184288\n",
      "99 Train Loss 1.5263599 Test MSE 7398.576048329068 Test RE 0.7203434886581377\n",
      "100 Train Loss 1.5199109 Test MSE 7414.875370151387 Test RE 0.721136523078653\n",
      "101 Train Loss 1.5089054 Test MSE 7397.804198759651 Test RE 0.7203059130956484\n",
      "102 Train Loss 1.4688451 Test MSE 7053.595055676692 Test RE 0.7033489208110708\n",
      "103 Train Loss 1.4261967 Test MSE 6805.0377872401195 Test RE 0.6908453429319135\n",
      "104 Train Loss 1.3135707 Test MSE 6382.256080911923 Test RE 0.6690409133299148\n",
      "105 Train Loss 1.2637296 Test MSE 6135.497959753208 Test RE 0.6559798085923519\n",
      "106 Train Loss 1.2493136 Test MSE 6037.032215495067 Test RE 0.6506947615633656\n",
      "107 Train Loss 1.2388237 Test MSE 5887.672953903852 Test RE 0.6425950898719852\n",
      "108 Train Loss 1.2101474 Test MSE 5733.260920362307 Test RE 0.6341126502177546\n",
      "109 Train Loss 1.1650922 Test MSE 5702.180828869245 Test RE 0.6323915475311243\n",
      "110 Train Loss 1.1361278 Test MSE 5545.42567140614 Test RE 0.6236386288719791\n",
      "111 Train Loss 1.0904438 Test MSE 5194.784683750295 Test RE 0.6036001505828927\n",
      "112 Train Loss 1.0612609 Test MSE 5014.2787365914755 Test RE 0.5930206271892974\n",
      "113 Train Loss 1.0185152 Test MSE 4725.867292001977 Test RE 0.5757133824371126\n",
      "114 Train Loss 0.9798979 Test MSE 4601.320405428731 Test RE 0.5680764708379714\n",
      "115 Train Loss 0.9332668 Test MSE 4267.220865465369 Test RE 0.5470639850165049\n",
      "116 Train Loss 0.8833327 Test MSE 4117.908253035105 Test RE 0.5374077150683132\n",
      "117 Train Loss 0.8306478 Test MSE 4026.6746106366004 Test RE 0.5314211463926581\n",
      "118 Train Loss 0.79450274 Test MSE 3763.5566733920987 Test RE 0.5137653300970685\n",
      "119 Train Loss 0.7749276 Test MSE 3632.577227367388 Test RE 0.5047461229881163\n",
      "120 Train Loss 0.7580685 Test MSE 3406.1477063630036 Test RE 0.4887618525970971\n",
      "121 Train Loss 0.7250952 Test MSE 3109.594541704401 Test RE 0.46700060845002117\n",
      "122 Train Loss 0.6911365 Test MSE 3101.0585662208155 Test RE 0.46635919924072833\n",
      "123 Train Loss 0.68107575 Test MSE 3080.6770112226177 Test RE 0.4648241113032078\n",
      "124 Train Loss 0.6659374 Test MSE 3020.6613052360217 Test RE 0.4602741449572576\n",
      "125 Train Loss 0.6450489 Test MSE 2973.2904201928554 Test RE 0.4566508070706544\n",
      "126 Train Loss 0.6202641 Test MSE 2769.0750623675726 Test RE 0.44068972830782954\n",
      "127 Train Loss 0.59300566 Test MSE 2494.838255762698 Test RE 0.4182989372525465\n",
      "128 Train Loss 0.554942 Test MSE 2305.4895274771734 Test RE 0.4021120990307482\n",
      "129 Train Loss 0.5365898 Test MSE 2170.3353964817543 Test RE 0.39014764563569454\n",
      "130 Train Loss 0.5235853 Test MSE 2022.0439184771576 Test RE 0.37658312570400426\n",
      "131 Train Loss 0.5135677 Test MSE 1878.9521154348322 Test RE 0.3630140380942405\n",
      "132 Train Loss 0.49633056 Test MSE 1763.1549414808758 Test RE 0.35165014682134266\n",
      "133 Train Loss 0.48728675 Test MSE 1646.5375605364054 Test RE 0.3398219165015661\n",
      "134 Train Loss 0.48460025 Test MSE 1582.8547219706554 Test RE 0.33318549913576234\n",
      "135 Train Loss 0.47794637 Test MSE 1510.4218819058274 Test RE 0.32547279851017175\n",
      "136 Train Loss 0.47182584 Test MSE 1423.4545201705944 Test RE 0.31596382427671\n",
      "137 Train Loss 0.46615207 Test MSE 1333.4008696758046 Test RE 0.30580594974917613\n",
      "138 Train Loss 0.4590742 Test MSE 1305.3396081995772 Test RE 0.3025710147664707\n",
      "139 Train Loss 0.4425813 Test MSE 1188.8452695429887 Test RE 0.2887541474160992\n",
      "140 Train Loss 0.40377122 Test MSE 932.5981735308678 Test RE 0.2557483520961497\n",
      "141 Train Loss 0.33649844 Test MSE 690.2316894150721 Test RE 0.22002043093749651\n",
      "142 Train Loss 0.2758364 Test MSE 446.1595332432285 Test RE 0.17689300201605934\n",
      "143 Train Loss 0.23638436 Test MSE 328.32450365581246 Test RE 0.15174598601750333\n",
      "144 Train Loss 0.21429253 Test MSE 261.2411863452086 Test RE 0.13535876629414134\n",
      "145 Train Loss 0.20118074 Test MSE 197.82596861071718 Test RE 0.1177896724693866\n",
      "146 Train Loss 0.18505928 Test MSE 187.07786326629446 Test RE 0.1145451655411619\n",
      "147 Train Loss 0.15514907 Test MSE 204.2391385323207 Test RE 0.11968371154017052\n",
      "148 Train Loss 0.14163746 Test MSE 197.25320514467535 Test RE 0.1176190312601199\n",
      "149 Train Loss 0.13397345 Test MSE 210.6537793861295 Test RE 0.12154866448700143\n",
      "150 Train Loss 0.120541796 Test MSE 214.62447105183014 Test RE 0.12268887456806321\n",
      "151 Train Loss 0.1028644 Test MSE 197.1012157629343 Test RE 0.11757370807053061\n",
      "152 Train Loss 0.09531394 Test MSE 208.0495321371965 Test RE 0.12079499369941721\n",
      "153 Train Loss 0.08801256 Test MSE 219.76529146645015 Test RE 0.12414954020278375\n",
      "154 Train Loss 0.08119893 Test MSE 200.83326920382845 Test RE 0.11868160002015228\n",
      "155 Train Loss 0.07510629 Test MSE 173.04826023090888 Test RE 0.11016640623884119\n",
      "156 Train Loss 0.07130809 Test MSE 164.29285022698423 Test RE 0.10734328846019596\n",
      "157 Train Loss 0.070013866 Test MSE 158.4024953804045 Test RE 0.10540144702502696\n",
      "158 Train Loss 0.069575496 Test MSE 153.3685303700573 Test RE 0.10371311819421582\n",
      "159 Train Loss 0.068782076 Test MSE 146.5156262704405 Test RE 0.10136955421770735\n",
      "160 Train Loss 0.06802305 Test MSE 148.86074616624919 Test RE 0.10217759106014523\n",
      "161 Train Loss 0.06792232 Test MSE 150.40450821316858 Test RE 0.1027060414489682\n",
      "162 Train Loss 0.06787508 Test MSE 153.19864106030923 Test RE 0.10365565975682052\n",
      "163 Train Loss 0.06753073 Test MSE 156.07422829415304 Test RE 0.10462396190267172\n",
      "164 Train Loss 0.06753073 Test MSE 156.07422829415304 Test RE 0.10462396190267172\n",
      "165 Train Loss 0.06753073 Test MSE 156.07422829415304 Test RE 0.10462396190267172\n",
      "166 Train Loss 0.06753073 Test MSE 156.07422829415304 Test RE 0.10462396190267172\n",
      "167 Train Loss 0.0675295 Test MSE 156.07422433374182 Test RE 0.10462396057524594\n",
      "168 Train Loss 0.06752844 Test MSE 156.3172318463205 Test RE 0.10470537863057235\n",
      "169 Train Loss 0.065561906 Test MSE 152.80427024964752 Test RE 0.10352215626034861\n",
      "170 Train Loss 0.058269158 Test MSE 106.80504410853987 Test RE 0.08654887868853472\n",
      "171 Train Loss 0.053260546 Test MSE 80.33371208414707 Test RE 0.07506102341345265\n",
      "172 Train Loss 0.052250125 Test MSE 68.89220170776765 Test RE 0.06951052937329226\n",
      "173 Train Loss 0.051984593 Test MSE 65.5295069639819 Test RE 0.06779286904204121\n",
      "174 Train Loss 0.051983148 Test MSE 65.52950697125488 Test RE 0.06779286904580331\n",
      "175 Train Loss 0.051978324 Test MSE 65.42739596284727 Test RE 0.06774002951333988\n",
      "176 Train Loss 0.05190149 Test MSE 63.61855465806466 Test RE 0.06679707768101653\n",
      "177 Train Loss 0.05190133 Test MSE 63.61855465806466 Test RE 0.06679707768101653\n",
      "178 Train Loss 0.05190133 Test MSE 63.61855465806466 Test RE 0.06679707768101653\n",
      "179 Train Loss 0.05190133 Test MSE 63.61855465806466 Test RE 0.06679707768101653\n",
      "180 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "181 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "182 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "183 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "184 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "185 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "186 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "187 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "188 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "189 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "190 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "191 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "192 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "193 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "194 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "195 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "196 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "197 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "198 Train Loss 0.05188483 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "199 Train Loss 0.051884823 Test MSE 63.61857720259847 Test RE 0.06679708951646965\n",
      "Training time: 96.38\n",
      "Training time: 96.38\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 3.4008954 Test MSE 14263.98161969372 Test RE 1.0001979245263868\n",
      "1 Train Loss 2.8583047 Test MSE 14257.362245828299 Test RE 0.9999658205999074\n",
      "2 Train Loss 2.832027 Test MSE 14258.560227642849 Test RE 1.0000078310252671\n",
      "3 Train Loss 2.8298154 Test MSE 14257.37326723328 Test RE 0.9999662071028927\n",
      "4 Train Loss 2.8281105 Test MSE 14257.088906687133 Test RE 0.9999562349870863\n",
      "5 Train Loss 2.827697 Test MSE 14257.113853332454 Test RE 0.9999571098340759\n",
      "6 Train Loss 2.8268876 Test MSE 14253.785784811764 Test RE 0.9998403919609008\n",
      "7 Train Loss 2.8268428 Test MSE 14253.365733694853 Test RE 0.9998256594830196\n",
      "8 Train Loss 2.826836 Test MSE 14253.311273386245 Test RE 0.9998237493774699\n",
      "9 Train Loss 2.8268309 Test MSE 14253.275715990576 Test RE 0.9998225022584619\n",
      "10 Train Loss 2.8268273 Test MSE 14253.255847809543 Test RE 0.999821805412986\n",
      "11 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "12 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "13 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "14 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "15 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "16 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "17 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "18 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "19 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "20 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "21 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "22 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "23 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "24 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "25 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "26 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "27 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "28 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "29 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "30 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "31 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "32 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "33 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "34 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "35 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "36 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "37 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "38 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "39 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "40 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "41 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "42 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "43 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "44 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "45 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "46 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "47 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "48 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "49 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "50 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "51 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "52 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "53 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "54 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "55 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "56 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "57 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "58 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "59 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "60 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "61 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "62 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "63 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "64 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "65 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "66 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "67 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "68 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "69 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "70 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "71 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "72 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "73 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "74 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "75 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "76 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "77 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "78 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "79 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "80 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "81 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "82 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "83 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "84 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "85 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "86 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "87 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "88 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "89 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "90 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "91 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "92 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "93 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "94 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "95 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "96 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "97 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "98 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "99 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "100 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "101 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "102 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "103 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "104 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "105 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "106 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "107 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "108 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "109 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "110 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "111 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "112 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "113 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "114 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "115 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "116 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "117 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "118 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "119 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "120 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "121 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "122 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "123 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "124 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "125 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "126 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "127 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "128 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "129 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "130 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "131 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "132 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "133 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "134 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "135 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "136 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "137 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "138 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "139 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "140 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "141 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "142 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "143 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "144 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "145 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "146 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "147 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "148 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "149 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "150 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "151 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "152 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "153 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "154 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "155 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "156 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "157 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "158 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "159 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "160 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "161 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "162 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "163 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "164 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "165 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "166 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "167 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "168 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "169 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "170 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "171 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "172 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "173 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "174 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "175 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "176 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "177 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "178 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "179 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "180 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "181 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "182 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "183 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "184 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "185 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "186 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "187 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "188 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "189 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "190 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "191 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "192 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "193 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "194 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "195 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "196 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "197 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "198 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "199 Train Loss 2.8268244 Test MSE 14253.245041385617 Test RE 0.9998214263943098\n",
      "Training time: 15.76\n",
      "Training time: 15.76\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f35b8387bd0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25567208616517456\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
