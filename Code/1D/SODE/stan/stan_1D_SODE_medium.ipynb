{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 2.4303029 Test MSE 382.2055473287947 Test RE 0.9965774950062527\n",
      "1 Train Loss 2.3773313 Test MSE 382.57359710711506 Test RE 0.9970572131446261\n",
      "2 Train Loss 2.36897 Test MSE 381.04791916257346 Test RE 0.9950671283061951\n",
      "3 Train Loss 2.3324924 Test MSE 374.17663517851156 Test RE 0.9860544903438507\n",
      "4 Train Loss 2.2876937 Test MSE 367.25101781907665 Test RE 0.9768864519219239\n",
      "5 Train Loss 2.2411335 Test MSE 353.9836587211701 Test RE 0.9590785751595624\n",
      "6 Train Loss 2.1023607 Test MSE 334.0861256739399 Test RE 0.9317336973375127\n",
      "7 Train Loss 2.0129035 Test MSE 315.6975999549479 Test RE 0.9057288941986591\n",
      "8 Train Loss 1.8481396 Test MSE 288.7520878931637 Test RE 0.8662139012286334\n",
      "9 Train Loss 1.6996993 Test MSE 262.9494373546239 Test RE 0.8266062895700891\n",
      "10 Train Loss 1.5816531 Test MSE 244.59261228901246 Test RE 0.7972311331102658\n",
      "11 Train Loss 1.3881605 Test MSE 216.71901314659164 Test RE 0.750431558935023\n",
      "12 Train Loss 1.2800635 Test MSE 195.33244844645537 Test RE 0.7124424352509262\n",
      "13 Train Loss 1.0706195 Test MSE 142.358085810221 Test RE 0.6082101344238193\n",
      "14 Train Loss 0.8559645 Test MSE 117.61749527247443 Test RE 0.552838838481005\n",
      "15 Train Loss 0.73256135 Test MSE 104.771661439056 Test RE 0.5217764792303919\n",
      "16 Train Loss 0.46857524 Test MSE 55.628521594546626 Test RE 0.3801993250837928\n",
      "17 Train Loss 0.3678565 Test MSE 46.33849772315066 Test RE 0.3470032621179902\n",
      "18 Train Loss 0.28322613 Test MSE 37.863540159800756 Test RE 0.3136701496264601\n",
      "19 Train Loss 0.19819798 Test MSE 22.249870786336388 Test RE 0.2404508569086512\n",
      "20 Train Loss 0.17713943 Test MSE 17.59349532520309 Test RE 0.21381523534181285\n",
      "21 Train Loss 0.07860082 Test MSE 8.394152185931661 Test RE 0.1476899698660641\n",
      "22 Train Loss 0.052084394 Test MSE 5.772131499634026 Test RE 0.1224702743732963\n",
      "23 Train Loss 0.040697023 Test MSE 2.801319083635001 Test RE 0.08531862986035971\n",
      "24 Train Loss 0.029347457 Test MSE 1.1343277764197517 Test RE 0.05429150314542219\n",
      "25 Train Loss 0.013572382 Test MSE 0.3450758277121849 Test RE 0.029944692087214047\n",
      "26 Train Loss 0.011717268 Test MSE 0.495535245527936 Test RE 0.035883918145770924\n",
      "27 Train Loss 0.008220796 Test MSE 0.3349698412352721 Test RE 0.029502949523098978\n",
      "28 Train Loss 0.0016709006 Test MSE 0.014072484525836036 Test RE 0.006047111443984571\n",
      "29 Train Loss 0.0014215879 Test MSE 0.002581660186450604 Test RE 0.0025900736948864184\n",
      "30 Train Loss 0.001414341 Test MSE 0.0023829460195520964 Test RE 0.002488397089804863\n",
      "31 Train Loss 0.0014080253 Test MSE 0.0023021475872530765 Test RE 0.0024458463093050913\n",
      "32 Train Loss 0.0014026517 Test MSE 0.0022532259594307525 Test RE 0.002419719116650972\n",
      "33 Train Loss 0.0013977139 Test MSE 0.0022851275332374625 Test RE 0.002436788318567126\n",
      "34 Train Loss 0.0013923008 Test MSE 0.0023451327863075262 Test RE 0.0024685748595166144\n",
      "35 Train Loss 0.0013893376 Test MSE 0.002465365032383546 Test RE 0.002531064417704107\n",
      "36 Train Loss 0.0013854972 Test MSE 0.0026166815553975275 Test RE 0.0026075822679740746\n",
      "37 Train Loss 0.0013816969 Test MSE 0.0028849177264133807 Test RE 0.0027379738671020728\n",
      "38 Train Loss 0.0013779895 Test MSE 0.0032151051128624835 Test RE 0.002890414788365654\n",
      "39 Train Loss 0.0013740134 Test MSE 0.0036920668281078713 Test RE 0.003097400464948585\n",
      "40 Train Loss 0.0013688101 Test MSE 0.004269078432430833 Test RE 0.0033306550003680893\n",
      "41 Train Loss 0.0013641617 Test MSE 0.005023490798630444 Test RE 0.0036129785906475957\n",
      "42 Train Loss 0.0013578202 Test MSE 0.005991099785290154 Test RE 0.003945625476045877\n",
      "43 Train Loss 0.0013511456 Test MSE 0.007132013696298566 Test RE 0.004304955502446727\n",
      "44 Train Loss 0.0013432404 Test MSE 0.008594808353718161 Test RE 0.00472585814272251\n",
      "45 Train Loss 0.0013341173 Test MSE 0.010282263400909222 Test RE 0.005169004913922553\n",
      "46 Train Loss 0.0010929402 Test MSE 0.03557163451784715 Test RE 0.009614229685344975\n",
      "47 Train Loss 0.0005677713 Test MSE 0.002950799249524576 Test RE 0.0027690603095805026\n",
      "48 Train Loss 0.0005317457 Test MSE 0.0005904490383137691 Test RE 0.0012386647891693267\n",
      "49 Train Loss 0.0005250033 Test MSE 0.00035070686769652346 Test RE 0.0009546292075657003\n",
      "50 Train Loss 0.0005200826 Test MSE 0.00022251940793418696 Test RE 0.0007604077428049823\n",
      "51 Train Loss 0.0005166686 Test MSE 0.00014721654631289014 Test RE 0.0006185016962588699\n",
      "52 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "53 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "54 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "55 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "56 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "57 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "58 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "59 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "60 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "61 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "62 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "63 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "64 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "65 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "66 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "67 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "68 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "69 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "70 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "71 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "72 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "73 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "74 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "75 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "76 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "77 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "78 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "79 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "80 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "81 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "82 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "83 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "84 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "85 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "86 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "87 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "88 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "89 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "90 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "91 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "92 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "93 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "94 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "95 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "96 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "97 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "98 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "99 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "100 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "101 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "102 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "103 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "104 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "105 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "106 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "107 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "108 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "109 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "110 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "111 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "112 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "113 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "114 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "115 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "116 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "117 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "118 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "119 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "120 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "121 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "122 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "123 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "124 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "125 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "126 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "127 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "128 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "129 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "130 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "131 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "132 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "133 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "134 Train Loss 0.00051405234 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "135 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "136 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "137 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "138 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "139 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "140 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "141 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "142 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "143 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "144 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "145 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "146 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "147 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "148 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "149 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "150 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "151 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "152 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "153 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "154 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "155 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "156 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "157 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "158 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "159 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "160 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "161 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "162 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "163 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "164 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "165 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "166 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "167 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "168 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "169 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "170 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "171 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "172 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "173 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "174 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "175 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "176 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "177 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "178 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "179 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "180 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "181 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "182 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "183 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "184 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "185 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "186 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "187 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "188 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "189 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "190 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "191 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "192 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "193 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "194 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "195 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "196 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "197 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "198 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "199 Train Loss 0.0005140524 Test MSE 0.00011276586259774865 Test RE 0.00054131667523227\n",
      "Training time: 22.53\n",
      "Training time: 22.53\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 2.504889 Test MSE 386.1131199359195 Test RE 1.00165891805106\n",
      "1 Train Loss 2.408879 Test MSE 384.8355082264547 Test RE 1.0000003477010584\n",
      "2 Train Loss 2.3869069 Test MSE 384.2122124711895 Test RE 0.9991901982445626\n",
      "3 Train Loss 2.3763878 Test MSE 382.99110529178756 Test RE 0.9976011163820104\n",
      "4 Train Loss 2.3654783 Test MSE 380.9482263027469 Test RE 0.9949369510082108\n",
      "5 Train Loss 2.3514712 Test MSE 375.15741355516883 Test RE 0.9873459499488993\n",
      "6 Train Loss 2.3090055 Test MSE 369.6325720369752 Test RE 0.9800487964424047\n",
      "7 Train Loss 2.230814 Test MSE 351.66583707120344 Test RE 0.9559334808867187\n",
      "8 Train Loss 2.1476307 Test MSE 338.5620834140326 Test RE 0.9379544362024045\n",
      "9 Train Loss 1.9661765 Test MSE 313.7026349652249 Test RE 0.9028626053221268\n",
      "10 Train Loss 1.6885478 Test MSE 256.5508917633686 Test RE 0.8164871362580741\n",
      "11 Train Loss 1.463442 Test MSE 215.82004854614354 Test RE 0.7488735220223982\n",
      "12 Train Loss 1.2460446 Test MSE 189.80898468180752 Test RE 0.7022972463186206\n",
      "13 Train Loss 1.0231282 Test MSE 143.54034329756317 Test RE 0.610730448712291\n",
      "14 Train Loss 0.8410497 Test MSE 116.53644208542045 Test RE 0.5502923303288121\n",
      "15 Train Loss 0.6745667 Test MSE 94.69536334660637 Test RE 0.49605169724906506\n",
      "16 Train Loss 0.55886626 Test MSE 81.05279989543934 Test RE 0.4589301534294494\n",
      "17 Train Loss 0.49308535 Test MSE 68.20974090088002 Test RE 0.4210035737748142\n",
      "18 Train Loss 0.34783307 Test MSE 34.92481215247265 Test RE 0.3012517801905105\n",
      "19 Train Loss 0.3054424 Test MSE 18.46891466028558 Test RE 0.21907018237849754\n",
      "20 Train Loss 0.13915661 Test MSE 7.559304377437511 Test RE 0.14015335515450392\n",
      "21 Train Loss 0.122190155 Test MSE 6.003919454653329 Test RE 0.12490505392304467\n",
      "22 Train Loss 0.10041012 Test MSE 0.9324819475371783 Test RE 0.049224668367980354\n",
      "23 Train Loss 0.073142126 Test MSE 0.4388070826302573 Test RE 0.033767537972029735\n",
      "24 Train Loss 0.05656763 Test MSE 2.9067963903724503 Test RE 0.08691002775947859\n",
      "25 Train Loss 0.021012165 Test MSE 1.0094932603728919 Test RE 0.05121701864071049\n",
      "26 Train Loss 0.012773026 Test MSE 0.04782041101080378 Test RE 0.011147288336983209\n",
      "27 Train Loss 0.011312245 Test MSE 0.04731258408510913 Test RE 0.01108794126858997\n",
      "28 Train Loss 0.010692441 Test MSE 0.0754360586283705 Test RE 0.014000775029019972\n",
      "29 Train Loss 0.009051683 Test MSE 0.03135164354944255 Test RE 0.009025946166698955\n",
      "30 Train Loss 0.0061824997 Test MSE 0.032116216961997075 Test RE 0.009135341230445751\n",
      "31 Train Loss 0.0055808737 Test MSE 0.0013373182879234976 Test RE 0.0018641462190961372\n",
      "32 Train Loss 0.0052272356 Test MSE 0.003057749798489012 Test RE 0.0028187954100054365\n",
      "33 Train Loss 0.0028583393 Test MSE 0.03430034635570439 Test RE 0.0094408660388636\n",
      "34 Train Loss 0.000615015 Test MSE 0.0006325666613433659 Test RE 0.001282081791165164\n",
      "35 Train Loss 0.0004280767 Test MSE 0.0056480967109415504 Test RE 0.0038310131679424437\n",
      "36 Train Loss 0.00031307962 Test MSE 0.0001176232213945259 Test RE 0.000552852295595223\n",
      "37 Train Loss 0.00021111578 Test MSE 0.0006177897693863028 Test RE 0.0012670184487256016\n",
      "38 Train Loss 0.00020172501 Test MSE 0.0004497214062272704 Test RE 0.0010810215770994268\n",
      "39 Train Loss 0.00019442386 Test MSE 0.00030804399371710077 Test RE 0.0008946825258504008\n",
      "40 Train Loss 0.0001879441 Test MSE 0.00017949733146384108 Test RE 0.000682954200892845\n",
      "41 Train Loss 0.00018358984 Test MSE 9.885527124907253e-05 Test RE 0.0005068302160060019\n",
      "42 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "43 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "44 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "45 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "46 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "47 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "48 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "49 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "50 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "51 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "52 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "53 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "54 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "55 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "56 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "57 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "58 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "59 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "60 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "61 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "62 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "63 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "64 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "65 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "66 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "67 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "68 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "69 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "70 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "71 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "72 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "73 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "74 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "75 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "76 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "77 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "78 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "79 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "80 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "81 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "82 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "83 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "84 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "85 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "86 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "87 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "88 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "89 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "90 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "91 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "92 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "93 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "94 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "95 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "96 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "97 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "98 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "99 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "100 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "101 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "102 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "103 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "104 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "105 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "106 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "107 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "108 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "109 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "110 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "111 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "112 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "113 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "114 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "115 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "116 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "117 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "118 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "119 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "120 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "121 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "122 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "123 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "124 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "125 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "126 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "127 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "128 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "129 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "130 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "131 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "132 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "133 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "134 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "135 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "136 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "137 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "138 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "139 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "140 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "141 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "142 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "143 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "144 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "145 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "146 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "147 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "148 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "149 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "150 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "151 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "152 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "153 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "154 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "155 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "156 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "157 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "158 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "159 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "160 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "161 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "162 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "163 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "164 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "165 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "166 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "167 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "168 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "169 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "170 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "171 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "172 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "173 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "174 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "175 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "176 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "177 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "178 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "179 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "180 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "181 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "182 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "183 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "184 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "185 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "186 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "187 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "188 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "189 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "190 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "191 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "192 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "193 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "194 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "195 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "196 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "197 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "198 Train Loss 0.00018058311 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "199 Train Loss 0.0001805831 Test MSE 4.6759928659858015e-05 Test RE 0.00034857762173145157\n",
      "Training time: 24.93\n",
      "Training time: 24.93\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 2.3809736 Test MSE 381.94167280979076 Test RE 0.9962334173057977\n",
      "1 Train Loss 2.372682 Test MSE 381.8467238391784 Test RE 0.9961095800504356\n",
      "2 Train Loss 2.3483589 Test MSE 376.1675840694175 Test RE 0.9886743486775795\n",
      "3 Train Loss 2.2835557 Test MSE 365.0567191179271 Test RE 0.9739636659996252\n",
      "4 Train Loss 2.2223816 Test MSE 348.0412092735634 Test RE 0.9509943067898655\n",
      "5 Train Loss 2.1134055 Test MSE 331.095424002257 Test RE 0.9275539341346591\n",
      "6 Train Loss 1.9860617 Test MSE 309.95177081373186 Test RE 0.8974487220695185\n",
      "7 Train Loss 1.8273996 Test MSE 275.60012828627293 Test RE 0.8462570314792419\n",
      "8 Train Loss 1.5788002 Test MSE 217.9630627418175 Test RE 0.7525823580911385\n",
      "9 Train Loss 1.3848555 Test MSE 201.81406407421812 Test RE 0.7241662768541138\n",
      "10 Train Loss 1.2051954 Test MSE 177.9106623643347 Test RE 0.6799290092803599\n",
      "11 Train Loss 1.1109273 Test MSE 165.64720767488723 Test RE 0.6560767419498151\n",
      "12 Train Loss 1.0186394 Test MSE 141.70668379452192 Test RE 0.6068170152137207\n",
      "13 Train Loss 0.7967665 Test MSE 118.59712453664133 Test RE 0.5551363456851304\n",
      "14 Train Loss 0.59725356 Test MSE 85.87865516698052 Test RE 0.47239489965089515\n",
      "15 Train Loss 0.4881313 Test MSE 70.15803621525325 Test RE 0.4269738668754876\n",
      "16 Train Loss 0.44450992 Test MSE 57.68028285070445 Test RE 0.38714733425438386\n",
      "17 Train Loss 0.33080775 Test MSE 34.34850881084029 Test RE 0.298755924472966\n",
      "18 Train Loss 0.24350527 Test MSE 31.066791786217454 Test RE 0.28412587480225815\n",
      "19 Train Loss 0.16843429 Test MSE 19.311574138846694 Test RE 0.22401207062169246\n",
      "20 Train Loss 0.12520868 Test MSE 11.705478735870212 Test RE 0.17440429369452623\n",
      "21 Train Loss 0.10226483 Test MSE 6.049737343568019 Test RE 0.12538074393557336\n",
      "22 Train Loss 0.063207686 Test MSE 2.2672043388083507 Test RE 0.0767552188890005\n",
      "23 Train Loss 0.03689767 Test MSE 0.032564875740642034 Test RE 0.009198929600422305\n",
      "24 Train Loss 0.023477852 Test MSE 0.45611530638013453 Test RE 0.034427057573903456\n",
      "25 Train Loss 0.014694307 Test MSE 0.2604042613155647 Test RE 0.02601277149406375\n",
      "26 Train Loss 0.0115520535 Test MSE 0.00873098051176268 Test RE 0.004763148174159532\n",
      "27 Train Loss 0.006899794 Test MSE 0.051061943540401306 Test RE 0.01151890649955398\n",
      "28 Train Loss 0.003944868 Test MSE 0.0003194705311024602 Test RE 0.0009111250444055441\n",
      "29 Train Loss 0.0036231237 Test MSE 0.00024613511149627123 Test RE 0.0007997410120442798\n",
      "30 Train Loss 0.0032000786 Test MSE 0.00227335677332677 Test RE 0.0024305042311748323\n",
      "31 Train Loss 0.0023005547 Test MSE 0.00073364078012563 Test RE 0.0013807158631862562\n",
      "32 Train Loss 0.0022811943 Test MSE 0.0005063370252870233 Test RE 0.0011470502012318044\n",
      "33 Train Loss 0.002277304 Test MSE 0.000460334971530548 Test RE 0.0010937034126272396\n",
      "34 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "35 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "36 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "37 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "38 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "39 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "40 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "41 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "42 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "43 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "44 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "45 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "46 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "47 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "48 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "49 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "50 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "51 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "52 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "53 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "54 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "55 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "56 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "57 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "58 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "59 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "60 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "61 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "62 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "63 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "64 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "65 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "66 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "67 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "68 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "69 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "70 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "71 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "72 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "73 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "74 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "75 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "76 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "77 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "78 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "79 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "80 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "81 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "82 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "83 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "84 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "85 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "86 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "87 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "88 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "89 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "90 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "91 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "92 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "93 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "94 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "95 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "96 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "97 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "98 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "99 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "100 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "101 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "102 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "103 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "104 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "105 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "106 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "107 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "108 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "109 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "110 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "111 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "112 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "113 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "114 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "115 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "116 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "117 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "118 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "119 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "120 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "121 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "122 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "123 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "124 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "125 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "126 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "127 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "128 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "129 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "130 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "131 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "132 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "133 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "134 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "135 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "136 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "137 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "138 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "139 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "140 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "141 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "142 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "143 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "144 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "145 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "146 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "147 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "148 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "149 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "150 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "151 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "152 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "153 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "154 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "155 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "156 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "157 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "158 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "159 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "160 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "161 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "162 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "163 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "164 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "165 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "166 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "167 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "168 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "169 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "170 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "171 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "172 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "173 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "174 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "175 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "176 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "177 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "178 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "179 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "180 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "181 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "182 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "183 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "184 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "185 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "186 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "187 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "188 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "189 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "190 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "191 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "192 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "193 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "194 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "195 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "196 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "197 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "198 Train Loss 0.0022749463 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "199 Train Loss 0.002274946 Test MSE 0.00040975382178925763 Test RE 0.0010318678682682309\n",
      "Training time: 21.82\n",
      "Training time: 21.82\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.1892853 Test MSE 383.7421117247881 Test RE 0.9985787343041175\n",
      "1 Train Loss 2.574312 Test MSE 378.84777019634623 Test RE 0.9921902391048864\n",
      "2 Train Loss 2.3430588 Test MSE 372.7784943482407 Test RE 0.9842105305956803\n",
      "3 Train Loss 2.2847137 Test MSE 365.3688448561008 Test RE 0.9743799494971905\n",
      "4 Train Loss 2.202347 Test MSE 351.5840042498949 Test RE 0.9558222513090102\n",
      "5 Train Loss 2.1305003 Test MSE 334.08794811642053 Test RE 0.9317362386414226\n",
      "6 Train Loss 2.015183 Test MSE 316.56091219533135 Test RE 0.9069664598356364\n",
      "7 Train Loss 1.8801398 Test MSE 298.3695800655007 Test RE 0.8805212755656674\n",
      "8 Train Loss 1.8154138 Test MSE 286.5887386915014 Test RE 0.8629629355583768\n",
      "9 Train Loss 1.6283052 Test MSE 249.39937274755226 Test RE 0.8050266560424448\n",
      "10 Train Loss 1.5454253 Test MSE 229.76892170086376 Test RE 0.7726952198938742\n",
      "11 Train Loss 1.2474506 Test MSE 168.31687181688557 Test RE 0.6613424624487374\n",
      "12 Train Loss 1.0330766 Test MSE 152.2346508335926 Test RE 0.6289546615214967\n",
      "13 Train Loss 0.88956994 Test MSE 123.5852189237239 Test RE 0.5666903903403742\n",
      "14 Train Loss 0.742311 Test MSE 87.23274502251655 Test RE 0.4761045720280723\n",
      "15 Train Loss 0.32992285 Test MSE 47.9449612537303 Test RE 0.3529669713943903\n",
      "16 Train Loss 0.26002592 Test MSE 32.002373075074146 Test RE 0.2883723884005283\n",
      "17 Train Loss 0.18617323 Test MSE 17.539778492564857 Test RE 0.21348857320518916\n",
      "18 Train Loss 0.074487746 Test MSE 0.07288858566110411 Test RE 0.013762341939409914\n",
      "19 Train Loss 0.026907673 Test MSE 0.14595368084063798 Test RE 0.019474670058287017\n",
      "20 Train Loss 0.021592077 Test MSE 0.3622649485917264 Test RE 0.030681440092542877\n",
      "21 Train Loss 0.011638488 Test MSE 0.4763396159441491 Test RE 0.03518203316083221\n",
      "22 Train Loss 0.008986769 Test MSE 0.22094141112957685 Test RE 0.02396079062858015\n",
      "23 Train Loss 0.007923705 Test MSE 0.06158780552663752 Test RE 0.012650565727730389\n",
      "24 Train Loss 0.004810122 Test MSE 0.0024327205365085387 Test RE 0.00251425135710233\n",
      "25 Train Loss 0.002828688 Test MSE 0.003335371384366639 Test RE 0.0029439788236170733\n",
      "26 Train Loss 0.002679298 Test MSE 0.011793598060505148 Test RE 0.005535868225732937\n",
      "27 Train Loss 0.0026736918 Test MSE 0.011344411601282682 Test RE 0.00542942164784973\n",
      "28 Train Loss 0.0026671025 Test MSE 0.010722825795725113 Test RE 0.005278581213283498\n",
      "29 Train Loss 0.0026614708 Test MSE 0.009945380708616853 Test RE 0.0050836224484269435\n",
      "30 Train Loss 0.0026539122 Test MSE 0.009085200949863141 Test RE 0.00485880926946189\n",
      "31 Train Loss 0.0026451002 Test MSE 0.00811658251014057 Test RE 0.004592500227296652\n",
      "32 Train Loss 0.00206308 Test MSE 0.00010826525608201011 Test RE 0.0005304044221563731\n",
      "33 Train Loss 0.0015291112 Test MSE 0.0026587475085239646 Test RE 0.0026284585365881842\n",
      "34 Train Loss 0.0015213924 Test MSE 0.0027989856315234225 Test RE 0.002696888038428016\n",
      "35 Train Loss 0.0015146837 Test MSE 0.0027085678354178523 Test RE 0.0026529706229992825\n",
      "36 Train Loss 0.001506922 Test MSE 0.0026195999916356623 Test RE 0.002609036006553491\n",
      "37 Train Loss 0.0014988848 Test MSE 0.0022869084501083315 Test RE 0.0024377376905720797\n",
      "38 Train Loss 0.0014897236 Test MSE 0.0019740310403760343 Test RE 0.002264850662578285\n",
      "39 Train Loss 0.001306562 Test MSE 0.0012911195892245466 Test RE 0.0018316640336203851\n",
      "40 Train Loss 0.0012406415 Test MSE 0.00014080629441615443 Test RE 0.0006048861173702153\n",
      "41 Train Loss 0.001231061 Test MSE 1.4273102508711374e-05 Test RE 0.00019258469715430893\n",
      "42 Train Loss 0.0012216154 Test MSE 2.4166340388775565e-05 Test RE 0.00025059247569253175\n",
      "43 Train Loss 0.001211692 Test MSE 0.00018820889332039466 Test RE 0.0006993307953941917\n",
      "44 Train Loss 0.0012019482 Test MSE 0.0004916192485544378 Test RE 0.0011302565222527576\n",
      "45 Train Loss 0.0011923418 Test MSE 0.000870233682265965 Test RE 0.0015037669195730212\n",
      "46 Train Loss 0.0011833715 Test MSE 0.0012304747509197225 Test RE 0.0017881293669478552\n",
      "47 Train Loss 0.0011755595 Test MSE 0.0014910319189784822 Test RE 0.001968366897748433\n",
      "48 Train Loss 0.0011695613 Test MSE 0.0016179478420759988 Test RE 0.002050429497972291\n",
      "49 Train Loss 0.0011642074 Test MSE 0.001594126237028108 Test RE 0.0020352789334893108\n",
      "50 Train Loss 0.0011604062 Test MSE 0.0014912390451071998 Test RE 0.0019685036104681074\n",
      "51 Train Loss 0.0011573686 Test MSE 0.0012678934679735358 Test RE 0.0018151142433363988\n",
      "52 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "53 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "54 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "55 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "56 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "57 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "58 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "59 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "60 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "61 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "62 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "63 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "64 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "65 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "66 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "67 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "68 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "69 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "70 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "71 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "72 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "73 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "74 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "75 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "76 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "77 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "78 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "79 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "80 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "81 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "82 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "83 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "84 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "85 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "86 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "87 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "88 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "89 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "90 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "91 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "92 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "93 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "94 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "95 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "96 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "97 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "98 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "99 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "100 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "101 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "102 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "103 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "104 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "105 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "106 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "107 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "108 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "109 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "110 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "111 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "112 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "113 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "114 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "115 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "116 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "117 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "118 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "119 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "120 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "121 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "122 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "123 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "124 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "125 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "126 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "127 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "128 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "129 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "130 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "131 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "132 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "133 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "134 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "135 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "136 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "137 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "138 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "139 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "140 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "141 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "142 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "143 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "144 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "145 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "146 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "147 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "148 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "149 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "150 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "151 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "152 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "153 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "154 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "155 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "156 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "157 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "158 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "159 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "160 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "161 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "162 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "163 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "164 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "165 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "166 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "167 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "168 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "169 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "170 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "171 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "172 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "173 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "174 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "175 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "176 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "177 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "178 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "179 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "180 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "181 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "182 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "183 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "184 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "185 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "186 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "187 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "188 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "189 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "190 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "191 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "192 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "193 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "194 Train Loss 0.0011549469 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "195 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "196 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "197 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "198 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "199 Train Loss 0.0011549467 Test MSE 0.0010936936804442558 Test RE 0.0016858170189030098\n",
      "Training time: 21.10\n",
      "Training time: 21.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 2.8545597 Test MSE 381.5756050155849 Test RE 0.9957558883786226\n",
      "1 Train Loss 2.3480337 Test MSE 375.92400936032385 Test RE 0.9883542053787063\n",
      "2 Train Loss 2.252932 Test MSE 355.86097977228195 Test RE 0.9616184064027831\n",
      "3 Train Loss 2.1630263 Test MSE 344.85239343307427 Test RE 0.9466276918390946\n",
      "4 Train Loss 2.051204 Test MSE 325.7554904820106 Test RE 0.920043697545333\n",
      "5 Train Loss 1.9499854 Test MSE 309.373040933409 Test RE 0.8966104899496145\n",
      "6 Train Loss 1.752754 Test MSE 267.0794907354246 Test RE 0.8330726031137657\n",
      "7 Train Loss 1.5420469 Test MSE 227.57620037478475 Test RE 0.7689994046927819\n",
      "8 Train Loss 1.4273939 Test MSE 208.90834227100913 Test RE 0.7367844882755988\n",
      "9 Train Loss 1.2750452 Test MSE 164.0484926551616 Test RE 0.652903060929752\n",
      "10 Train Loss 0.740667 Test MSE 89.01386954324398 Test RE 0.48094058013201957\n",
      "11 Train Loss 0.4336229 Test MSE 43.707414700269695 Test RE 0.337007946809021\n",
      "12 Train Loss 0.29059967 Test MSE 23.461179266763235 Test RE 0.24690933029537487\n",
      "13 Train Loss 0.1809499 Test MSE 18.281721295330176 Test RE 0.21795715200760798\n",
      "14 Train Loss 0.12045047 Test MSE 11.18844006899084 Test RE 0.1705090177168869\n",
      "15 Train Loss 0.07589516 Test MSE 2.8209572600628525 Test RE 0.08561716352377612\n",
      "16 Train Loss 0.045965526 Test MSE 0.6894992408722458 Test RE 0.042328168630859295\n",
      "17 Train Loss 0.040522292 Test MSE 0.46359877050126747 Test RE 0.03470833018373173\n",
      "18 Train Loss 0.029722545 Test MSE 0.4294170964128742 Test RE 0.03340429020651498\n",
      "19 Train Loss 0.026178397 Test MSE 0.6421136257831419 Test RE 0.040847786303972226\n",
      "20 Train Loss 0.023645798 Test MSE 1.2484045754033979 Test RE 0.056956101903500145\n",
      "21 Train Loss 0.018804265 Test MSE 0.9038354976938087 Test RE 0.04846266353736197\n",
      "22 Train Loss 0.012047257 Test MSE 0.13241957770256185 Test RE 0.018549776520804064\n",
      "23 Train Loss 0.0045629805 Test MSE 0.00021557582279353206 Test RE 0.0007484496828757877\n",
      "24 Train Loss 0.0020560683 Test MSE 3.6615874344109954e-06 Test RE 9.754326470882533e-05\n",
      "25 Train Loss 0.0011462934 Test MSE 0.0015817103358419509 Test RE 0.002027337524025302\n",
      "26 Train Loss 0.00092096365 Test MSE 7.00962721977113e-07 Test RE 4.267858810830983e-05\n",
      "27 Train Loss 0.00091410533 Test MSE 1.645845868452961e-05 Test RE 0.00020680315654350326\n",
      "28 Train Loss 0.00090885576 Test MSE 5.778218921167878e-05 Test RE 0.00038748917867393164\n",
      "29 Train Loss 0.00090494624 Test MSE 0.00010546661029706887 Test RE 0.0005235040869444413\n",
      "30 Train Loss 0.00090211414 Test MSE 0.00016453507652632558 Test RE 0.0006538706311463724\n",
      "31 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "32 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "33 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "34 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "35 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "36 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "37 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "38 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "39 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "40 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "41 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "42 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "43 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "44 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "45 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "46 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "47 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "48 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "49 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "50 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "51 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "52 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "53 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "54 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "55 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "56 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "57 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "58 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "59 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "60 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "61 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "62 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "63 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "64 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "65 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "66 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "67 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "68 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "69 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "70 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "71 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "72 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "73 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "74 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "75 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "76 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "77 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "78 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "79 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "80 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "81 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "82 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "83 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "84 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "85 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "86 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "87 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "88 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "89 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "90 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "91 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "92 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "93 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "94 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "95 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "96 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "97 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "98 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "99 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "100 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "101 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "102 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "103 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "104 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "105 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "106 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "107 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "108 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "109 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "110 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "111 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "112 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "113 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "114 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "115 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "116 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "117 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "118 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "119 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "120 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "121 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "122 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "123 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "124 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "125 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "126 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "127 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "128 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "129 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "130 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "131 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "132 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "133 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "134 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "135 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "136 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "137 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "138 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "139 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "140 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "141 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "142 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "143 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "144 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "145 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "146 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "147 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "148 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "149 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "150 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "151 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "152 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "153 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "154 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "155 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "156 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "157 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "158 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "159 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "160 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "161 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "162 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "163 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "164 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "165 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "166 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "167 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "168 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "169 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "170 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "171 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "172 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "173 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "174 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "175 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "176 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "177 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "178 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "179 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "180 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "181 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "182 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "183 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "184 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "185 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "186 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "187 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "188 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "189 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "190 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "191 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "192 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "193 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "194 Train Loss 0.0008996147 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "195 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "196 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "197 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "198 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "199 Train Loss 0.00089961477 Test MSE 0.00022744239801753726 Test RE 0.0007687733066619924\n",
      "Training time: 20.08\n",
      "Training time: 20.08\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 2.5137892 Test MSE 375.3171241150823 Test RE 0.987556092131381\n",
      "1 Train Loss 2.3001533 Test MSE 369.3874769927853 Test RE 0.9797238184059652\n",
      "2 Train Loss 2.224379 Test MSE 354.729476876799 Test RE 0.9600883983848884\n",
      "3 Train Loss 2.0990598 Test MSE 332.90956298093437 Test RE 0.930091590861949\n",
      "4 Train Loss 1.9253604 Test MSE 303.1538681567137 Test RE 0.8875526797251477\n",
      "5 Train Loss 1.7041103 Test MSE 260.9908555272034 Test RE 0.8235220424729507\n",
      "6 Train Loss 1.5048878 Test MSE 233.21339115507976 Test RE 0.7784654180440798\n",
      "7 Train Loss 1.3735803 Test MSE 214.7664189088557 Test RE 0.7470432918961178\n",
      "8 Train Loss 1.143038 Test MSE 171.82414050211912 Test RE 0.6681972325605251\n",
      "9 Train Loss 0.90958345 Test MSE 125.44547984745526 Test RE 0.5709395009247448\n",
      "10 Train Loss 0.74729395 Test MSE 98.12405608406112 Test RE 0.5049522695841451\n",
      "11 Train Loss 0.67348963 Test MSE 98.79299293791237 Test RE 0.5066705406420163\n",
      "12 Train Loss 0.4553771 Test MSE 61.842975309765926 Test RE 0.4008738899508402\n",
      "13 Train Loss 0.4005761 Test MSE 51.945614852068104 Test RE 0.36739820327589984\n",
      "14 Train Loss 0.26086858 Test MSE 32.22617507308807 Test RE 0.2893789680612948\n",
      "15 Train Loss 0.24413243 Test MSE 31.348886982462844 Test RE 0.2854129310745096\n",
      "16 Train Loss 0.17383829 Test MSE 19.623496163013094 Test RE 0.22581395378742508\n",
      "17 Train Loss 0.1450606 Test MSE 15.587636059323767 Test RE 0.20125779545427172\n",
      "18 Train Loss 0.08497095 Test MSE 6.073803352654129 Test RE 0.1256298803086051\n",
      "19 Train Loss 0.032725483 Test MSE 2.4287201716479503 Test RE 0.07944221094673246\n",
      "20 Train Loss 0.012825865 Test MSE 0.15651855580017127 Test RE 0.02016719504410274\n",
      "21 Train Loss 0.010645755 Test MSE 0.01636016403329749 Test RE 0.006520132319808531\n",
      "22 Train Loss 0.010036405 Test MSE 0.0005892074770988123 Test RE 0.00123736180842793\n",
      "23 Train Loss 0.007880767 Test MSE 0.002327558751180014 Test RE 0.002459307920895929\n",
      "24 Train Loss 0.006126804 Test MSE 0.004158163136731715 Test RE 0.0032871032405050096\n",
      "25 Train Loss 0.0057950774 Test MSE 0.00036767274941600856 Test RE 0.0009774471932897568\n",
      "26 Train Loss 0.0036076594 Test MSE 0.004887028369516371 Test RE 0.0035635676896891566\n",
      "27 Train Loss 0.0031901302 Test MSE 0.00369371442143458 Test RE 0.003098091498909478\n",
      "28 Train Loss 0.0031831157 Test MSE 0.003995897741937158 Test RE 0.00322232815098514\n",
      "29 Train Loss 0.0031780442 Test MSE 0.004395597194926754 Test RE 0.0033796484411620867\n",
      "30 Train Loss 0.0031733587 Test MSE 0.004813934818579006 Test RE 0.003536817781293206\n",
      "31 Train Loss 0.0031688288 Test MSE 0.00538844381679138 Test RE 0.0037419179641879114\n",
      "32 Train Loss 0.003164197 Test MSE 0.005970682463427476 Test RE 0.003938896506359213\n",
      "33 Train Loss 0.003158667 Test MSE 0.006750378392508476 Test RE 0.004188192564898031\n",
      "34 Train Loss 0.0031517497 Test MSE 0.007685948830712098 Test RE 0.004469010004996985\n",
      "35 Train Loss 0.0031430272 Test MSE 0.008874922047854 Test RE 0.004802251005031353\n",
      "36 Train Loss 0.0027335598 Test MSE 0.031427143978459776 Test RE 0.00903680768706281\n",
      "37 Train Loss 0.00239773 Test MSE 0.0010157597074991447 Test RE 0.0016246434848704134\n",
      "38 Train Loss 0.0020252317 Test MSE 0.005793866785780458 Test RE 0.0038801349930056893\n",
      "39 Train Loss 0.0014623582 Test MSE 0.0011215977116380308 Test RE 0.001707187176119859\n",
      "40 Train Loss 0.0013951416 Test MSE 0.0013958885685739558 Test RE 0.0019045306132712582\n",
      "41 Train Loss 0.0013875976 Test MSE 0.0012745869772353186 Test RE 0.0018198991450230344\n",
      "42 Train Loss 0.0013825949 Test MSE 0.001214791416260641 Test RE 0.0017766972894951456\n",
      "43 Train Loss 0.0013785156 Test MSE 0.0011072397365338574 Test RE 0.0016962248217879962\n",
      "44 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "45 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "46 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "47 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "48 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "49 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "50 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "51 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "52 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "53 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "54 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "55 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "56 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "57 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "58 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "59 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "60 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "61 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "62 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "63 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "64 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "65 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "66 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "67 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "68 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "69 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "70 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "71 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "72 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "73 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "74 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "75 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "76 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "77 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "78 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "79 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "80 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "81 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "82 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "83 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "84 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "85 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "86 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "87 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "88 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "89 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "90 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "91 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "92 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "93 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "94 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "95 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "96 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "97 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "98 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "99 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "100 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "101 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "102 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "103 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "104 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "105 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "106 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "107 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "108 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "109 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "110 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "111 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "112 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "113 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "114 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "115 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "116 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "117 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "118 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "119 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "120 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "121 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "122 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "123 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "124 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "125 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "126 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "127 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "128 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "129 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "130 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "131 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "132 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "133 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "134 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "135 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "136 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "137 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "138 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "139 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "140 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "141 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "142 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "143 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "144 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "145 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "146 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "147 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "148 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "149 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "150 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "151 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "152 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "153 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "154 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "155 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "156 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "157 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "158 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "159 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "160 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "161 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "162 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "163 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "164 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "165 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "166 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "167 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "168 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "169 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "170 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "171 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "172 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "173 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "174 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "175 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "176 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "177 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "178 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "179 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "180 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "181 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "182 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "183 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "184 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "185 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "186 Train Loss 0.0013754001 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "187 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "188 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "189 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "190 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "191 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "192 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "193 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "194 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "195 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "196 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "197 Train Loss 0.0013753999 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "198 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "199 Train Loss 0.0013754 Test MSE 0.0010230950960422886 Test RE 0.0016304991773451812\n",
      "Training time: 23.65\n",
      "Training time: 23.65\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 2.4459372 Test MSE 379.35067748831716 Test RE 0.9928485697864423\n",
      "1 Train Loss 2.3531444 Test MSE 378.3079909351902 Test RE 0.9914831548099794\n",
      "2 Train Loss 2.2909138 Test MSE 364.8624416755005 Test RE 0.9737044674399171\n",
      "3 Train Loss 2.1949937 Test MSE 341.85846185714325 Test RE 0.9425095288524389\n",
      "4 Train Loss 2.0579355 Test MSE 324.6221865474394 Test RE 0.91844188632004\n",
      "5 Train Loss 1.8682472 Test MSE 283.71036732909425 Test RE 0.8586183890839937\n",
      "6 Train Loss 1.6334033 Test MSE 228.9229895881136 Test RE 0.7712715059446823\n",
      "7 Train Loss 1.467452 Test MSE 202.94077925821495 Test RE 0.7261849505558565\n",
      "8 Train Loss 1.2643263 Test MSE 190.02557791721583 Test RE 0.7026978318424432\n",
      "9 Train Loss 1.133537 Test MSE 172.6684348179907 Test RE 0.6698368851646431\n",
      "10 Train Loss 0.9981583 Test MSE 138.02663950800564 Test RE 0.5988858330790195\n",
      "11 Train Loss 0.78532505 Test MSE 113.08716124807052 Test RE 0.5420873012030305\n",
      "12 Train Loss 0.6609123 Test MSE 97.53135293723278 Test RE 0.5034249167098459\n",
      "13 Train Loss 0.5630908 Test MSE 73.14459760702486 Test RE 0.43596709330207845\n",
      "14 Train Loss 0.5021539 Test MSE 52.50882277519932 Test RE 0.3693845473137053\n",
      "15 Train Loss 0.34425083 Test MSE 29.267872815689774 Test RE 0.2757770757014673\n",
      "16 Train Loss 0.11984676 Test MSE 6.920158000163979 Test RE 0.13409747109386697\n",
      "17 Train Loss 0.083717205 Test MSE 4.737528260592924 Test RE 0.11095285692530131\n",
      "18 Train Loss 0.053165693 Test MSE 2.422730918781772 Test RE 0.07934419776489166\n",
      "19 Train Loss 0.037887022 Test MSE 2.440203807468424 Test RE 0.07962980142951921\n",
      "20 Train Loss 0.031731695 Test MSE 2.4630226283875705 Test RE 0.08000125196228101\n",
      "21 Train Loss 0.030664137 Test MSE 2.0181494377409823 Test RE 0.07241678551591137\n",
      "22 Train Loss 0.018839417 Test MSE 0.19990511061290486 Test RE 0.02279158513683861\n",
      "23 Train Loss 0.014423975 Test MSE 0.1981394239756598 Test RE 0.02269070713979719\n",
      "24 Train Loss 0.01412191 Test MSE 0.2549424467622989 Test RE 0.025738525129075818\n",
      "25 Train Loss 0.013294761 Test MSE 0.10592253210142706 Test RE 0.016590396273835555\n",
      "26 Train Loss 0.012723684 Test MSE 0.012820634798643269 Test RE 0.005771880715557599\n",
      "27 Train Loss 0.008712653 Test MSE 0.036129416939319606 Test RE 0.009689314641447304\n",
      "28 Train Loss 0.0027961126 Test MSE 0.0003934471044308037 Test RE 0.0010111271206246614\n",
      "29 Train Loss 0.0022743987 Test MSE 0.004208944778999261 Test RE 0.003307114233546789\n",
      "30 Train Loss 0.0019394894 Test MSE 8.062726134720868e-05 Test RE 0.0004577238442409818\n",
      "31 Train Loss 0.0014773424 Test MSE 0.0011864749535543686 Test RE 0.0017558680235422893\n",
      "32 Train Loss 0.001300097 Test MSE 2.5542165407358915e-05 Test RE 0.00025762703721187477\n",
      "33 Train Loss 0.0012932621 Test MSE 1.64363163201054e-05 Test RE 0.00020666399855471352\n",
      "34 Train Loss 0.0012875038 Test MSE 1.419161129949573e-05 Test RE 0.00019203413623431877\n",
      "35 Train Loss 0.0012832888 Test MSE 1.647500486936127e-05 Test RE 0.0002069070831547078\n",
      "36 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "37 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "38 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "39 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "40 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "41 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "42 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "43 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "44 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "45 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "46 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "47 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "48 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "49 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "50 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "51 Train Loss 0.0012805957 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "52 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "53 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "54 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "55 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "56 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "57 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "58 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "59 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "60 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "61 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "62 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "63 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "64 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "65 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "66 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "67 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "68 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "69 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "70 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "71 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "72 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "73 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "74 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "75 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "76 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "77 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "78 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "79 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "80 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "81 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "82 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "83 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "84 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "85 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "86 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "87 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "88 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "89 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "90 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "91 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "92 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "93 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "94 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "95 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "96 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "97 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "98 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "99 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "100 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "101 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "102 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "103 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "104 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "105 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "106 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "107 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "108 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "109 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "110 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "111 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "112 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "113 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "114 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "115 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "116 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "117 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "118 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "119 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "120 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "121 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "122 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "123 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "124 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "125 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "126 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "127 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "128 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "129 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "130 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "131 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "132 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "133 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "134 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "135 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "136 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "137 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "138 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "139 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "140 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "141 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "142 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "143 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "144 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "145 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "146 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "147 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "148 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "149 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "150 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "151 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "152 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "153 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "154 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "155 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "156 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "157 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "158 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "159 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "160 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "161 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "162 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "163 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "164 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "165 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "166 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "167 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "168 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "169 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "170 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "171 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "172 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "173 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "174 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "175 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "176 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "177 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "178 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "179 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "180 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "181 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "182 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "183 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "184 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "185 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "186 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "187 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "188 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "189 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "190 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "191 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "192 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "193 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "194 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "195 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "196 Train Loss 0.0012805958 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "197 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "198 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "199 Train Loss 0.0012805959 Test MSE 1.9182095902007222e-05 Test RE 0.00022325984119964118\n",
      "Training time: 27.90\n",
      "Training time: 27.90\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 2.3879538 Test MSE 383.16956991574165 Test RE 0.9978335183494217\n",
      "1 Train Loss 2.3605866 Test MSE 379.54370652627404 Test RE 0.9931011385073991\n",
      "2 Train Loss 2.294094 Test MSE 364.85611942169925 Test RE 0.9736960313380674\n",
      "3 Train Loss 2.196231 Test MSE 349.6435232530304 Test RE 0.9531808893658618\n",
      "4 Train Loss 2.1221328 Test MSE 337.63452200555525 Test RE 0.9366686941008173\n",
      "5 Train Loss 1.9933127 Test MSE 309.2420246090187 Test RE 0.8964206171507698\n",
      "6 Train Loss 1.6455412 Test MSE 252.86904890455241 Test RE 0.8106071312099052\n",
      "7 Train Loss 1.3017572 Test MSE 188.68716489508333 Test RE 0.7002187922835622\n",
      "8 Train Loss 0.9334852 Test MSE 127.74768242694067 Test RE 0.5761546847705551\n",
      "9 Train Loss 0.6077547 Test MSE 83.02815009247776 Test RE 0.4644888180553283\n",
      "10 Train Loss 0.4063856 Test MSE 51.823665651478834 Test RE 0.36696669193459525\n",
      "11 Train Loss 0.2331712 Test MSE 23.690405807997745 Test RE 0.24811260731003693\n",
      "12 Train Loss 0.19441372 Test MSE 15.8424296095309 Test RE 0.2028959955175501\n",
      "13 Train Loss 0.12549157 Test MSE 10.508704082837358 Test RE 0.16524836224085723\n",
      "14 Train Loss 0.08703668 Test MSE 3.109161223125616 Test RE 0.08988437518363228\n",
      "15 Train Loss 0.059225168 Test MSE 0.12603955000212216 Test RE 0.01809739245581855\n",
      "16 Train Loss 0.04903486 Test MSE 0.09311424140549525 Test RE 0.015555022271268813\n",
      "17 Train Loss 0.019854441 Test MSE 0.5444928267992138 Test RE 0.03761479193496089\n",
      "18 Train Loss 0.008897863 Test MSE 0.31356830998517615 Test RE 0.0285449091549479\n",
      "19 Train Loss 0.006973058 Test MSE 0.23793551335749025 Test RE 0.02486521487305041\n",
      "20 Train Loss 0.0051836856 Test MSE 0.06363446719034098 Test RE 0.012859047145576888\n",
      "21 Train Loss 0.004675139 Test MSE 0.04281666759719048 Test RE 0.01054797311525243\n",
      "22 Train Loss 0.0033234248 Test MSE 0.10175695174165962 Test RE 0.016260901790695406\n",
      "23 Train Loss 0.0026110923 Test MSE 0.1061365922388524 Test RE 0.016607151679926966\n",
      "24 Train Loss 0.0022665379 Test MSE 0.07452989425218952 Test RE 0.013916429859388887\n",
      "25 Train Loss 0.0013346492 Test MSE 0.02360273163335586 Test RE 0.0078314777201597\n",
      "26 Train Loss 0.000998416 Test MSE 0.005740536149017829 Test RE 0.0038622360265123303\n",
      "27 Train Loss 0.00091238355 Test MSE 0.0014172964127026214 Test RE 0.001919079324283644\n",
      "28 Train Loss 0.0009034454 Test MSE 0.0011165309345968653 Test RE 0.0017033267332128147\n",
      "29 Train Loss 0.0008969484 Test MSE 0.0007795441448523084 Test RE 0.001423255725202119\n",
      "30 Train Loss 0.000893255 Test MSE 0.0006629546339933917 Test RE 0.001312515642008873\n",
      "31 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "32 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "33 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "34 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "35 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "36 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "37 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "38 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "39 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "40 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "41 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "42 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "43 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "44 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "45 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "46 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "47 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "48 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "49 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "50 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "51 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "52 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "53 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "54 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "55 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "56 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "57 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "58 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "59 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "60 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "61 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "62 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "63 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "64 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "65 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "66 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "67 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "68 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "69 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "70 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "71 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "72 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "73 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "74 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "75 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "76 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "77 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "78 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "79 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "80 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "81 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "82 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "83 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "84 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "85 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "86 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "87 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "88 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "89 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "90 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "91 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "92 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "93 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "94 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "95 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "96 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "97 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "98 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "99 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "100 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "101 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "102 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "103 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "104 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "105 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "106 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "107 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "108 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "109 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "110 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "111 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "112 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "113 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "114 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "115 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "116 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "117 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "118 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "119 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "120 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "121 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "122 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "123 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "124 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "125 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "126 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "127 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "128 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "129 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "130 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "131 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "132 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "133 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "134 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "135 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "136 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "137 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "138 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "139 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "140 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "141 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "142 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "143 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "144 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "145 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "146 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "147 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "148 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "149 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "150 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "151 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "152 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "153 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "154 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "155 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "156 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "157 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "158 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "159 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "160 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "161 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "162 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "163 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "164 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "165 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "166 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "167 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "168 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "169 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "170 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "171 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "172 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "173 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "174 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "175 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "176 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "177 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "178 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "179 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "180 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "181 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "182 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "183 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "184 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "185 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "186 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "187 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "188 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "189 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "190 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "191 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "192 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "193 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "194 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "195 Train Loss 0.00089054496 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "196 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "197 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "198 Train Loss 0.000890545 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "199 Train Loss 0.0008905449 Test MSE 0.0005876531058705612 Test RE 0.0012357286063657178\n",
      "Training time: 25.55\n",
      "Training time: 25.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 3.099979 Test MSE 386.2742074753085 Test RE 1.001867843798688\n",
      "1 Train Loss 2.4008327 Test MSE 384.66501902546145 Test RE 0.9997788138985888\n",
      "2 Train Loss 2.384023 Test MSE 383.8374067137945 Test RE 0.9987027155399157\n",
      "3 Train Loss 2.3662162 Test MSE 380.68152553435897 Test RE 0.9945886137670256\n",
      "4 Train Loss 2.3362656 Test MSE 373.15351204681303 Test RE 0.9847054673905722\n",
      "5 Train Loss 2.299068 Test MSE 365.85863197848533 Test RE 0.9750328225150166\n",
      "6 Train Loss 2.2093375 Test MSE 350.2044399710318 Test RE 0.9539451546845098\n",
      "7 Train Loss 2.1409066 Test MSE 337.5652487205843 Test RE 0.9365725998899292\n",
      "8 Train Loss 2.0503335 Test MSE 323.25940385347417 Test RE 0.9165120227195542\n",
      "9 Train Loss 1.8542472 Test MSE 282.58860262501094 Test RE 0.8569192587907993\n",
      "10 Train Loss 1.7421305 Test MSE 266.94925677246295 Test RE 0.8328694658838428\n",
      "11 Train Loss 1.5159516 Test MSE 234.00758928279808 Test RE 0.7797898066393406\n",
      "12 Train Loss 1.4019262 Test MSE 214.08889683708819 Test RE 0.7458640150566331\n",
      "13 Train Loss 1.3018314 Test MSE 198.45729033809866 Test RE 0.718118493631\n",
      "14 Train Loss 1.1941156 Test MSE 187.2118754482859 Test RE 0.6974760181304128\n",
      "15 Train Loss 1.0605135 Test MSE 154.96375884334887 Test RE 0.6345672488550648\n",
      "16 Train Loss 0.88858724 Test MSE 129.09854438422929 Test RE 0.5791929344632206\n",
      "17 Train Loss 0.77821064 Test MSE 107.73861186200367 Test RE 0.5291128030358638\n",
      "18 Train Loss 0.6837909 Test MSE 88.77444229642848 Test RE 0.4802933337199642\n",
      "19 Train Loss 0.52027845 Test MSE 72.21790725288484 Test RE 0.43319659279183614\n",
      "20 Train Loss 0.43749994 Test MSE 60.54694967895697 Test RE 0.3966511491114881\n",
      "21 Train Loss 0.30599907 Test MSE 36.14506443361419 Test RE 0.3064693760617785\n",
      "22 Train Loss 0.15800746 Test MSE 18.726090194022987 Test RE 0.22059016114125873\n",
      "23 Train Loss 0.100820415 Test MSE 9.578099551463268 Test RE 0.15776194642783892\n",
      "24 Train Loss 0.06044264 Test MSE 5.544729893106603 Test RE 0.12003358573881732\n",
      "25 Train Loss 0.040485825 Test MSE 1.9764585397330139 Test RE 0.07166488970448942\n",
      "26 Train Loss 0.019814653 Test MSE 0.04063238416876609 Test RE 0.010275399963579882\n",
      "27 Train Loss 0.011213355 Test MSE 0.0028371862651759465 Test RE 0.002715229271526219\n",
      "28 Train Loss 0.009940969 Test MSE 0.08151930835904478 Test RE 0.014554350373376313\n",
      "29 Train Loss 0.008780693 Test MSE 0.12929454592531425 Test RE 0.018329587234923422\n",
      "30 Train Loss 0.007983104 Test MSE 0.08303156657428042 Test RE 0.01468872832740662\n",
      "31 Train Loss 0.00678561 Test MSE 0.08968636623718262 Test RE 0.015266018934566349\n",
      "32 Train Loss 0.0064914096 Test MSE 0.11808977517397194 Test RE 0.017517363031090142\n",
      "33 Train Loss 0.0058249203 Test MSE 0.06428714768266192 Test RE 0.012924824691086415\n",
      "34 Train Loss 0.005549881 Test MSE 0.04191808008889109 Test RE 0.010436701780741506\n",
      "35 Train Loss 0.005541921 Test MSE 0.041972595546321205 Test RE 0.010443486164754564\n",
      "36 Train Loss 0.0055379826 Test MSE 0.04223803453158134 Test RE 0.010476456956338474\n",
      "37 Train Loss 0.005533042 Test MSE 0.042562215406118716 Test RE 0.010516584003413307\n",
      "38 Train Loss 0.0055282097 Test MSE 0.04306206805513932 Test RE 0.010578157378222739\n",
      "39 Train Loss 0.0055240896 Test MSE 0.043585764734002864 Test RE 0.01064228579752748\n",
      "40 Train Loss 0.005517817 Test MSE 0.04426690386682317 Test RE 0.010725119905757629\n",
      "41 Train Loss 0.005510456 Test MSE 0.04515792358326858 Test RE 0.01083252162495145\n",
      "42 Train Loss 0.005501412 Test MSE 0.04602638993693204 Test RE 0.010936189796918596\n",
      "43 Train Loss 0.0047375388 Test MSE 0.10681128116668752 Test RE 0.016659852219175056\n",
      "44 Train Loss 0.002258616 Test MSE 0.0812864415561942 Test RE 0.01453354763981203\n",
      "45 Train Loss 0.0011252239 Test MSE 0.0002902863188385628 Test RE 0.0008685120864027824\n",
      "46 Train Loss 0.0007810215 Test MSE 3.9398716101571336e-05 Test RE 0.00031996584176996305\n",
      "47 Train Loss 0.0007724179 Test MSE 0.00012553317341190966 Test RE 0.0005711390257421872\n",
      "48 Train Loss 0.0007653813 Test MSE 0.00021657290852656027 Test RE 0.000750178558513955\n",
      "49 Train Loss 0.00076020043 Test MSE 0.00032602488475395495 Test RE 0.0009204240492011092\n",
      "50 Train Loss 0.0007561744 Test MSE 0.00040141817545926323 Test RE 0.001021318265129701\n",
      "51 Train Loss 0.0007528014 Test MSE 0.0004927663764302076 Test RE 0.0011315744052664055\n",
      "52 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "53 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "54 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "55 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "56 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "57 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "58 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "59 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "60 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "61 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "62 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "63 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "64 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "65 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "66 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "67 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "68 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "69 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "70 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "71 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "72 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "73 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "74 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "75 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "76 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "77 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "78 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "79 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "80 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "81 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "82 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "83 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "84 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "85 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "86 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "87 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "88 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "89 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "90 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "91 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "92 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "93 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "94 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "95 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "96 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "97 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "98 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "99 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "100 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "101 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "102 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "103 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "104 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "105 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "106 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "107 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "108 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "109 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "110 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "111 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "112 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "113 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "114 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "115 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "116 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "117 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "118 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "119 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "120 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "121 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "122 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "123 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "124 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "125 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "126 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "127 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "128 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "129 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "130 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "131 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "132 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "133 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "134 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "135 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "136 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "137 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "138 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "139 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "140 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "141 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "142 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "143 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "144 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "145 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "146 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "147 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "148 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "149 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "150 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "151 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "152 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "153 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "154 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "155 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "156 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "157 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "158 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "159 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "160 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "161 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "162 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "163 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "164 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "165 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "166 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "167 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "168 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "169 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "170 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "171 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "172 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "173 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "174 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "175 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "176 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "177 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "178 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "179 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "180 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "181 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "182 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "183 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "184 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "185 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "186 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "187 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "188 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "189 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "190 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "191 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "192 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "193 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "194 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "195 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "196 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "197 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "198 Train Loss 0.00075037417 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "199 Train Loss 0.0007503742 Test MSE 0.0005403599665738736 Test RE 0.001184961296659897\n",
      "Training time: 31.74\n",
      "Training time: 31.74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 2.5437882 Test MSE 385.82647238858226 Test RE 1.0012870369019797\n",
      "1 Train Loss 2.3922005 Test MSE 383.84372278434154 Test RE 0.9987109323665557\n",
      "2 Train Loss 2.3706043 Test MSE 380.9939092686059 Test RE 0.9949966051883296\n",
      "3 Train Loss 2.3200724 Test MSE 370.82625434449506 Test RE 0.9816299936019547\n",
      "4 Train Loss 2.2714455 Test MSE 362.4127323451775 Test RE 0.9704302058348222\n",
      "5 Train Loss 2.1941829 Test MSE 347.56427503743294 Test RE 0.9503424913998939\n",
      "6 Train Loss 1.9431483 Test MSE 302.9891205645868 Test RE 0.8873114787154054\n",
      "7 Train Loss 1.7173742 Test MSE 263.3305054485586 Test RE 0.8272050344207934\n",
      "8 Train Loss 1.6076704 Test MSE 237.99382676571165 Test RE 0.7864034835419544\n",
      "9 Train Loss 1.3938714 Test MSE 208.56238875078384 Test RE 0.7361741756422241\n",
      "10 Train Loss 1.2109225 Test MSE 174.74095719720185 Test RE 0.673844887518641\n",
      "11 Train Loss 1.0109781 Test MSE 149.06332275218924 Test RE 0.6223690415317873\n",
      "12 Train Loss 0.69148934 Test MSE 90.37646291630028 Test RE 0.4846076355384815\n",
      "13 Train Loss 0.5765887 Test MSE 60.54939913846596 Test RE 0.39665917239843995\n",
      "14 Train Loss 0.3052161 Test MSE 30.191259622777853 Test RE 0.2800936086435022\n",
      "15 Train Loss 0.21900484 Test MSE 23.45168437051781 Test RE 0.24685936231015668\n",
      "16 Train Loss 0.09734715 Test MSE 7.436085611422486 Test RE 0.13900639264365547\n",
      "17 Train Loss 0.07886991 Test MSE 6.642942250431066 Test RE 0.13138410320312283\n",
      "18 Train Loss 0.06740499 Test MSE 4.63178200054073 Test RE 0.10970758067180662\n",
      "19 Train Loss 0.049234927 Test MSE 1.6422550713984043 Test RE 0.06532552194190877\n",
      "20 Train Loss 0.030318085 Test MSE 0.3641292330979213 Test RE 0.030760285055607586\n",
      "21 Train Loss 0.025956789 Test MSE 0.06150994438425855 Test RE 0.012642566587312347\n",
      "22 Train Loss 0.023267137 Test MSE 0.000390130633073724 Test RE 0.00100685657108949\n",
      "23 Train Loss 0.019343799 Test MSE 0.005216268332178484 Test RE 0.0036816503821385945\n",
      "24 Train Loss 0.014137981 Test MSE 0.022625628419015772 Test RE 0.007667661070440983\n",
      "25 Train Loss 0.00861124 Test MSE 0.016337818307961854 Test RE 0.006515677998011848\n",
      "26 Train Loss 0.007841835 Test MSE 0.004096772052314668 Test RE 0.0032627476278723646\n",
      "27 Train Loss 0.007581463 Test MSE 1.8373264399443835e-05 Test RE 0.00021850216522016513\n",
      "28 Train Loss 0.0047062477 Test MSE 0.004706844311228735 Test RE 0.0034972566060822584\n",
      "29 Train Loss 0.0024878008 Test MSE 3.0618911853222125e-05 Test RE 0.00028207036387595594\n",
      "30 Train Loss 0.0024534767 Test MSE 8.761281743514463e-05 Test RE 0.00047714063673918533\n",
      "31 Train Loss 0.0024452186 Test MSE 0.0001202131309072265 Test RE 0.0005589056973092485\n",
      "32 Train Loss 0.0024365825 Test MSE 0.00015477765027031433 Test RE 0.0006341860826847856\n",
      "33 Train Loss 0.0022407544 Test MSE 0.005012825273174207 Test RE 0.0036091411405793886\n",
      "34 Train Loss 0.0020099843 Test MSE 0.014621440355926168 Test RE 0.0061639294745796285\n",
      "35 Train Loss 0.0019506264 Test MSE 0.014849939139022161 Test RE 0.006211906631357312\n",
      "36 Train Loss 0.0019323664 Test MSE 0.014031723776107966 Test RE 0.006038347407343732\n",
      "37 Train Loss 0.0019257247 Test MSE 0.013607206318035364 Test RE 0.005946303439065827\n",
      "38 Train Loss 0.0019203632 Test MSE 0.013170642401035 Test RE 0.005850137337485647\n",
      "39 Train Loss 0.0019165416 Test MSE 0.012785577434633632 Test RE 0.005763983858549455\n",
      "40 Train Loss 0.0019137056 Test MSE 0.01248415148855191 Test RE 0.0056956343099877705\n",
      "41 Train Loss 0.001911201 Test MSE 0.012154238108739678 Test RE 0.005619872369680068\n",
      "42 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "43 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "44 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "45 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "46 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "47 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "48 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "49 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "50 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "51 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "52 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "53 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "54 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "55 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "56 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "57 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "58 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "59 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "60 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "61 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "62 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "63 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "64 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "65 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "66 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "67 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "68 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "69 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "70 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "71 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "72 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "73 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "74 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "75 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "76 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "77 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "78 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "79 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "80 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "81 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "82 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "83 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "84 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "85 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "86 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "87 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "88 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "89 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "90 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "91 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "92 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "93 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "94 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "95 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "96 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "97 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "98 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "99 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "100 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "101 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "102 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "103 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "104 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "105 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "106 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "107 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "108 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "109 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "110 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "111 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "112 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "113 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "114 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "115 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "116 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "117 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "118 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "119 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "120 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "121 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "122 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "123 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "124 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "125 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "126 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "127 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "128 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "129 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "130 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "131 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "132 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "133 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "134 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "135 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "136 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "137 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "138 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "139 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "140 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "141 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "142 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "143 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "144 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "145 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "146 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "147 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "148 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "149 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "150 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "151 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "152 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "153 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "154 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "155 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "156 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "157 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "158 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "159 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "160 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "161 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "162 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "163 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "164 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "165 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "166 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "167 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "168 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "169 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "170 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "171 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "172 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "173 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "174 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "175 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "176 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "177 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "178 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "179 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "180 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "181 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "182 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "183 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "184 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "185 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "186 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "187 Train Loss 0.001908976 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "188 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "189 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "190 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "191 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "192 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "193 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "194 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "195 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "196 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "197 Train Loss 0.0019089758 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "198 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "199 Train Loss 0.0019089759 Test MSE 0.011948745254063666 Test RE 0.005572161991883281\n",
      "Training time: 28.74\n",
      "Training time: 28.74\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2c8854bb10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014222963404250672\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
