{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2.0*x) + np.exp(-3.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_tanhALR_\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.lambdas = torch.ones((2,),device = device)\n",
    "        \n",
    "        self.lambda_alpha = 0.1\n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.lambdas[1]*self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def lambda_update(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc1.backward()\n",
    "        bc1_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc1_grads.append(param.grad.view(-1))\n",
    "        bc1_grads = torch.cat(bc1_grads)\n",
    "        bc1_grads = torch.mean(torch.abs(bc1_grads))\n",
    "        \n",
    "        \n",
    "        loss_bc2 = self.lambdas[1]*self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_bc2.backward()\n",
    "        bc2_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc2_grads.append(param.grad.view(-1))\n",
    "        bc2_grads = torch.cat(bc2_grads)\n",
    "        bc2_grads = torch.mean(torch.abs(bc2_grads))\n",
    "        \n",
    "    \n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        loss_f.backward()\n",
    "        f_grads = []\n",
    "        for param in self.parameters():\n",
    "            f_grads.append(param.grad.view(-1))   \n",
    "        f_grads = torch.cat(f_grads)\n",
    "        f_grads = torch.max(torch.abs(f_grads))\n",
    "    \n",
    "        self.lambdas[0] = (1.0-self.lambda_alpha)*self.lambdas[0] + self.lambda_alpha*f_grads/bc1_grads\n",
    "        self.lambdas[1] = (1.0-self.lambda_alpha)*self.lambdas[1] + self.lambda_alpha*f_grads/bc2_grads\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    PINN.lambda_update(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        print\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 8.567006 Test MSE 384.7808846347728 Test RE 0.9999293751063201\n",
      "1 Train Loss 10.137734 Test MSE 385.14023735798406 Test RE 1.0003961907696532\n",
      "2 Train Loss 8.320732 Test MSE 385.3736087236702 Test RE 1.0006992342420633\n",
      "3 Train Loss 7.8856044 Test MSE 383.96196827599454 Test RE 0.9988647501376681\n",
      "4 Train Loss 7.200137 Test MSE 382.98076460054165 Test RE 0.9975876487653722\n",
      "5 Train Loss 6.693535 Test MSE 382.86861867124344 Test RE 0.9974415793078224\n",
      "6 Train Loss 6.1921167 Test MSE 383.03903622175005 Test RE 0.9976635387890116\n",
      "7 Train Loss 5.7063518 Test MSE 383.1807465372162 Test RE 0.9978480710789184\n",
      "8 Train Loss 5.217752 Test MSE 383.2176923243015 Test RE 0.9978961755232358\n",
      "9 Train Loss 4.7905626 Test MSE 383.2872561532298 Test RE 0.9979867432739633\n",
      "10 Train Loss 4.4084454 Test MSE 383.3314599956591 Test RE 0.9980442896392409\n",
      "11 Train Loss 4.059435 Test MSE 383.38764343283253 Test RE 0.9981174267412312\n",
      "12 Train Loss 3.739053 Test MSE 383.4296286073024 Test RE 0.9981720776771541\n",
      "13 Train Loss 3.4383936 Test MSE 383.48083962752094 Test RE 0.9982387335899167\n",
      "14 Train Loss 3.1569548 Test MSE 383.5217234268225 Test RE 0.998291944467905\n",
      "15 Train Loss 2.8856723 Test MSE 383.57025946387034 Test RE 0.9983551111661122\n",
      "16 Train Loss 2.646853 Test MSE 383.61317813264185 Test RE 0.9984109638720338\n",
      "17 Train Loss 2.426712 Test MSE 383.65952716480905 Test RE 0.9984712772121743\n",
      "18 Train Loss 2.2397184 Test MSE 383.7015331270491 Test RE 0.9985259358298243\n",
      "19 Train Loss 2.168081 Test MSE 383.74490520629143 Test RE 0.9985823689145262\n",
      "20 Train Loss 2.33332 Test MSE 383.78746742787337 Test RE 0.9986377451619264\n",
      "21 Train Loss 2.3559713 Test MSE 383.76407248340945 Test RE 0.9986073071817636\n",
      "22 Train Loss 2.172076 Test MSE 383.773982539838 Test RE 0.9986202007687172\n",
      "23 Train Loss 2.1884596 Test MSE 383.8294442291821 Test RE 0.9986923567315936\n",
      "24 Train Loss 2.0901244 Test MSE 383.8318817691244 Test RE 0.9986955278648684\n",
      "25 Train Loss 1.9631014 Test MSE 383.85007840127065 Test RE 0.9987192005724137\n",
      "26 Train Loss 1.8537256 Test MSE 383.85959694287436 Test RE 0.9987315833904242\n",
      "27 Train Loss 1.7794092 Test MSE 383.87497439539567 Test RE 0.9987515878319183\n",
      "28 Train Loss 1.7207936 Test MSE 383.8891798861308 Test RE 0.9987700673188871\n",
      "29 Train Loss 1.6503757 Test MSE 383.90264990295555 Test RE 0.9987875897346035\n",
      "30 Train Loss 1.6090864 Test MSE 383.9170744557152 Test RE 0.9988063535135968\n",
      "31 Train Loss 1.5638119 Test MSE 383.92589773095307 Test RE 0.9988178308524797\n",
      "32 Train Loss 1.531382 Test MSE 383.9347039142958 Test RE 0.998829285826681\n",
      "33 Train Loss 1.5172743 Test MSE 383.94067885254316 Test RE 0.9988370578775309\n",
      "34 Train Loss 1.490079 Test MSE 383.9440192481441 Test RE 0.9988414029545712\n",
      "35 Train Loss 1.4623886 Test MSE 383.94956333255584 Test RE 0.998848614476022\n",
      "36 Train Loss 1.4390317 Test MSE 383.9548134859704 Test RE 0.9988554436158853\n",
      "37 Train Loss 1.404267 Test MSE 383.95944455786616 Test RE 0.9988614674463753\n",
      "38 Train Loss 1.4118743 Test MSE 383.9647086323128 Test RE 0.9988683146058592\n",
      "39 Train Loss 1.3562348 Test MSE 383.9647086323128 Test RE 0.9988683146058592\n",
      "40 Train Loss 1.3541994 Test MSE 383.97391680015494 Test RE 0.9988802918680171\n",
      "41 Train Loss 1.2764981 Test MSE 383.97618088586563 Test RE 0.998883236791048\n",
      "42 Train Loss 1.3039991 Test MSE 383.9878312330466 Test RE 0.9988983903979767\n",
      "43 Train Loss 1.2736117 Test MSE 383.9878312330466 Test RE 0.9988983903979767\n",
      "44 Train Loss 1.3312393 Test MSE 383.9878312330466 Test RE 0.9988983903979767\n",
      "45 Train Loss 1.5243493 Test MSE 383.9878312330466 Test RE 0.9988983903979767\n",
      "46 Train Loss 2.9367898 Test MSE 383.9878312330466 Test RE 0.9988983903979767\n",
      "47 Train Loss 4.345244 Test MSE 383.76613971088113 Test RE 0.9986099967843433\n",
      "48 Train Loss 4.3206253 Test MSE 383.7849467664145 Test RE 0.9986344657012579\n",
      "49 Train Loss 4.0007176 Test MSE 383.6632413724758 Test RE 0.9984761103004138\n",
      "50 Train Loss 3.6727817 Test MSE 383.52844659956065 Test RE 0.9983006945065104\n",
      "51 Train Loss 3.3674994 Test MSE 383.5206686072925 Test RE 0.9982905716401105\n",
      "52 Train Loss 3.0723572 Test MSE 383.657990821235 Test RE 0.9984692780483878\n",
      "53 Train Loss 2.7941506 Test MSE 383.71919699065614 Test RE 0.9985489193488215\n",
      "54 Train Loss 2.5922608 Test MSE 383.7373684669059 Test RE 0.9985725628012457\n",
      "55 Train Loss 2.4074442 Test MSE 383.77350741804474 Test RE 0.9986195826101278\n",
      "56 Train Loss 2.2408984 Test MSE 383.80161331800724 Test RE 0.9986561492147624\n",
      "57 Train Loss 2.1120408 Test MSE 383.8263851308686 Test RE 0.9986883769637082\n",
      "58 Train Loss 1.9845083 Test MSE 383.8465498247635 Test RE 0.9987146101536466\n",
      "59 Train Loss 1.8727643 Test MSE 383.8651696908696 Test RE 0.9987388329932391\n",
      "60 Train Loss 1.7950094 Test MSE 383.8810698293594 Test RE 0.9987595172367889\n",
      "61 Train Loss 1.732294 Test MSE 383.8928556100899 Test RE 0.9987748488997056\n",
      "62 Train Loss 1.661813 Test MSE 383.90271229920546 Test RE 0.998787670901792\n",
      "63 Train Loss 1.6091834 Test MSE 383.91245589124674 Test RE 0.998800345620596\n",
      "64 Train Loss 1.5772489 Test MSE 383.9195132218351 Test RE 0.9988095258799607\n",
      "65 Train Loss 1.5524044 Test MSE 383.92417336177243 Test RE 0.99881558779903\n",
      "66 Train Loss 1.5143843 Test MSE 383.92855199935417 Test RE 0.9988212835057653\n",
      "67 Train Loss 1.4746014 Test MSE 383.9337950115405 Test RE 0.9988281035433162\n",
      "68 Train Loss 1.4807554 Test MSE 383.9382130832541 Test RE 0.9988338504735854\n",
      "69 Train Loss 1.3946636 Test MSE 383.9425162248593 Test RE 0.9988394478735173\n",
      "70 Train Loss 1.3829255 Test MSE 383.9508879504863 Test RE 0.998850337477908\n",
      "71 Train Loss 1.3988512 Test MSE 383.95202009553674 Test RE 0.9988518101175963\n",
      "72 Train Loss 1.3120633 Test MSE 383.95202009553674 Test RE 0.9988518101175963\n",
      "73 Train Loss 1.3336742 Test MSE 383.96211586490443 Test RE 0.9988649421115584\n",
      "74 Train Loss 1.2103083 Test MSE 383.96211586490443 Test RE 0.9988649421115584\n",
      "75 Train Loss 1.2385672 Test MSE 383.9775024563585 Test RE 0.9988849557693874\n",
      "76 Train Loss 1.1777674 Test MSE 383.9775024563585 Test RE 0.9988849557693874\n",
      "77 Train Loss 1.4717134 Test MSE 383.9775024563585 Test RE 0.9988849557693874\n",
      "78 Train Loss 2.5888278 Test MSE 383.9775024563585 Test RE 0.9988849557693874\n",
      "79 Train Loss 3.3077645 Test MSE 383.76472329739704 Test RE 0.9986081539355046\n",
      "80 Train Loss 3.4112327 Test MSE 383.86134179187195 Test RE 0.9987338532748242\n",
      "81 Train Loss 3.1787734 Test MSE 383.73108488502606 Test RE 0.9985643871078375\n",
      "82 Train Loss 2.9380105 Test MSE 383.72847104162116 Test RE 0.9985609861644615\n",
      "83 Train Loss 2.674878 Test MSE 383.73217375237726 Test RE 0.9985658038596082\n",
      "84 Train Loss 2.4582343 Test MSE 383.7439090134956 Test RE 0.9985810727654519\n",
      "85 Train Loss 2.2673414 Test MSE 383.7729726428634 Test RE 0.9986188868388666\n",
      "86 Train Loss 2.100018 Test MSE 383.8055549972246 Test RE 0.9986612773486021\n",
      "87 Train Loss 1.9587829 Test MSE 383.84159335684456 Test RE 0.9987081621174358\n",
      "88 Train Loss 1.8659599 Test MSE 383.8726545371704 Test RE 0.998748569967065\n",
      "89 Train Loss 1.7891043 Test MSE 383.8923468586485 Test RE 0.9987741870896395\n",
      "90 Train Loss 1.7371194 Test MSE 383.9061644241077 Test RE 0.9987921615343522\n",
      "91 Train Loss 1.694128 Test MSE 383.9142797201189 Test RE 0.998802718086918\n",
      "92 Train Loss 1.6406075 Test MSE 383.920367949032 Test RE 0.9988106377135114\n",
      "93 Train Loss 1.6071053 Test MSE 383.92587566631556 Test RE 0.9988178021509075\n",
      "94 Train Loss 1.5794641 Test MSE 383.9291465466112 Test RE 0.9988220568869343\n",
      "95 Train Loss 1.5426607 Test MSE 383.9317394125946 Test RE 0.9988254296542607\n",
      "96 Train Loss 1.530196 Test MSE 383.93417850086354 Test RE 0.9988286023790824\n",
      "97 Train Loss 1.4978086 Test MSE 383.9354542737625 Test RE 0.9988302618789061\n",
      "98 Train Loss 1.4817656 Test MSE 383.93758369745547 Test RE 0.9988330317844359\n",
      "99 Train Loss 1.4637235 Test MSE 383.938886967269 Test RE 0.9988347270441468\n",
      "Training time: 18.72\n",
      "Training time: 18.72\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8dc1d2d90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1lUlEQVR4nO3deXxU1d3H8W9IyJBAEgUkIRJoqFFEwAUURRQQiSKiVqtsKhRrRZaHFJVFXBBrAogUKYJLXaCIWAvuG8EFVLQEMIKgCJICKmkEYyZASCC5zx+nM1mBLDNzZ/m8X6/zumfu3Jn8ch9qvs+5954TZlmWJQAAAB9pZHcBAAAgtBA+AACATxE+AACATxE+AACATxE+AACATxE+AACATxE+AACATxE+AACAT0XYXUBVZWVl+umnnxQTE6OwsDC7ywEAALVgWZYKCwuVmJioRo2OP7bhd+Hjp59+UlJSkt1lAACAetizZ4/atGlz3GP8LnzExMRIMsXHxsbaXA0AAKgNp9OppKQk99/x4/G78OG61BIbG0v4AAAgwNTmlgluOAUAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAIERYljRsmDRvnlRYaF8dhA8AAELEpk3S0qXSxIkmiNiF8AEAQIhYtsxsBwyQYmPtq4PwAQBACLCs8vAxaJC9tRA+AAAIAVlZ0n/+IzVtakY+7ET4AAAgBLhGPa65xgQQOxE+AAAIcmVl0j//afp2X3KRCB8AAAS9zz6TfvxRiouTrrzS7moIHwAABL2XXzbb666THA5bS5FE+AAAIKgdPSq98orpDx5sby0uhA8AAILYxx9LeXlSixZS3752V2MQPgAACGKuSy433CA1bmxvLS6EDwAAglRJibR8uen7yyUXifABAEDQWrVKys+XEhKkSy+1u5pyhA8AAIKUa2KxG2+UwsPtraUiwgcAAEHo4EHp1VdNf8gQe2upivABAEAQeuMN6cABqX176cIL7a6mMsIHAABB6MUXzXboUCkszN5aqiJ8AAAQZPbtk95/3/SHDbO3lpoQPgAACDL//KeZ2bRrV6lDB7urqY7wAQBAkFmyxGz9cdRDInwAABBUdu6UPv9catTIvyYWq4jwAQBAEFm61Gz79pVat7a3lmMhfAAAECQsq/wpF3+95CIRPgAACBobN0rffis1aSL97nd2V3NshA8AAIKEa9Tjmmuk2Fh7azmeOoWPadOmKSwsrFJLSEhwv29ZlqZNm6bExERFRUWpd+/e2rJli8eLBgAAlZWWlq/l4s+XXKR6jHycddZZ2rt3r7tt3rzZ/d6sWbM0Z84czZ8/X1lZWUpISFC/fv1UWFjo0aIBAEBlH34o7d0rNW8uXXml3dUcX53DR0REhBISEtztlFNOkWRGPebOnaupU6fq+uuvV6dOnbRo0SIdOnRIS1233gIAAK9YtMhsBw+WIiPtreVE6hw+tm/frsTERCUnJ2vw4MHauXOnJCknJ0e5ublKTU11H+twONSrVy+tXbv2mN9XXFwsp9NZqQEAgNpzOqUVK0x/+HB7a6mNOoWP7t27a/HixXr//ff1zDPPKDc3Vz169ND+/fuVm5srSYqPj6/0mfj4ePd7NcnIyFBcXJy7JSUl1ePXAAAgdL3yilRUZKZSP/98u6s5sTqFj/79++uGG25Q586ddfnll+vtt9+WJC1yjfVICquydJ5lWdX2VTRlyhQVFBS42549e+pSEgAAIc/1Z3j4cP9bwbYmDXrUtmnTpurcubO2b9/ufuql6ihHXl5etdGQihwOh2JjYys1AABQOzt3Sp98YkLHzTfbXU3tNCh8FBcX65tvvlHr1q2VnJyshIQEZWZmut8vKSnR6tWr1aNHjwYXCgAAqlu82Gwvv1xq08beWmoroi4H33333Ro4cKDatm2rvLw8/eUvf5HT6dTw4cMVFhamtLQ0paenKyUlRSkpKUpPT1d0dLSGDh3qrfoBAAhZZWXl4SMQbjR1qVP4+OGHHzRkyBDt27dPp5xyii688EJ98cUXateunSRp4sSJKioq0ujRo5Wfn6/u3btr5cqViomJ8UrxAACEsk8/lXJypJgY/55Ovaowy7Isu4uoyOl0Ki4uTgUFBdz/AQDAcdx2m/Tcc9LIkdKzz9pbS13+frO2CwAAAejQIfOIrRRYl1wkwgcAAAHp1VelwkIpOVnq2dPuauqG8AEAQAByze1x661SowD7ax5g5QIAgF27pFWrTP/WW+2tpT4IHwAABJjnn5csS7rsMql9e7urqTvCBwAAAaS01IQPyTztEogIHwAABJAPPpB275ZOOimw5vaoiPABAEAAcc3nMWyYFBVlby31RfgAACBA7N8vvfaa6QfqJReJ8AEAQMB48UWppEQ691zTAhXhAwCAAGBZ5ZdcAnnUQyJ8AAAQEDZskDZtkhwOKdAXiyd8AAAQAFyjHjfcIJ18sr21NBThAwAAP3fokLR0qekH+iUXifABAIDf+9e/JKfTLCLXu7fd1TQc4QMAAD/31FNm+8c/Bt4icjUJgl8BAIDg9fXX0tq1UkSENHKk3dV4BuEDAAA/5hr1uPZaKSHB3lo8hfABAICfOnRI+sc/TP+OO+ytxZMIHwAA+KmXX5YKCqT27aW+fe2uxnMIHwAA+CnXJZfbbw+OG01dguhXAQAgeHz1lfTvf5sbTf/wB7ur8SzCBwAAfsg16vG730nx8fbW4mmEDwAA/MzBg9KSJaYfTDeauhA+AADwM8uWSYWF0mmnSX362F2N5xE+AADwM65LLn/6U3DdaOoShL8SAACBa906KStLioyURoywuxrvIHwAAOBHnnjCbAcNkk45xd5avIXwAQCAn9i3z0wsJkljxthbizcRPgAA8BPPPisVF0tdu0oXXGB3Nd5D+AAAwA+UlkoLF5r+2LFSWJi99XgT4QMAAD/w9tvSrl1S8+bmfo9gRvgAAMAPuG40ve02KSrK3lq8jfABAIDNvvtOWrnSXGq58067q/E+wgcAADZbsMBsr7pKSk62txZfIHwAAGCjgwelF14w/bFjbS3FZwgfAADYaMkSqaDArOOSmmp3Nb5B+AAAwCaWJT3+uOmPHh2c67jUJER+TQAA/E9mpvTNN1KzZtLIkXZX4zuEDwAAbDJ3rtmOHCnFxdlaik8RPgAAsMG2bdK775rHa8eNs7sa3yJ8AABgg3nzzPbqq83NpqGE8AEAgI/l55c/XpuWZmcl9iB8AADgY88+Kx06JHXuLPXpY3c1vkf4AADAh44elf72N9MfPz64V689FsIHAAA+9Prr0u7dUsuW0tChdldjD8IHAAA+5Hq8dtSo4F+99lgIHwAA+Mi6ddKnn0qNG4fG6rXHQvgAAMBHZs8226FDpcREe2uxE+EDAAAf2LlTWr7c9O+6y95a7Nag8JGRkaGwsDClVXhI2bIsTZs2TYmJiYqKilLv3r21ZcuWhtYJAEBAmztXKiuTrrjCPGIbyuodPrKysvT000+rS5culfbPmjVLc+bM0fz585WVlaWEhAT169dPhYWFDS4WAIBAtH+/mdtDku65x95a/EG9wseBAwc0bNgwPfPMMzr55JPd+y3L0ty5czV16lRdf/316tSpkxYtWqRDhw5p6dKlHisaAIBA8uSTZlKxc86RLrvM7mrsV6/wMWbMGA0YMECXX355pf05OTnKzc1Vamqqe5/D4VCvXr20du3aGr+ruLhYTqezUgMAIFgcPlw+qdjdd4fmpGJVRdT1A8uWLdPGjRuVlZVV7b3c3FxJUnx8fKX98fHx2rVrV43fl5GRoYceeqiuZQAAEBCWLJH++18pKUm66Sa7q/EPdRr52LNnj8aPH68lS5aoSZMmxzwurEqssyyr2j6XKVOmqKCgwN327NlTl5IAAPBbZWXSY4+Zflqamd8DdRz52LBhg/Ly8tS1a1f3vtLSUq1Zs0bz58/Xtm3bJJkRkNatW7uPycvLqzYa4uJwOORwOOpTOwAAfu2dd6Rvv5ViY6U//tHuavxHnUY++vbtq82bNys7O9vdunXrpmHDhik7O1vt27dXQkKCMjMz3Z8pKSnR6tWr1aNHD48XDwCAP5sxw2zvuMMEEBh1GvmIiYlRp06dKu1r2rSpWrRo4d6flpam9PR0paSkKCUlRenp6YqOjtbQUF09BwAQkj75RPrsMyky0lxyQbk633B6IhMnTlRRUZFGjx6t/Px8de/eXStXrlRMTIynfxQAAH4rI8NsR4wI7anUaxJmWZZldxEVOZ1OxcXFqaCgQLGMUQEAAlB2tnTuuVKjRtJ330m//a3dFXlfXf5+s7YLAAAe5rrX46abQiN41BXhAwAAD9qxQ3rlFdOfPNneWvwV4QMAAA+aNcvM73HVVdLZZ9tdjX8ifAAA4CE//ii98ILpT5liayl+jfABAICHzJkjHTki9expGmpG+AAAwAP275eeesr0GfU4PsIHAAAe8Ne/SgcPSuecI/Xvb3c1/o3wAQBAA+XnS/Pmmf7990vHWEsV/0P4AACggR5/XCoslDp1kq67zu5q/B/hAwCABigokObONf377zezmuL4OEUAADTA3/5mAsiZZ0o33GB3NYGB8AEAQD0VFprHayUz6hEebm89gYLwAQBAPT3xhLnZ9PTTzTouqB3CBwAA9XDggPTYY6Z/332MetQF4QMAgHp48klp3z6zau2QIXZXE1gIHwAA1NGBA9LMmaY/daoUEWFvPYGG8AEAQB3Nm2dGPU47TbrlFrurCTyEDwAA6qCgQJo92/QffJBRj/ogfAAAUAd//at5wuXMM7nXo74IHwAA1NL+/SZ8SNJDD/GES30RPgAAqKXZsyWnU+rShdlMG4LwAQBALeTlla9cO306a7g0BKcOAIBamDlTOnRI6tZNuuYau6sJbIQPAABO4KefpAULTP/hh6WwMHvrCXSEDwAATmD6dOnwYalHD+mKK+yuJvARPgAAOI7vvpP+/nfTnzGDUQ9PIHwAAHAc990nlZZKAwZIl1xidzXBgfABAMAxrF8vvfKKGe1IT7e7muBB+AAA4BimTDHbYcPM3B7wDMIHAAA1WLXKtMaNzQ2n8BzCBwAAVZSVSZMnm/6dd0rJyfbWE2wIHwAAVLF8ubRhg9SsmTR1qt3VBB/CBwAAFZSUSPfea/p33SW1amVvPcGI8AEAQAVPPint2CHFx5vwAc8jfAAA8D/5+dJDD5n+9OlSTIy99QQrwgcAAP+Tni798ovUsaM0cqTd1QQvwgcAAJJycqR580x/9mwpIsLeeoIZ4QMAAJkJxUpKpMsvl6680u5qghvhAwAQ8r74Qnr5ZTON+uzZLB7nbYQPAEBIs6zyp1pGjJDOPtvWckIC4QMAENL+9S9p7VopOlp6+GG7qwkNhA8AQMgqKpLuucf077lHOvVUe+sJFYQPAEDImj1b2rVLatNGmjjR7mpCB+EDABCS9uyRMjJM/9FHzWUX+AbhAwAQkiZPNpddevaUBg2yu5rQQvgAAISczz6Tli41j9Q+/jiP1voa4QMAEFLKyqTx403/ttuk886zt55QRPgAAISURYukDRuk2FjpL3+xu5rQRPgAAISMX38193pI0gMPSPHxtpYTsggfAICQ8cADUl6edMYZ0rhxdlcTuggfAICQkJ0tPfGE6c+fL0VG2lpOSKtT+Fi4cKG6dOmi2NhYxcbG6qKLLtK7777rft+yLE2bNk2JiYmKiopS7969tWXLFo8XDQBAXZSVSaNHm+1NN5mVa2GfOoWPNm3aaMaMGVq/fr3Wr1+vyy67TNdee607YMyaNUtz5szR/PnzlZWVpYSEBPXr10+FhYVeKR4AgNpYtEj6/HOpaVNpzhy7q0GYZVlWQ76gefPmevTRRzVy5EglJiYqLS1NkyZNkiQVFxcrPj5eM2fO1B133FGr73M6nYqLi1NBQYFiY2MbUhoAAMrPl04/Xdq3z8xkevfddlcUnOry97ve93yUlpZq2bJlOnjwoC666CLl5OQoNzdXqamp7mMcDod69eqltWvXHvN7iouL5XQ6KzUAADxl6lQTPDp2LJ/fA/aqc/jYvHmzmjVrJofDoVGjRunVV19Vx44dlZubK0mKr/LcUnx8vPu9mmRkZCguLs7dkpKS6loSAAA12rBBevJJ03/iCalxY3vrgVHn8HHGGWcoOztbX3zxhe68804NHz5cW7dudb8fVmWOWsuyqu2raMqUKSooKHC3PXv21LUkAACqOXpU+tOfJMuShg6Veve2uyK4RNT1A5GRkTrttNMkSd26dVNWVpYef/xx930eubm5at26tfv4vLy8aqMhFTkcDjkcjrqWAQDAcc2fL23cKJ10kvTYY3ZXg4oaPM+HZVkqLi5WcnKyEhISlJmZ6X6vpKREq1evVo8ePRr6YwAAqLXdu6X77jP9mTOlhAR760FldRr5uPfee9W/f38lJSWpsLBQy5Yt08cff6z33ntPYWFhSktLU3p6ulJSUpSSkqL09HRFR0dr6NCh3qofAIBKLEsaO1Y6eFC6+GLpj3+0uyJUVafw8d///le33HKL9u7dq7i4OHXp0kXvvfee+vXrJ0maOHGiioqKNHr0aOXn56t79+5auXKlYmJivFI8AABVvfqq9Oab5ubSp5+WGjGXt99p8DwfnsY8HwCA+iooMI/U/vSTuezy8MN2VxQ6fDLPBwAA/ua++0zwOO006d577a4Gx0L4AAAEhc8+K1847sknpagoe+vBsRE+AAAB7/Bh6bbbzM2mf/iD1Lev3RXheAgfAICAN326tG2beaSWOT38H+EDABDQNm6UZs0y/YULpZNPtrcenBjhAwAQsI4ckUaOlEpLpUGDpOuus7si1AbhAwAQsGbOlL76SmrRQvrb3+yuBrVF+AAABKStW8vn8Zg3TzrlFHvrQe0RPgAAAefIEWn4cKmkRBo4UBoyxO6KUBeEDwBAwJkxQ1q/3txc+uSTUliY3RWhLggfAICAsnGjebRWMpOKJSbaWw/qjvABAAgYxcXSrbdKR49Kv/+9NHiw3RWhPggfAICA8cAD0pYtUqtW0oIFXG4JVIQPAEBAWLtWevRR03/qKZ5uCWSEDwCA3ztwwDzdYlnmsguTiQU2wgcAwO/9+c/Sjh1SmzbS44/bXQ0aivABAPBrr74q/f3v5v6OxYulk06yuyI0FOEDAOC3fvpJuv1207/7bqlPH3vrgWcQPgAAfqmsTBoxQtq/Xzr33PKp1BH4CB8AAL80b56UmSk1aSK9+KLkcNhdETyF8AEA8DubNkmTJpn+Y49JZ55pbz3wLMIHAMCvHDxoZi4tKZEGDJDuvNPuiuBphA8AgF8ZP1765hspIUF67jlmMQ1GhA8AgN946SXp2WdN4FiyxEyjjuBD+AAA+IUdO6Q//cn0p06V+va1tx54D+EDAGC74mJp0CAzjfoll0gPPmh3RfAmwgcAwHaTJ0sbN0rNm0tLl0oREXZXBG8ifAAAbPXaa9Lcuab/wgtm/RYEN8IHAMA2O3aY1Wols3jcwIH21gPfIHwAAGxRVCT9/veS0yldfLE0c6bdFcFXCB8AAFuMHSt99ZV5nPbll6XGje2uCL5C+AAA+Nxzz5nWqJG5wfTUU+2uCL5E+AAA+FR2tjRmjOlPn858HqGI8AEA8JlffpFuuEE6fNis2zJlit0VwQ6EDwCAT5SWSkOGSDt3SsnJ0uLF5rILQg//ZwcA+MTUqdLKlVJ0tJnbo3lzuyuCXQgfAACv++c/yx+lff55qUsXe+uBvQgfAACv2rRJ+sMfTH/iROmmm+ytB/YjfAAAvOaXX6Tf/U46dEhKTZXS0+2uCP6A8AEA8IojR8wMpq4bTF96SQoPt7sq+APCBwDA4yxLGjdO+ugjqVkz6fXXucEU5QgfAACPmz9feuopKSzMjHh07mx3RfAnhA8AgEe9/76Ulmb6M2dKV19taznwQ4QPAIDHfPutNGiQVFYmDR8u3X233RXBHxE+AAAe8fPPZpSjoEC6+OLyyy5AVYQPAECDFRVJ114rff+99JvfSCtWSA6H3VXBXxE+AAANUlYm3Xqr9Pnn0kknSe+8I7VqZXdV8GeEDwBAg0yeLP3rX1LjxmbNljPPtLsi+DvCBwCg3hYulB591PSff17q1cveehAYCB8AgHp56y1p7FjTf/hhadgwe+tB4KhT+MjIyND555+vmJgYtWrVStddd522bdtW6RjLsjRt2jQlJiYqKipKvXv31pYtWzxaNADAXp9/bhaIKyszi8ZNnWp3RQgkdQofq1ev1pgxY/TFF18oMzNTR48eVWpqqg4ePOg+ZtasWZozZ47mz5+vrKwsJSQkqF+/fiosLPR48QAA39u6VRowwDzhctVVPFKLuguzLMuq74d//vlntWrVSqtXr9all14qy7KUmJiotLQ0TZo0SZJUXFys+Ph4zZw5U3fccccJv9PpdCouLk4FBQWKjY2tb2kAAC/44QfpoovMtnt36YMPpKZN7a4K/qAuf78bdM9HQUGBJKn5/1YLysnJUW5urlJTU93HOBwO9erVS2vXrq3xO4qLi+V0Ois1AID/yc+XrrzSBI8zzjD3fBA8UB/1Dh+WZWnChAnq2bOnOnXqJEnKzc2VJMXHx1c6Nj4+3v1eVRkZGYqLi3O3pKSk+pYEAPCSQ4ekgQOlLVukxESzfkvLlnZXhUBV7/AxduxYbdq0SS+99FK198KqXPyzLKvaPpcpU6aooKDA3fbs2VPfkgAAXlBcLF1/vfTZZ1JcnPTee1K7dnZXhUAWUZ8PjRs3Tm+88YbWrFmjNm3auPcnJCRIMiMgrVu3du/Py8urNhri4nA45GAOXgDwS0ePmkdo339fio42s5d27mx3VQh0dRr5sCxLY8eO1YoVK/Thhx8qOTm50vvJyclKSEhQZmame19JSYlWr16tHj16eKZiAIBPlJVJt98uLV8uRUaa2Uv5Tzk8oU4jH2PGjNHSpUv1+uuvKyYmxn0fR1xcnKKiohQWFqa0tDSlp6crJSVFKSkpSk9PV3R0tIYOHeqVXwAA4HmWJU2YIL3wghQeLi1bJvXrZ3dVCBZ1Ch8LFy6UJPXu3bvS/ueff14jRoyQJE2cOFFFRUUaPXq08vPz1b17d61cuVIxMTEeKRgA4H0PPCA9/rjpP/ec9Lvf2VsPgkuD5vnwBub5AAB7TZ8uPfig6c+fL40ZY289CAw+m+cDABBcMjLKg8djjxE84B2EDwCAJGn2bOnee00/I8Pc8wF4A+EDAKB586R77jH96dOlyZPtrQfBjfABACFu7lxp/HjTv/9+0wBvInwAQAh79FHpz382/SlTpIcesrcehAbCBwCEqEcekSZONP0HHzSvj7ESBuBR9ZpeHQAQuCzL3NcxbZp5/fDD0n332VoSQgzhAwBCiGVJU6eap1kkacYMadIke2tC6CF8AECIKCuT/u//pCeeMK9nz5buusvemhCaCB8AEAKOHpVGjpT+8Q9zX8eCBdKoUXZXhVBF+ACAIFdcLA0ebFalDQ+XFi+WWOsTdiJ8AEAQO3DALAq3apXkcEivvCINHGh3VQh1hA8ACFJ5edKAAdL69VKzZtIbb0h9+thdFUD4AICglJMjpaZKO3ZILVtKb78tXXCB3VUBBuEDAIJMdrbUv7+Umyu1ayetXCmdfrrdVQHlmOEUAILIRx9JvXqZ4NGli7R2LcED/ofwAQBBYskS6YorJKfTBJA1a6TERLurAqojfABAgHNNl37LLdKRI9KNN0rvvSfFxdldGVAzwgcABLCSEjN52IMPmtcTJ0rLlklNmthbF3A83HAKAAHq11+l3/9e+uADqVEjM206s5YiEBA+ACAAbd9uJgvbtk1q2lT65z+lq66yuyqgdggfABBgPvzQjHjk50tt2khvvimdc47dVQG1xz0fABBAnnrKPNGSny917y5lZRE8EHgIHwAQAI4ckcaNM/d0HD0qDRsmffyxlJBgd2VA3XHZBQD8XF6eeXx2zRrz+pFHpClTpLAwe+sC6ovwAQB+bP16syrtDz9IMTHSP/4hXXut3VUBDcNlFwDwU4sXSz17muBxxhnSunUEDwQHwgcA+JniYmnMGGn4cNO/+mrp3/+WOnSwuzLAMwgfAOBHdu2SLrlEWrDAvH7gAen115kqHcGFez4AwE+88450883mMdrmzc1Ccf37210V4HmMfACAzY4ele6/XxowwASP88+XNm4keCB4MfIBADb64Qdp6FDpk0/M69GjpTlzJIfD3roAbyJ8AIBN3npLGjFC2r/fPEb71FPSkCF2VwV4H5ddAMDHSkqkCRPMwnD790vnnWcusxA8ECoIHwDgQ998I114ofTXv5rX48dLa9dKp51mb12ALxE+AMAHLEtauFDq2lX68kupRQvzCO3cudzfgdDDPR8A4GV5edJtt5l7PCQpNVV6/nkpMdHeugC7MPIBAF70+utS584meDgc5nLLu+8SPBDaGPkAAC/49VdzP8fixeZ1p07S0qUmiAChjpEPAPCwzEwTMhYvlho1kiZNMqvTEjwAg5EPAPAQp1OaONHM1yGZJ1gWLZJ69LC3LsDfMPIBAB7wzjvSWWeVB4+xY6XsbIIHUBNGPgCgAfbvl9LSzCJwkvTb30rPPiv16mVrWYBfY+QDAOrBsqSXXpI6djTBo1Ej6a67pE2bCB7AiTDyAQB19P33ZgG4lSvN644dpeeek7p3t7cuIFAw8gEAtXTkiDRjhnlsduVKM2/Hww+bGUsJHkDtMfIBALXw8cfSmDHS1q3m9WWXmenSTz/d1rKAgMTIBwAcx9690rBhUp8+Jni0bGken121iuAB1BfhAwBqcOSIWfTtjDPMzKRhYdKdd0rbtkm33mpeA6gfLrsAQBXvvy/9+c/SN9+Y1xdcIC1YYFakBdBwjHwAwP9s3y4NHChdeaUJHi1bSs88I33+OcED8KQ6h481a9Zo4MCBSkxMVFhYmF577bVK71uWpWnTpikxMVFRUVHq3bu3tmzZ4ql6AcDj8vOlu+82M5S+9ZYUEWFGPrZvl/74RzOHBwDPqfP/pA4ePKizzz5b8+fPr/H9WbNmac6cOZo/f76ysrKUkJCgfv36qbCwsMHFAoAnlZSY+zp++1vpscfMfR79+0ubN0tz5kgnnWR3hUBwqvM9H/3791f//v1rfM+yLM2dO1dTp07V9ddfL0latGiR4uPjtXTpUt1xxx0NqxYAPMCypOXLpcmTzYRhkhn1ePRREz4AeJdHBxNzcnKUm5ur1NRU9z6Hw6FevXpp7dq1NX6muLhYTqezUgMAb/noI+nCC6UbbzTBIz5eevppswgcwQPwDY+Gj9zcXElSfHx8pf3x8fHu96rKyMhQXFycuyUlJXmyJACQZGYhvfJKMznYunVSdLR0//3Sjh3S7beb+zwA+IZXbqMKq/IAvGVZ1fa5TJkyRQUFBe62Z88eb5QEIER9+600eLB03nnmEdqICDNT6fffS9OnS82a2V0hEHo8mvUTEhIkmRGQ1q1bu/fn5eVVGw1xcTgccjgcniwDALRjhwkXL74olZWZScGGDjX72re3uzogtHl05CM5OVkJCQnKzMx07yspKdHq1avVo0cPT/4oAKhRTo55PLZDB+kf/zDB49przWWXJUsIHoA/qPPIx4EDB7Rjxw7365ycHGVnZ6t58+Zq27at0tLSlJ6erpSUFKWkpCg9PV3R0dEaOnSoRwsHgIq2b5fS003gKC01+/r3NyMd3brZWxuAyuocPtavX68+ffq4X0+YMEGSNHz4cL3wwguaOHGiioqKNHr0aOXn56t79+5auXKlYmJiPFc1APzPt99Kjzxi1l8pKzP7UlOladOkiy6ytTQAxxBmWZZldxEVOZ1OxcXFqaCgQLGxsXaXA8BPZWVJM2ZIr75q5u2QpAEDzBMs3bvbWxsQiury95uHywAEDMuSPvxQysiQPvigfP9110n33cf6K0CgIHwA8HtHjkj/+pc0e7a0caPZFx4uDRsmTZokdexob30A6obwAcBvOZ3S3/8uPf64tHu32RcVJd12m1kIrl07e+sDUD+EDwB+Z+dO6W9/k557zgQQSTrlFGncOOnOO81S9wACF+EDgF+wLLPuyuOPS2++WX4TaYcO0l13STffLDVpYm+NADyD8AHAVoWFZm6OBQukLVvK9195pTR+vHlstpFXFoIAYBfCBwBbbNliAsfixdKBA2ZfdLQ0YoS5vNKhg63lAfAiwgcAnykqMk+tPP209Omn5fvPOEMaPVoaPlyKi7OvPgC+QfgA4HVbtkjPPGNGOfLzzb7wcOmaa8wKs5ddZhZ+AxAaCB8AvKKgQFq2zDyxsm5d+f527aTbb5f+8AcpMdG++gDYh/ABwGNKS80TK4sWScuXm8sskhQRIQ0cKP3pT1K/fmbUA0DoInwAaLCtW80llSVLpB9/LN/fsaOZEOzmm6VWreyrD4B/IXwAqJcff5Refll68cXyKc8l6eSTpcGDzc2jF1zAvRwAqiN8AKi1X36RVqwwy9d//HH5RGAREdJVV5nAMWCA5HDYWiYAP0f4AHBcv/4qvf66GeXIzJSOHi1/r2dPaehQ6fe/N9OfA0BtED4AVPPLL9Ibb5ibRleulEpKyt/r0kUaMsRcWvnNb2wrEUAAI3wAkCTl5poRjuXLzRMrFUc4OnaUBg2SbrqJmUcBNBzhAwhh27aZwPHaa9IXX5TfwyFJnTpJN9xgLql06mRbiQCCEOEDCCFHj0qffSa99ZZZOXbbtsrvd+smXX+9CR2nn25PjQCCH+EDCHI//yy9/770zjvSu++aG0hdGjeW+vSRrr3WTHXepo1tZQIIIYQPIMiUlkpZWSZovPuutH595cspLVqYx2Kvvlq64goWcgPge4QPIAj85z/mMdiVK6VVqyqPbkjS2WdL/fubKc67d2d6cwD2InwAAWjfPvNEygcfmLZjR+X34+LMGir9+0tXXskCbgD8C+EDCAC//ip98omZVfSjj6Ts7MqXUsLDpQsvlFJTTevWzcw6CgD+iP88AX5o3z7p00+lNWuk1aulL7+sHDYk6ayzpL59TevVi3s3AAQOwgdgM8uScnLMI7CffWYCxzffVD/u9NOl3r1N69NHSkjwdaUA4BmED8DHDh82q8B+8YUJG2vXmtlFq+rYUbrkEunSS03g4L4NAMGC8AF4kWWZm0HXrTPtiy/MJZQjRyof17ix1LWrdPHFJnBcfLHUsqU9NQOAtxE+AA+xLOmHH8y8Ghs2mLCxfr2Un1/92FatzA2iF11kgka3blJUlO9rBgA7ED6AenDdp7FxoxnJ+PJLEzR+/rn6sQ6HdN550vnnm8Bx4YVmNdiwMJ+XDQB+gfABnEBRkfT119KmTdJXX5W3goLqx4aHm0XYunY1oxndu5vXkZG+rxsA/BXhA/if0lLp++9N0Ni82bSvv5a2b5fKyqofHxkpde4snXuuaV27Sl26cPkEAE6E8IGQc+SIuQn022+lrVulLVvM9ttvpeLimj9zyilmivIuXcz2nHOkM880N4oCAOqG8IGgZFlmoq5t26TvvjNt2zYzf8b335ul5WsSFWUece3c2Vwu6dzZtIQE7tEAAE8hfCBgWZb03/9KO3eaQLF9uxnRcG2rLq5WUbNmUocOZvTirLNM4DjrLKldOxZdAwBvI3zArxUWmhVbc3LKtzk5JnDs3CkdPHjsz4aFSW3bmplBzzjDbF2B49RTGckAALsQPmCb0lIzs+fu3dKePeVt1y4TNHbtqnmOjIrCwqSkJOm3v5VSUqTTTjMtJUVq316KjvbJrwIAqAPCBzzOsiSnU9q7V/rpp8rthx+kH3802717TQA5kZNOkpKTK7f27U3gaNfOzKMBAAgchA/USlmZGYX4+WfT8vLM/RYVW25ueTt8uHbfGx5uLoG0bWtGMJKSTKBwtbZtpdhY7/5uAADfInyEGMsy91Hk55sbMvPzpV9+qdz27zdt377K29qMUlQUG2uCRWJieTv1VKlNG9NOPVWKj+cGTwAINYQPP3f0qJlhs6jI3Fx56JBpBw+aduBAeXM6TbAoLDR9VysoqNxqmjCrtk46ycx5ccopJjjEx5t1Slq1klq3No+ktm5t9nO/BQCgJiETPkpLzZoakZHVW0SEmSwqIqK8hYdLjRqZFh5ubmx0NclsLcv0Lau8lZaaP+6lpeXtyBETIo4eNf2Sksrbw4fN5FauVlRk9hUVHXs+ioaKjJROPtm05s3L28knm9VUW7Qo37ZoYcJFy5ZMqgUAaLiQCR9HjpiFvwJZdLRpTZuabbNmpu/axsZKMTGmufpxcZXbSSeZxhTgAAC7hEz4iIiQ3n7bjDZUbMXFlUclXP2qoxeukQ2pvF9xNCQsrPJIiavvGlFp3Njsb9zYjDpU3DocUpMmZuvqR0WZVrHPvBQAgGAQUuHjqqvsrgIAADSyuwAAABBaCB8AAMCnCB8AAMCnCB8AAMCnCB8AAMCnvPa0y4IFC/Too49q7969OuusszR37lxdcskl3vpxJ2ZZ0uWXm2dWXRNmREeXP8dasV/1Odeatq7mcJhHaQAAQK145a/myy+/rLS0NC1YsEAXX3yxnnrqKfXv319bt25V27ZtvfEjT+zwYenDD73z3eHh5UGk6tbVIiNrfl1xe7xW0wQhVfvHa66pW5ksBABgszDLck2d5Tndu3fXeeedp4ULF7r3nXnmmbruuuuUkZFx3M86nU7FxcWpoKBAsZ5czvTIEWn58vLFUQ4dKl8wxbV4iqsdOmTCimuOc9d85xX3eWvec29zzXRW05zyVV9X3B8eXnnu+RNtXe1Erys21wxtNb2uqV91RreK/bruq6lVnDmuLq8rbgl7AEJEXf5+e3zko6SkRBs2bNDkyZMr7U9NTdXatWs9/eNqr3FjafBgz33f0aPli7G4QomrX3FbtbmmVXVti4vLF3qp+F7FBWAqHnPkSM0LxNTUasqVrilb4TvHCiXHCyxVp86tqX+iY2vbXDU25JiK73ur73pdl2Mqbo/3Xk3H1PS6Np+vzfcc6/vq+rnjHXOizzTkc946pr689d2e///NG/bd9a2nps9FREhz59bv+zzA4+Fj3759Ki0tVXx8fKX98fHxys3NrXZ8cXGxiouL3a+dTqenS/IO1yhA06Z2V3JsFVe1cwWSivPIV9xXcY750tKa97n2V319vG3FVtM+V6s6n/2xXte0PdF7FY+xrOPvO9YxDeFacRAA/IXDEVzhwyWsShK1LKvaPknKyMjQQw895K0yQpvrMgUarurSxa5+WVnN+1z7Kx5TdVvxM1WPrdqv6fWx9tW2lZVV/91qaq5jjnVsxf01HVN1X9Xja/r+E/Vr2tb3vdp+3lvHHG+fp46pSW2+pzafq+8xnvpZ/sBTIy/1/Z76fM7mByU8/tNbtmyp8PDwaqMceXl51UZDJGnKlCmaMGGC+7XT6VRSUpKnywIapuLQPoEOABrE4/N8REZGqmvXrsrMzKy0PzMzUz169Kh2vMPhUGxsbKUGAACCl1fGXSZMmKBbbrlF3bp100UXXaSnn35au3fv1qhRo7zx4wAAQADxSvgYNGiQ9u/fr+nTp2vv3r3q1KmT3nnnHbVr184bPw4AAAQQr8zz0RBem+cDAAB4TV3+frO2CwAA8CnCBwAA8CnCBwAA8CnCBwAA8CnCBwAA8CnCBwAA8CnCBwAA8CnCBwAA8CnCBwAA8Cl719StgWvCVafTaXMlAACgtlx/t2szcbrfhY/CwkJJUlJSks2VAACAuiosLFRcXNxxj/G7tV3Kysr0008/KSYmRmFhYR79bqfTqaSkJO3Zs4d1Y7yMc+07nGvf4Vz7Dufadzx1ri3LUmFhoRITE9Wo0fHv6vC7kY9GjRqpTZs2Xv0ZsbGx/GP2Ec6173CufYdz7Tuca9/xxLk+0YiHCzecAgAAnyJ8AAAAnwqp8OFwOPTggw/K4XDYXUrQ41z7DufadzjXvsO59h07zrXf3XAKAACCW0iNfAAAAPsRPgAAgE8RPgAAgE8RPgAAgE+FTPhYsGCBkpOT1aRJE3Xt2lWffPKJ3SUFvIyMDJ1//vmKiYlRq1atdN1112nbtm2VjrEsS9OmTVNiYqKioqLUu3dvbdmyxaaKg0dGRobCwsKUlpbm3se59pwff/xRN998s1q0aKHo6Gidc8452rBhg/t9zrXnHD16VPfdd5+Sk5MVFRWl9u3ba/r06SorK3Mfw/munzVr1mjgwIFKTExUWFiYXnvttUrv1+a8FhcXa9y4cWrZsqWaNm2qa665Rj/88EPDi7NCwLJly6zGjRtbzzzzjLV161Zr/PjxVtOmTa1du3bZXVpAu+KKK6znn3/e+vrrr63s7GxrwIABVtu2ba0DBw64j5kxY4YVExNjLV++3Nq8ebM1aNAgq3Xr1pbT6bSx8sC2bt066ze/+Y3VpUsXa/z48e79nGvP+OWXX6x27dpZI0aMsP79739bOTk51qpVq6wdO3a4j+Fce85f/vIXq0WLFtZbb71l5eTkWK+88orVrFkza+7cue5jON/1884771hTp061li9fbkmyXn311Urv1+a8jho1yjr11FOtzMxMa+PGjVafPn2ss88+2zp69GiDaguJ8HHBBRdYo0aNqrSvQ4cO1uTJk22qKDjl5eVZkqzVq1dblmVZZWVlVkJCgjVjxgz3MYcPH7bi4uKsJ5980q4yA1phYaGVkpJiZWZmWr169XKHD86150yaNMnq2bPnMd/nXHvWgAEDrJEjR1bad/3111s333yzZVmcb0+pGj5qc15//fVXq3HjxtayZcvcx/z4449Wo0aNrPfee69B9QT9ZZeSkhJt2LBBqamplfanpqZq7dq1NlUVnAoKCiRJzZs3lyTl5OQoNze30rl3OBzq1asX576exowZowEDBujyyy+vtJ9z7TlvvPGGunXrphtvvFGtWrXSueeeq2eeecb9Pufas3r27KkPPvhA3333nSTpq6++0qeffqqrrrpKEufbW2pzXjds2KAjR45UOiYxMVGdOnVq8Ln3u4XlPG3fvn0qLS1VfHx8pf3x8fHKzc21qargY1mWJkyYoJ49e6pTp06S5D6/NZ37Xbt2+bzGQLds2TJt3LhRWVlZ1d7jXHvOzp07tXDhQk2YMEH33nuv1q1bp//7v/+Tw+HQrbfeyrn2sEmTJqmgoEAdOnRQeHi4SktL9cgjj2jIkCGS+LftLbU5r7m5uYqMjNTJJ59c7ZiG/v0M+vDhEhYWVum1ZVnV9qH+xo4dq02bNunTTz+t9h7nvuH27Nmj8ePHa+XKlWrSpMkxj+NcN1xZWZm6deum9PR0SdK5556rLVu2aOHChbr11lvdx3GuPePll1/WkiVLtHTpUp111lnKzs5WWlqaEhMTNXz4cPdxnG/vqM959cS5D/rLLi1btlR4eHi1lJaXl1ct8aF+xo0bpzfeeEMfffSR2rRp496fkJAgSZx7D9iwYYPy8vLUtWtXRUREKCIiQqtXr9a8efMUERHhPp+c64Zr3bq1OnbsWGnfmWeeqd27d0vi37Wn3XPPPZo8ebIGDx6szp0765ZbbtGf//xnZWRkSOJ8e0ttzmtCQoJKSkqUn59/zGPqK+jDR2RkpLp27arMzMxK+zMzM9WjRw+bqgoOlmVp7NixWrFihT788EMlJydXej85OVkJCQmVzn1JSYlWr17Nua+jvn37avPmzcrOzna3bt26adiwYcrOzlb79u051x5y8cUXV3tk/LvvvlO7du0k8e/a0w4dOqRGjSr/KQoPD3c/asv59o7anNeuXbuqcePGlY7Zu3evvv7664af+wbdrhogXI/aPvvss9bWrVuttLQ0q2nTptZ//vMfu0sLaHfeeacVFxdnffzxx9bevXvd7dChQ+5jZsyYYcXFxVkrVqywNm/ebA0ZMoRH5Dyk4tMulsW59pR169ZZERER1iOPPGJt377devHFF63o6GhryZIl7mM4154zfPhw69RTT3U/artixQqrZcuW1sSJE93HcL7rp7Cw0Pryyy+tL7/80pJkzZkzx/ryyy/d00zU5ryOGjXKatOmjbVq1Spr48aN1mWXXcajtnXxxBNPWO3atbMiIyOt8847z/04KOpPUo3t+eefdx9TVlZmPfjgg1ZCQoLlcDisSy+91Nq8ebN9RQeRquGDc+05b775ptWpUyfL4XBYHTp0sJ5++ulK73OuPcfpdFrjx4+32rZtazVp0sRq3769NXXqVKu4uNh9DOe7fj766KMa/xs9fPhwy7Jqd16LioqssWPHWs2bN7eioqKsq6++2tq9e3eDawuzLMtq2NgJAABA7QX9PR8AAMC/ED4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBPET4AAIBP/T/YPoAFfT2xNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26014/3367627263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_re_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
