{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "4c900037-3d26-473d-c9aa-392edcfda7eb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "a1b70a95-2d46-4fe5-8a51-d369c604c433"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "2d3abfaa-8287-4fe9-d053-d79e6506dc03"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dmSz5jcVVt4p"
   },
   "outputs": [],
   "source": [
    "# lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = extent*np.sin(x)/2 + np.square(x)/2\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_FODE_stan_\" + level\n",
    "extent = 20.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(extent,-1.0*extent,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f16363d83d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - extent*torch.cos(g)/2.0 -g \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(i)\n",
    "\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 179.75754 Test MSE 7375.9231570873435 Test RE 0.9568911636326142\n",
      "1 Train Loss 142.09407 Test MSE 2622.789428371568 Test RE 0.5706056460260531\n",
      "2 Train Loss 85.52339 Test MSE 3839.7845170476594 Test RE 0.6904111219667582\n",
      "3 Train Loss 57.08626 Test MSE 620.7804334370247 Test RE 0.27760249318319175\n",
      "4 Train Loss 41.21875 Test MSE 109.49895065106405 Test RE 0.11658947078496482\n",
      "5 Train Loss 32.08556 Test MSE 157.97122716517327 Test RE 0.14003717732984083\n",
      "6 Train Loss 25.404058 Test MSE 75.00596309387856 Test RE 0.09649443071799145\n",
      "7 Train Loss 22.466093 Test MSE 28.593437702677214 Test RE 0.0595782220253735\n",
      "8 Train Loss 18.582008 Test MSE 38.51021865583743 Test RE 0.06914206476737524\n",
      "9 Train Loss 16.173584 Test MSE 35.24498441574306 Test RE 0.06614591276624768\n",
      "10 Train Loss 15.081959 Test MSE 51.72486165784735 Test RE 0.08013163815474934\n",
      "11 Train Loss 10.902446 Test MSE 59.14149729600398 Test RE 0.08568415340345482\n",
      "12 Train Loss 9.648 Test MSE 54.683584586924255 Test RE 0.08239158166861434\n",
      "13 Train Loss 8.448542 Test MSE 22.198024273868093 Test RE 0.05249422318060127\n",
      "14 Train Loss 6.984647 Test MSE 31.41955254225221 Test RE 0.06245315030532008\n",
      "15 Train Loss 4.9925003 Test MSE 8.006621573660873 Test RE 0.031526735528409705\n",
      "16 Train Loss 4.4880238 Test MSE 5.198557965143444 Test RE 0.025403631116055495\n",
      "17 Train Loss 3.8209865 Test MSE 3.8640785216913507 Test RE 0.021901674684523722\n",
      "18 Train Loss 3.0872579 Test MSE 5.3745864687008655 Test RE 0.02583014710632257\n",
      "19 Train Loss 2.8941393 Test MSE 2.9497198959053197 Test RE 0.0191357170696116\n",
      "20 Train Loss 2.4996238 Test MSE 6.590140492777412 Test RE 0.02860234817430535\n",
      "21 Train Loss 2.2736995 Test MSE 7.25363166571758 Test RE 0.030007658013218787\n",
      "22 Train Loss 2.0295863 Test MSE 4.559097669415785 Test RE 0.023789964854259313\n",
      "23 Train Loss 1.8896478 Test MSE 4.1591824303934715 Test RE 0.022722616422576593\n",
      "24 Train Loss 1.6882979 Test MSE 4.581066196541624 Test RE 0.023847213289134136\n",
      "25 Train Loss 1.6142014 Test MSE 1.2201561519523083 Test RE 0.01230727937889444\n",
      "26 Train Loss 1.5077894 Test MSE 2.38885915660018 Test RE 0.017220653366552742\n",
      "27 Train Loss 1.4469665 Test MSE 4.2680531637147725 Test RE 0.02301808887443502\n",
      "28 Train Loss 1.3368821 Test MSE 7.135017105448312 Test RE 0.02976129753344024\n",
      "29 Train Loss 1.2767833 Test MSE 3.652086246830698 Test RE 0.021292412192539852\n",
      "30 Train Loss 1.2594882 Test MSE 3.6869941121933807 Test RE 0.021393930192604726\n",
      "31 Train Loss 1.2027838 Test MSE 3.332311875625032 Test RE 0.020338886500319557\n",
      "32 Train Loss 1.0728338 Test MSE 0.6025492974277992 Test RE 0.008648696249749765\n",
      "33 Train Loss 0.9952432 Test MSE 0.4561084607897192 Test RE 0.007524686602015255\n",
      "34 Train Loss 0.88930637 Test MSE 0.3710885527847686 Test RE 0.0067872388251012964\n",
      "35 Train Loss 0.8429977 Test MSE 0.4931903830675592 Test RE 0.007824591114308827\n",
      "36 Train Loss 0.7730433 Test MSE 0.2309743118318666 Test RE 0.005354712933900214\n",
      "37 Train Loss 0.74709684 Test MSE 0.27659373458596814 Test RE 0.005859701882462945\n",
      "38 Train Loss 0.7146673 Test MSE 0.509335516312489 Test RE 0.007951633098585773\n",
      "39 Train Loss 0.6653499 Test MSE 1.1446061440496564 Test RE 0.011920168341940278\n",
      "40 Train Loss 0.6317794 Test MSE 0.3084712177504974 Test RE 0.006188162024737458\n",
      "41 Train Loss 0.6096102 Test MSE 0.1241552412454959 Test RE 0.003925878741362243\n",
      "42 Train Loss 0.6026105 Test MSE 0.14289799640419373 Test RE 0.004211796914636474\n",
      "43 Train Loss 0.58787465 Test MSE 0.15440491418449848 Test RE 0.004378092282758423\n",
      "44 Train Loss 0.5663708 Test MSE 0.235784548679832 Test RE 0.00541018384455672\n",
      "45 Train Loss 0.5419865 Test MSE 0.1515693523792838 Test RE 0.00433770536745862\n",
      "46 Train Loss 0.5318147 Test MSE 0.27481515631359515 Test RE 0.005840831703470334\n",
      "47 Train Loss 0.5190384 Test MSE 0.21616692104342328 Test RE 0.005180229141994985\n",
      "48 Train Loss 0.5032449 Test MSE 0.13766557023039513 Test RE 0.004133967151082964\n",
      "49 Train Loss 0.49700162 Test MSE 0.1089587899076384 Test RE 0.003677777808955229\n",
      "50 Train Loss 0.48514363 Test MSE 0.16267146238114755 Test RE 0.004493761693148668\n",
      "51 Train Loss 0.47790146 Test MSE 0.13365972830539272 Test RE 0.004073377304694227\n",
      "52 Train Loss 0.47020397 Test MSE 0.11960395861938058 Test RE 0.003853249488004328\n",
      "53 Train Loss 0.46288183 Test MSE 0.1424951489331353 Test RE 0.004205855931284611\n",
      "54 Train Loss 0.4584407 Test MSE 0.15915154936794548 Test RE 0.004444877412071198\n",
      "55 Train Loss 0.44668686 Test MSE 0.1475630689597288 Test RE 0.0042799943117127785\n",
      "56 Train Loss 0.42346543 Test MSE 0.19643822337769482 Test RE 0.004938184945727065\n",
      "57 Train Loss 0.41445744 Test MSE 0.28883863771113993 Test RE 0.005988002822122728\n",
      "58 Train Loss 0.40443364 Test MSE 0.4461545302628465 Test RE 0.007442125782470943\n",
      "59 Train Loss 0.39022467 Test MSE 0.07953516337986712 Test RE 0.003142200845287568\n",
      "60 Train Loss 0.3835074 Test MSE 0.1042813151829852 Test RE 0.0035979705334230863\n",
      "61 Train Loss 0.374627 Test MSE 0.12149044389427263 Test RE 0.0038835187989141656\n",
      "62 Train Loss 0.36442187 Test MSE 0.07641813256963098 Test RE 0.0030800130929371874\n",
      "63 Train Loss 0.35393825 Test MSE 0.09772975376512086 Test RE 0.003483114527426987\n",
      "64 Train Loss 0.34812796 Test MSE 0.10489025445275224 Test RE 0.0036084602189606413\n",
      "65 Train Loss 0.34414434 Test MSE 0.11885774402484299 Test RE 0.0038412103801655282\n",
      "66 Train Loss 0.33920616 Test MSE 0.14643006414753307 Test RE 0.004263531526124899\n",
      "67 Train Loss 0.33350664 Test MSE 0.11147178245953587 Test RE 0.0037199476241432912\n",
      "68 Train Loss 0.33051202 Test MSE 0.0944226456656982 Test RE 0.0034236742371322433\n",
      "69 Train Loss 0.32289165 Test MSE 0.06919529100893877 Test RE 0.0029308434571196093\n",
      "70 Train Loss 0.31143567 Test MSE 0.23100910746907252 Test RE 0.005355116255010837\n",
      "71 Train Loss 0.29839247 Test MSE 0.18731589320835018 Test RE 0.004822160553984118\n",
      "72 Train Loss 0.2884471 Test MSE 0.10292328787130019 Test RE 0.0035744660618842123\n",
      "73 Train Loss 0.2819794 Test MSE 0.13156386934953992 Test RE 0.004041314706212351\n",
      "74 Train Loss 0.27197134 Test MSE 0.07707407114926428 Test RE 0.00309320356566465\n",
      "75 Train Loss 0.26653755 Test MSE 0.0659628551679124 Test RE 0.00286156803000016\n",
      "76 Train Loss 0.26316643 Test MSE 0.0586366191177259 Test RE 0.0026979805421105717\n",
      "77 Train Loss 0.25556198 Test MSE 0.08629011333348902 Test RE 0.0032729160967917502\n",
      "78 Train Loss 0.24780446 Test MSE 0.12276318183356184 Test RE 0.003903807736642945\n",
      "79 Train Loss 0.23306592 Test MSE 0.10395637944068496 Test RE 0.003592360605277347\n",
      "80 Train Loss 0.21221249 Test MSE 0.16952860526615915 Test RE 0.004587497566335142\n",
      "81 Train Loss 0.20489866 Test MSE 0.1227089988032004 Test RE 0.003902946144941096\n",
      "82 Train Loss 0.19670814 Test MSE 0.18666362773647915 Test RE 0.004813757444952339\n",
      "83 Train Loss 0.18967643 Test MSE 0.062631960566333 Test RE 0.0027883825464623873\n",
      "84 Train Loss 0.17827234 Test MSE 0.13739848320705791 Test RE 0.004129955018515947\n",
      "85 Train Loss 0.17092185 Test MSE 0.13602410430643644 Test RE 0.0041092474068967315\n",
      "86 Train Loss 0.16490002 Test MSE 0.2151490429110978 Test RE 0.005168018523153731\n",
      "87 Train Loss 0.16253042 Test MSE 0.18126225286937564 Test RE 0.0047435997671359436\n",
      "88 Train Loss 0.15657145 Test MSE 0.06824403360610756 Test RE 0.0029106279566040384\n",
      "89 Train Loss 0.15204632 Test MSE 0.04442401113701786 Test RE 0.002348352227889547\n",
      "90 Train Loss 0.14584573 Test MSE 0.0946055078430681 Test RE 0.0034269878368609237\n",
      "91 Train Loss 0.1388544 Test MSE 0.20298137710477468 Test RE 0.005019754175552471\n",
      "92 Train Loss 0.13003398 Test MSE 0.06538344172335447 Test RE 0.0028489723951266805\n",
      "93 Train Loss 0.12277277 Test MSE 0.07569336441970458 Test RE 0.0030653724999697178\n",
      "94 Train Loss 0.11898949 Test MSE 0.029867412053419785 Test RE 0.0019255426793252342\n",
      "95 Train Loss 0.11581924 Test MSE 0.02055349041545263 Test RE 0.001597339181723323\n",
      "96 Train Loss 0.10830771 Test MSE 0.036957573137221436 Test RE 0.002141933951638333\n",
      "97 Train Loss 0.10180739 Test MSE 0.07088522646178613 Test RE 0.002966417113376853\n",
      "98 Train Loss 0.09483818 Test MSE 0.04457897634542486 Test RE 0.002352444565278561\n",
      "99 Train Loss 0.089165084 Test MSE 0.0219837243038041 Test RE 0.0016519807671536103\n",
      "100 Train Loss 0.083314784 Test MSE 0.033681138424704125 Test RE 0.002044785390117903\n",
      "101 Train Loss 0.07742398 Test MSE 0.053517332915614245 Test RE 0.0025775172665936015\n",
      "102 Train Loss 0.07266012 Test MSE 0.03665057521053625 Test RE 0.0021330191272022755\n",
      "103 Train Loss 0.07077949 Test MSE 0.021181857643293518 Test RE 0.0016215725157274507\n",
      "104 Train Loss 0.06873794 Test MSE 0.01513757490539062 Test RE 0.0013708265428601237\n",
      "105 Train Loss 0.06548413 Test MSE 0.02334236378464905 Test RE 0.001702263428794853\n",
      "106 Train Loss 0.06101831 Test MSE 0.018433482389620504 Test RE 0.0015127182504124437\n",
      "107 Train Loss 0.056943335 Test MSE 0.04504394931952884 Test RE 0.0023646811146867913\n",
      "108 Train Loss 0.055436697 Test MSE 0.04196715543340991 Test RE 0.0022824912379968924\n",
      "109 Train Loss 0.053101774 Test MSE 0.013649174706700393 Test RE 0.00130168994071206\n",
      "110 Train Loss 0.0487076 Test MSE 0.0214473999653338 Test RE 0.0016317051256744494\n",
      "111 Train Loss 0.047096156 Test MSE 0.009178214175469185 Test RE 0.0010674153558117214\n",
      "112 Train Loss 0.045908898 Test MSE 0.011407841031032486 Test RE 0.0011900249932430796\n",
      "113 Train Loss 0.04442983 Test MSE 0.008398992879155957 Test RE 0.001021099253940119\n",
      "114 Train Loss 0.04131801 Test MSE 0.01874773670196131 Test RE 0.0015255581807058316\n",
      "115 Train Loss 0.03796355 Test MSE 0.025241647416274434 Test RE 0.001770162761619252\n",
      "116 Train Loss 0.0366058 Test MSE 0.008869321378918759 Test RE 0.0010492996989192045\n",
      "117 Train Loss 0.034294855 Test MSE 0.00983620189672557 Test RE 0.0011050147358570733\n",
      "118 Train Loss 0.032091588 Test MSE 0.004334260240404296 Test RE 0.0007335198050969291\n",
      "119 Train Loss 0.030709352 Test MSE 0.004841788400109352 Test RE 0.0007752776339630525\n",
      "120 Train Loss 0.029765083 Test MSE 0.003995464687288871 Test RE 0.0007042680729323256\n",
      "121 Train Loss 0.02922664 Test MSE 0.0030382324937254495 Test RE 0.0006141364220107002\n",
      "122 Train Loss 0.028686225 Test MSE 0.0027560279807190523 Test RE 0.0005849195855203841\n",
      "123 Train Loss 0.027599227 Test MSE 0.003949714202493212 Test RE 0.0007002243162564851\n",
      "124 Train Loss 0.025875363 Test MSE 0.008904084785069807 Test RE 0.0010513540589258232\n",
      "125 Train Loss 0.024903294 Test MSE 0.012148137713235438 Test RE 0.001228030647889717\n",
      "126 Train Loss 0.023799263 Test MSE 0.00796651072499484 Test RE 0.0009944625317917925\n",
      "127 Train Loss 0.022526294 Test MSE 0.008542068231372399 Test RE 0.0010297596498425306\n",
      "128 Train Loss 0.02115498 Test MSE 0.014412203278396709 Test RE 0.001337579308451109\n",
      "129 Train Loss 0.018385956 Test MSE 0.004183951880968097 Test RE 0.0007206886673065397\n",
      "130 Train Loss 0.017681578 Test MSE 0.003147294336094797 Test RE 0.0006250619067702846\n",
      "131 Train Loss 0.017218798 Test MSE 0.001837897056871466 Test RE 0.00047765567671653617\n",
      "132 Train Loss 0.016544182 Test MSE 0.001750964649191425 Test RE 0.00046622229875904726\n",
      "133 Train Loss 0.016066812 Test MSE 0.0016824710015638942 Test RE 0.00045701257082319955\n",
      "134 Train Loss 0.015638934 Test MSE 0.003863296406976133 Test RE 0.0006925216696571814\n",
      "135 Train Loss 0.015087945 Test MSE 0.005805662196944104 Test RE 0.0008489463271024879\n",
      "136 Train Loss 0.014563766 Test MSE 0.005574818175486527 Test RE 0.000831897283220897\n",
      "137 Train Loss 0.013497027 Test MSE 0.009091801501665537 Test RE 0.0010623786271831866\n",
      "138 Train Loss 0.01266177 Test MSE 0.007939306515350403 Test RE 0.0009927631238938386\n",
      "139 Train Loss 0.01205455 Test MSE 0.0012201647079563087 Test RE 0.0003891917109151621\n",
      "140 Train Loss 0.011683822 Test MSE 0.0033030361402741322 Test RE 0.0006403405653988381\n",
      "141 Train Loss 0.0112209795 Test MSE 0.002002928873515023 Test RE 0.0004986400023903905\n",
      "142 Train Loss 0.010950339 Test MSE 0.0016993618744280301 Test RE 0.00045930089068923314\n",
      "143 Train Loss 0.010634966 Test MSE 0.0020600137511281127 Test RE 0.0005056958761408796\n",
      "144 Train Loss 0.010264146 Test MSE 0.0015602487755245209 Test RE 0.0004400999077806572\n",
      "145 Train Loss 0.009893287 Test MSE 0.001508679792054321 Test RE 0.0004327657564758383\n",
      "146 Train Loss 0.009487815 Test MSE 0.0026321144229284133 Test RE 0.0005716191039562822\n",
      "147 Train Loss 0.009202765 Test MSE 0.004098120410591064 Test RE 0.0007132580956049421\n",
      "148 Train Loss 0.008913949 Test MSE 0.002484512162812974 Test RE 0.0005553604086928\n",
      "149 Train Loss 0.008582852 Test MSE 0.0009584846061292623 Test RE 0.00034494267222343115\n",
      "150 Train Loss 0.008473913 Test MSE 0.0015244509142118366 Test RE 0.00043502185392773924\n",
      "151 Train Loss 0.008280109 Test MSE 0.0010957793330512058 Test RE 0.0003688212241187106\n",
      "152 Train Loss 0.0078075635 Test MSE 0.00503782196487373 Test RE 0.0007908165693209183\n",
      "153 Train Loss 0.007108579 Test MSE 0.004432768770065384 Test RE 0.0007418086463801991\n",
      "154 Train Loss 0.006659274 Test MSE 0.001053361776080623 Test RE 0.0003616122462707502\n",
      "155 Train Loss 0.006253293 Test MSE 0.0018156958254119402 Test RE 0.0004747619444468496\n",
      "156 Train Loss 0.005865239 Test MSE 0.0008830674238834494 Test RE 0.0003310939799700908\n",
      "157 Train Loss 0.005545335 Test MSE 0.0012412921642515375 Test RE 0.000392546725795415\n",
      "158 Train Loss 0.0051863636 Test MSE 0.0004129818073685739 Test RE 0.00022642262063045916\n",
      "159 Train Loss 0.0050099385 Test MSE 0.0005277504315150441 Test RE 0.00025595796632487945\n",
      "160 Train Loss 0.0048375735 Test MSE 0.0005360749119552357 Test RE 0.00025796874675445224\n",
      "161 Train Loss 0.004679473 Test MSE 0.000884501991078666 Test RE 0.00033136280645082217\n",
      "162 Train Loss 0.0045962096 Test MSE 0.0014985880927464261 Test RE 0.000431315922711846\n",
      "163 Train Loss 0.0045444043 Test MSE 0.00139303250438187 Test RE 0.0004158483450232696\n",
      "164 Train Loss 0.004502973 Test MSE 0.000884653243774783 Test RE 0.0003313911372945666\n",
      "165 Train Loss 0.0044312603 Test MSE 0.0007258998985642265 Test RE 0.0003001875803352564\n",
      "166 Train Loss 0.0043584076 Test MSE 0.0005407450319593899 Test RE 0.0002590899823345279\n",
      "167 Train Loss 0.0043477532 Test MSE 0.0004941540916169496 Test RE 0.000247676926759486\n",
      "168 Train Loss 0.004326305 Test MSE 0.0004311852541261006 Test RE 0.00023135894885346285\n",
      "169 Train Loss 0.0043028276 Test MSE 0.000588940332344675 Test RE 0.00027038961094401706\n",
      "170 Train Loss 0.0042817006 Test MSE 0.0007299892245873792 Test RE 0.0003010319397852298\n",
      "171 Train Loss 0.004255448 Test MSE 0.00034498814315106556 Test RE 0.00020694571430691894\n",
      "172 Train Loss 0.004235584 Test MSE 0.00035290974531880693 Test RE 0.00020930816870781246\n",
      "173 Train Loss 0.004191428 Test MSE 0.000767074585889256 Test RE 0.00030858381897936606\n",
      "174 Train Loss 0.0040865485 Test MSE 0.0004080789168847288 Test RE 0.00022507457111765877\n",
      "175 Train Loss 0.0040284693 Test MSE 0.0004371643294398212 Test RE 0.00023295750778776943\n",
      "176 Train Loss 0.0039549447 Test MSE 0.0012819459162507609 Test RE 0.000398923117181541\n",
      "177 Train Loss 0.0038962387 Test MSE 0.002904595067776821 Test RE 0.0006004780685161772\n",
      "178 Train Loss 0.003845093 Test MSE 0.002759355446683302 Test RE 0.0005852725777466245\n",
      "179 Train Loss 0.0037415002 Test MSE 0.001966976976141188 Test RE 0.0004941445280959124\n",
      "180 Train Loss 0.0036291748 Test MSE 0.0010394574624827768 Test RE 0.00035921768780325814\n",
      "181 Train Loss 0.0035731434 Test MSE 0.000546814255056104 Test RE 0.0002605399144733019\n",
      "182 Train Loss 0.0035139532 Test MSE 0.0003378563733978253 Test RE 0.00020479550034757034\n",
      "183 Train Loss 0.0034269623 Test MSE 0.00038210301180175896 Test RE 0.00021779333380915272\n",
      "184 Train Loss 0.0032946784 Test MSE 0.0003968642848362512 Test RE 0.00022196032950655848\n",
      "185 Train Loss 0.003212842 Test MSE 0.0004885124421842522 Test RE 0.0002462590314789028\n",
      "186 Train Loss 0.0031345661 Test MSE 0.0003109877101957836 Test RE 0.00019648344569570293\n",
      "187 Train Loss 0.0030243397 Test MSE 0.0005560815062192079 Test RE 0.00026273841658567263\n",
      "188 Train Loss 0.0029595003 Test MSE 0.0006542239742848955 Test RE 0.00028498209830828033\n",
      "189 Train Loss 0.0028802257 Test MSE 0.0008184141177963953 Test RE 0.0003187431854462693\n",
      "190 Train Loss 0.0028335117 Test MSE 0.0008983045624574449 Test RE 0.00033393824089261243\n",
      "191 Train Loss 0.002778376 Test MSE 0.00030676291830674247 Test RE 0.0001951442605718028\n",
      "192 Train Loss 0.0027535118 Test MSE 0.00021885082352830475 Test RE 0.00016482703445929326\n",
      "193 Train Loss 0.0027344874 Test MSE 0.0002455297245429441 Test RE 0.00017458478428990888\n",
      "194 Train Loss 0.0027150058 Test MSE 0.00021744229158839122 Test RE 0.00016429576183247025\n",
      "195 Train Loss 0.0026956163 Test MSE 0.0001636417027375345 Test RE 0.0001425283794567419\n",
      "196 Train Loss 0.0026690445 Test MSE 0.00017978068298597793 Test RE 0.0001493914922320732\n",
      "197 Train Loss 0.002626629 Test MSE 0.0004740984522530228 Test RE 0.0002425987847970363\n",
      "198 Train Loss 0.0025415064 Test MSE 0.0003961794840622372 Test RE 0.0002217687473494035\n",
      "199 Train Loss 0.002485519 Test MSE 0.0002489472793157039 Test RE 0.0001757956176612304\n",
      "Training time: 28.93\n",
      "Training time: 28.93\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 179.90916 Test MSE 7578.269464794652 Test RE 0.9699277262030098\n",
      "1 Train Loss 119.51515 Test MSE 827.3182138100411 Test RE 0.32047240913932085\n",
      "2 Train Loss 74.658676 Test MSE 840.3304376053746 Test RE 0.3229828032388012\n",
      "3 Train Loss 56.285465 Test MSE 105.65373389068122 Test RE 0.11452407094846859\n",
      "4 Train Loss 49.914448 Test MSE 89.53017278309454 Test RE 0.10542389645907009\n",
      "5 Train Loss 47.66751 Test MSE 138.316009397141 Test RE 0.1310359849807151\n",
      "6 Train Loss 45.523106 Test MSE 81.69988039432738 Test RE 0.10070825310523564\n",
      "7 Train Loss 44.12813 Test MSE 51.6542203938246 Test RE 0.08007690108988101\n",
      "8 Train Loss 42.61586 Test MSE 77.94750814002523 Test RE 0.09836836894349485\n",
      "9 Train Loss 40.403503 Test MSE 59.583451621031 Test RE 0.08600370905255394\n",
      "10 Train Loss 38.1706 Test MSE 49.08080401411284 Test RE 0.07805669998335019\n",
      "11 Train Loss 35.51485 Test MSE 52.08161030621459 Test RE 0.08040749903425726\n",
      "12 Train Loss 32.669563 Test MSE 56.16517190187576 Test RE 0.08350027361177031\n",
      "13 Train Loss 28.6042 Test MSE 63.9921062706959 Test RE 0.0891286964600251\n",
      "14 Train Loss 21.323364 Test MSE 180.14755781456367 Test RE 0.14954384465476883\n",
      "15 Train Loss 16.613125 Test MSE 66.03291160214474 Test RE 0.09053876702971769\n",
      "16 Train Loss 12.72517 Test MSE 85.4049081802753 Test RE 0.1029664565633023\n",
      "17 Train Loss 8.984631 Test MSE 33.40746340005834 Test RE 0.06439855209247385\n",
      "18 Train Loss 7.71702 Test MSE 21.29167887556772 Test RE 0.051411385278180885\n",
      "19 Train Loss 6.5790195 Test MSE 5.969550463449886 Test RE 0.0272223219704632\n",
      "20 Train Loss 5.9811916 Test MSE 7.094158636253545 Test RE 0.0296759615845863\n",
      "21 Train Loss 4.816329 Test MSE 8.648896837716505 Test RE 0.03276685134273812\n",
      "22 Train Loss 4.250367 Test MSE 7.401558358353682 Test RE 0.030312093802213294\n",
      "23 Train Loss 3.5850174 Test MSE 5.576051961623018 Test RE 0.026309812826595152\n",
      "24 Train Loss 3.2265122 Test MSE 2.222287987510364 Test RE 0.016609422085369436\n",
      "25 Train Loss 2.832619 Test MSE 0.9313081372903608 Test RE 0.010752292028487497\n",
      "26 Train Loss 2.6492484 Test MSE 2.3263173566624746 Test RE 0.01699373465744755\n",
      "27 Train Loss 2.1740773 Test MSE 1.275062227511105 Test RE 0.012581141374142532\n",
      "28 Train Loss 1.9786899 Test MSE 4.225169761930969 Test RE 0.022902159443011993\n",
      "29 Train Loss 1.5514 Test MSE 0.717085741877495 Test RE 0.00943495636589772\n",
      "30 Train Loss 1.4499205 Test MSE 1.890534502787984 Test RE 0.015319572851444677\n",
      "31 Train Loss 1.2796028 Test MSE 2.779383677506257 Test RE 0.018574990644781377\n",
      "32 Train Loss 1.1036693 Test MSE 1.2346819555133686 Test RE 0.012380320931100745\n",
      "33 Train Loss 0.98914385 Test MSE 0.7977191096570663 Test RE 0.009951289259163849\n",
      "34 Train Loss 0.92311895 Test MSE 0.5727326515216399 Test RE 0.008431994679551977\n",
      "35 Train Loss 0.8544628 Test MSE 0.24595564806898637 Test RE 0.0055256421058924\n",
      "36 Train Loss 0.7846888 Test MSE 0.8975488615856578 Test RE 0.010555611620371112\n",
      "37 Train Loss 0.7144126 Test MSE 0.42249273479942157 Test RE 0.007242090955638643\n",
      "38 Train Loss 0.6485927 Test MSE 0.6995185136819572 Test RE 0.009318670568545734\n",
      "39 Train Loss 0.53938216 Test MSE 1.2090448538843286 Test RE 0.012251113370289092\n",
      "40 Train Loss 0.44274712 Test MSE 1.3936260648773526 Test RE 0.013153080633195435\n",
      "41 Train Loss 0.36306942 Test MSE 0.57924500566191 Test RE 0.008479797890680462\n",
      "42 Train Loss 0.28415203 Test MSE 0.15990921044840403 Test RE 0.004455445050416694\n",
      "43 Train Loss 0.23178788 Test MSE 0.7506046062908236 Test RE 0.009652947962311908\n",
      "44 Train Loss 0.2067101 Test MSE 0.5217310853974461 Test RE 0.008047809896016247\n",
      "45 Train Loss 0.19863656 Test MSE 0.39257119805796975 Test RE 0.006980934594167416\n",
      "46 Train Loss 0.16353898 Test MSE 0.15889594306390956 Test RE 0.004441306616706682\n",
      "47 Train Loss 0.14587376 Test MSE 0.1157087423446805 Test RE 0.0037899845447158962\n",
      "48 Train Loss 0.13150798 Test MSE 0.029986323560853016 Test RE 0.0019293719654930588\n",
      "49 Train Loss 0.11928144 Test MSE 0.06478491187163052 Test RE 0.0028359024545767694\n",
      "50 Train Loss 0.11141325 Test MSE 0.03709761698262993 Test RE 0.002145988344470609\n",
      "51 Train Loss 0.10821059 Test MSE 0.0838385201939459 Test RE 0.003226087589026819\n",
      "52 Train Loss 0.096588165 Test MSE 0.04985502432946889 Test RE 0.0024877619360141245\n",
      "53 Train Loss 0.090516835 Test MSE 0.1679236999769028 Test RE 0.004565731307408019\n",
      "54 Train Loss 0.079991646 Test MSE 0.03729611907369862 Test RE 0.0021517220666028886\n",
      "55 Train Loss 0.07782438 Test MSE 0.025435333464373117 Test RE 0.00177694125410246\n",
      "56 Train Loss 0.07435212 Test MSE 0.04325236076597095 Test RE 0.0023171772754941055\n",
      "57 Train Loss 0.0720634 Test MSE 0.021908696230128408 Test RE 0.001649159341936796\n",
      "58 Train Loss 0.07057637 Test MSE 0.024516690372927974 Test RE 0.001744557440861256\n",
      "59 Train Loss 0.066816844 Test MSE 0.06739451705510763 Test RE 0.002892455160315018\n",
      "60 Train Loss 0.06342194 Test MSE 0.02246963703205754 Test RE 0.0016701380899193103\n",
      "61 Train Loss 0.060810197 Test MSE 0.026435924204677486 Test RE 0.0018115553225346909\n",
      "62 Train Loss 0.05479856 Test MSE 0.040960004583217156 Test RE 0.0022549366774062907\n",
      "63 Train Loss 0.05155634 Test MSE 0.036807230196570374 Test RE 0.002137572831744838\n",
      "64 Train Loss 0.050017267 Test MSE 0.014756780870204938 Test RE 0.0013534747761148609\n",
      "65 Train Loss 0.04766502 Test MSE 0.017702153474075315 Test RE 0.0014824068160553172\n",
      "66 Train Loss 0.042753946 Test MSE 0.0239269742617222 Test RE 0.001723448234860395\n",
      "67 Train Loss 0.039415285 Test MSE 0.022360380389606166 Test RE 0.0016660726918808228\n",
      "68 Train Loss 0.037358858 Test MSE 0.03689398594984635 Test RE 0.0021400905109197153\n",
      "69 Train Loss 0.035164095 Test MSE 0.040714905854207804 Test RE 0.0022481799473391896\n",
      "70 Train Loss 0.034422103 Test MSE 0.03350579683528441 Test RE 0.002039455941890105\n",
      "71 Train Loss 0.03387552 Test MSE 0.016816138910913162 Test RE 0.0014448324779719756\n",
      "72 Train Loss 0.033201374 Test MSE 0.01132467720072076 Test RE 0.0011856793828128765\n",
      "73 Train Loss 0.031852514 Test MSE 0.011301456650972005 Test RE 0.0011844631779566652\n",
      "74 Train Loss 0.0306997 Test MSE 0.0255553276450324 Test RE 0.0017811277870982143\n",
      "75 Train Loss 0.02875858 Test MSE 0.025772648687689696 Test RE 0.0017886850588050493\n",
      "76 Train Loss 0.02548633 Test MSE 0.013752281619559364 Test RE 0.0013065972234874552\n",
      "77 Train Loss 0.02418982 Test MSE 0.008425427003611163 Test RE 0.0010227048428869076\n",
      "78 Train Loss 0.022538872 Test MSE 0.0244367476021939 Test RE 0.0017417108365882859\n",
      "79 Train Loss 0.02105262 Test MSE 0.028638755985286646 Test RE 0.001885521230847154\n",
      "80 Train Loss 0.019361783 Test MSE 0.022309515921409267 Test RE 0.0016641766563965316\n",
      "81 Train Loss 0.018524261 Test MSE 0.021836514120003803 Test RE 0.0016464403756573014\n",
      "82 Train Loss 0.017516807 Test MSE 0.021270822652672824 Test RE 0.0016249742960468124\n",
      "83 Train Loss 0.016942903 Test MSE 0.017878311259793524 Test RE 0.0014897644253089612\n",
      "84 Train Loss 0.01649539 Test MSE 0.012706086386720978 Test RE 0.0012559150195097925\n",
      "85 Train Loss 0.015825167 Test MSE 0.005378065095444101 Test RE 0.0008170852668106393\n",
      "86 Train Loss 0.015003279 Test MSE 0.013326404718812706 Test RE 0.0012862069489751123\n",
      "87 Train Loss 0.014403362 Test MSE 0.0033907142875611498 Test RE 0.0006487837303772621\n",
      "88 Train Loss 0.013820114 Test MSE 0.005292204240397773 Test RE 0.0008105366377984298\n",
      "89 Train Loss 0.012483675 Test MSE 0.004337879126285221 Test RE 0.0007338259669852286\n",
      "90 Train Loss 0.011844652 Test MSE 0.0028431674660626778 Test RE 0.000594094556115549\n",
      "91 Train Loss 0.0115274545 Test MSE 0.0015167385372891627 Test RE 0.0004339200451894829\n",
      "92 Train Loss 0.010983358 Test MSE 0.0018871448893709526 Test RE 0.00048401294268131975\n",
      "93 Train Loss 0.010652829 Test MSE 0.002493255982220672 Test RE 0.000556336798797099\n",
      "94 Train Loss 0.010362197 Test MSE 0.003118073108987564 Test RE 0.0006221534292469875\n",
      "95 Train Loss 0.010106004 Test MSE 0.004691191169567531 Test RE 0.0007631254157932705\n",
      "96 Train Loss 0.009769116 Test MSE 0.007261289387810335 Test RE 0.0009494262279991151\n",
      "97 Train Loss 0.009192474 Test MSE 0.007784927377068718 Test RE 0.0009830636438834003\n",
      "98 Train Loss 0.008865367 Test MSE 0.007537452697787475 Test RE 0.0009673121709285994\n",
      "99 Train Loss 0.008650672 Test MSE 0.005435270808372312 Test RE 0.0008214193817972405\n",
      "100 Train Loss 0.008351953 Test MSE 0.002627473096225604 Test RE 0.0005711149006453765\n",
      "101 Train Loss 0.008239974 Test MSE 0.0019941766193893847 Test RE 0.0004975493490629389\n",
      "102 Train Loss 0.007975029 Test MSE 0.0013493815333064981 Test RE 0.00040928114102005387\n",
      "103 Train Loss 0.0077216905 Test MSE 0.003288129497241359 Test RE 0.0006388939990068234\n",
      "104 Train Loss 0.0072404393 Test MSE 0.0048378209134641095 Test RE 0.0007749599275928106\n",
      "105 Train Loss 0.006742505 Test MSE 0.0025315182346047566 Test RE 0.0005605894009864423\n",
      "106 Train Loss 0.0062329136 Test MSE 0.0009720423956145482 Test RE 0.0003473737171121625\n",
      "107 Train Loss 0.006029662 Test MSE 0.0009090854716438944 Test RE 0.0003359361270240718\n",
      "108 Train Loss 0.005931709 Test MSE 0.0008856742900272952 Test RE 0.0003315823241262879\n",
      "109 Train Loss 0.0058642332 Test MSE 0.0007941194813100993 Test RE 0.000313976596549381\n",
      "110 Train Loss 0.005782023 Test MSE 0.0008945487905877123 Test RE 0.00033323941914221175\n",
      "111 Train Loss 0.0056327716 Test MSE 0.0011461928922791736 Test RE 0.00037721000894494185\n",
      "112 Train Loss 0.0053378353 Test MSE 0.001641797944675973 Test RE 0.000451454728526754\n",
      "113 Train Loss 0.0049929335 Test MSE 0.0018339588112009355 Test RE 0.00047714364199898915\n",
      "114 Train Loss 0.004693764 Test MSE 0.0016664974492714563 Test RE 0.0004548379348433354\n",
      "115 Train Loss 0.0044486686 Test MSE 0.0017202268506034386 Test RE 0.00046211196502718763\n",
      "116 Train Loss 0.004196945 Test MSE 0.0005992681821680224 Test RE 0.00027275012744235305\n",
      "117 Train Loss 0.0041162614 Test MSE 0.0006472976460933901 Test RE 0.0002834695186187905\n",
      "118 Train Loss 0.004069725 Test MSE 0.0007192219254248913 Test RE 0.0002988035903364274\n",
      "119 Train Loss 0.003925711 Test MSE 0.0015914871675643486 Test RE 0.00044448378564809646\n",
      "120 Train Loss 0.003701013 Test MSE 0.0013869791114392831 Test RE 0.00041494383127423036\n",
      "121 Train Loss 0.0035933882 Test MSE 0.0011348289531080684 Test RE 0.0003753354250566927\n",
      "122 Train Loss 0.0034382693 Test MSE 0.0015379843333161928 Test RE 0.000436948555663558\n",
      "123 Train Loss 0.0033762588 Test MSE 0.0015461041597673978 Test RE 0.0004381004776358886\n",
      "124 Train Loss 0.0033019208 Test MSE 0.0008282854860875017 Test RE 0.0003206596971254309\n",
      "125 Train Loss 0.0032649126 Test MSE 0.000878943345763079 Test RE 0.00033031994201671494\n",
      "126 Train Loss 0.0032371008 Test MSE 0.0011024727139306194 Test RE 0.00036994594990822824\n",
      "127 Train Loss 0.0031564229 Test MSE 0.002501070067475127 Test RE 0.0005572079212083414\n",
      "128 Train Loss 0.0030994255 Test MSE 0.002505545623665378 Test RE 0.000557706248053912\n",
      "129 Train Loss 0.0030332461 Test MSE 0.0039400880406390325 Test RE 0.0006993705096005875\n",
      "130 Train Loss 0.0029810988 Test MSE 0.0042302433043120554 Test RE 0.0007246645653686418\n",
      "131 Train Loss 0.002923015 Test MSE 0.0039929972352221125 Test RE 0.0007040505743153582\n",
      "132 Train Loss 0.002835264 Test MSE 0.0030305029626600992 Test RE 0.0006133547159370643\n",
      "133 Train Loss 0.0027561537 Test MSE 0.0030312361725314163 Test RE 0.0006134289099800379\n",
      "134 Train Loss 0.0027325754 Test MSE 0.002822015263929098 Test RE 0.0005918804994182574\n",
      "135 Train Loss 0.0026932003 Test MSE 0.00200654895108688 Test RE 0.0004990904179321971\n",
      "136 Train Loss 0.0026649248 Test MSE 0.0016565273220593387 Test RE 0.00045347531799831415\n",
      "137 Train Loss 0.002630474 Test MSE 0.0012728418599615558 Test RE 0.0003975040676215186\n",
      "138 Train Loss 0.0026295425 Test MSE 0.0012586571782174662 Test RE 0.00039528294890502866\n",
      "139 Train Loss 0.0025953227 Test MSE 0.0010966877865692972 Test RE 0.00036897407766901265\n",
      "140 Train Loss 0.0025615087 Test MSE 0.0009758108320705102 Test RE 0.0003480464189893025\n",
      "141 Train Loss 0.0025304363 Test MSE 0.0008643088680424352 Test RE 0.0003275584722323397\n",
      "142 Train Loss 0.0024939626 Test MSE 0.0007608021388129648 Test RE 0.0003073195685272861\n",
      "143 Train Loss 0.002457818 Test MSE 0.0006429798248993732 Test RE 0.00028252249009671815\n",
      "144 Train Loss 0.002424745 Test MSE 0.0005709195725371996 Test RE 0.00026622069833967635\n",
      "145 Train Loss 0.0023839045 Test MSE 0.000495269890248657 Test RE 0.00024795639601096065\n",
      "146 Train Loss 0.002310324 Test MSE 0.00031125749475345495 Test RE 0.00019656865277377063\n",
      "147 Train Loss 0.0022493615 Test MSE 0.0002670864051214034 Test RE 0.00018208754631542835\n",
      "148 Train Loss 0.0021799025 Test MSE 0.0003355069491657729 Test RE 0.00020408219294575368\n",
      "149 Train Loss 0.0021091215 Test MSE 0.00037488412385882345 Test RE 0.00021572619165937708\n",
      "150 Train Loss 0.0020413008 Test MSE 0.0007127598233150574 Test RE 0.00029745820857825955\n",
      "151 Train Loss 0.001984218 Test MSE 0.001267533165803074 Test RE 0.00039667425816445424\n",
      "152 Train Loss 0.001962862 Test MSE 0.0009854659368694727 Test RE 0.0003497640434653042\n",
      "153 Train Loss 0.001954972 Test MSE 0.0009567428950925887 Test RE 0.00034462912329663597\n",
      "154 Train Loss 0.0019380946 Test MSE 0.000803100748461566 Test RE 0.00031574709799130275\n",
      "155 Train Loss 0.0019158546 Test MSE 0.0004730513374019849 Test RE 0.0002423307294921525\n",
      "156 Train Loss 0.0018831795 Test MSE 0.00048194467581555775 Test RE 0.00024459802486567197\n",
      "157 Train Loss 0.0018182718 Test MSE 0.0019209254344929206 Test RE 0.0004883257275997158\n",
      "158 Train Loss 0.0017353215 Test MSE 0.000821493940084418 Test RE 0.0003193423628869845\n",
      "159 Train Loss 0.0016632404 Test MSE 0.00024179434149805941 Test RE 0.00017325166578235606\n",
      "160 Train Loss 0.0016326008 Test MSE 0.00029358855389014096 Test RE 0.0001909079052047926\n",
      "161 Train Loss 0.0015989797 Test MSE 0.00043521864137734915 Test RE 0.00023243851747786426\n",
      "162 Train Loss 0.0015680054 Test MSE 0.0004258093181252505 Test RE 0.00022991215536655578\n",
      "163 Train Loss 0.0015438368 Test MSE 0.0003047062757693363 Test RE 0.0001944890038395881\n",
      "164 Train Loss 0.0015167514 Test MSE 0.00015098291866327834 Test RE 0.00013690466932267023\n",
      "165 Train Loss 0.001494135 Test MSE 0.0001515284673112126 Test RE 0.00013715178605477134\n",
      "166 Train Loss 0.0014817038 Test MSE 0.0001230875126086186 Test RE 0.00012361220403179026\n",
      "167 Train Loss 0.001452026 Test MSE 0.00019622460816157448 Test RE 0.00015607418925202312\n",
      "168 Train Loss 0.001425264 Test MSE 0.00020371474402676793 Test RE 0.0001590250656851672\n",
      "169 Train Loss 0.0013965166 Test MSE 0.0002411372224267415 Test RE 0.00017301608452734108\n",
      "170 Train Loss 0.0013445969 Test MSE 0.0002293693350119633 Test RE 0.00016874154845514694\n",
      "171 Train Loss 0.0013077931 Test MSE 0.00037019403081941615 Test RE 0.0002143724928449395\n",
      "172 Train Loss 0.0012729832 Test MSE 0.00043911454741379734 Test RE 0.0002334765487181794\n",
      "173 Train Loss 0.0012398381 Test MSE 0.0002736272642720168 Test RE 0.0001843036925870268\n",
      "174 Train Loss 0.0012201476 Test MSE 0.0003134046911721418 Test RE 0.00019724549767516768\n",
      "175 Train Loss 0.0011988443 Test MSE 0.0004999549487284066 Test RE 0.00024912642055025533\n",
      "176 Train Loss 0.0011902871 Test MSE 0.0005432617591557032 Test RE 0.0002596922087316987\n",
      "177 Train Loss 0.001153995 Test MSE 0.0002268756343979688 Test RE 0.00016782176362183302\n",
      "178 Train Loss 0.0011302955 Test MSE 0.00013912006912101987 Test RE 0.00013141630277911534\n",
      "179 Train Loss 0.0011009428 Test MSE 0.00012500865378254597 Test RE 0.0001245731342767729\n",
      "180 Train Loss 0.0010596985 Test MSE 0.00012473616687089692 Test RE 0.00012443729141548585\n",
      "181 Train Loss 0.0010422892 Test MSE 0.00011495205023602528 Test RE 0.0001194573051243087\n",
      "182 Train Loss 0.0010301007 Test MSE 0.00014508288813108624 Test RE 0.00013420306931219937\n",
      "183 Train Loss 0.0010118848 Test MSE 9.872195661099638e-05 Test RE 0.00011070346883142854\n",
      "184 Train Loss 0.0010043324 Test MSE 8.880687168009994e-05 Test RE 0.00010499718078811293\n",
      "185 Train Loss 0.0010008146 Test MSE 9.415281632141544e-05 Test RE 0.00010811128017645998\n",
      "186 Train Loss 0.0009869725 Test MSE 0.00011821263513246574 Test RE 0.00012113964638668918\n",
      "187 Train Loss 0.0009868495 Test MSE 0.00011828680497276973 Test RE 0.00012117764359111064\n",
      "188 Train Loss 0.0009865817 Test MSE 0.00011824240221745461 Test RE 0.00012115489749402324\n",
      "189 Train Loss 0.0009758707 Test MSE 0.0001276629445822863 Test RE 0.00012588870934169142\n",
      "190 Train Loss 0.00097336073 Test MSE 0.0001337580392629507 Test RE 0.000128858864279271\n",
      "191 Train Loss 0.0009572235 Test MSE 0.00023392287561035147 Test RE 0.00017040828252415816\n",
      "192 Train Loss 0.0009556005 Test MSE 0.00023814614299703685 Test RE 0.0001719396856481207\n",
      "193 Train Loss 0.0009556005 Test MSE 0.00023814614299703685 Test RE 0.0001719396856481207\n",
      "194 Train Loss 0.0009556003 Test MSE 0.00023814485494498802 Test RE 0.00017193922066565016\n",
      "195 Train Loss 0.00095560006 Test MSE 0.00023814431418621765 Test RE 0.00017193902545317258\n",
      "196 Train Loss 0.00095560006 Test MSE 0.00023814431418621765 Test RE 0.00017193902545317258\n",
      "197 Train Loss 0.00095560006 Test MSE 0.00023814431418621765 Test RE 0.00017193902545317258\n",
      "198 Train Loss 0.00095560006 Test MSE 0.00023814431418621765 Test RE 0.00017193902545317258\n",
      "199 Train Loss 0.00095560006 Test MSE 0.00023814431418621765 Test RE 0.00017193902545317258\n",
      "Training time: 31.98\n",
      "Training time: 31.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 178.60507 Test MSE 7033.986436567852 Test RE 0.9344479500353576\n",
      "1 Train Loss 132.52664 Test MSE 2102.1759070327403 Test RE 0.5108446853855653\n",
      "2 Train Loss 103.33978 Test MSE 2901.0416808156433 Test RE 0.60011065345951\n",
      "3 Train Loss 61.08812 Test MSE 482.9690052449025 Test RE 0.24485782228573932\n",
      "4 Train Loss 48.56373 Test MSE 356.15854918101735 Test RE 0.21026938241119086\n",
      "5 Train Loss 40.47007 Test MSE 281.62520680588193 Test RE 0.18697782955693978\n",
      "6 Train Loss 37.08645 Test MSE 189.93577628207228 Test RE 0.15355280010254949\n",
      "7 Train Loss 30.653048 Test MSE 161.09485568766834 Test RE 0.14141490574986876\n",
      "8 Train Loss 26.554878 Test MSE 103.86251017114525 Test RE 0.1135491165474158\n",
      "9 Train Loss 22.741274 Test MSE 42.37685488994015 Test RE 0.07253017293845014\n",
      "10 Train Loss 19.946932 Test MSE 48.31716293834149 Test RE 0.07744708304333281\n",
      "11 Train Loss 17.828743 Test MSE 30.041185590530407 Test RE 0.06106788604478674\n",
      "12 Train Loss 15.675151 Test MSE 64.81134038436475 Test RE 0.08969739987595876\n",
      "13 Train Loss 13.092459 Test MSE 66.73669624310276 Test RE 0.09101997335620342\n",
      "14 Train Loss 11.143541 Test MSE 30.996193709923148 Test RE 0.062030964727735186\n",
      "15 Train Loss 9.377669 Test MSE 34.27748276385407 Test RE 0.06523171744117144\n",
      "16 Train Loss 7.87063 Test MSE 32.71970902552378 Test RE 0.06373222325060893\n",
      "17 Train Loss 6.46262 Test MSE 55.426144228401306 Test RE 0.08294910149766192\n",
      "18 Train Loss 5.142329 Test MSE 23.21705668793751 Test RE 0.05368561508246591\n",
      "19 Train Loss 4.238921 Test MSE 27.72595595384997 Test RE 0.05866750470496153\n",
      "20 Train Loss 3.6706991 Test MSE 19.886298610199564 Test RE 0.04968569059428423\n",
      "21 Train Loss 2.7270582 Test MSE 13.760610659672652 Test RE 0.04133074236250642\n",
      "22 Train Loss 2.2414958 Test MSE 2.6727297342655327 Test RE 0.018215113216955018\n",
      "23 Train Loss 1.7567691 Test MSE 8.61098397210821 Test RE 0.03269495491181587\n",
      "24 Train Loss 1.4001639 Test MSE 13.922866201154608 Test RE 0.041573699935551225\n",
      "25 Train Loss 1.1362953 Test MSE 7.0295015874685385 Test RE 0.02954041682255967\n",
      "26 Train Loss 0.8584388 Test MSE 0.4677338889981831 Test RE 0.007619978938714333\n",
      "27 Train Loss 0.731448 Test MSE 0.3153780874440996 Test RE 0.0062570569802090645\n",
      "28 Train Loss 0.6709587 Test MSE 0.533527130104545 Test RE 0.008138279601634421\n",
      "29 Train Loss 0.5891788 Test MSE 0.4356507466505581 Test RE 0.00735399929160266\n",
      "30 Train Loss 0.48994842 Test MSE 1.1599896440147968 Test RE 0.012000004484760834\n",
      "31 Train Loss 0.42357847 Test MSE 0.23627949796960174 Test RE 0.005415859286129958\n",
      "32 Train Loss 0.40335765 Test MSE 0.1428037750010275 Test RE 0.004210408137809032\n",
      "33 Train Loss 0.38872093 Test MSE 0.7001048652619661 Test RE 0.009322575306283066\n",
      "34 Train Loss 0.36447805 Test MSE 0.4193113889556272 Test RE 0.007214773170227579\n",
      "35 Train Loss 0.3434726 Test MSE 0.21040674894057101 Test RE 0.005110744681362226\n",
      "36 Train Loss 0.28728914 Test MSE 0.40175969730891287 Test RE 0.007062159735534791\n",
      "37 Train Loss 0.24653165 Test MSE 0.33749969289097725 Test RE 0.00647278293735085\n",
      "38 Train Loss 0.234164 Test MSE 0.06328384649187922 Test RE 0.0028028560038701722\n",
      "39 Train Loss 0.20974144 Test MSE 0.10384700575966303 Test RE 0.003590470326365858\n",
      "40 Train Loss 0.1799572 Test MSE 0.07622221752050627 Test RE 0.003076062406470972\n",
      "41 Train Loss 0.1448962 Test MSE 0.13301164243879304 Test RE 0.004063489860871132\n",
      "42 Train Loss 0.12953274 Test MSE 0.3140296821649477 Test RE 0.006243666566412058\n",
      "43 Train Loss 0.119602114 Test MSE 0.05411986175625393 Test RE 0.002591986239724406\n",
      "44 Train Loss 0.10695363 Test MSE 0.06859783905978613 Test RE 0.002918163155773027\n",
      "45 Train Loss 0.09853506 Test MSE 0.05731579781691122 Test RE 0.0026674207384074643\n",
      "46 Train Loss 0.09465864 Test MSE 0.07765350613560308 Test RE 0.0031048089884795874\n",
      "47 Train Loss 0.08751415 Test MSE 0.042756149127684426 Test RE 0.0023038470530743734\n",
      "48 Train Loss 0.07966328 Test MSE 0.017840220418475106 Test RE 0.0014881765617474718\n",
      "49 Train Loss 0.07522691 Test MSE 0.012717165734108768 Test RE 0.0012564624613569076\n",
      "50 Train Loss 0.07083687 Test MSE 0.04223803560895018 Test RE 0.002289845645763681\n",
      "51 Train Loss 0.06759443 Test MSE 0.011891981794112704 Test RE 0.0012150145261182663\n",
      "52 Train Loss 0.06603984 Test MSE 0.011788238906238178 Test RE 0.0012097031646721341\n",
      "53 Train Loss 0.06411018 Test MSE 0.027457718229001716 Test RE 0.0018462332761468331\n",
      "54 Train Loss 0.06248597 Test MSE 0.025850427618108636 Test RE 0.0017913820498456044\n",
      "55 Train Loss 0.06052036 Test MSE 0.021506896853429474 Test RE 0.0016339668017441966\n",
      "56 Train Loss 0.056357592 Test MSE 0.013315512888733588 Test RE 0.0012856812253661636\n",
      "57 Train Loss 0.054957323 Test MSE 0.006670648303585287 Test RE 0.000909993676265858\n",
      "58 Train Loss 0.05294222 Test MSE 0.012744050411881836 Test RE 0.0012577898701151009\n",
      "59 Train Loss 0.05114077 Test MSE 0.008752900767927563 Test RE 0.0010423902841770983\n",
      "60 Train Loss 0.049245395 Test MSE 0.0092519570740149 Test RE 0.0010716948825320938\n",
      "61 Train Loss 0.04871475 Test MSE 0.0093072343399622 Test RE 0.001074891619217559\n",
      "62 Train Loss 0.0481273 Test MSE 0.006485024408513674 Test RE 0.0008972431674945388\n",
      "63 Train Loss 0.04696772 Test MSE 0.00619148504324405 Test RE 0.0008767015339205299\n",
      "64 Train Loss 0.04440222 Test MSE 0.01850729085338655 Test RE 0.00151574371968458\n",
      "65 Train Loss 0.042994153 Test MSE 0.00940066216071291 Test RE 0.0010802731328903013\n",
      "66 Train Loss 0.04157438 Test MSE 0.004696742104048353 Test RE 0.0007635767731003581\n",
      "67 Train Loss 0.040568884 Test MSE 0.015836198238469067 Test RE 0.0014021026727139994\n",
      "68 Train Loss 0.038786575 Test MSE 0.027590682894312407 Test RE 0.0018506980920996708\n",
      "69 Train Loss 0.037726525 Test MSE 0.011142328948825867 Test RE 0.0011760948282771586\n",
      "70 Train Loss 0.037048943 Test MSE 0.009010009418162369 Test RE 0.0010575891211799678\n",
      "71 Train Loss 0.036329787 Test MSE 0.00471575648218264 Test RE 0.0007651208511189986\n",
      "72 Train Loss 0.035886973 Test MSE 0.008647742748555598 Test RE 0.0010361096849708124\n",
      "73 Train Loss 0.035448115 Test MSE 0.00923023985230876 Test RE 0.0010704363428955524\n",
      "74 Train Loss 0.034811776 Test MSE 0.005468455801695064 Test RE 0.0008239231499730157\n",
      "75 Train Loss 0.033195715 Test MSE 0.01060224352857051 Test RE 0.0011472372508677638\n",
      "76 Train Loss 0.031894043 Test MSE 0.01628723665927697 Test RE 0.0014219294718541128\n",
      "77 Train Loss 0.031433888 Test MSE 0.018080164717222878 Test RE 0.001498150845519085\n",
      "78 Train Loss 0.030897586 Test MSE 0.004186529599769033 Test RE 0.0007209106400648978\n",
      "79 Train Loss 0.030420035 Test MSE 0.01007493455790384 Test RE 0.0011183441473088785\n",
      "80 Train Loss 0.029451672 Test MSE 0.004654603759438051 Test RE 0.0007601437172607441\n",
      "81 Train Loss 0.02895232 Test MSE 0.0031848853029294985 Test RE 0.000628783664937598\n",
      "82 Train Loss 0.028788703 Test MSE 0.002569296883740789 Test RE 0.0005647568372435933\n",
      "83 Train Loss 0.027970813 Test MSE 0.0068043891518936505 Test RE 0.000919070706571052\n",
      "84 Train Loss 0.026816744 Test MSE 0.014378723958550615 Test RE 0.0013360248173922872\n",
      "85 Train Loss 0.026086438 Test MSE 0.0066291271693244905 Test RE 0.0009071571491608154\n",
      "86 Train Loss 0.025240082 Test MSE 0.0024368360489299833 Test RE 0.000550006101916031\n",
      "87 Train Loss 0.024963355 Test MSE 0.003528526461312961 Test RE 0.0006618369983933945\n",
      "88 Train Loss 0.024395293 Test MSE 0.003929174065755393 Test RE 0.0006984012158704659\n",
      "89 Train Loss 0.02386212 Test MSE 0.009453484691592679 Test RE 0.001083303920782397\n",
      "90 Train Loss 0.022648877 Test MSE 0.010149963372479582 Test RE 0.0011225006208206466\n",
      "91 Train Loss 0.02171103 Test MSE 0.013629724443895123 Test RE 0.0013007621469767322\n",
      "92 Train Loss 0.021176277 Test MSE 0.005272963325844597 Test RE 0.0008090618584388041\n",
      "93 Train Loss 0.02081429 Test MSE 0.003989362150249629 Test RE 0.0007037300298411677\n",
      "94 Train Loss 0.020610193 Test MSE 0.004996077434580074 Test RE 0.0007875333113923522\n",
      "95 Train Loss 0.020351179 Test MSE 0.0033577240799199863 Test RE 0.0006456198208376159\n",
      "96 Train Loss 0.019943686 Test MSE 0.0026674679012872507 Test RE 0.0005754451756052832\n",
      "97 Train Loss 0.019299334 Test MSE 0.0048168402218559765 Test RE 0.0007732776762120454\n",
      "98 Train Loss 0.018549554 Test MSE 0.006091356692165905 Test RE 0.0008695836549629827\n",
      "99 Train Loss 0.017671032 Test MSE 0.010624460548231768 Test RE 0.0011484386406407892\n",
      "100 Train Loss 0.017084636 Test MSE 0.010972204422260736 Test RE 0.0011670818026283778\n",
      "101 Train Loss 0.016703013 Test MSE 0.005384520623875748 Test RE 0.0008175755114579088\n",
      "102 Train Loss 0.016324038 Test MSE 0.002377697528001969 Test RE 0.0005432911811212657\n",
      "103 Train Loss 0.016107952 Test MSE 0.0033078275716212223 Test RE 0.0006408048406684916\n",
      "104 Train Loss 0.015739223 Test MSE 0.0020055057151788127 Test RE 0.0004989606586410887\n",
      "105 Train Loss 0.015472107 Test MSE 0.0015081929904759291 Test RE 0.00043269593117184024\n",
      "106 Train Loss 0.015218343 Test MSE 0.0017000771184819995 Test RE 0.00045939753804739825\n",
      "107 Train Loss 0.014916676 Test MSE 0.0012621599483299381 Test RE 0.00039583259155444376\n",
      "108 Train Loss 0.014580569 Test MSE 0.002054805118878473 Test RE 0.0005050561593120882\n",
      "109 Train Loss 0.014265342 Test MSE 0.0024418909353862805 Test RE 0.0005505762629848246\n",
      "110 Train Loss 0.013975799 Test MSE 0.0017103707816393953 Test RE 0.00046078622415859346\n",
      "111 Train Loss 0.013667267 Test MSE 0.0017479801253129758 Test RE 0.00046582479072738916\n",
      "112 Train Loss 0.0134312175 Test MSE 0.0017990968842893785 Test RE 0.0004725868446650703\n",
      "113 Train Loss 0.013285706 Test MSE 0.0019183048542736085 Test RE 0.0004879925201084647\n",
      "114 Train Loss 0.012976264 Test MSE 0.001348036080446978 Test RE 0.00040907704536485146\n",
      "115 Train Loss 0.012778112 Test MSE 0.001062429185595755 Test RE 0.00036316530258332123\n",
      "116 Train Loss 0.012575358 Test MSE 0.001470717063617611 Test RE 0.00042728625044393314\n",
      "117 Train Loss 0.012466407 Test MSE 0.0013957953932376137 Test RE 0.0004162605298165904\n",
      "118 Train Loss 0.012341464 Test MSE 0.0011617509446797474 Test RE 0.0003797614435012668\n",
      "119 Train Loss 0.012158512 Test MSE 0.0015901228911049815 Test RE 0.00044429323155825197\n",
      "120 Train Loss 0.0119549725 Test MSE 0.0018833191676404617 Test RE 0.00048352208523273565\n",
      "121 Train Loss 0.01176145 Test MSE 0.0022915628130096163 Test RE 0.0005333597460651648\n",
      "122 Train Loss 0.011652858 Test MSE 0.0013331416079964537 Test RE 0.0004068108186388133\n",
      "123 Train Loss 0.011550202 Test MSE 0.0011600887783700332 Test RE 0.0003794896758636509\n",
      "124 Train Loss 0.011445987 Test MSE 0.002094094226473194 Test RE 0.0005098617849733748\n",
      "125 Train Loss 0.011293467 Test MSE 0.0011669593251364286 Test RE 0.0003806117678154748\n",
      "126 Train Loss 0.011198636 Test MSE 0.001071136388355034 Test RE 0.0003646504375341547\n",
      "127 Train Loss 0.0110892365 Test MSE 0.0017177501863068546 Test RE 0.0004617791866953812\n",
      "128 Train Loss 0.010872 Test MSE 0.00100191295546992 Test RE 0.000352670674589802\n",
      "129 Train Loss 0.010718817 Test MSE 0.0013067879058105544 Test RE 0.000402769805776236\n",
      "130 Train Loss 0.010605298 Test MSE 0.0016152221749567714 Test RE 0.00044778597445851586\n",
      "131 Train Loss 0.010527104 Test MSE 0.000973303262533879 Test RE 0.0003475989388150657\n",
      "132 Train Loss 0.010481243 Test MSE 0.0009426610584564124 Test RE 0.0003420835068313672\n",
      "133 Train Loss 0.010446381 Test MSE 0.0012861641318493897 Test RE 0.00039957890215224564\n",
      "134 Train Loss 0.010392077 Test MSE 0.0013994912358615728 Test RE 0.000416811261098888\n",
      "135 Train Loss 0.010318402 Test MSE 0.001116081235252858 Test RE 0.00037222218617297505\n",
      "136 Train Loss 0.010271758 Test MSE 0.0008331591810202444 Test RE 0.00032160170649355287\n",
      "137 Train Loss 0.010245792 Test MSE 0.000750923377440581 Test RE 0.00030531782839684596\n",
      "138 Train Loss 0.010208113 Test MSE 0.0007633265971201182 Test RE 0.0003078290130300087\n",
      "139 Train Loss 0.010115503 Test MSE 0.001289168700190837 Test RE 0.0004000453518682449\n",
      "140 Train Loss 0.009956374 Test MSE 0.0016947614549063033 Test RE 0.00045867877223747103\n",
      "141 Train Loss 0.009797102 Test MSE 0.001377430294667992 Test RE 0.0004135130001416168\n",
      "142 Train Loss 0.00972394 Test MSE 0.001983464282601557 Test RE 0.000496211179399984\n",
      "143 Train Loss 0.009646444 Test MSE 0.002139070761431127 Test RE 0.0005153080513538033\n",
      "144 Train Loss 0.009476211 Test MSE 0.001450888570396238 Test RE 0.00042439609786071693\n",
      "145 Train Loss 0.009375445 Test MSE 0.0007575010292666782 Test RE 0.0003066521161280442\n",
      "146 Train Loss 0.009243232 Test MSE 0.000908817939713347 Test RE 0.0003358866925888407\n",
      "147 Train Loss 0.0091463635 Test MSE 0.0010133132612105463 Test RE 0.00035467143777834475\n",
      "148 Train Loss 0.009009745 Test MSE 0.0009260834986392228 Test RE 0.0003390622387678575\n",
      "149 Train Loss 0.008941255 Test MSE 0.0007477032840360571 Test RE 0.0003046624964390912\n",
      "150 Train Loss 0.0088562295 Test MSE 0.0007006010901476408 Test RE 0.0002949101749142353\n",
      "151 Train Loss 0.008788712 Test MSE 0.0007996409035271313 Test RE 0.00031506622755005613\n",
      "152 Train Loss 0.008743852 Test MSE 0.0008810951840328734 Test RE 0.000330724041202213\n",
      "153 Train Loss 0.00871104 Test MSE 0.0011385619275420375 Test RE 0.0003759522436544466\n",
      "154 Train Loss 0.008609509 Test MSE 0.00362872546100403 Test RE 0.0006711682570709821\n",
      "155 Train Loss 0.008352815 Test MSE 0.003782199986094814 Test RE 0.0006852145821868174\n",
      "156 Train Loss 0.008206835 Test MSE 0.000876335214532848 Test RE 0.00032982949071701305\n",
      "157 Train Loss 0.008115019 Test MSE 0.0010792381486929004 Test RE 0.000366026893944345\n",
      "158 Train Loss 0.008063055 Test MSE 0.0010848289870884172 Test RE 0.00036697374417877013\n",
      "159 Train Loss 0.007998828 Test MSE 0.0012646785575822343 Test RE 0.00039622733184795013\n",
      "160 Train Loss 0.007933861 Test MSE 0.0007301247822962548 Test RE 0.0003010598890373563\n",
      "161 Train Loss 0.007853468 Test MSE 0.0007894701700024332 Test RE 0.00031305613188406224\n",
      "162 Train Loss 0.0077070254 Test MSE 0.0017666620658970203 Test RE 0.00046830747970319866\n",
      "163 Train Loss 0.0075970045 Test MSE 0.0012672012999963467 Test RE 0.00039662232609512124\n",
      "164 Train Loss 0.0075253067 Test MSE 0.0005823980043059505 Test RE 0.00026888358592878377\n",
      "165 Train Loss 0.007399521 Test MSE 0.000870124712293471 Test RE 0.0003286586779454378\n",
      "166 Train Loss 0.007192964 Test MSE 0.0009454903110579182 Test RE 0.0003425964778109125\n",
      "167 Train Loss 0.0070664054 Test MSE 0.0006372002347717446 Test RE 0.0002812498606367758\n",
      "168 Train Loss 0.006977638 Test MSE 0.0010648289671465823 Test RE 0.0003635752244025243\n",
      "169 Train Loss 0.006854674 Test MSE 0.0011211496829588252 Test RE 0.00037306641309372814\n",
      "170 Train Loss 0.0066671097 Test MSE 0.0013062015259056447 Test RE 0.00040267943050590504\n",
      "171 Train Loss 0.006476815 Test MSE 0.0011416547853194208 Test RE 0.0003764625269385462\n",
      "172 Train Loss 0.00640339 Test MSE 0.0012554985124884493 Test RE 0.00039478664576314163\n",
      "173 Train Loss 0.0063064345 Test MSE 0.001013746151410564 Test RE 0.0003547471879937542\n",
      "174 Train Loss 0.0061070174 Test MSE 0.0007321096763731333 Test RE 0.0003014688372206445\n",
      "175 Train Loss 0.0060445936 Test MSE 0.0005288061724873379 Test RE 0.0002562138546081654\n",
      "176 Train Loss 0.005963161 Test MSE 0.0004807812775065221 Test RE 0.00024430262076334004\n",
      "177 Train Loss 0.005892078 Test MSE 0.0006493716232965895 Test RE 0.00028392328139118086\n",
      "178 Train Loss 0.0058589047 Test MSE 0.0006514274082323299 Test RE 0.00028437234934783947\n",
      "179 Train Loss 0.0058199805 Test MSE 0.0010286806994608663 Test RE 0.0003573507089995841\n",
      "180 Train Loss 0.005759745 Test MSE 0.0013090302429046195 Test RE 0.00040311521706048705\n",
      "181 Train Loss 0.005672213 Test MSE 0.0009685049007542659 Test RE 0.0003467410529467403\n",
      "182 Train Loss 0.005471136 Test MSE 0.0004896373448259641 Test RE 0.00024654240005027176\n",
      "183 Train Loss 0.005186324 Test MSE 0.0010560011295713842 Test RE 0.0003620649992813215\n",
      "184 Train Loss 0.0050774748 Test MSE 0.0007067666807009313 Test RE 0.00029620500051149527\n",
      "185 Train Loss 0.0049716444 Test MSE 0.0005927179896909147 Test RE 0.00027125540881377713\n",
      "186 Train Loss 0.0049172766 Test MSE 0.0004536827889894936 Test RE 0.00023731790422668828\n",
      "187 Train Loss 0.004869023 Test MSE 0.000518916070046126 Test RE 0.000253806600701994\n",
      "188 Train Loss 0.004809846 Test MSE 0.0006235541566856456 Test RE 0.00027822198298885133\n",
      "189 Train Loss 0.004764818 Test MSE 0.0004604009007685358 Test RE 0.00023906854295795656\n",
      "190 Train Loss 0.004751226 Test MSE 0.0007968033446279286 Test RE 0.00031450671796265313\n",
      "191 Train Loss 0.00470925 Test MSE 0.0006196860997086594 Test RE 0.0002773577015016433\n",
      "192 Train Loss 0.004677245 Test MSE 0.0005489753663195105 Test RE 0.00026105425779008364\n",
      "193 Train Loss 0.0046394076 Test MSE 0.0017400310526483778 Test RE 0.00046476439695265076\n",
      "194 Train Loss 0.0045838663 Test MSE 0.0030764841412312164 Test RE 0.0006179903487614098\n",
      "195 Train Loss 0.0044855075 Test MSE 0.0012502818039113023 Test RE 0.00039396560509492547\n",
      "196 Train Loss 0.0044450886 Test MSE 0.0009656435688516446 Test RE 0.0003462284715901031\n",
      "197 Train Loss 0.0043911496 Test MSE 0.0013601264036557132 Test RE 0.0004109074242139982\n",
      "198 Train Loss 0.004331843 Test MSE 0.0011422000656449575 Test RE 0.00037655241974433006\n",
      "199 Train Loss 0.004277746 Test MSE 0.0015842169992015878 Test RE 0.00044346738697835064\n",
      "Training time: 33.89\n",
      "Training time: 33.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 162.27762 Test MSE 4051.195760552337 Test RE 0.7091628343454283\n",
      "1 Train Loss 93.693985 Test MSE 2452.8555063018657 Test RE 0.5518109764035758\n",
      "2 Train Loss 68.937836 Test MSE 280.7117779217179 Test RE 0.1866743594522921\n",
      "3 Train Loss 55.360477 Test MSE 387.14548600928254 Test RE 0.2192256933467935\n",
      "4 Train Loss 47.597313 Test MSE 88.58261102885129 Test RE 0.10486452427224899\n",
      "5 Train Loss 44.128788 Test MSE 104.78706901173418 Test RE 0.11405339013255944\n",
      "6 Train Loss 38.508614 Test MSE 90.52225169986457 Test RE 0.1060063854694578\n",
      "7 Train Loss 33.06268 Test MSE 104.05729034580297 Test RE 0.11365553972652531\n",
      "8 Train Loss 22.847763 Test MSE 49.323169525374055 Test RE 0.07824918821627334\n",
      "9 Train Loss 19.644323 Test MSE 34.43778364045963 Test RE 0.06538406971135194\n",
      "10 Train Loss 17.21757 Test MSE 23.07006555933837 Test RE 0.05351539886476008\n",
      "11 Train Loss 13.769931 Test MSE 41.1890275406442 Test RE 0.07150643418794143\n",
      "12 Train Loss 11.361223 Test MSE 22.825173589634257 Test RE 0.053230604346381265\n",
      "13 Train Loss 9.095391 Test MSE 13.65777451832869 Test RE 0.04117601577256581\n",
      "14 Train Loss 6.4828606 Test MSE 29.828915067840782 Test RE 0.06085175122809022\n",
      "15 Train Loss 4.3603683 Test MSE 14.878223671838118 Test RE 0.04297638623211297\n",
      "16 Train Loss 2.1984322 Test MSE 7.685449247319758 Test RE 0.030887942619037382\n",
      "17 Train Loss 1.6683173 Test MSE 1.0473940443772753 Test RE 0.011402744740579557\n",
      "18 Train Loss 1.2724233 Test MSE 0.6747537920020131 Test RE 0.009152231974192605\n",
      "19 Train Loss 1.1427488 Test MSE 1.1112615102910717 Test RE 0.011745256038322447\n",
      "20 Train Loss 0.9468799 Test MSE 1.8145830463532864 Test RE 0.015008689631956916\n",
      "21 Train Loss 0.76954454 Test MSE 0.8491763530122309 Test RE 0.010267230174155475\n",
      "22 Train Loss 0.60576016 Test MSE 1.1996431905327742 Test RE 0.012203387418379761\n",
      "23 Train Loss 0.4839508 Test MSE 1.0185238923069104 Test RE 0.011244495186431008\n",
      "24 Train Loss 0.44395456 Test MSE 1.0127528795336433 Test RE 0.011212593967532748\n",
      "25 Train Loss 0.39040917 Test MSE 0.1844845721071443 Test RE 0.0047855777774732655\n",
      "26 Train Loss 0.34684256 Test MSE 0.18139962773889468 Test RE 0.004745396964200614\n",
      "27 Train Loss 0.31533852 Test MSE 0.18355529211893068 Test RE 0.004773509678742279\n",
      "28 Train Loss 0.30378193 Test MSE 0.09317089184686343 Test RE 0.0034009048280347716\n",
      "29 Train Loss 0.27624977 Test MSE 0.1693560800170038 Test RE 0.004585162678295957\n",
      "30 Train Loss 0.26190355 Test MSE 0.07759732490057254 Test RE 0.0031036856421338068\n",
      "31 Train Loss 0.24393359 Test MSE 0.06611165881595112 Test RE 0.0028647938749650805\n",
      "32 Train Loss 0.23777188 Test MSE 0.08538793845933787 Test RE 0.0032557617496944613\n",
      "33 Train Loss 0.22831634 Test MSE 0.09666212561542391 Test RE 0.0034640370060496223\n",
      "34 Train Loss 0.1981938 Test MSE 0.19011454194686672 Test RE 0.004858050456078808\n",
      "35 Train Loss 0.17521238 Test MSE 0.14317102500103937 Test RE 0.004215818637368999\n",
      "36 Train Loss 0.15602702 Test MSE 0.18649655354867486 Test RE 0.004811602674178993\n",
      "37 Train Loss 0.13669044 Test MSE 0.04831023279441013 Test RE 0.002448916162278332\n",
      "38 Train Loss 0.122746125 Test MSE 0.12406170568467344 Test RE 0.00392439963160488\n",
      "39 Train Loss 0.10770293 Test MSE 0.07841653568516048 Test RE 0.0031200257500507973\n",
      "40 Train Loss 0.10041917 Test MSE 0.041722227484112945 Test RE 0.0022758209746119706\n",
      "41 Train Loss 0.08817023 Test MSE 0.03086824203098403 Test RE 0.0019575384478168877\n",
      "42 Train Loss 0.07888228 Test MSE 0.013355628395208248 Test RE 0.0012876164480993197\n",
      "43 Train Loss 0.0737901 Test MSE 0.01696201303322458 Test RE 0.0014510856543773966\n",
      "44 Train Loss 0.06678979 Test MSE 0.02665482524667957 Test RE 0.0018190400963063358\n",
      "45 Train Loss 0.05560617 Test MSE 0.006797617433489377 Test RE 0.0009186132638012526\n",
      "46 Train Loss 0.04951814 Test MSE 0.00662520263636838 Test RE 0.0009068885847548042\n",
      "47 Train Loss 0.045414526 Test MSE 0.02191194711910775 Test RE 0.0016492816914141144\n",
      "48 Train Loss 0.039028943 Test MSE 0.010066040881701621 Test RE 0.00111785042764014\n",
      "49 Train Loss 0.035896912 Test MSE 0.006565730774965019 Test RE 0.0009028090162933388\n",
      "50 Train Loss 0.03349084 Test MSE 0.003988360876992983 Test RE 0.0007036417111763571\n",
      "51 Train Loss 0.029461684 Test MSE 0.00414741673667236 Test RE 0.0007175351657449581\n",
      "52 Train Loss 0.027824087 Test MSE 0.007731650492510073 Test RE 0.0009796940245383073\n",
      "53 Train Loss 0.025546884 Test MSE 0.006038766814758058 Test RE 0.0008658217317906568\n",
      "54 Train Loss 0.024029197 Test MSE 0.002444760464306531 Test RE 0.000550899666175234\n",
      "55 Train Loss 0.0223087 Test MSE 0.004350339606686481 Test RE 0.0007348791622057101\n",
      "56 Train Loss 0.02066109 Test MSE 0.0038756489868518275 Test RE 0.0006936279272704753\n",
      "57 Train Loss 0.019408545 Test MSE 0.002425333627636115 Test RE 0.0005487064894354573\n",
      "58 Train Loss 0.01878098 Test MSE 0.0029426100177070173 Test RE 0.0006043947829518926\n",
      "59 Train Loss 0.018011326 Test MSE 0.0018539204275039051 Test RE 0.00047973333511743937\n",
      "60 Train Loss 0.017000511 Test MSE 0.001963821818148672 Test RE 0.0004937480491833398\n",
      "61 Train Loss 0.016271746 Test MSE 0.0011179491771869919 Test RE 0.00037253354287886\n",
      "62 Train Loss 0.01602594 Test MSE 0.0010087370594717808 Test RE 0.0003538696696012066\n",
      "63 Train Loss 0.015584647 Test MSE 0.0009298991261925863 Test RE 0.0003397600189120278\n",
      "64 Train Loss 0.0149069475 Test MSE 0.0015844598958893613 Test RE 0.0004435013825202943\n",
      "65 Train Loss 0.014153967 Test MSE 0.0025005166915465287 Test RE 0.0005571462750928421\n",
      "66 Train Loss 0.013796039 Test MSE 0.002173440124123061 Test RE 0.0005194313916629476\n",
      "67 Train Loss 0.013283378 Test MSE 0.0008494790789077013 Test RE 0.0003247361938235584\n",
      "68 Train Loss 0.013029797 Test MSE 0.0009576103510649301 Test RE 0.0003447853214041099\n",
      "69 Train Loss 0.012741472 Test MSE 0.0010013031086875599 Test RE 0.0003525633260357063\n",
      "70 Train Loss 0.012263288 Test MSE 0.0010527394931458425 Test RE 0.0003615054176489523\n",
      "71 Train Loss 0.011886593 Test MSE 0.001735920469688658 Test RE 0.0004642151015141394\n",
      "72 Train Loss 0.011445278 Test MSE 0.0007554972035790904 Test RE 0.000306246252457239\n",
      "73 Train Loss 0.011052167 Test MSE 0.0015224320919686686 Test RE 0.00043473370993474714\n",
      "74 Train Loss 0.010789947 Test MSE 0.0017325244757429394 Test RE 0.0004637608055159462\n",
      "75 Train Loss 0.01044111 Test MSE 0.0009438807209223434 Test RE 0.0003423047377498624\n",
      "76 Train Loss 0.010276212 Test MSE 0.0008831907455976229 Test RE 0.0003311170980529387\n",
      "77 Train Loss 0.0100163855 Test MSE 0.0012648662987999892 Test RE 0.0003962567406810565\n",
      "78 Train Loss 0.009702261 Test MSE 0.0011063770514297883 Test RE 0.00037060044107098785\n",
      "79 Train Loss 0.00931923 Test MSE 0.0005288137600845358 Test RE 0.00025621569274910706\n",
      "80 Train Loss 0.009194359 Test MSE 0.0006917139553192877 Test RE 0.00029303373536785954\n",
      "81 Train Loss 0.008881512 Test MSE 0.0008994027013984738 Test RE 0.00033414229116575356\n",
      "82 Train Loss 0.008718254 Test MSE 0.0005844470821590961 Test RE 0.0002693561833858347\n",
      "83 Train Loss 0.008443592 Test MSE 0.0012009407585968413 Test RE 0.00038611364059867463\n",
      "84 Train Loss 0.008357891 Test MSE 0.0010394867658088942 Test RE 0.000359222751117173\n",
      "85 Train Loss 0.008288994 Test MSE 0.0008730075459037012 Test RE 0.0003292026716950505\n",
      "86 Train Loss 0.008196172 Test MSE 0.0004922381875678798 Test RE 0.0002471963215406255\n",
      "87 Train Loss 0.008036504 Test MSE 0.0008687081781750543 Test RE 0.00032839104627382205\n",
      "88 Train Loss 0.007777948 Test MSE 0.0009342667444625935 Test RE 0.0003405569890523667\n",
      "89 Train Loss 0.007438283 Test MSE 0.0006787334223416699 Test RE 0.0002902712144974302\n",
      "90 Train Loss 0.007250725 Test MSE 0.0013165331036396545 Test RE 0.0004042688175527727\n",
      "91 Train Loss 0.007046386 Test MSE 0.0014794020628577753 Test RE 0.00042854601629960183\n",
      "92 Train Loss 0.0067134122 Test MSE 0.0013785117723674294 Test RE 0.0004136753014008656\n",
      "93 Train Loss 0.0062399297 Test MSE 0.0015439139668411157 Test RE 0.0004377900636787326\n",
      "94 Train Loss 0.00594368 Test MSE 0.00173566299703344 Test RE 0.0004641806739210808\n",
      "95 Train Loss 0.005618019 Test MSE 0.001116828489587956 Test RE 0.00037234677302750184\n",
      "96 Train Loss 0.005458203 Test MSE 0.0008165876980232279 Test RE 0.0003183873240299823\n",
      "97 Train Loss 0.005360299 Test MSE 0.0004344151164842294 Test RE 0.00023222384788059073\n",
      "98 Train Loss 0.0052500893 Test MSE 0.00035334853124267525 Test RE 0.0002094382486356724\n",
      "99 Train Loss 0.005136403 Test MSE 0.00047722099788025297 Test RE 0.00024339638552997331\n",
      "100 Train Loss 0.0049996986 Test MSE 0.001143325519714569 Test RE 0.00037673788992708453\n",
      "101 Train Loss 0.0048392834 Test MSE 0.002326797015901342 Test RE 0.0005374444734940932\n",
      "102 Train Loss 0.0046390397 Test MSE 0.000938738889223131 Test RE 0.0003413711044619044\n",
      "103 Train Loss 0.004344128 Test MSE 0.0004034588661694881 Test RE 0.0002237968576138293\n",
      "104 Train Loss 0.00411248 Test MSE 0.0005390805602839334 Test RE 0.0002586909215201349\n",
      "105 Train Loss 0.003936605 Test MSE 0.002382129753845959 Test RE 0.0005437973153240205\n",
      "106 Train Loss 0.0035873281 Test MSE 0.0018530533312527205 Test RE 0.000479621134070264\n",
      "107 Train Loss 0.003335853 Test MSE 0.0009009057893540481 Test RE 0.0003344213850514795\n",
      "108 Train Loss 0.003168918 Test MSE 0.0002252352547399628 Test RE 0.00016721396185160345\n",
      "109 Train Loss 0.0030056976 Test MSE 0.00047711672483184773 Test RE 0.00024336979295575407\n",
      "110 Train Loss 0.0028745574 Test MSE 0.0008138650706104721 Test RE 0.00031785610502075575\n",
      "111 Train Loss 0.0027474253 Test MSE 0.00021474200640067437 Test RE 0.0001632724296889394\n",
      "112 Train Loss 0.0026421954 Test MSE 0.00017314141315752626 Test RE 0.00014660704197667635\n",
      "113 Train Loss 0.002554067 Test MSE 0.00025952600959240266 Test RE 0.00017949187609469207\n",
      "114 Train Loss 0.002481176 Test MSE 0.0001594889927824215 Test RE 0.00014070829805289107\n",
      "115 Train Loss 0.002392528 Test MSE 0.0002904660019177117 Test RE 0.00018988996123528473\n",
      "116 Train Loss 0.002343787 Test MSE 0.0001317037046727141 Test RE 0.0001278654902243046\n",
      "117 Train Loss 0.00228171 Test MSE 0.000506147235629375 Test RE 0.0002506644740239096\n",
      "118 Train Loss 0.0021744238 Test MSE 0.00014524967955357982 Test RE 0.0001342801889901946\n",
      "119 Train Loss 0.0020852801 Test MSE 0.0005356534545631552 Test RE 0.00025786732043281386\n",
      "120 Train Loss 0.0019792642 Test MSE 0.00032113565303192287 Test RE 0.00019966346999030984\n",
      "121 Train Loss 0.0019307334 Test MSE 0.00012787487544148676 Test RE 0.000125993158753322\n",
      "122 Train Loss 0.0018880778 Test MSE 0.0001437816381887405 Test RE 0.00013359987939496053\n",
      "123 Train Loss 0.0018157953 Test MSE 0.00010998761574572695 Test RE 0.00011684933529399857\n",
      "124 Train Loss 0.00175747 Test MSE 0.0003369879495197202 Test RE 0.00020453212846910574\n",
      "125 Train Loss 0.0017067376 Test MSE 0.00015343741621860185 Test RE 0.00013801299822324538\n",
      "126 Train Loss 0.0016661938 Test MSE 7.800549439952083e-05 Test RE 9.840495095537669e-05\n",
      "127 Train Loss 0.0015905543 Test MSE 0.00015334826073310244 Test RE 0.00013797289586719243\n",
      "128 Train Loss 0.0015385826 Test MSE 0.00024898651789060456 Test RE 0.00017580947139311318\n",
      "129 Train Loss 0.0014479447 Test MSE 0.00030590888263865956 Test RE 0.0001948724279817727\n",
      "130 Train Loss 0.0013530258 Test MSE 9.916269234498817e-05 Test RE 0.00011095030672696295\n",
      "131 Train Loss 0.0012635626 Test MSE 0.00018642005479051468 Test RE 0.00015212502688487166\n",
      "132 Train Loss 0.0012291874 Test MSE 0.00022684426034507403 Test RE 0.00016781015939878641\n",
      "133 Train Loss 0.0011901521 Test MSE 9.992531292009819e-05 Test RE 0.00011137612679165415\n",
      "134 Train Loss 0.0011488382 Test MSE 8.711652055248351e-05 Test RE 0.00010399312117420251\n",
      "135 Train Loss 0.0011165492 Test MSE 0.0001355840505247191 Test RE 0.00012973544757141316\n",
      "136 Train Loss 0.0010866256 Test MSE 5.945443534011144e-05 Test RE 8.591054651006651e-05\n",
      "137 Train Loss 0.0010649306 Test MSE 0.00011706979052593917 Test RE 0.00012055265318163327\n",
      "138 Train Loss 0.0010271776 Test MSE 7.344570286975048e-05 Test RE 9.548552670190911e-05\n",
      "139 Train Loss 0.0010049148 Test MSE 6.962821547267116e-05 Test RE 9.297088919294276e-05\n",
      "140 Train Loss 0.0009889223 Test MSE 5.817701550130031e-05 Test RE 8.498261123054272e-05\n",
      "141 Train Loss 0.000963098 Test MSE 0.00014553385706829602 Test RE 0.0001344114827880304\n",
      "142 Train Loss 0.0009277573 Test MSE 5.883300836671501e-05 Test RE 8.546039197503931e-05\n",
      "143 Train Loss 0.00089481607 Test MSE 9.686795348326412e-05 Test RE 0.00010965903370061946\n",
      "144 Train Loss 0.00086315005 Test MSE 0.00014258943004265019 Test RE 0.00013304483488594743\n",
      "145 Train Loss 0.0008335621 Test MSE 7.784495441356772e-05 Test RE 9.830363715554538e-05\n",
      "146 Train Loss 0.0008070948 Test MSE 3.941522740949224e-05 Test RE 6.994978285529504e-05\n",
      "147 Train Loss 0.00079196616 Test MSE 3.8363990151232454e-05 Test RE 6.90106690349277e-05\n",
      "148 Train Loss 0.0007786577 Test MSE 6.429712533390157e-05 Test RE 8.934086038559667e-05\n",
      "149 Train Loss 0.0007778729 Test MSE 6.53107051371747e-05 Test RE 9.004229146081718e-05\n",
      "150 Train Loss 0.0007605296 Test MSE 8.893734495613423e-05 Test RE 0.00010507428235633451\n",
      "151 Train Loss 0.0007469816 Test MSE 3.934156319440645e-05 Test RE 6.988438673821602e-05\n",
      "152 Train Loss 0.0007372857 Test MSE 4.4624936951579824e-05 Test RE 7.442916728317444e-05\n",
      "153 Train Loss 0.00072902377 Test MSE 4.677059789032231e-05 Test RE 7.619751589880426e-05\n",
      "154 Train Loss 0.0007239133 Test MSE 4.2778950143738464e-05 Test RE 7.287346391041841e-05\n",
      "155 Train Loss 0.0007117957 Test MSE 3.547479362809152e-05 Test RE 6.636120924038278e-05\n",
      "156 Train Loss 0.00070081564 Test MSE 3.975115760222001e-05 Test RE 7.024723627352839e-05\n",
      "157 Train Loss 0.00068296766 Test MSE 3.6752174186219454e-05 Test RE 6.754541439411472e-05\n",
      "158 Train Loss 0.0006666265 Test MSE 3.345070662025936e-05 Test RE 6.44402179343873e-05\n",
      "159 Train Loss 0.0006346135 Test MSE 8.54424673449802e-05 Test RE 0.00010298909524464556\n",
      "160 Train Loss 0.0005953745 Test MSE 5.9310022823450506e-05 Test RE 8.580614638344955e-05\n",
      "161 Train Loss 0.0005631358 Test MSE 3.4027729407635696e-05 Test RE 6.499363657658224e-05\n",
      "162 Train Loss 0.0005304537 Test MSE 7.78159888285227e-05 Test RE 9.828534639213177e-05\n",
      "163 Train Loss 0.00051815156 Test MSE 8.34665933388425e-05 Test RE 0.00010179130834743044\n",
      "164 Train Loss 0.00051627407 Test MSE 7.318770521787359e-05 Test RE 9.531766995136485e-05\n",
      "165 Train Loss 0.00050408725 Test MSE 3.161222378225356e-05 Test RE 6.264434550356633e-05\n",
      "166 Train Loss 0.0004925557 Test MSE 5.5129899956764384e-05 Test RE 8.272712936216023e-05\n",
      "167 Train Loss 0.00048555384 Test MSE 6.447156349535372e-05 Test RE 8.946196921823069e-05\n",
      "168 Train Loss 0.00048474123 Test MSE 5.959374468494296e-05 Test RE 8.601113731847094e-05\n",
      "169 Train Loss 0.00048060025 Test MSE 3.53704115374552e-05 Test RE 6.626350571550806e-05\n",
      "170 Train Loss 0.00047391665 Test MSE 2.69992960262105e-05 Test RE 5.7893602198942414e-05\n",
      "171 Train Loss 0.00047128493 Test MSE 3.352906268811856e-05 Test RE 6.451564726241905e-05\n",
      "172 Train Loss 0.0004591661 Test MSE 4.397122825830828e-05 Test RE 7.388200127668681e-05\n",
      "173 Train Loss 0.00044504533 Test MSE 4.857130567871499e-05 Test RE 7.765049730128116e-05\n",
      "174 Train Loss 0.00043543222 Test MSE 5.8921719047016773e-05 Test RE 8.552479794414362e-05\n",
      "175 Train Loss 0.00042098918 Test MSE 6.47285274602301e-05 Test RE 8.964007597040645e-05\n",
      "176 Train Loss 0.00042037846 Test MSE 6.303322693461282e-05 Test RE 8.845840878123895e-05\n",
      "177 Train Loss 0.0004031925 Test MSE 4.837712056894872e-05 Test RE 7.749512087961586e-05\n",
      "178 Train Loss 0.00037491834 Test MSE 3.899909160766047e-05 Test RE 6.95795471689383e-05\n",
      "179 Train Loss 0.00035940626 Test MSE 1.6851118124112534e-05 Test RE 4.5737109425172326e-05\n",
      "180 Train Loss 0.000350543 Test MSE 1.641962435487868e-05 Test RE 4.514773434578857e-05\n",
      "181 Train Loss 0.00034796787 Test MSE 1.5226369757411234e-05 Test RE 4.34762961448327e-05\n",
      "182 Train Loss 0.00034138488 Test MSE 1.8504277231605065e-05 Test RE 4.79281223852016e-05\n",
      "183 Train Loss 0.0003318302 Test MSE 3.719314124246745e-05 Test RE 6.794942437808349e-05\n",
      "184 Train Loss 0.0003308901 Test MSE 3.6215760699876745e-05 Test RE 6.705067561956414e-05\n",
      "185 Train Loss 0.00032960792 Test MSE 3.435363531901492e-05 Test RE 6.530413824887723e-05\n",
      "186 Train Loss 0.00032769496 Test MSE 3.3826268586023e-05 Test RE 6.480095388113467e-05\n",
      "187 Train Loss 0.0003269633 Test MSE 3.223273470268285e-05 Test RE 6.325617527704984e-05\n",
      "188 Train Loss 0.00032637463 Test MSE 2.914876692990209e-05 Test RE 6.015399095648771e-05\n",
      "189 Train Loss 0.00032578557 Test MSE 2.579617025093926e-05 Test RE 5.6588993506153446e-05\n",
      "190 Train Loss 0.00032507358 Test MSE 2.2205651131579517e-05 Test RE 5.250324046029539e-05\n",
      "191 Train Loss 0.00032265062 Test MSE 1.417261926118892e-05 Test RE 4.194492406047725e-05\n",
      "192 Train Loss 0.0003219442 Test MSE 1.473028482954657e-05 Test RE 4.2762188602645005e-05\n",
      "193 Train Loss 0.00032130122 Test MSE 1.5726651154240077e-05 Test RE 4.4184757834271705e-05\n",
      "194 Train Loss 0.00032036484 Test MSE 1.784085901342818e-05 Test RE 4.7061117215358956e-05\n",
      "195 Train Loss 0.00030969983 Test MSE 3.126214467314141e-05 Test RE 6.229651279810381e-05\n",
      "196 Train Loss 0.00030079085 Test MSE 1.6580857357926303e-05 Test RE 4.536885761862982e-05\n",
      "197 Train Loss 0.0002911612 Test MSE 2.034981813280078e-05 Test RE 5.02614042886845e-05\n",
      "198 Train Loss 0.0002861228 Test MSE 3.0983005223280594e-05 Test RE 6.201776662996033e-05\n",
      "199 Train Loss 0.00028316435 Test MSE 3.271108402455344e-05 Test RE 6.372382270651045e-05\n",
      "Training time: 30.82\n",
      "Training time: 30.82\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 150.6237 Test MSE 1960.1601838642996 Test RE 0.4932875266721055\n",
      "1 Train Loss 86.01635 Test MSE 1270.3346922235053 Test RE 0.397112384783782\n",
      "2 Train Loss 59.872345 Test MSE 605.2807992844972 Test RE 0.2741149997541391\n",
      "3 Train Loss 49.481327 Test MSE 310.87192034092885 Test RE 0.19644686401256312\n",
      "4 Train Loss 44.094456 Test MSE 224.47629794756946 Test RE 0.16693200052899573\n",
      "5 Train Loss 40.34151 Test MSE 89.82439707231096 Test RE 0.10559698240250075\n",
      "6 Train Loss 35.337906 Test MSE 159.39354948447212 Test RE 0.14066618958644814\n",
      "7 Train Loss 26.840757 Test MSE 71.56967881629546 Test RE 0.09425814453815977\n",
      "8 Train Loss 18.808302 Test MSE 95.39650910322469 Test RE 0.10882297479809032\n",
      "9 Train Loss 12.813031 Test MSE 41.201082110618906 Test RE 0.07151689712318833\n",
      "10 Train Loss 9.578193 Test MSE 47.95332115539366 Test RE 0.07715493288682128\n",
      "11 Train Loss 6.9229417 Test MSE 14.019938713382455 Test RE 0.04171837753015353\n",
      "12 Train Loss 5.990069 Test MSE 12.083368362399879 Test RE 0.03873007695781246\n",
      "13 Train Loss 3.8930786 Test MSE 14.793003251952614 Test RE 0.042853128060986866\n",
      "14 Train Loss 2.8574471 Test MSE 5.020570779649877 Test RE 0.02496496142899884\n",
      "15 Train Loss 2.1826687 Test MSE 3.08610729141353 Test RE 0.01957311116490667\n",
      "16 Train Loss 1.4882908 Test MSE 1.457806695180257 Test RE 0.013452540945328483\n",
      "17 Train Loss 0.90495944 Test MSE 0.5882932238942842 Test RE 0.008545771482146082\n",
      "18 Train Loss 0.7224771 Test MSE 1.2278097003319421 Test RE 0.012345818343203094\n",
      "19 Train Loss 0.58724314 Test MSE 1.0887006625552458 Test RE 0.011625418502090493\n",
      "20 Train Loss 0.50102097 Test MSE 0.38070304393739474 Test RE 0.0068746014761640935\n",
      "21 Train Loss 0.4294863 Test MSE 0.30823482441738215 Test RE 0.0061857904572657045\n",
      "22 Train Loss 0.36723858 Test MSE 1.1814604975583627 Test RE 0.012110552454709834\n",
      "23 Train Loss 0.33372405 Test MSE 1.6559275801489974 Test RE 0.014337552531497415\n",
      "24 Train Loss 0.27927288 Test MSE 0.15160736103206437 Test RE 0.004338249210919904\n",
      "25 Train Loss 0.23398826 Test MSE 0.32135161209923674 Test RE 0.0063160359577628545\n",
      "26 Train Loss 0.21223012 Test MSE 0.31510661456904565 Test RE 0.0062543634085724074\n",
      "27 Train Loss 0.16347905 Test MSE 0.17155468776195995 Test RE 0.004614829362358904\n",
      "28 Train Loss 0.13521919 Test MSE 0.05112239501143977 Test RE 0.002519184341561601\n",
      "29 Train Loss 0.12196563 Test MSE 0.02190152024686081 Test RE 0.0016488892366568221\n",
      "30 Train Loss 0.114966296 Test MSE 0.023640770894327603 Test RE 0.0017131096814318586\n",
      "31 Train Loss 0.10083762 Test MSE 0.08628875962863351 Test RE 0.003272890424211096\n",
      "32 Train Loss 0.08966976 Test MSE 0.08580498270791305 Test RE 0.003263702817634019\n",
      "33 Train Loss 0.07267177 Test MSE 0.04720357937035273 Test RE 0.0024207047267867884\n",
      "34 Train Loss 0.0682591 Test MSE 0.016871042041472963 Test RE 0.0014471891778129846\n",
      "35 Train Loss 0.0632161 Test MSE 0.007938835418896313 Test RE 0.0009927336695501545\n",
      "36 Train Loss 0.057867114 Test MSE 0.014501906527957135 Test RE 0.0013417354768888246\n",
      "37 Train Loss 0.052581128 Test MSE 0.0234812177866349 Test RE 0.0017073189497880316\n",
      "38 Train Loss 0.04649011 Test MSE 0.03144120379202527 Test RE 0.0019756223703973038\n",
      "39 Train Loss 0.043131348 Test MSE 0.0043340649182961755 Test RE 0.0007335032769882206\n",
      "40 Train Loss 0.04098855 Test MSE 0.010961162147812805 Test RE 0.0011664943873247336\n",
      "41 Train Loss 0.037695393 Test MSE 0.009624055173799495 Test RE 0.0010930333281299323\n",
      "42 Train Loss 0.034493167 Test MSE 0.01645544627864513 Test RE 0.0014292532382624986\n",
      "43 Train Loss 0.03268779 Test MSE 0.013778946200356105 Test RE 0.0013078633041501758\n",
      "44 Train Loss 0.03199577 Test MSE 0.016844491197155132 Test RE 0.0014460499706473885\n",
      "45 Train Loss 0.029815197 Test MSE 0.008380775812012279 Test RE 0.0010199912918710326\n",
      "46 Train Loss 0.028554687 Test MSE 0.0034366691759319507 Test RE 0.0006531654681316203\n",
      "47 Train Loss 0.027827559 Test MSE 0.006754933078168728 Test RE 0.0009157245926633863\n",
      "48 Train Loss 0.024823764 Test MSE 0.0039536967856171395 Test RE 0.0007005772530487401\n",
      "49 Train Loss 0.024028976 Test MSE 0.005567833742095323 Test RE 0.0008313759969658901\n",
      "50 Train Loss 0.022258647 Test MSE 0.002857177800561745 Test RE 0.0005955565232377613\n",
      "51 Train Loss 0.021015294 Test MSE 0.0030466290978703116 Test RE 0.0006149844648372021\n",
      "52 Train Loss 0.020215077 Test MSE 0.004741524882237752 Test RE 0.0007672084355865053\n",
      "53 Train Loss 0.019783502 Test MSE 0.005458280447366034 Test RE 0.0008231562411877862\n",
      "54 Train Loss 0.018944368 Test MSE 0.004134980381574602 Test RE 0.0007164585652726342\n",
      "55 Train Loss 0.017434552 Test MSE 0.0076473369133936145 Test RE 0.0009743376043146767\n",
      "56 Train Loss 0.016367164 Test MSE 0.002420815419815474 Test RE 0.0005481951524331044\n",
      "57 Train Loss 0.015660979 Test MSE 0.0018515626915248397 Test RE 0.00047942818599324886\n",
      "58 Train Loss 0.014927975 Test MSE 0.0032976016510336058 Test RE 0.0006398135720017709\n",
      "59 Train Loss 0.014562146 Test MSE 0.001352703512641678 Test RE 0.0004097846263906774\n",
      "60 Train Loss 0.014098043 Test MSE 0.002229884378529864 Test RE 0.0005261329788162766\n",
      "61 Train Loss 0.01376021 Test MSE 0.0022219525767924295 Test RE 0.000525196405561331\n",
      "62 Train Loss 0.01361633 Test MSE 0.0030735268472664784 Test RE 0.0006176932533678424\n",
      "63 Train Loss 0.013315249 Test MSE 0.002679270738520099 Test RE 0.0005767168664930894\n",
      "64 Train Loss 0.012705809 Test MSE 0.0007647741548986338 Test RE 0.0003081207552718502\n",
      "65 Train Loss 0.012389659 Test MSE 0.001819935454334749 Test RE 0.00047531590311054037\n",
      "66 Train Loss 0.012118978 Test MSE 0.001000991409463351 Test RE 0.0003525084464153947\n",
      "67 Train Loss 0.0119628515 Test MSE 0.000747140929943853 Test RE 0.00030454790524245815\n",
      "68 Train Loss 0.011678484 Test MSE 0.000749214309420158 Test RE 0.0003049701856185917\n",
      "69 Train Loss 0.01133622 Test MSE 0.0008474158041022436 Test RE 0.00032434158295755367\n",
      "70 Train Loss 0.010723072 Test MSE 0.001184204729332182 Test RE 0.0003834138068835254\n",
      "71 Train Loss 0.0104287565 Test MSE 0.0008304517016516879 Test RE 0.00032107873408342965\n",
      "72 Train Loss 0.010152411 Test MSE 0.000747162947827046 Test RE 0.0003045523926494079\n",
      "73 Train Loss 0.009974519 Test MSE 0.0008005899620082727 Test RE 0.0003152531412036516\n",
      "74 Train Loss 0.009641441 Test MSE 0.001894786105656496 Test RE 0.0004849918583242229\n",
      "75 Train Loss 0.00943211 Test MSE 0.001180149212440362 Test RE 0.0003827567098847717\n",
      "76 Train Loss 0.009313478 Test MSE 0.0006359488744876117 Test RE 0.0002809735598390164\n",
      "77 Train Loss 0.009180109 Test MSE 0.0012121541805783607 Test RE 0.00038791206210794544\n",
      "78 Train Loss 0.0090277465 Test MSE 0.0019473086066650476 Test RE 0.0004916677743465186\n",
      "79 Train Loss 0.008794105 Test MSE 0.003998388776752251 Test RE 0.0007045257358535114\n",
      "80 Train Loss 0.008511237 Test MSE 0.0028831464569706072 Test RE 0.0005982568836536951\n",
      "81 Train Loss 0.008298744 Test MSE 0.0020531790669590594 Test RE 0.00050485628389184\n",
      "82 Train Loss 0.0080841025 Test MSE 0.002304253431077858 Test RE 0.0005348345735227824\n",
      "83 Train Loss 0.007984122 Test MSE 0.0009032758069623539 Test RE 0.0003348609781814963\n",
      "84 Train Loss 0.007864393 Test MSE 0.0007129148577391067 Test RE 0.00029749055731137416\n",
      "85 Train Loss 0.0077064675 Test MSE 0.002009121419355622 Test RE 0.0004994102414347649\n",
      "86 Train Loss 0.0074195233 Test MSE 0.001207389372468204 Test RE 0.00038714889743546657\n",
      "87 Train Loss 0.0072651827 Test MSE 0.000787325109616195 Test RE 0.00031263054197502247\n",
      "88 Train Loss 0.0070811785 Test MSE 0.0009331596079800971 Test RE 0.00034035514368138816\n",
      "89 Train Loss 0.006679806 Test MSE 0.003998989449759872 Test RE 0.0007045786538817108\n",
      "90 Train Loss 0.0064609763 Test MSE 0.0020274735706134616 Test RE 0.0005016859668945808\n",
      "91 Train Loss 0.0060249153 Test MSE 0.0015572657509695315 Test RE 0.00043967899511950946\n",
      "92 Train Loss 0.0058105183 Test MSE 0.0010208253511824602 Test RE 0.00035598366971675475\n",
      "93 Train Loss 0.005555842 Test MSE 0.0006462509606147557 Test RE 0.00028324023965370154\n",
      "94 Train Loss 0.0054048495 Test MSE 0.00033956553714206754 Test RE 0.00020531286142473181\n",
      "95 Train Loss 0.005320963 Test MSE 0.0006185622883916207 Test RE 0.0002771060909124003\n",
      "96 Train Loss 0.0051068272 Test MSE 0.0005976583373885268 Test RE 0.0002723835297629126\n",
      "97 Train Loss 0.00487391 Test MSE 0.0016845591739497081 Test RE 0.00045729608985650244\n",
      "98 Train Loss 0.004726117 Test MSE 0.002720117448311257 Test RE 0.00058109639406863\n",
      "99 Train Loss 0.0046052826 Test MSE 0.0003795384155656359 Test RE 0.00021706121141467713\n",
      "100 Train Loss 0.00455233 Test MSE 0.00112826506998316 Test RE 0.0003742483754784542\n",
      "101 Train Loss 0.004488851 Test MSE 0.0012052652855380733 Test RE 0.00038680820373606403\n",
      "102 Train Loss 0.004425302 Test MSE 0.00029927473698528273 Test RE 0.00019274777846705002\n",
      "103 Train Loss 0.004390553 Test MSE 0.0003418782625319803 Test RE 0.00020601085114397064\n",
      "104 Train Loss 0.004340749 Test MSE 0.0003586716025036121 Test RE 0.00021100990838804476\n",
      "105 Train Loss 0.004240609 Test MSE 0.0016658881905433321 Test RE 0.00045475478460685424\n",
      "106 Train Loss 0.004115234 Test MSE 0.0008799205433690232 Test RE 0.0003305035136826874\n",
      "107 Train Loss 0.0040516052 Test MSE 0.0003373258980507579 Test RE 0.00020463466034236435\n",
      "108 Train Loss 0.0039782887 Test MSE 0.000231389986610885 Test RE 0.0001694831912337467\n",
      "109 Train Loss 0.003943743 Test MSE 0.0002617532293397762 Test RE 0.00018026041914516707\n",
      "110 Train Loss 0.0039191246 Test MSE 0.00022329140061415952 Test RE 0.0001664908426131438\n",
      "111 Train Loss 0.003895543 Test MSE 0.000225579296385376 Test RE 0.00016734162085199186\n",
      "112 Train Loss 0.003853818 Test MSE 0.0003175078502735592 Test RE 0.00019853248843143182\n",
      "113 Train Loss 0.00379855 Test MSE 0.000196407385495517 Test RE 0.0001561468615447539\n",
      "114 Train Loss 0.003755542 Test MSE 0.0002489554172631935 Test RE 0.0001757984909680166\n",
      "115 Train Loss 0.0036959294 Test MSE 0.00022271555315269619 Test RE 0.00016627602193327776\n",
      "116 Train Loss 0.0036551643 Test MSE 0.00020918355621173338 Test RE 0.00016114547810702935\n",
      "117 Train Loss 0.0036284795 Test MSE 0.0002545707343680028 Test RE 0.0001777700481475959\n",
      "118 Train Loss 0.003607731 Test MSE 0.000153628026317888 Test RE 0.0001380986960505469\n",
      "119 Train Loss 0.0035828413 Test MSE 0.00020618317433034302 Test RE 0.00015998562532823718\n",
      "120 Train Loss 0.0035258934 Test MSE 0.00027001308287577644 Test RE 0.00018308246710018497\n",
      "121 Train Loss 0.0034582783 Test MSE 0.0003028928890389731 Test RE 0.00019390941276708468\n",
      "122 Train Loss 0.003336815 Test MSE 0.0001420460212121383 Test RE 0.0001327910757138542\n",
      "123 Train Loss 0.0032471328 Test MSE 0.0002318214831823034 Test RE 0.00016964114394995037\n",
      "124 Train Loss 0.0031851728 Test MSE 0.00013986237844206348 Test RE 0.00013176643833307569\n",
      "125 Train Loss 0.0031566676 Test MSE 0.00022993548233710156 Test RE 0.00016894967064961974\n",
      "126 Train Loss 0.0031242575 Test MSE 0.00044042047433252607 Test RE 0.0002338234706688111\n",
      "127 Train Loss 0.0030716958 Test MSE 0.00021511063221270614 Test RE 0.00016341250620714135\n",
      "128 Train Loss 0.0030708562 Test MSE 0.00021315461773802803 Test RE 0.00016266784945301562\n",
      "129 Train Loss 0.0030467408 Test MSE 0.00014026261302796058 Test RE 0.0001319548371417564\n",
      "130 Train Loss 0.0030093547 Test MSE 0.000205677572091126 Test RE 0.00015978934660518213\n",
      "131 Train Loss 0.0029788641 Test MSE 0.0003454490900230666 Test RE 0.0002070839207051767\n",
      "132 Train Loss 0.0029057502 Test MSE 0.00045756516964729225 Test RE 0.00023833116252774397\n",
      "133 Train Loss 0.0028572222 Test MSE 0.000200766890746992 Test RE 0.00015787028716167509\n",
      "134 Train Loss 0.0028164824 Test MSE 0.0001710126817207743 Test RE 0.00014570300574851875\n",
      "135 Train Loss 0.0027967945 Test MSE 0.00010755919916966666 Test RE 0.00011555217710004029\n",
      "136 Train Loss 0.0027850526 Test MSE 9.972856668705226e-05 Test RE 0.00011126642670859672\n",
      "137 Train Loss 0.0027512885 Test MSE 0.00018367614116558678 Test RE 0.00015100131373207079\n",
      "138 Train Loss 0.0027030867 Test MSE 0.00016768131505680173 Test RE 0.00014427686203211581\n",
      "139 Train Loss 0.0026477135 Test MSE 0.0001781549149055798 Test RE 0.00014871447983587642\n",
      "140 Train Loss 0.0025712785 Test MSE 0.00043381455648803727 Test RE 0.0002320632726817756\n",
      "141 Train Loss 0.0025347918 Test MSE 0.00024007891911884648 Test RE 0.00017263600044906173\n",
      "142 Train Loss 0.0025164282 Test MSE 0.00011558624571181752 Test RE 0.00011978637745377625\n",
      "143 Train Loss 0.0024930434 Test MSE 0.00011172798392789255 Test RE 0.00011777017854401736\n",
      "144 Train Loss 0.0024784778 Test MSE 0.00015651496025538057 Test RE 0.00013939021244063426\n",
      "145 Train Loss 0.0024642248 Test MSE 0.0001941184391050971 Test RE 0.00015523432143692742\n",
      "146 Train Loss 0.0024388125 Test MSE 0.00013645660310765255 Test RE 0.00013015223500538822\n",
      "147 Train Loss 0.0024196547 Test MSE 0.00016296640692857424 Test RE 0.00014223399141886166\n",
      "148 Train Loss 0.002396116 Test MSE 0.00030477795679703987 Test RE 0.00019451187890481473\n",
      "149 Train Loss 0.002345627 Test MSE 0.0004419242133807469 Test RE 0.00023422230529122356\n",
      "150 Train Loss 0.0022950328 Test MSE 0.00014445240834359051 Test RE 0.0001339111518851169\n",
      "151 Train Loss 0.0022473908 Test MSE 0.00021734078416091185 Test RE 0.00016425740869794708\n",
      "152 Train Loss 0.0022294258 Test MSE 0.0001729863819654489 Test RE 0.0001465413911477591\n",
      "153 Train Loss 0.0022212625 Test MSE 8.367956363550896e-05 Test RE 0.00010192108909774009\n",
      "154 Train Loss 0.002208566 Test MSE 0.00013583186584193696 Test RE 0.0001298539561775128\n",
      "155 Train Loss 0.002168852 Test MSE 0.0002802214415561954 Test RE 0.00018651125045443886\n",
      "156 Train Loss 0.0021340447 Test MSE 0.00018086978437141182 Test RE 0.0001498433116694392\n",
      "157 Train Loss 0.002079122 Test MSE 0.00011862391194432544 Test RE 0.00012135019354398486\n",
      "158 Train Loss 0.0020301759 Test MSE 0.00012039174378802318 Test RE 0.00012225108009301784\n",
      "159 Train Loss 0.0019760071 Test MSE 0.0001680518207903539 Test RE 0.0001444361699091659\n",
      "160 Train Loss 0.0019396592 Test MSE 0.00016389273877197842 Test RE 0.00014263766103663738\n",
      "161 Train Loss 0.001912707 Test MSE 7.078096284542334e-05 Test RE 9.373733138772332e-05\n",
      "162 Train Loss 0.0018881611 Test MSE 9.41384086911984e-05 Test RE 0.00010810300805566186\n",
      "163 Train Loss 0.0018476321 Test MSE 0.0001054014489695905 Test RE 0.00011438725626339595\n",
      "164 Train Loss 0.0018037425 Test MSE 8.348975818042967e-05 Test RE 0.00010180543265666695\n",
      "165 Train Loss 0.0017408737 Test MSE 0.0004212575760078815 Test RE 0.00022868001633150075\n",
      "166 Train Loss 0.0016951275 Test MSE 0.0004598971425474876 Test RE 0.00023893771599247217\n",
      "167 Train Loss 0.0016356342 Test MSE 0.0002449315316784123 Test RE 0.00017437198101338488\n",
      "168 Train Loss 0.0015921644 Test MSE 6.021117429070106e-05 Test RE 8.645555458267885e-05\n",
      "169 Train Loss 0.00154891 Test MSE 8.130187662222178e-05 Test RE 0.00010046265177006284\n",
      "170 Train Loss 0.0015111385 Test MSE 7.903453194145455e-05 Test RE 9.905189649808563e-05\n",
      "171 Train Loss 0.001505615 Test MSE 7.564633691598303e-05 Test RE 9.690547254966839e-05\n",
      "172 Train Loss 0.0014721265 Test MSE 5.337305512200214e-05 Test RE 8.139830919319006e-05\n",
      "173 Train Loss 0.0014393218 Test MSE 6.97022320695286e-05 Test RE 9.302029129937255e-05\n",
      "174 Train Loss 0.0014068292 Test MSE 5.652467334958674e-05 Test RE 8.376708118910581e-05\n",
      "175 Train Loss 0.0013529059 Test MSE 0.00032654015071344346 Test RE 0.00020133656126025554\n",
      "176 Train Loss 0.001304396 Test MSE 0.00022897340442891144 Test RE 0.00016859584720892498\n",
      "177 Train Loss 0.0012820868 Test MSE 4.61809192908422e-05 Test RE 7.571564725735781e-05\n",
      "178 Train Loss 0.0012690487 Test MSE 8.200219468428164e-05 Test RE 0.00010089440656983753\n",
      "179 Train Loss 0.0012470951 Test MSE 0.0001160814852443986 Test RE 0.00012004272085073213\n",
      "180 Train Loss 0.0012261465 Test MSE 5.967864614223167e-05 Test RE 8.6072384282285e-05\n",
      "181 Train Loss 0.0012030461 Test MSE 4.796753377717797e-05 Test RE 7.716636580339154e-05\n",
      "182 Train Loss 0.0011686402 Test MSE 8.209320823927623e-05 Test RE 0.00010095038197453525\n",
      "183 Train Loss 0.0011243821 Test MSE 0.00012101702794633155 Test RE 0.00012256813948423813\n",
      "184 Train Loss 0.0010967396 Test MSE 0.00022078251344590302 Test RE 0.00016555286043578611\n",
      "185 Train Loss 0.001068514 Test MSE 0.00018707252249824854 Test RE 0.0001523910121354726\n",
      "186 Train Loss 0.001033103 Test MSE 4.861330940773579e-05 Test RE 7.7684065532136e-05\n",
      "187 Train Loss 0.0010151801 Test MSE 4.397795553789951e-05 Test RE 7.388765276619696e-05\n",
      "188 Train Loss 0.0009951887 Test MSE 4.9625888335797615e-05 Test RE 7.848894639945945e-05\n",
      "189 Train Loss 0.0009799085 Test MSE 5.059247474378671e-05 Test RE 7.924964293086154e-05\n",
      "190 Train Loss 0.00096043746 Test MSE 5.392494451394434e-05 Test RE 8.181806530120453e-05\n",
      "191 Train Loss 0.0009443181 Test MSE 3.892799786585482e-05 Test RE 6.951609791142019e-05\n",
      "192 Train Loss 0.00093394634 Test MSE 3.5793153949224355e-05 Test RE 6.665831580381578e-05\n",
      "193 Train Loss 0.0009216741 Test MSE 0.00014096358869559585 Test RE 0.00013228415459192236\n",
      "194 Train Loss 0.00089619943 Test MSE 0.0003095211775082535 Test RE 0.0001960196172260665\n",
      "195 Train Loss 0.0008632604 Test MSE 0.00010574713129580495 Test RE 0.00011457467913732428\n",
      "196 Train Loss 0.0008476237 Test MSE 3.232564138746447e-05 Test RE 6.334727354744189e-05\n",
      "197 Train Loss 0.00083246705 Test MSE 7.756751854504818e-05 Test RE 9.812830595746658e-05\n",
      "198 Train Loss 0.000819529 Test MSE 5.677084372998723e-05 Test RE 8.394928986185599e-05\n",
      "199 Train Loss 0.00080574444 Test MSE 4.316470448823851e-05 Test RE 7.320129064317703e-05\n",
      "Training time: 33.86\n",
      "Training time: 33.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 118.73029 Test MSE 2855.386189924642 Test RE 0.5953697702953417\n",
      "1 Train Loss 91.76314 Test MSE 1788.916754742317 Test RE 0.4712478894425031\n",
      "2 Train Loss 59.29354 Test MSE 408.0189067318034 Test RE 0.22505802130894523\n",
      "3 Train Loss 49.85259 Test MSE 71.59170916005307 Test RE 0.09427265053837297\n",
      "4 Train Loss 46.888218 Test MSE 88.68688604144343 Test RE 0.10492622675197237\n",
      "5 Train Loss 44.815006 Test MSE 154.19350871670525 Test RE 0.1383526231954393\n",
      "6 Train Loss 41.78779 Test MSE 254.2895486769136 Test RE 0.17767184321313367\n",
      "7 Train Loss 36.007668 Test MSE 158.82949949690308 Test RE 0.14041707950552818\n",
      "8 Train Loss 30.92258 Test MSE 130.7014038946282 Test RE 0.12737801553495467\n",
      "9 Train Loss 25.940735 Test MSE 150.98643608376514 Test RE 0.13690626403447026\n",
      "10 Train Loss 21.171577 Test MSE 59.760826000794815 Test RE 0.08613162643509145\n",
      "11 Train Loss 18.359245 Test MSE 34.223221019821416 Test RE 0.06518006562220739\n",
      "12 Train Loss 16.197083 Test MSE 27.81094438163714 Test RE 0.058757352714101714\n",
      "13 Train Loss 14.251283 Test MSE 45.07189092330963 Test RE 0.07480097205650969\n",
      "14 Train Loss 12.514265 Test MSE 19.5039275187704 Test RE 0.04920569717428017\n",
      "15 Train Loss 11.607444 Test MSE 13.030154119789259 Test RE 0.04021880186367773\n",
      "16 Train Loss 10.842127 Test MSE 16.973372866843135 Test RE 0.0459027207932809\n",
      "17 Train Loss 9.675588 Test MSE 22.356937352201683 Test RE 0.05268178811464058\n",
      "18 Train Loss 8.863107 Test MSE 13.57326399081795 Test RE 0.041048425199579276\n",
      "19 Train Loss 7.8786316 Test MSE 10.008960033940136 Test RE 0.03524916469472037\n",
      "20 Train Loss 6.7890615 Test MSE 6.788936551455212 Test RE 0.029030547553743215\n",
      "21 Train Loss 6.132919 Test MSE 7.664768500855661 Test RE 0.030846356499918206\n",
      "22 Train Loss 5.4671254 Test MSE 6.3773678351201575 Test RE 0.028136824768111947\n",
      "23 Train Loss 4.963622 Test MSE 8.261424555596177 Test RE 0.03202446055920573\n",
      "24 Train Loss 4.5142107 Test MSE 4.7165005549219785 Test RE 0.024197154488929673\n",
      "25 Train Loss 3.6984875 Test MSE 7.097460153733087 Test RE 0.029682866160199425\n",
      "26 Train Loss 3.1017783 Test MSE 11.33222671312893 Test RE 0.037506969868886096\n",
      "27 Train Loss 2.0043259 Test MSE 2.8230522192991025 Test RE 0.01872034327493083\n",
      "28 Train Loss 1.3366618 Test MSE 1.2694688215299916 Test RE 0.012553515751066427\n",
      "29 Train Loss 0.85953647 Test MSE 0.5738706436474964 Test RE 0.008440367505672664\n",
      "30 Train Loss 0.66290957 Test MSE 0.40638553970933233 Test RE 0.007102700063423323\n",
      "31 Train Loss 0.5454158 Test MSE 0.6481531767120259 Test RE 0.008970015207034904\n",
      "32 Train Loss 0.48279676 Test MSE 0.6274752753590572 Test RE 0.008825771132720579\n",
      "33 Train Loss 0.37330234 Test MSE 0.6524112872649735 Test RE 0.008999431702213062\n",
      "34 Train Loss 0.29487985 Test MSE 0.8317639246522706 Test RE 0.010161419768172683\n",
      "35 Train Loss 0.23363613 Test MSE 0.14975115486116397 Test RE 0.0043116097217887036\n",
      "36 Train Loss 0.20936958 Test MSE 0.0452755076359999 Test RE 0.0023707514046373327\n",
      "37 Train Loss 0.19282635 Test MSE 0.031202620763280207 Test RE 0.001968112359672563\n",
      "38 Train Loss 0.1682354 Test MSE 0.219736919411364 Test RE 0.005222829737078468\n",
      "39 Train Loss 0.14005181 Test MSE 0.09268823501991069 Test RE 0.003392084469726241\n",
      "40 Train Loss 0.1289816 Test MSE 0.06323411584963753 Test RE 0.002801754496679752\n",
      "41 Train Loss 0.10715405 Test MSE 0.02966122634285057 Test RE 0.001918884804970022\n",
      "42 Train Loss 0.09307843 Test MSE 0.03311972356068965 Test RE 0.0020276719995309606\n",
      "43 Train Loss 0.07855886 Test MSE 0.043908017098608164 Test RE 0.0023346740959168505\n",
      "44 Train Loss 0.06690961 Test MSE 0.012858817949769568 Test RE 0.001263440738582126\n",
      "45 Train Loss 0.063230574 Test MSE 0.03821839392482049 Test RE 0.0021781639634028604\n",
      "46 Train Loss 0.05356908 Test MSE 0.07998134184529501 Test RE 0.003151002119894343\n",
      "47 Train Loss 0.04387885 Test MSE 0.00834175424747265 Test RE 0.001017613940715355\n",
      "48 Train Loss 0.03788489 Test MSE 0.007235471067146886 Test RE 0.000947736829765312\n",
      "49 Train Loss 0.032896962 Test MSE 0.016649791679326863 Test RE 0.001437668490455627\n",
      "50 Train Loss 0.030659735 Test MSE 0.008558978298216105 Test RE 0.0010307784133679117\n",
      "51 Train Loss 0.029660841 Test MSE 0.011948348845096012 Test RE 0.0012178906582553256\n",
      "52 Train Loss 0.027826952 Test MSE 0.011981526725233177 Test RE 0.0012195803904254121\n",
      "53 Train Loss 0.025720982 Test MSE 0.009757095065461803 Test RE 0.0011005622713351068\n",
      "54 Train Loss 0.023556411 Test MSE 0.012271677055957756 Test RE 0.0012342590242490428\n",
      "55 Train Loss 0.022591323 Test MSE 0.012860389828384779 Test RE 0.0012635179585411447\n",
      "56 Train Loss 0.02179794 Test MSE 0.0029324456510856456 Test RE 0.0006033500294264566\n",
      "57 Train Loss 0.020315133 Test MSE 0.010395551615492512 Test RE 0.0011359994520598882\n",
      "58 Train Loss 0.018705769 Test MSE 0.0029637985247823154 Test RE 0.0006065668772179584\n",
      "59 Train Loss 0.017921336 Test MSE 0.002908035728816509 Test RE 0.0006008336137768763\n",
      "60 Train Loss 0.017423779 Test MSE 0.0016763002955075355 Test RE 0.000456173721071533\n",
      "61 Train Loss 0.016833663 Test MSE 0.0012004081027224157 Test RE 0.0003860280041893488\n",
      "62 Train Loss 0.01600855 Test MSE 0.0014425541927793142 Test RE 0.00042317540745603243\n",
      "63 Train Loss 0.015631568 Test MSE 0.0025558650576044477 Test RE 0.0005632786787856607\n",
      "64 Train Loss 0.015079415 Test MSE 0.003259710074347496 Test RE 0.0006361270152057387\n",
      "65 Train Loss 0.01455089 Test MSE 0.0015033928479662756 Test RE 0.00043200680935332326\n",
      "66 Train Loss 0.014171211 Test MSE 0.000940614095387514 Test RE 0.0003417118924287905\n",
      "67 Train Loss 0.013945603 Test MSE 0.0010105164875836507 Test RE 0.00035418164792247535\n",
      "68 Train Loss 0.013723615 Test MSE 0.0009200517943960449 Test RE 0.0003379562563831848\n",
      "69 Train Loss 0.0132678775 Test MSE 0.002403195510423651 Test RE 0.0005461964892709008\n",
      "70 Train Loss 0.012860575 Test MSE 0.003124862784061735 Test RE 0.0006228304375145979\n",
      "71 Train Loss 0.012500839 Test MSE 0.0009618144988184142 Test RE 0.0003455413392186313\n",
      "72 Train Loss 0.011869758 Test MSE 0.0015192253290924732 Test RE 0.0004342756196165969\n",
      "73 Train Loss 0.0114428885 Test MSE 0.0007133574059596718 Test RE 0.0002975828779372869\n",
      "74 Train Loss 0.010969872 Test MSE 0.001453444163088313 Test RE 0.00042476969870525996\n",
      "75 Train Loss 0.01053192 Test MSE 0.0006264241753946448 Test RE 0.00027886153096088446\n",
      "76 Train Loss 0.009866756 Test MSE 0.0008988618066373419 Test RE 0.0003340418005868481\n",
      "77 Train Loss 0.009548167 Test MSE 0.0005499923417043946 Test RE 0.00026129594706378746\n",
      "78 Train Loss 0.009425261 Test MSE 0.0005586342499123006 Test RE 0.00026334078859234776\n",
      "79 Train Loss 0.009212376 Test MSE 0.0013915162622921474 Test RE 0.00041562196895922755\n",
      "80 Train Loss 0.008944325 Test MSE 0.000554288124011339 Test RE 0.0002623144041778243\n",
      "81 Train Loss 0.008792349 Test MSE 0.0008567820077019273 Test RE 0.00032612907693304076\n",
      "82 Train Loss 0.008667604 Test MSE 0.0009226662642836601 Test RE 0.00033843609330454823\n",
      "83 Train Loss 0.008520834 Test MSE 0.0006142946387782144 Test RE 0.0002761485167472479\n",
      "84 Train Loss 0.008330847 Test MSE 0.00073495019031235 Test RE 0.0003020531058444094\n",
      "85 Train Loss 0.008211972 Test MSE 0.0013809846770028776 Test RE 0.00041404618006240287\n",
      "86 Train Loss 0.008082344 Test MSE 0.001564535982844879 Test RE 0.0004407041400127253\n",
      "87 Train Loss 0.00790446 Test MSE 0.0008721971816750848 Test RE 0.0003290498459795218\n",
      "88 Train Loss 0.007688897 Test MSE 0.00036340384875998714 Test RE 0.00021239735963755435\n",
      "89 Train Loss 0.007404168 Test MSE 0.0004639738668445995 Test RE 0.00023999440230907054\n",
      "90 Train Loss 0.0072514233 Test MSE 0.000558517228857147 Test RE 0.00026331320522187497\n",
      "91 Train Loss 0.0072134025 Test MSE 0.000453888874974203 Test RE 0.00023737179908705258\n",
      "92 Train Loss 0.0071727275 Test MSE 0.000374225671886016 Test RE 0.00021553665607370608\n",
      "93 Train Loss 0.007092446 Test MSE 0.0003912753589860442 Test RE 0.00022039188612635942\n",
      "94 Train Loss 0.0070199356 Test MSE 0.00035295138036222295 Test RE 0.00020932051506236083\n",
      "95 Train Loss 0.006923812 Test MSE 0.00037978578124187045 Test RE 0.00021713193514810817\n",
      "96 Train Loss 0.006803762 Test MSE 0.0004937930919599876 Test RE 0.0002475864411976673\n",
      "97 Train Loss 0.006746633 Test MSE 0.0005514473288208902 Test RE 0.00026164134380826854\n",
      "98 Train Loss 0.0066722184 Test MSE 0.0005822929205169667 Test RE 0.0002688593271053863\n",
      "99 Train Loss 0.0065533286 Test MSE 0.0006485119319827402 Test RE 0.00028373527870600507\n",
      "100 Train Loss 0.00643282 Test MSE 0.00040022789778158146 Test RE 0.0002228989543859715\n",
      "101 Train Loss 0.0063259783 Test MSE 0.00043329267585654065 Test RE 0.00023192364415714666\n",
      "102 Train Loss 0.006227821 Test MSE 0.0005335259262936418 Test RE 0.00025735470742603133\n",
      "103 Train Loss 0.0060918494 Test MSE 0.0006665271597652336 Test RE 0.000287649271029124\n",
      "104 Train Loss 0.0058751875 Test MSE 0.0008191062175433428 Test RE 0.000318877931080745\n",
      "105 Train Loss 0.005760993 Test MSE 0.0008155175420407136 Test RE 0.00031817862886766824\n",
      "106 Train Loss 0.0056669144 Test MSE 0.00041107638996372875 Test RE 0.00022589968187288322\n",
      "107 Train Loss 0.0054904753 Test MSE 0.0008510943035197746 Test RE 0.0003250447787005276\n",
      "108 Train Loss 0.0054025315 Test MSE 0.0004593440257993393 Test RE 0.0002387939879700557\n",
      "109 Train Loss 0.005342775 Test MSE 0.00023642258668632414 Test RE 0.00017131635857753043\n",
      "110 Train Loss 0.0052841534 Test MSE 0.0003757309689888115 Test RE 0.00021596971173593865\n",
      "111 Train Loss 0.005204604 Test MSE 0.00045887383908221386 Test RE 0.0002386717413512999\n",
      "112 Train Loss 0.0051301797 Test MSE 0.0002741737388368691 Test RE 0.00018448764180670736\n",
      "113 Train Loss 0.0050516096 Test MSE 0.0003376011029224062 Test RE 0.00020471811817068488\n",
      "114 Train Loss 0.0049401107 Test MSE 0.00027467903821923556 Test RE 0.00018465756796343184\n",
      "115 Train Loss 0.004786599 Test MSE 0.0004300850230351406 Test RE 0.00023106358752918163\n",
      "116 Train Loss 0.004570009 Test MSE 0.00032559969927030305 Test RE 0.0002010464226843794\n",
      "117 Train Loss 0.004360179 Test MSE 0.00039030172927694576 Test RE 0.00022011750935188145\n",
      "118 Train Loss 0.0042896676 Test MSE 0.00019751825588779727 Test RE 0.00015658781835325476\n",
      "119 Train Loss 0.0042424602 Test MSE 0.0002634988257614233 Test RE 0.00018086048638423847\n",
      "120 Train Loss 0.0042201076 Test MSE 0.0002307213553623673 Test RE 0.00016923814229629085\n",
      "121 Train Loss 0.004178482 Test MSE 0.00018658777994789806 Test RE 0.00015219344618014647\n",
      "122 Train Loss 0.0041141356 Test MSE 0.00029828104251840943 Test RE 0.00019242751813425437\n",
      "123 Train Loss 0.0040781004 Test MSE 0.00024800081634976015 Test RE 0.0001754611241778409\n",
      "124 Train Loss 0.0040590805 Test MSE 0.00022066322049497326 Test RE 0.0001655081287281043\n",
      "125 Train Loss 0.004007809 Test MSE 0.0004578716254790855 Test RE 0.0002384109607228803\n",
      "126 Train Loss 0.0039302115 Test MSE 0.000337044757232484 Test RE 0.00020454936723505394\n",
      "127 Train Loss 0.0038929693 Test MSE 0.0002030252601985601 Test RE 0.00015875572302172968\n",
      "128 Train Loss 0.0038717124 Test MSE 0.00019406291684894589 Test RE 0.00015521211958894628\n",
      "129 Train Loss 0.0038532675 Test MSE 0.00017816117179402272 Test RE 0.00014871709127587234\n",
      "130 Train Loss 0.0038230345 Test MSE 0.0002862624302071234 Test RE 0.00018851092709477922\n",
      "131 Train Loss 0.00375423 Test MSE 0.0005046469652879437 Test RE 0.00025029270121121544\n",
      "132 Train Loss 0.0037021078 Test MSE 0.00020441042935017553 Test RE 0.0001592963693510403\n",
      "133 Train Loss 0.003603395 Test MSE 0.00018010114324185908 Test RE 0.00014952457861287574\n",
      "134 Train Loss 0.0034899886 Test MSE 0.00031180096931739907 Test RE 0.00019674018835503056\n",
      "135 Train Loss 0.0033308119 Test MSE 0.0008452796890081504 Test RE 0.00032393253458335987\n",
      "136 Train Loss 0.003155388 Test MSE 0.00028013443106361685 Test RE 0.0001864822917572806\n",
      "137 Train Loss 0.0030865637 Test MSE 0.0001925561402447406 Test RE 0.0001546083831084299\n",
      "138 Train Loss 0.003031412 Test MSE 0.0001351769223561714 Test RE 0.00012954051805927165\n",
      "139 Train Loss 0.003014499 Test MSE 0.00013871708113829704 Test RE 0.00013122582847030026\n",
      "140 Train Loss 0.0029987653 Test MSE 0.0002102194206250927 Test RE 0.00016154397674804862\n",
      "141 Train Loss 0.0029704457 Test MSE 0.00031193232036796014 Test RE 0.00019678162393726667\n",
      "142 Train Loss 0.0029437807 Test MSE 0.00020036140901693038 Test RE 0.00015771078409057522\n",
      "143 Train Loss 0.002893527 Test MSE 0.0002372421767106474 Test RE 0.00017161304702891644\n",
      "144 Train Loss 0.0028334404 Test MSE 0.00035838698516585854 Test RE 0.0002109261702317811\n",
      "145 Train Loss 0.0027639407 Test MSE 0.00013918889675103942 Test RE 0.00013144880691123116\n",
      "146 Train Loss 0.0026947325 Test MSE 0.0006418823137187461 Test RE 0.0002822812663373799\n",
      "147 Train Loss 0.0026230179 Test MSE 0.0004235910929531464 Test RE 0.0002293125174029182\n",
      "148 Train Loss 0.0025292488 Test MSE 0.00010661490524596256 Test RE 0.00011504382558155119\n",
      "149 Train Loss 0.0024655217 Test MSE 0.00012644599952642835 Test RE 0.00012528725648161066\n",
      "150 Train Loss 0.0023776684 Test MSE 0.00026745553520044347 Test RE 0.0001822133310458139\n",
      "151 Train Loss 0.0023057065 Test MSE 0.0003823812980153662 Test RE 0.0002178726289847652\n",
      "152 Train Loss 0.0022668839 Test MSE 0.0002535441753732413 Test RE 0.00017741125633892566\n",
      "153 Train Loss 0.002225952 Test MSE 0.0002124146929902154 Test RE 0.00016238526912767717\n",
      "154 Train Loss 0.0022008228 Test MSE 0.00026324838394297344 Test RE 0.000180774516747144\n",
      "155 Train Loss 0.0021814397 Test MSE 0.00033687499664004407 Test RE 0.00020449784767486768\n",
      "156 Train Loss 0.0021104873 Test MSE 0.00021758962642264078 Test RE 0.00016435141427612127\n",
      "157 Train Loss 0.002069645 Test MSE 0.00018771530612854046 Test RE 0.00015265259638032994\n",
      "158 Train Loss 0.0019937216 Test MSE 0.00015095663631677113 Test RE 0.00013689275296620033\n",
      "159 Train Loss 0.0019464804 Test MSE 0.00010611635030173994 Test RE 0.00011477452516288815\n",
      "160 Train Loss 0.0018977204 Test MSE 0.00011568428945588984 Test RE 0.00011983716989646387\n",
      "161 Train Loss 0.0018074957 Test MSE 0.00018672212159112926 Test RE 0.00015224822532687392\n",
      "162 Train Loss 0.0017310216 Test MSE 0.0001656273933582344 Test RE 0.0001433905187270267\n",
      "163 Train Loss 0.001688502 Test MSE 0.00010830838761194355 Test RE 0.00011595390996232338\n",
      "164 Train Loss 0.0016624546 Test MSE 0.00011590126744359445 Test RE 0.00011994950082375877\n",
      "165 Train Loss 0.0016461295 Test MSE 0.0001029195347585925 Test RE 0.00011303248081594904\n",
      "166 Train Loss 0.0016391738 Test MSE 7.05090489838012e-05 Test RE 9.355710633035054e-05\n",
      "167 Train Loss 0.0016306455 Test MSE 0.00011862495522983117 Test RE 0.00012135072717425214\n",
      "168 Train Loss 0.0016280322 Test MSE 0.0001304988384046455 Test RE 0.00012727926986554628\n",
      "169 Train Loss 0.0016035001 Test MSE 8.842811325352258e-05 Test RE 0.00010477303675615644\n",
      "170 Train Loss 0.001603219 Test MSE 8.762030830399121e-05 Test RE 0.00010429337958748398\n",
      "171 Train Loss 0.001575028 Test MSE 0.0001301469744476116 Test RE 0.00012710756252587321\n",
      "172 Train Loss 0.0015476032 Test MSE 0.0002408031869499596 Test RE 0.00017289620768252585\n",
      "173 Train Loss 0.0015176621 Test MSE 0.0001485348952037723 Test RE 0.00013579025360741333\n",
      "174 Train Loss 0.0014926621 Test MSE 6.547336639285983e-05 Test RE 9.015435030335017e-05\n",
      "175 Train Loss 0.0014727882 Test MSE 7.178450666067658e-05 Test RE 9.439950399194661e-05\n",
      "176 Train Loss 0.0014329083 Test MSE 7.600327241516373e-05 Test RE 9.713382661644063e-05\n",
      "177 Train Loss 0.0014026157 Test MSE 0.0001152565114917946 Test RE 0.00011961539743530746\n",
      "178 Train Loss 0.0013720433 Test MSE 0.0001173419047528602 Test RE 0.00012069267671689448\n",
      "179 Train Loss 0.0013511679 Test MSE 0.00010541981427278406 Test RE 0.00011439722133105647\n",
      "180 Train Loss 0.0013198957 Test MSE 8.830706154700264e-05 Test RE 0.00010470129883302427\n",
      "181 Train Loss 0.0012727116 Test MSE 6.510409814718216e-05 Test RE 8.989975661309997e-05\n",
      "182 Train Loss 0.001239022 Test MSE 6.730212186165701e-05 Test RE 9.140474273969212e-05\n",
      "183 Train Loss 0.0012232412 Test MSE 7.641805599494746e-05 Test RE 9.739851716915276e-05\n",
      "184 Train Loss 0.0011852344 Test MSE 7.253829188126386e-05 Test RE 9.489383856176805e-05\n",
      "185 Train Loss 0.0011578928 Test MSE 5.702730404668844e-05 Test RE 8.413869519691376e-05\n",
      "186 Train Loss 0.0011490688 Test MSE 5.383461053552232e-05 Test RE 8.174950659149679e-05\n",
      "187 Train Loss 0.0011386105 Test MSE 4.965903727870204e-05 Test RE 7.851515642153985e-05\n",
      "188 Train Loss 0.0011207075 Test MSE 5.182518080706825e-05 Test RE 8.02093072247813e-05\n",
      "189 Train Loss 0.0010977029 Test MSE 8.645219701172842e-05 Test RE 0.0001035958527344611\n",
      "190 Train Loss 0.0010699362 Test MSE 8.065599804038423e-05 Test RE 0.00010006280824608125\n",
      "191 Train Loss 0.0010499287 Test MSE 7.699248380598369e-05 Test RE 9.776389986419643e-05\n",
      "192 Train Loss 0.0010346564 Test MSE 5.966287479261172e-05 Test RE 8.60610103030619e-05\n",
      "193 Train Loss 0.0009889829 Test MSE 5.2629543957641314e-05 Test RE 8.082936291269246e-05\n",
      "194 Train Loss 0.00095394446 Test MSE 0.00010661631093844684 Test RE 0.0001150445839919609\n",
      "195 Train Loss 0.0009277407 Test MSE 8.983814743391365e-05 Test RE 0.0001056050645114582\n",
      "196 Train Loss 0.00090939674 Test MSE 4.373125798778385e-05 Test RE 7.368012222804409e-05\n",
      "197 Train Loss 0.00090435386 Test MSE 3.835600305731148e-05 Test RE 6.900348491011962e-05\n",
      "198 Train Loss 0.00090356247 Test MSE 3.8755647662926115e-05 Test RE 6.936203907198887e-05\n",
      "199 Train Loss 0.00090260326 Test MSE 4.0001730542783784e-05 Test RE 7.046829153196646e-05\n",
      "Training time: 33.15\n",
      "Training time: 33.15\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 166.47118 Test MSE 3493.4466042759327 Test RE 0.658538859473133\n",
      "1 Train Loss 116.87964 Test MSE 1695.0789692094982 Test RE 0.4587217370599487\n",
      "2 Train Loss 73.94224 Test MSE 920.8061378810454 Test RE 0.3380947718821721\n",
      "3 Train Loss 56.38903 Test MSE 288.9094003299822 Test RE 0.1893804694466586\n",
      "4 Train Loss 45.30258 Test MSE 57.33852016503148 Test RE 0.08436796861523205\n",
      "5 Train Loss 41.24132 Test MSE 47.94184605478468 Test RE 0.07714570085066129\n",
      "6 Train Loss 32.948105 Test MSE 141.9522887339921 Test RE 0.1327472557936977\n",
      "7 Train Loss 26.859049 Test MSE 172.18166186196234 Test RE 0.1462001438586955\n",
      "8 Train Loss 19.82591 Test MSE 141.40480142830984 Test RE 0.1324910160041166\n",
      "9 Train Loss 14.254059 Test MSE 31.54590602135366 Test RE 0.06257860172846891\n",
      "10 Train Loss 11.549107 Test MSE 20.165497689145397 Test RE 0.0500332627475952\n",
      "11 Train Loss 9.13072 Test MSE 18.13764693721532 Test RE 0.04745094044199403\n",
      "12 Train Loss 6.6639853 Test MSE 19.479897971726714 Test RE 0.04917537622929379\n",
      "13 Train Loss 5.6327415 Test MSE 8.097118074247792 Test RE 0.03170440363920726\n",
      "14 Train Loss 4.0913744 Test MSE 13.82274264233929 Test RE 0.04142394566719555\n",
      "15 Train Loss 3.381766 Test MSE 6.639026709544671 Test RE 0.02870823947239783\n",
      "16 Train Loss 2.8884358 Test MSE 4.871585810224316 Test RE 0.02459175541423117\n",
      "17 Train Loss 2.3812194 Test MSE 2.608624264410618 Test RE 0.017995342511796523\n",
      "18 Train Loss 1.7599721 Test MSE 5.328638751933344 Test RE 0.02571949824264674\n",
      "19 Train Loss 1.3351007 Test MSE 3.073853895159141 Test RE 0.01953421497520938\n",
      "20 Train Loss 1.0431466 Test MSE 0.5459951372965101 Test RE 0.008232822266149378\n",
      "21 Train Loss 0.8007524 Test MSE 0.5162737642889024 Test RE 0.008005609097158034\n",
      "22 Train Loss 0.7167945 Test MSE 0.28887993966999004 Test RE 0.00598843092863855\n",
      "23 Train Loss 0.5429284 Test MSE 0.1214617994712333 Test RE 0.003883060953385174\n",
      "24 Train Loss 0.35750222 Test MSE 0.3777182462143072 Test RE 0.006847599233109918\n",
      "25 Train Loss 0.26563781 Test MSE 1.060985014290495 Test RE 0.011476487216392472\n",
      "26 Train Loss 0.20488818 Test MSE 0.41709484923122064 Test RE 0.007195678743538962\n",
      "27 Train Loss 0.18580516 Test MSE 0.06768825087053351 Test RE 0.0028987515787565563\n",
      "28 Train Loss 0.15438321 Test MSE 0.056356573921587674 Test RE 0.002645005892970116\n",
      "29 Train Loss 0.13995855 Test MSE 0.031205456376940784 Test RE 0.0019682017861272897\n",
      "30 Train Loss 0.12436737 Test MSE 0.09129319403239593 Test RE 0.00336646073381919\n",
      "31 Train Loss 0.10851467 Test MSE 0.19823621152479518 Test RE 0.004960732934384146\n",
      "32 Train Loss 0.10034009 Test MSE 0.22739878037772887 Test RE 0.005313105234514275\n",
      "33 Train Loss 0.08916793 Test MSE 0.022246999265134217 Test RE 0.0016618433079699088\n",
      "34 Train Loss 0.08081056 Test MSE 0.11545010154213028 Test RE 0.0037857463470807582\n",
      "35 Train Loss 0.06684827 Test MSE 0.0779556326052958 Test RE 0.0031108430646162224\n",
      "36 Train Loss 0.06321531 Test MSE 0.03167866869331645 Test RE 0.0019830689441269886\n",
      "37 Train Loss 0.05802993 Test MSE 0.017321996630092484 Test RE 0.0014664029554046494\n",
      "38 Train Loss 0.05444095 Test MSE 0.008863748521030135 Test RE 0.0010489699940467374\n",
      "39 Train Loss 0.05139871 Test MSE 0.007712667632336593 Test RE 0.000978490605948601\n",
      "40 Train Loss 0.05087248 Test MSE 0.009605810155100649 Test RE 0.0010919967653874566\n",
      "41 Train Loss 0.04975443 Test MSE 0.007773776784658365 Test RE 0.0009823593553589572\n",
      "42 Train Loss 0.045533456 Test MSE 0.014673644982626959 Test RE 0.0013496568277344505\n",
      "43 Train Loss 0.041557785 Test MSE 0.018615692778426115 Test RE 0.0015201762873607385\n",
      "44 Train Loss 0.03849973 Test MSE 0.008058634157801716 Test RE 0.0010001959058759313\n",
      "45 Train Loss 0.03566904 Test MSE 0.011760900889459679 Test RE 0.0012082996436902557\n",
      "46 Train Loss 0.03433542 Test MSE 0.005718545824214257 Test RE 0.0008425528561460261\n",
      "47 Train Loss 0.032011844 Test MSE 0.00708724443656455 Test RE 0.000937978876745844\n",
      "48 Train Loss 0.03018579 Test MSE 0.012678070671404622 Test RE 0.001254529668699071\n",
      "49 Train Loss 0.028186573 Test MSE 0.017160731112346436 Test RE 0.001459560984067277\n",
      "50 Train Loss 0.024023252 Test MSE 0.004542108660480228 Test RE 0.0007509017425353786\n",
      "51 Train Loss 0.022960922 Test MSE 0.010432357314732505 Test RE 0.0011380086918120439\n",
      "52 Train Loss 0.021469155 Test MSE 0.004722796023273698 Test RE 0.0007656917129879189\n",
      "53 Train Loss 0.020119114 Test MSE 0.005359778212376989 Test RE 0.0008156949279274346\n",
      "54 Train Loss 0.018637115 Test MSE 0.005571967273186708 Test RE 0.0008316845443394711\n",
      "55 Train Loss 0.017513355 Test MSE 0.003994761674423505 Test RE 0.0007042061112663405\n",
      "56 Train Loss 0.016606363 Test MSE 0.0016101143928944836 Test RE 0.0004470774019000253\n",
      "57 Train Loss 0.015311062 Test MSE 0.002249522284780926 Test RE 0.0005284446461419019\n",
      "58 Train Loss 0.013848483 Test MSE 0.0020887230156746506 Test RE 0.0005092074845575871\n",
      "59 Train Loss 0.013570299 Test MSE 0.0021914153653085923 Test RE 0.0005215749241333183\n",
      "60 Train Loss 0.013141882 Test MSE 0.0014934015982800768 Test RE 0.0004305689007152754\n",
      "61 Train Loss 0.012590803 Test MSE 0.0011044180786627035 Test RE 0.00037027219952749473\n",
      "62 Train Loss 0.011290384 Test MSE 0.0020017549811845695 Test RE 0.0004984938575437276\n",
      "63 Train Loss 0.009466533 Test MSE 0.0014831998538248957 Test RE 0.0004290957265701361\n",
      "64 Train Loss 0.0085728485 Test MSE 0.0020964894934349834 Test RE 0.0005101532967040219\n",
      "65 Train Loss 0.007981054 Test MSE 0.0012757274783207536 Test RE 0.00039795439680230825\n",
      "66 Train Loss 0.0072602686 Test MSE 0.0011127168098217196 Test RE 0.0003716607310793091\n",
      "67 Train Loss 0.0068574217 Test MSE 0.0006207770457526752 Test RE 0.0002776017357246333\n",
      "68 Train Loss 0.0066824662 Test MSE 0.0006799456596537594 Test RE 0.0002905303152058587\n",
      "69 Train Loss 0.006109108 Test MSE 0.0011746619732163628 Test RE 0.00038186583749664154\n",
      "70 Train Loss 0.0055478285 Test MSE 0.0004978838328496325 Test RE 0.0002486098688478527\n",
      "71 Train Loss 0.0052040024 Test MSE 0.0005528048946352663 Test RE 0.0002619632032017855\n",
      "72 Train Loss 0.0049753096 Test MSE 0.0009497292503517381 Test RE 0.0003433636044389884\n",
      "73 Train Loss 0.0045383032 Test MSE 0.0008862846098272265 Test RE 0.0003316965514491337\n",
      "74 Train Loss 0.0041032517 Test MSE 0.0015022167659233623 Test RE 0.0004318378000170573\n",
      "75 Train Loss 0.003848525 Test MSE 0.0005605209145208575 Test RE 0.00026378510163289963\n",
      "76 Train Loss 0.0035603028 Test MSE 0.00039386227913474416 Test RE 0.0002211192472227691\n",
      "77 Train Loss 0.0032931848 Test MSE 0.0010752454044427836 Test RE 0.0003653491908909918\n",
      "78 Train Loss 0.0029983488 Test MSE 0.00152704573782358 Test RE 0.0004353919298124391\n",
      "79 Train Loss 0.002879881 Test MSE 0.001089480530850085 Test RE 0.0003677596599982236\n",
      "80 Train Loss 0.0026575727 Test MSE 0.0011207054743426702 Test RE 0.0003729924997993212\n",
      "81 Train Loss 0.0024507982 Test MSE 0.00039761121566070254 Test RE 0.00022216910500547573\n",
      "82 Train Loss 0.0024059962 Test MSE 0.0002226215937189167 Test RE 0.00016624094389588683\n",
      "83 Train Loss 0.0023722497 Test MSE 0.00019930911652768385 Test RE 0.00015729609258386106\n",
      "84 Train Loss 0.002324671 Test MSE 0.00043803532753049695 Test RE 0.00023318946241567568\n",
      "85 Train Loss 0.0022522688 Test MSE 0.00015595452526356155 Test RE 0.0001391404305396936\n",
      "86 Train Loss 0.0021789507 Test MSE 0.0004422343608280473 Test RE 0.0002343044808168658\n",
      "87 Train Loss 0.0020431664 Test MSE 0.0005163564490373003 Test RE 0.00025317985984158007\n",
      "88 Train Loss 0.001905421 Test MSE 0.0006167952493954326 Test RE 0.00027671000511981185\n",
      "89 Train Loss 0.0018356737 Test MSE 0.0009694999555707963 Test RE 0.00034691913040349064\n",
      "90 Train Loss 0.0017101504 Test MSE 0.00034236096313097715 Test RE 0.0002061562340131364\n",
      "91 Train Loss 0.0015748185 Test MSE 0.0013886147843203322 Test RE 0.0004151884320695405\n",
      "92 Train Loss 0.0014022168 Test MSE 0.0005295583558792801 Test RE 0.00025639601144834075\n",
      "93 Train Loss 0.0013049032 Test MSE 0.0005222297789048367 Test RE 0.00025461569374569276\n",
      "94 Train Loss 0.0012430031 Test MSE 0.001024775547725058 Test RE 0.00035667176377639773\n",
      "95 Train Loss 0.001174943 Test MSE 0.0006840304242861164 Test RE 0.0002914016870058293\n",
      "96 Train Loss 0.0011166782 Test MSE 0.000560639246927394 Test RE 0.0002638129441966865\n",
      "97 Train Loss 0.0010424141 Test MSE 0.0005451784419735148 Test RE 0.00026014991565908945\n",
      "98 Train Loss 0.0009869215 Test MSE 0.00017228395347122758 Test RE 0.00014624356551963885\n",
      "99 Train Loss 0.00093872845 Test MSE 0.0005988970432187036 Test RE 0.0002726656545162388\n",
      "100 Train Loss 0.00088258693 Test MSE 0.00050839440856439 Test RE 0.0002512203029921713\n",
      "101 Train Loss 0.0008443208 Test MSE 7.620395107594814e-05 Test RE 9.72619779149606e-05\n",
      "102 Train Loss 0.00080787565 Test MSE 5.079412756841117e-05 Test RE 7.940742352765387e-05\n",
      "103 Train Loss 0.00077808223 Test MSE 5.5304469289414794e-05 Test RE 8.285800394619265e-05\n",
      "104 Train Loss 0.0007243555 Test MSE 0.0003382234104758466 Test RE 0.00020490671201041664\n",
      "105 Train Loss 0.00067609915 Test MSE 0.0003771568539166871 Test RE 0.00021637912215530957\n",
      "106 Train Loss 0.0006481945 Test MSE 9.388911120755688e-05 Test RE 0.00010795977388561232\n",
      "107 Train Loss 0.0006302769 Test MSE 6.969792282101301e-05 Test RE 9.301741582655081e-05\n",
      "108 Train Loss 0.00061755895 Test MSE 0.00011637129521883625 Test RE 0.000120192477251876\n",
      "109 Train Loss 0.00059477746 Test MSE 3.5594720278822065e-05 Test RE 6.647328547756068e-05\n",
      "110 Train Loss 0.0005797336 Test MSE 3.304116087185494e-05 Test RE 6.404452383830928e-05\n",
      "111 Train Loss 0.00056362426 Test MSE 4.482632608442473e-05 Test RE 7.459692496765035e-05\n",
      "112 Train Loss 0.00055078976 Test MSE 6.764444611969225e-05 Test RE 9.16369075841882e-05\n",
      "113 Train Loss 0.00053618633 Test MSE 9.390997506047787e-05 Test RE 0.0001079717685231935\n",
      "114 Train Loss 0.0005090141 Test MSE 3.81847794939682e-05 Test RE 6.884929473273756e-05\n",
      "115 Train Loss 0.0004891164 Test MSE 4.8206989692327e-05 Test RE 7.735873487061293e-05\n",
      "116 Train Loss 0.00047994123 Test MSE 3.604474101326997e-05 Test RE 6.689217342541197e-05\n",
      "117 Train Loss 0.00046789876 Test MSE 2.1745035919633278e-05 Test RE 5.1955845544038276e-05\n",
      "118 Train Loss 0.000458225 Test MSE 2.1486400393092715e-05 Test RE 5.1645939786251956e-05\n",
      "119 Train Loss 0.00045778207 Test MSE 2.1696969715597596e-05 Test RE 5.189839100920575e-05\n",
      "120 Train Loss 0.00045430032 Test MSE 2.0748178748513656e-05 Test RE 5.075096947016468e-05\n",
      "121 Train Loss 0.0004542454 Test MSE 2.0738412173801295e-05 Test RE 5.073902332577324e-05\n",
      "122 Train Loss 0.00045167177 Test MSE 2.0938313665944315e-05 Test RE 5.098297839269758e-05\n",
      "123 Train Loss 0.00045165946 Test MSE 2.0866403808661473e-05 Test RE 5.089535596219856e-05\n",
      "124 Train Loss 0.00045110512 Test MSE 1.9848294196737003e-05 Test RE 4.9638191092427473e-05\n",
      "125 Train Loss 0.00045036984 Test MSE 1.874781911920675e-05 Test RE 4.824249158080256e-05\n",
      "126 Train Loss 0.00044647782 Test MSE 1.8821202561684248e-05 Test RE 4.833681568724081e-05\n",
      "127 Train Loss 0.00044384986 Test MSE 2.2593914891127377e-05 Test RE 5.296025854318966e-05\n",
      "128 Train Loss 0.00042994664 Test MSE 2.8430025820390643e-05 Test RE 5.940773291807603e-05\n",
      "129 Train Loss 0.00041538803 Test MSE 3.6740996733403195e-05 Test RE 6.753514230666254e-05\n",
      "130 Train Loss 0.00040132093 Test MSE 0.00013369708383715334 Test RE 0.00012882949953209205\n",
      "131 Train Loss 0.00037307388 Test MSE 0.00025064950603341274 Test RE 0.0001763956125765033\n",
      "132 Train Loss 0.00036071526 Test MSE 0.00010375368202948773 Test RE 0.00011348961202527284\n",
      "133 Train Loss 0.0003585429 Test MSE 6.536631789412637e-05 Test RE 9.008061928570164e-05\n",
      "134 Train Loss 0.00035628967 Test MSE 3.864726153104528e-05 Test RE 6.926498035719417e-05\n",
      "135 Train Loss 0.00035532415 Test MSE 3.0962932818953746e-05 Test RE 6.199767420570107e-05\n",
      "136 Train Loss 0.00035433337 Test MSE 2.6796527504560753e-05 Test RE 5.767579793360347e-05\n",
      "137 Train Loss 0.00035028902 Test MSE 2.7041862568559582e-05 Test RE 5.793922116487599e-05\n",
      "138 Train Loss 0.00034935257 Test MSE 2.7385429292668108e-05 Test RE 5.830611823930786e-05\n",
      "139 Train Loss 0.00034237912 Test MSE 3.354516413816995e-05 Test RE 6.453113637402917e-05\n",
      "140 Train Loss 0.00033173317 Test MSE 2.4987779098052186e-05 Test RE 5.569525302874377e-05\n",
      "141 Train Loss 0.00033078226 Test MSE 2.8089665574924017e-05 Test RE 5.905105168409257e-05\n",
      "142 Train Loss 0.00032561866 Test MSE 6.881233355676705e-05 Test RE 9.242458203531771e-05\n",
      "143 Train Loss 0.0003249857 Test MSE 7.346662104211359e-05 Test RE 9.549912341641118e-05\n",
      "144 Train Loss 0.00031794977 Test MSE 0.00016831655698593 Test RE 0.00014454989207737053\n",
      "145 Train Loss 0.00031703827 Test MSE 0.0001753535789620302 Test RE 0.00014754064219782857\n",
      "146 Train Loss 0.0003161421 Test MSE 0.0001791683885721671 Test RE 0.00014913687757967613\n",
      "147 Train Loss 0.0003076952 Test MSE 0.00014347053197327647 Test RE 0.00013345526336107689\n",
      "148 Train Loss 0.00029382334 Test MSE 4.8075000344410084e-05 Test RE 7.725275928850091e-05\n",
      "149 Train Loss 0.0002771101 Test MSE 6.891101736051516e-05 Test RE 9.2490831362634e-05\n",
      "150 Train Loss 0.00025672434 Test MSE 6.097084254904538e-05 Test RE 8.699923836713225e-05\n",
      "151 Train Loss 0.00023963422 Test MSE 2.0788280025094626e-05 Test RE 5.0799990549524755e-05\n",
      "152 Train Loss 0.0002308748 Test MSE 5.1001331212534736e-05 Test RE 7.956922138519587e-05\n",
      "153 Train Loss 0.00022898497 Test MSE 5.414479171801418e-05 Test RE 8.198467814620283e-05\n",
      "154 Train Loss 0.00022577551 Test MSE 4.5819403998319854e-05 Test RE 7.54187048745327e-05\n",
      "155 Train Loss 0.00022480893 Test MSE 4.219459597662515e-05 Test RE 7.237403210522413e-05\n",
      "156 Train Loss 0.00022346446 Test MSE 3.849603332790728e-05 Test RE 6.912932927490574e-05\n",
      "157 Train Loss 0.0002224877 Test MSE 3.3810043625226075e-05 Test RE 6.478541094531637e-05\n",
      "158 Train Loss 0.00022192695 Test MSE 3.577383783310326e-05 Test RE 6.664032697932035e-05\n",
      "159 Train Loss 0.00021829414 Test MSE 2.4479818141027914e-05 Test RE 5.5126249442514286e-05\n",
      "160 Train Loss 0.0002175908 Test MSE 2.077819457817677e-05 Test RE 5.0787666231130804e-05\n",
      "161 Train Loss 0.00021000732 Test MSE 1.344036914501204e-05 Test RE 4.0846979821630497e-05\n",
      "162 Train Loss 0.00020931949 Test MSE 1.4243163051745819e-05 Test RE 4.204918428903043e-05\n",
      "163 Train Loss 0.00020868322 Test MSE 1.4735425174063881e-05 Test RE 4.276964919186471e-05\n",
      "164 Train Loss 0.00020715475 Test MSE 1.524172289412378e-05 Test RE 4.349820975097085e-05\n",
      "165 Train Loss 0.00020692099 Test MSE 1.5109006690806114e-05 Test RE 4.330841693304123e-05\n",
      "166 Train Loss 0.00020654235 Test MSE 1.443158577521828e-05 Test RE 4.2326404675209474e-05\n",
      "167 Train Loss 0.00020633898 Test MSE 1.3996967279041033e-05 Test RE 4.168418608809044e-05\n",
      "168 Train Loss 0.00020578389 Test MSE 1.2743303116058583e-05 Test RE 3.977364188390665e-05\n",
      "169 Train Loss 0.00020542147 Test MSE 1.1902163500694438e-05 Test RE 3.843857758680182e-05\n",
      "170 Train Loss 0.00020540923 Test MSE 1.1874581436913263e-05 Test RE 3.8394012989497545e-05\n",
      "171 Train Loss 0.00020525766 Test MSE 1.1573033239650804e-05 Test RE 3.790338114137166e-05\n",
      "172 Train Loss 0.0002051419 Test MSE 1.1384792394001288e-05 Test RE 3.759385916271995e-05\n",
      "173 Train Loss 0.00020488749 Test MSE 1.1000194129633988e-05 Test RE 3.6953410559877975e-05\n",
      "174 Train Loss 0.00020443245 Test MSE 1.029323459378055e-05 Test RE 3.574623349120346e-05\n",
      "175 Train Loss 0.00020410497 Test MSE 9.789412106536923e-06 Test RE 3.486042344144537e-05\n",
      "176 Train Loss 0.00020369532 Test MSE 9.520420324884843e-06 Test RE 3.437814300111175e-05\n",
      "177 Train Loss 0.0002031875 Test MSE 9.451653574633322e-06 Test RE 3.425375996168064e-05\n",
      "178 Train Loss 0.00020297446 Test MSE 9.718475967612526e-06 Test RE 3.473389082434355e-05\n",
      "179 Train Loss 0.00020272803 Test MSE 1.0097915193414506e-05 Test RE 3.540545760143558e-05\n",
      "180 Train Loss 0.00020213515 Test MSE 1.111926791403014e-05 Test RE 3.7152876984909e-05\n",
      "181 Train Loss 0.00020155228 Test MSE 1.2240093087775648e-05 Test RE 3.898043781876119e-05\n",
      "182 Train Loss 0.00020055265 Test MSE 1.5106803875733604e-05 Test RE 4.330525974630438e-05\n",
      "183 Train Loss 0.00019975501 Test MSE 1.7050883079462163e-05 Test RE 4.600741056338711e-05\n",
      "184 Train Loss 0.00019896775 Test MSE 1.7986503600345887e-05 Test RE 4.725281945207538e-05\n",
      "185 Train Loss 0.00019746114 Test MSE 1.2421110744979598e-05 Test RE 3.9267619069654096e-05\n",
      "186 Train Loss 0.00019580538 Test MSE 1.0529751921155118e-05 Test RE 3.6154588430110945e-05\n",
      "187 Train Loss 0.00019533858 Test MSE 1.013393604721183e-05 Test RE 3.5468549808166287e-05\n",
      "188 Train Loss 0.00019497752 Test MSE 1.0046070057804662e-05 Test RE 3.5314450552566665e-05\n",
      "189 Train Loss 0.00019179701 Test MSE 1.204595702818814e-05 Test RE 3.86700743547117e-05\n",
      "190 Train Loss 0.00019108172 Test MSE 1.257941331606337e-05 Test RE 3.951705266312821e-05\n",
      "191 Train Loss 0.00019063671 Test MSE 1.2042099343553763e-05 Test RE 3.866388186636936e-05\n",
      "192 Train Loss 0.00018688092 Test MSE 1.2075602068621995e-05 Test RE 3.8717628545521493e-05\n",
      "193 Train Loss 0.00018645922 Test MSE 1.2238664625861359e-05 Test RE 3.8978163175296665e-05\n",
      "194 Train Loss 0.00018630824 Test MSE 1.2806309637927654e-05 Test RE 3.987184675494356e-05\n",
      "195 Train Loss 0.00018586284 Test MSE 1.3383507737635571e-05 Test RE 4.0760483740847654e-05\n",
      "196 Train Loss 0.00018512108 Test MSE 1.4065172098764882e-05 Test RE 4.178562261054644e-05\n",
      "197 Train Loss 0.000181977 Test MSE 1.136593342084187e-05 Test RE 3.7562709036841566e-05\n",
      "198 Train Loss 0.00017963107 Test MSE 1.002420623039443e-05 Test RE 3.5276001208625355e-05\n",
      "199 Train Loss 0.0001786755 Test MSE 8.70334105430067e-06 Test RE 3.2869822097340954e-05\n",
      "Training time: 25.86\n",
      "Training time: 25.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 159.57216 Test MSE 3850.8358628030114 Test RE 0.6914039498049066\n",
      "1 Train Loss 110.41916 Test MSE 1854.3627987583307 Test RE 0.47979056723375124\n",
      "2 Train Loss 80.94231 Test MSE 503.3581717239579 Test RE 0.24997289166055245\n",
      "3 Train Loss 64.94423 Test MSE 779.0194627591185 Test RE 0.3109771695532706\n",
      "4 Train Loss 41.964172 Test MSE 200.88516532727047 Test RE 0.1579167821108735\n",
      "5 Train Loss 38.41698 Test MSE 114.57985153995611 Test RE 0.11926375528411559\n",
      "6 Train Loss 36.0172 Test MSE 157.1174947998096 Test RE 0.1396582594751958\n",
      "7 Train Loss 32.9114 Test MSE 78.17005673150192 Test RE 0.09850869503549382\n",
      "8 Train Loss 31.01464 Test MSE 53.29691750385747 Test RE 0.08134023040092002\n",
      "9 Train Loss 29.453337 Test MSE 45.66059248278143 Test RE 0.075287889605487\n",
      "10 Train Loss 25.248814 Test MSE 118.13337838014561 Test RE 0.12109902998062076\n",
      "11 Train Loss 22.802208 Test MSE 38.47947960044011 Test RE 0.06911446448203018\n",
      "12 Train Loss 19.23811 Test MSE 57.00438973716338 Test RE 0.08412178948965848\n",
      "13 Train Loss 16.45326 Test MSE 54.58141474081711 Test RE 0.0823145761930113\n",
      "14 Train Loss 12.8015995 Test MSE 125.21669828968516 Test RE 0.12467675103278063\n",
      "15 Train Loss 9.862048 Test MSE 132.7597116689643 Test RE 0.1283770827768365\n",
      "16 Train Loss 7.915039 Test MSE 56.752956201492495 Test RE 0.0839360633223137\n",
      "17 Train Loss 6.241609 Test MSE 30.596620263434517 Test RE 0.06162984572593595\n",
      "18 Train Loss 4.637038 Test MSE 3.8374213475117736 Test RE 0.021825997239994465\n",
      "19 Train Loss 3.300082 Test MSE 15.809012225853547 Test RE 0.04430030538067237\n",
      "20 Train Loss 2.6068554 Test MSE 9.282308167915254 Test RE 0.03394551044983805\n",
      "21 Train Loss 1.9988389 Test MSE 6.916757608153886 Test RE 0.02930256450058191\n",
      "22 Train Loss 1.5344313 Test MSE 1.3665739942962718 Test RE 0.013024795951581495\n",
      "23 Train Loss 1.2983739 Test MSE 0.9715753151369707 Test RE 0.010982281929326935\n",
      "24 Train Loss 1.0760659 Test MSE 1.026663835411682 Test RE 0.011289338221078707\n",
      "25 Train Loss 1.0193115 Test MSE 0.9291000466139754 Test RE 0.010739537858108552\n",
      "26 Train Loss 0.8791026 Test MSE 1.5299437899683954 Test RE 0.013781360390897232\n",
      "27 Train Loss 0.80468154 Test MSE 1.2937595418044017 Test RE 0.012673049618968209\n",
      "28 Train Loss 0.7747329 Test MSE 1.2561781790084423 Test RE 0.012487628633363928\n",
      "29 Train Loss 0.7098191 Test MSE 0.5884378372871479 Test RE 0.008546821772222648\n",
      "30 Train Loss 0.66880846 Test MSE 0.3029156567252954 Test RE 0.006132184498243281\n",
      "31 Train Loss 0.6057205 Test MSE 0.7529250328785796 Test RE 0.009667857058612834\n",
      "32 Train Loss 0.515185 Test MSE 1.809557554767996 Test RE 0.014987891923306568\n",
      "33 Train Loss 0.47348902 Test MSE 1.1571221087431534 Test RE 0.01198516309002253\n",
      "34 Train Loss 0.43288708 Test MSE 0.22078388422685685 Test RE 0.005235257373427668\n",
      "35 Train Loss 0.37045237 Test MSE 0.2646616499453531 Test RE 0.005731916566045121\n",
      "36 Train Loss 0.35656002 Test MSE 0.2940902169105094 Test RE 0.006042193664690763\n",
      "37 Train Loss 0.3347882 Test MSE 0.12574989986905172 Test RE 0.00395101043144387\n",
      "38 Train Loss 0.2973583 Test MSE 0.5626966706775403 Test RE 0.008357791354442628\n",
      "39 Train Loss 0.27297693 Test MSE 0.8075172961615661 Test RE 0.010012217351227164\n",
      "40 Train Loss 0.22398853 Test MSE 0.6628708138859505 Test RE 0.009071284777777279\n",
      "41 Train Loss 0.19380584 Test MSE 0.3475700675376198 Test RE 0.006568641147334354\n",
      "42 Train Loss 0.18026836 Test MSE 0.12473485845417404 Test RE 0.003935032028980981\n",
      "43 Train Loss 0.16897106 Test MSE 0.05296517308555922 Test RE 0.002564186151103999\n",
      "44 Train Loss 0.160953 Test MSE 0.07793544241767167 Test RE 0.0031104401907612023\n",
      "45 Train Loss 0.14746965 Test MSE 0.2313675237807548 Test RE 0.005359268942172825\n",
      "46 Train Loss 0.117933266 Test MSE 0.47381262138293134 Test RE 0.007669334233743031\n",
      "47 Train Loss 0.101649314 Test MSE 0.04302427822753837 Test RE 0.0023110596179462305\n",
      "48 Train Loss 0.097371705 Test MSE 0.05731534315005057 Test RE 0.0026674101585127006\n",
      "49 Train Loss 0.090257816 Test MSE 0.10598554035178029 Test RE 0.003627251437757042\n",
      "50 Train Loss 0.08565061 Test MSE 0.09430613345530196 Test RE 0.0034215612748169917\n",
      "51 Train Loss 0.08161134 Test MSE 0.1103442244177409 Test RE 0.003701085820362274\n",
      "52 Train Loss 0.07949861 Test MSE 0.07368746060519935 Test RE 0.0030244829979674515\n",
      "53 Train Loss 0.07760607 Test MSE 0.03317356376555668 Test RE 0.0020293194460176286\n",
      "54 Train Loss 0.07634818 Test MSE 0.0216181411919219 Test RE 0.001638187194685593\n",
      "55 Train Loss 0.074357815 Test MSE 0.028857391875148308 Test RE 0.0018927048321344723\n",
      "56 Train Loss 0.071647674 Test MSE 0.02075137673148004 Test RE 0.0016050102482512234\n",
      "57 Train Loss 0.07001741 Test MSE 0.028178511837385168 Test RE 0.0018703090661460727\n",
      "58 Train Loss 0.06827972 Test MSE 0.03416459014925918 Test RE 0.002059408305589337\n",
      "59 Train Loss 0.065407656 Test MSE 0.03829072222729315 Test RE 0.0021802240765941734\n",
      "60 Train Loss 0.06311867 Test MSE 0.07989696261582599 Test RE 0.00314933954905889\n",
      "61 Train Loss 0.06141927 Test MSE 0.022678556842387145 Test RE 0.0016778844904607605\n",
      "62 Train Loss 0.060063943 Test MSE 0.03544137031964207 Test RE 0.0020975368772309048\n",
      "63 Train Loss 0.055859387 Test MSE 0.07200382373228438 Test RE 0.0029897311225001674\n",
      "64 Train Loss 0.05329266 Test MSE 0.07602092492699018 Test RE 0.0030719979883863835\n",
      "65 Train Loss 0.051024973 Test MSE 0.04228909185863467 Test RE 0.002291229181017512\n",
      "66 Train Loss 0.04843322 Test MSE 0.01036138176675234 Test RE 0.0011341309182428295\n",
      "67 Train Loss 0.04722956 Test MSE 0.009654865082588197 Test RE 0.001094781517810476\n",
      "68 Train Loss 0.04591227 Test MSE 0.012832721797822166 Test RE 0.0012621580512184158\n",
      "69 Train Loss 0.045135457 Test MSE 0.011603420689643101 Test RE 0.0012001827250949638\n",
      "70 Train Loss 0.044116013 Test MSE 0.010507713202373026 Test RE 0.0011421113768494136\n",
      "71 Train Loss 0.043010026 Test MSE 0.012377024464517834 Test RE 0.001239545511389611\n",
      "72 Train Loss 0.042149454 Test MSE 0.011878522815247846 Test RE 0.0012143267734524245\n",
      "73 Train Loss 0.041636396 Test MSE 0.009539076976899782 Test RE 0.001088197011939297\n",
      "74 Train Loss 0.041355655 Test MSE 0.007273110932009224 Test RE 0.000950198757444656\n",
      "75 Train Loss 0.04108857 Test MSE 0.007645288083888643 Test RE 0.000974207076172786\n",
      "76 Train Loss 0.040270414 Test MSE 0.007348296874837856 Test RE 0.0009550974800181516\n",
      "77 Train Loss 0.0390299 Test MSE 0.007241718666012179 Test RE 0.0009481459117637695\n",
      "78 Train Loss 0.037985336 Test MSE 0.012192155564953898 Test RE 0.0012302534739089194\n",
      "79 Train Loss 0.03662122 Test MSE 0.021135775593718768 Test RE 0.001619807654668849\n",
      "80 Train Loss 0.03534069 Test MSE 0.021027318459042727 Test RE 0.0016156463299734903\n",
      "81 Train Loss 0.034822956 Test MSE 0.013882971189026318 Test RE 0.0013127909179115958\n",
      "82 Train Loss 0.034221545 Test MSE 0.004608804071155083 Test RE 0.0007563946967938249\n",
      "83 Train Loss 0.033680324 Test MSE 0.0046158206460934995 Test RE 0.0007569702562590412\n",
      "84 Train Loss 0.03327879 Test MSE 0.005850333995113776 Test RE 0.0008522061866439446\n",
      "85 Train Loss 0.032928586 Test MSE 0.0038835022037534365 Test RE 0.0006943303198188651\n",
      "86 Train Loss 0.032631285 Test MSE 0.0037888088783634586 Test RE 0.0006858129816442089\n",
      "87 Train Loss 0.032362215 Test MSE 0.0040065027488999155 Test RE 0.0007052402242782917\n",
      "88 Train Loss 0.032220736 Test MSE 0.004246881275842622 Test RE 0.0007260882561000166\n",
      "89 Train Loss 0.032132696 Test MSE 0.005649669057118785 Test RE 0.0008374634399852036\n",
      "90 Train Loss 0.031947654 Test MSE 0.010186753605843901 Test RE 0.001124533125960168\n",
      "91 Train Loss 0.031721894 Test MSE 0.009345379137619689 Test RE 0.0010770920365218847\n",
      "92 Train Loss 0.03146727 Test MSE 0.005567763857084683 Test RE 0.000831370779415816\n",
      "93 Train Loss 0.031086922 Test MSE 0.003901543978850435 Test RE 0.0006959412930136864\n",
      "94 Train Loss 0.030547496 Test MSE 0.008550158152751081 Test RE 0.0010302471607640507\n",
      "95 Train Loss 0.030081972 Test MSE 0.008440497821712084 Test RE 0.001023619105890065\n",
      "96 Train Loss 0.029642265 Test MSE 0.0027714940101737843 Test RE 0.0005865584889835216\n",
      "97 Train Loss 0.02886374 Test MSE 0.01122684015805011 Test RE 0.0011805465654792001\n",
      "98 Train Loss 0.027296914 Test MSE 0.013660528581132411 Test RE 0.0013022312244255038\n",
      "99 Train Loss 0.02623985 Test MSE 0.01344511878095049 Test RE 0.0012919231309299342\n",
      "100 Train Loss 0.025887832 Test MSE 0.012889527365470985 Test RE 0.0012649485128339909\n",
      "101 Train Loss 0.02551281 Test MSE 0.0054455850475807754 Test RE 0.0008221983954591526\n",
      "102 Train Loss 0.02490167 Test MSE 0.004818607415533926 Test RE 0.0007734195125656618\n",
      "103 Train Loss 0.024411205 Test MSE 0.006746373388984266 Test RE 0.0009151442166250397\n",
      "104 Train Loss 0.023800552 Test MSE 0.0054543846207542775 Test RE 0.0008228624264554022\n",
      "105 Train Loss 0.023226922 Test MSE 0.005563808918440611 Test RE 0.0008310754539846197\n",
      "106 Train Loss 0.022833576 Test MSE 0.002444700532163945 Test RE 0.0005508929136119103\n",
      "107 Train Loss 0.022656526 Test MSE 0.002778491478977832 Test RE 0.0005872984937003929\n",
      "108 Train Loss 0.022445468 Test MSE 0.0049046864132923015 Test RE 0.000780297068102707\n",
      "109 Train Loss 0.022059122 Test MSE 0.005466524834154343 Test RE 0.0008237776692936448\n",
      "110 Train Loss 0.02180817 Test MSE 0.0026769973499343705 Test RE 0.000576472139493043\n",
      "111 Train Loss 0.021496115 Test MSE 0.003567910697259711 Test RE 0.0006655203507862056\n",
      "112 Train Loss 0.02119351 Test MSE 0.005298730024419057 Test RE 0.0008110362176735094\n",
      "113 Train Loss 0.020882694 Test MSE 0.003972660325276568 Test RE 0.0007022553701068282\n",
      "114 Train Loss 0.020655366 Test MSE 0.0060722902747767276 Test RE 0.0008682216563022017\n",
      "115 Train Loss 0.020448377 Test MSE 0.0030664065346146076 Test RE 0.0006169773462805425\n",
      "116 Train Loss 0.02029264 Test MSE 0.003074446230930016 Test RE 0.000617785631709085\n",
      "117 Train Loss 0.02019676 Test MSE 0.0037597133753062392 Test RE 0.0006831746155227469\n",
      "118 Train Loss 0.019968366 Test MSE 0.004772423325068329 Test RE 0.0007697041571349499\n",
      "119 Train Loss 0.019605545 Test MSE 0.0032093807742204503 Test RE 0.0006311970721085468\n",
      "120 Train Loss 0.019280476 Test MSE 0.0028026977505155024 Test RE 0.0005898512238203999\n",
      "121 Train Loss 0.019028043 Test MSE 0.004365912857014849 Test RE 0.0007361933392959047\n",
      "122 Train Loss 0.018867366 Test MSE 0.004156756078975089 Test RE 0.0007183426006637357\n",
      "123 Train Loss 0.018732913 Test MSE 0.002598053417238822 Test RE 0.0005679085281306957\n",
      "124 Train Loss 0.01841323 Test MSE 0.00271956473158812 Test RE 0.000581037352865774\n",
      "125 Train Loss 0.017984925 Test MSE 0.00594237966306274 Test RE 0.0008588840747653194\n",
      "126 Train Loss 0.01761502 Test MSE 0.00408671762183616 Test RE 0.0007122651041898787\n",
      "127 Train Loss 0.017130677 Test MSE 0.0023201141782669774 Test RE 0.0005366721163108397\n",
      "128 Train Loss 0.016803907 Test MSE 0.0052884907131691555 Test RE 0.0008102522120713108\n",
      "129 Train Loss 0.016624982 Test MSE 0.0024909974836317588 Test RE 0.0005560847647986441\n",
      "130 Train Loss 0.016339302 Test MSE 0.0032440284441251793 Test RE 0.0006345950484435751\n",
      "131 Train Loss 0.016019987 Test MSE 0.005366663478642585 Test RE 0.0008162186878679759\n",
      "132 Train Loss 0.015656332 Test MSE 0.0037862902599113206 Test RE 0.0006855849959582117\n",
      "133 Train Loss 0.015224068 Test MSE 0.0022833935578992497 Test RE 0.0005324082028702453\n",
      "134 Train Loss 0.0145621225 Test MSE 0.0022791563289711048 Test RE 0.0005319139860559635\n",
      "135 Train Loss 0.01420637 Test MSE 0.002096076742114922 Test RE 0.0005101030754145116\n",
      "136 Train Loss 0.014024772 Test MSE 0.0011161955847784337 Test RE 0.0003722412539338542\n",
      "137 Train Loss 0.013960655 Test MSE 0.0010395098755254302 Test RE 0.00035922674418860297\n",
      "138 Train Loss 0.013829039 Test MSE 0.0012298707523432785 Test RE 0.0003907365964063733\n",
      "139 Train Loss 0.013724252 Test MSE 0.001811373382017009 Test RE 0.0004741964988519264\n",
      "140 Train Loss 0.01362426 Test MSE 0.004495506362810952 Test RE 0.0007470396630413204\n",
      "141 Train Loss 0.013463063 Test MSE 0.004837529589946771 Test RE 0.0007749365940037009\n",
      "142 Train Loss 0.013167784 Test MSE 0.0013234927432483805 Test RE 0.0004053359599963479\n",
      "143 Train Loss 0.012893657 Test MSE 0.0010974811266412888 Test RE 0.0003691075108145091\n",
      "144 Train Loss 0.012627332 Test MSE 0.0012295234959531869 Test RE 0.00039068142989443514\n",
      "145 Train Loss 0.012429022 Test MSE 0.003768150989094559 Test RE 0.0006839407820470965\n",
      "146 Train Loss 0.012137892 Test MSE 0.006801624706055955 Test RE 0.0009188839903763847\n",
      "147 Train Loss 0.011857645 Test MSE 0.00496605990040467 Test RE 0.0007851639102205164\n",
      "148 Train Loss 0.011621894 Test MSE 0.0038580639431257625 Test RE 0.0006920525337167149\n",
      "149 Train Loss 0.011522262 Test MSE 0.003436182926310787 Test RE 0.0006531192587526808\n",
      "150 Train Loss 0.011375492 Test MSE 0.0015866442932457713 Test RE 0.00044380699125787037\n",
      "151 Train Loss 0.011192192 Test MSE 0.0015140375086679391 Test RE 0.00043353350766517503\n",
      "152 Train Loss 0.010961163 Test MSE 0.002961607015704142 Test RE 0.0006063425801500565\n",
      "153 Train Loss 0.010462803 Test MSE 0.0023594270449115995 Test RE 0.0005411998018944094\n",
      "154 Train Loss 0.010259041 Test MSE 0.0015032731227781094 Test RE 0.00043198960722069584\n",
      "155 Train Loss 0.010112505 Test MSE 0.001128486394046479 Test RE 0.00037428508055410356\n",
      "156 Train Loss 0.009978454 Test MSE 0.0012993464733641155 Test RE 0.00040162139320467624\n",
      "157 Train Loss 0.009833252 Test MSE 0.002365471547632671 Test RE 0.0005418925953472889\n",
      "158 Train Loss 0.009615092 Test MSE 0.0029826591897432293 Test RE 0.0006084938151836209\n",
      "159 Train Loss 0.009446118 Test MSE 0.0026379072083463385 Test RE 0.0005722477709540792\n",
      "160 Train Loss 0.009321366 Test MSE 0.0018821388557570436 Test RE 0.0004833705452497257\n",
      "161 Train Loss 0.009219927 Test MSE 0.001237914567477977 Test RE 0.0003920122957264245\n",
      "162 Train Loss 0.009032033 Test MSE 0.0026636645691393854 Test RE 0.0005750347883176568\n",
      "163 Train Loss 0.0087489495 Test MSE 0.0027910481211404197 Test RE 0.0005886240662449181\n",
      "164 Train Loss 0.0085752485 Test MSE 0.0023291122713449556 Test RE 0.0005377117963097571\n",
      "165 Train Loss 0.008500748 Test MSE 0.002604003013779376 Test RE 0.0005685584174685719\n",
      "166 Train Loss 0.008336772 Test MSE 0.003095153918078477 Test RE 0.0006198626630603488\n",
      "167 Train Loss 0.00816466 Test MSE 0.003998758830502919 Test RE 0.0007045583372804297\n",
      "168 Train Loss 0.008041325 Test MSE 0.0017022275718894051 Test RE 0.00045968799569228775\n",
      "169 Train Loss 0.0078906175 Test MSE 0.0007314858625910878 Test RE 0.000301340372525293\n",
      "170 Train Loss 0.007644513 Test MSE 0.0018299503655668098 Test RE 0.00047662191529348883\n",
      "171 Train Loss 0.0074498877 Test MSE 0.0025421232984017103 Test RE 0.0005617623874146311\n",
      "172 Train Loss 0.0073767016 Test MSE 0.002377682953216043 Test RE 0.0005432895159883581\n",
      "173 Train Loss 0.007261017 Test MSE 0.0016295427280425723 Test RE 0.00044976662832272815\n",
      "174 Train Loss 0.0071917507 Test MSE 0.0022995696487677535 Test RE 0.0005342907264443679\n",
      "175 Train Loss 0.007170569 Test MSE 0.0025425075739083005 Test RE 0.000561804844711805\n",
      "176 Train Loss 0.007117039 Test MSE 0.0018684442673547732 Test RE 0.00048160881397303504\n",
      "177 Train Loss 0.0070511256 Test MSE 0.0014015032863112099 Test RE 0.00041711077854135145\n",
      "178 Train Loss 0.006989653 Test MSE 0.0013932117180130064 Test RE 0.0004158750936077071\n",
      "179 Train Loss 0.0069518583 Test MSE 0.0010916785858094474 Test RE 0.0003681304553510032\n",
      "180 Train Loss 0.006908559 Test MSE 0.001214302745722301 Test RE 0.0003882557004617356\n",
      "181 Train Loss 0.0068193413 Test MSE 0.0017093790563290376 Test RE 0.0004606526158126535\n",
      "182 Train Loss 0.006716487 Test MSE 0.0014610702109440804 Test RE 0.0004258825987290423\n",
      "183 Train Loss 0.006610006 Test MSE 0.0009895783435394075 Test RE 0.00035049307653073667\n",
      "184 Train Loss 0.006519951 Test MSE 0.0014067563577879539 Test RE 0.0004178917483144047\n",
      "185 Train Loss 0.0063966117 Test MSE 0.0020960893687583387 Test RE 0.0005101046118277092\n",
      "186 Train Loss 0.006334968 Test MSE 0.0015226482591821418 Test RE 0.00043476457234219824\n",
      "187 Train Loss 0.006290537 Test MSE 0.0012354583255382659 Test RE 0.00039162319167551597\n",
      "188 Train Loss 0.0062487726 Test MSE 0.0012835357444780848 Test RE 0.0003991704063656772\n",
      "189 Train Loss 0.0061388775 Test MSE 0.0018322416263834697 Test RE 0.0004769202084910973\n",
      "190 Train Loss 0.0060222363 Test MSE 0.0019369423799235194 Test RE 0.0004903573655408687\n",
      "191 Train Loss 0.005971723 Test MSE 0.0014461964331835783 Test RE 0.00042370929897537677\n",
      "192 Train Loss 0.0058903666 Test MSE 0.0010364288073287612 Test RE 0.00035869398184039587\n",
      "193 Train Loss 0.005748397 Test MSE 0.002027980578674312 Test RE 0.0005017486909993726\n",
      "194 Train Loss 0.0056421165 Test MSE 0.001884192395303903 Test RE 0.0004836341681809342\n",
      "195 Train Loss 0.0054987124 Test MSE 0.0012208187086167302 Test RE 0.0003892959991053569\n",
      "196 Train Loss 0.0052023 Test MSE 0.0007669408534676931 Test RE 0.00030855691842696683\n",
      "197 Train Loss 0.004892763 Test MSE 0.003984104361143076 Test RE 0.0007032661356299498\n",
      "198 Train Loss 0.0046639587 Test MSE 0.005747881670188588 Test RE 0.0008447112180254419\n",
      "199 Train Loss 0.0044907713 Test MSE 0.004599966732043984 Test RE 0.0007556691589119138\n",
      "Training time: 30.24\n",
      "Training time: 30.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 141.12416 Test MSE 2717.2915609971164 Test RE 0.5807944696715183\n",
      "1 Train Loss 124.42378 Test MSE 2274.7204812091045 Test RE 0.5313961105152284\n",
      "2 Train Loss 87.87582 Test MSE 2109.6845983852463 Test RE 0.511756206620167\n",
      "3 Train Loss 55.97347 Test MSE 576.7101088692418 Test RE 0.26756736045157825\n",
      "4 Train Loss 51.515667 Test MSE 255.2342531656221 Test RE 0.1780015692797577\n",
      "5 Train Loss 45.17661 Test MSE 234.2672769935377 Test RE 0.1705336812527969\n",
      "6 Train Loss 43.612553 Test MSE 136.33235574144513 Test RE 0.13009296797557163\n",
      "7 Train Loss 39.417793 Test MSE 159.29639999438504 Test RE 0.14062331541884782\n",
      "8 Train Loss 35.031094 Test MSE 160.22882127696099 Test RE 0.14103427529556167\n",
      "9 Train Loss 33.061367 Test MSE 98.5904139239108 Test RE 0.11062969048272836\n",
      "10 Train Loss 29.714272 Test MSE 61.23018375020362 Test RE 0.08718406888925562\n",
      "11 Train Loss 27.644373 Test MSE 62.65338340163376 Test RE 0.0881914771256105\n",
      "12 Train Loss 23.750866 Test MSE 82.63528951754463 Test RE 0.10128313344386483\n",
      "13 Train Loss 18.34015 Test MSE 48.504081829135934 Test RE 0.07759674362235952\n",
      "14 Train Loss 16.389105 Test MSE 22.737803654728967 Test RE 0.05312862892256506\n",
      "15 Train Loss 13.040774 Test MSE 18.182792388508986 Test RE 0.04750995754545352\n",
      "16 Train Loss 9.815921 Test MSE 41.7134534387184 Test RE 0.07196021057573161\n",
      "17 Train Loss 8.2991295 Test MSE 24.423047835465102 Test RE 0.055062291644442124\n",
      "18 Train Loss 7.384439 Test MSE 18.67324768179926 Test RE 0.04814645117146011\n",
      "19 Train Loss 5.989034 Test MSE 19.250585615510452 Test RE 0.04888507942527861\n",
      "20 Train Loss 5.0641303 Test MSE 15.258167025939928 Test RE 0.043521668302019215\n",
      "21 Train Loss 4.0923004 Test MSE 5.854229223527378 Test RE 0.026958095905103292\n",
      "22 Train Loss 3.7897375 Test MSE 3.957685279200229 Test RE 0.02216536973797291\n",
      "23 Train Loss 2.3908534 Test MSE 1.96085102392192 Test RE 0.015601869896119664\n",
      "24 Train Loss 1.9587687 Test MSE 1.3732402933445242 Test RE 0.013056525501005055\n",
      "25 Train Loss 1.5976812 Test MSE 2.034584610776576 Test RE 0.015892500361271447\n",
      "26 Train Loss 1.3147793 Test MSE 1.311080057887693 Test RE 0.012757599322703895\n",
      "27 Train Loss 1.2507111 Test MSE 0.6095564573030677 Test RE 0.008698839552821082\n",
      "28 Train Loss 0.97933424 Test MSE 1.622399183594177 Test RE 0.01419166045764091\n",
      "29 Train Loss 0.9135174 Test MSE 0.5562498168721322 Test RE 0.00830977553778205\n",
      "30 Train Loss 0.81092656 Test MSE 1.1780073795079329 Test RE 0.01209284142294584\n",
      "31 Train Loss 0.6547692 Test MSE 0.34975084185711103 Test RE 0.006589215883073373\n",
      "32 Train Loss 0.5900581 Test MSE 0.6168933402385999 Test RE 0.008751034444312421\n",
      "33 Train Loss 0.50487393 Test MSE 0.15738778048132304 Test RE 0.0044201790087686375\n",
      "34 Train Loss 0.4597096 Test MSE 0.41879427352410414 Test RE 0.007210322991168929\n",
      "35 Train Loss 0.40504783 Test MSE 0.19667996170299412 Test RE 0.004941222494899361\n",
      "36 Train Loss 0.3521195 Test MSE 0.08303814367289446 Test RE 0.003210651504820982\n",
      "37 Train Loss 0.30479687 Test MSE 0.2112399607116882 Test RE 0.005120853970206615\n",
      "38 Train Loss 0.2647991 Test MSE 0.7952403623153106 Test RE 0.009935816442340763\n",
      "39 Train Loss 0.24095555 Test MSE 0.1928750908377297 Test RE 0.004893193883057806\n",
      "40 Train Loss 0.2107016 Test MSE 0.23440258648131435 Test RE 0.005394305668388272\n",
      "41 Train Loss 0.17951077 Test MSE 0.330199336720382 Test RE 0.006402394792196077\n",
      "42 Train Loss 0.15371825 Test MSE 0.07664911283356572 Test RE 0.0030846643807970193\n",
      "43 Train Loss 0.14535211 Test MSE 0.02698742804021087 Test RE 0.0018303540351767497\n",
      "44 Train Loss 0.13920511 Test MSE 0.02714662736981532 Test RE 0.001835744742793301\n",
      "45 Train Loss 0.13130544 Test MSE 0.03229361482307672 Test RE 0.002002224092350231\n",
      "46 Train Loss 0.1262172 Test MSE 0.051843430911383985 Test RE 0.002536887565027183\n",
      "47 Train Loss 0.11866321 Test MSE 0.042509248145285304 Test RE 0.0022971854888741124\n",
      "48 Train Loss 0.115529194 Test MSE 0.027214902748718395 Test RE 0.0018380517965968666\n",
      "49 Train Loss 0.11017995 Test MSE 0.01564137031044238 Test RE 0.0013934511598691966\n",
      "50 Train Loss 0.10067127 Test MSE 0.04173859219617036 Test RE 0.0022762672535612755\n",
      "51 Train Loss 0.09764282 Test MSE 0.02316514123933938 Test RE 0.0016957890576519159\n",
      "52 Train Loss 0.09480133 Test MSE 0.025332114045110665 Test RE 0.0017733320758983014\n",
      "53 Train Loss 0.090041496 Test MSE 0.015945853407817373 Test RE 0.0014069486139810334\n",
      "54 Train Loss 0.0849819 Test MSE 0.011153682845686885 Test RE 0.0011766938888268163\n",
      "55 Train Loss 0.08136363 Test MSE 0.011501469771008643 Test RE 0.0011948985208217215\n",
      "56 Train Loss 0.07671809 Test MSE 0.03590242433686548 Test RE 0.0021111361388800763\n",
      "57 Train Loss 0.07157473 Test MSE 0.03383691469027322 Test RE 0.0020495085318306535\n",
      "58 Train Loss 0.0674565 Test MSE 0.020245182251305335 Test RE 0.0015853136451683357\n",
      "59 Train Loss 0.062694944 Test MSE 0.014598880976223904 Test RE 0.0013462141039909776\n",
      "60 Train Loss 0.0603897 Test MSE 0.02261556423350115 Test RE 0.0016755526000498942\n",
      "61 Train Loss 0.055894427 Test MSE 0.02425346453800366 Test RE 0.001735166861651875\n",
      "62 Train Loss 0.05275736 Test MSE 0.0233062337200893 Test RE 0.0017009455094610846\n",
      "63 Train Loss 0.050135348 Test MSE 0.011613800858729388 Test RE 0.0012007194338385756\n",
      "64 Train Loss 0.047986675 Test MSE 0.024148324520325695 Test RE 0.001731401757892428\n",
      "65 Train Loss 0.043778304 Test MSE 0.04039283891678609 Test RE 0.0022392704092033195\n",
      "66 Train Loss 0.0411414 Test MSE 0.008269756953524004 Test RE 0.0010132129345577171\n",
      "67 Train Loss 0.039355185 Test MSE 0.008088615059424308 Test RE 0.0010020547156904633\n",
      "68 Train Loss 0.037816804 Test MSE 0.024330464559043447 Test RE 0.001737919087172109\n",
      "69 Train Loss 0.034977593 Test MSE 0.009226826409773164 Test RE 0.0010702383951241038\n",
      "70 Train Loss 0.033963796 Test MSE 0.004644415519016654 Test RE 0.0007593113403391309\n",
      "71 Train Loss 0.03328728 Test MSE 0.00469925323448431 Test RE 0.0007637808703883758\n",
      "72 Train Loss 0.031762198 Test MSE 0.007815734371415365 Test RE 0.0009850068433597881\n",
      "73 Train Loss 0.030193562 Test MSE 0.014617131846094242 Test RE 0.0013470553295725123\n",
      "74 Train Loss 0.029632092 Test MSE 0.007681326772645779 Test RE 0.0009765005063024191\n",
      "75 Train Loss 0.028840112 Test MSE 0.003275217277410725 Test RE 0.0006376383224084636\n",
      "76 Train Loss 0.028471874 Test MSE 0.005704952024722926 Test RE 0.0008415508261116793\n",
      "77 Train Loss 0.027802942 Test MSE 0.004897447505809274 Test RE 0.0007797210288285817\n",
      "78 Train Loss 0.02726423 Test MSE 0.004114053806614787 Test RE 0.0007146433158573037\n",
      "79 Train Loss 0.026612801 Test MSE 0.0033039898023389464 Test RE 0.0006404329992093869\n",
      "80 Train Loss 0.026117323 Test MSE 0.0031256453218857514 Test RE 0.000622908418197187\n",
      "81 Train Loss 0.025921999 Test MSE 0.003927890304296994 Test RE 0.0006982871137978251\n",
      "82 Train Loss 0.02573275 Test MSE 0.0030952436786001736 Test RE 0.0006198716511097492\n",
      "83 Train Loss 0.02530099 Test MSE 0.0040220262203171046 Test RE 0.0007066051543887533\n",
      "84 Train Loss 0.02483037 Test MSE 0.0035544572096928876 Test RE 0.0006642644300582015\n",
      "85 Train Loss 0.023811601 Test MSE 0.007278181935311332 Test RE 0.000950529951447132\n",
      "86 Train Loss 0.02238124 Test MSE 0.005184091771806737 Test RE 0.0008022148422930397\n",
      "87 Train Loss 0.022117585 Test MSE 0.0040216446202960415 Test RE 0.0007065716331083042\n",
      "88 Train Loss 0.021973655 Test MSE 0.0030873160450118065 Test RE 0.0006190773249735346\n",
      "89 Train Loss 0.021895884 Test MSE 0.0023634988136251363 Test RE 0.0005416665869376594\n",
      "90 Train Loss 0.021864586 Test MSE 0.0029086953892443227 Test RE 0.0006009017566267878\n",
      "91 Train Loss 0.021816006 Test MSE 0.004364849485952865 Test RE 0.0007361036794249414\n",
      "92 Train Loss 0.02156773 Test MSE 0.00491676873291094 Test RE 0.0007812575779860944\n",
      "93 Train Loss 0.021072198 Test MSE 0.0034615783116230775 Test RE 0.0006555282810415234\n",
      "94 Train Loss 0.020685855 Test MSE 0.006048295093037777 Test RE 0.0008665045316885942\n",
      "95 Train Loss 0.020274583 Test MSE 0.007215807033802701 Test RE 0.0009464481086995674\n",
      "96 Train Loss 0.019591453 Test MSE 0.003834497798299303 Test RE 0.0006899356699420643\n",
      "97 Train Loss 0.019000199 Test MSE 0.00195500706179691 Test RE 0.0004926386909991085\n",
      "98 Train Loss 0.018490363 Test MSE 0.00459035430088801 Test RE 0.0007548791948846653\n",
      "99 Train Loss 0.018230898 Test MSE 0.00231343090221685 Test RE 0.0005358985951998903\n",
      "100 Train Loss 0.018056393 Test MSE 0.0035985389210983366 Test RE 0.0006683707791601842\n",
      "101 Train Loss 0.017983431 Test MSE 0.003887006028596882 Test RE 0.0006946434731535359\n",
      "102 Train Loss 0.01777374 Test MSE 0.003461536755245659 Test RE 0.000655524346208918\n",
      "103 Train Loss 0.017419254 Test MSE 0.002783020702089902 Test RE 0.0005877769768618851\n",
      "104 Train Loss 0.017026449 Test MSE 0.0018159339608972483 Test RE 0.0004747930768528728\n",
      "105 Train Loss 0.016858675 Test MSE 0.001593849558813903 Test RE 0.000444813557463602\n",
      "106 Train Loss 0.016674928 Test MSE 0.0016063268377657742 Test RE 0.0004465512506793355\n",
      "107 Train Loss 0.016540816 Test MSE 0.0018234974406946527 Test RE 0.0004757808210142581\n",
      "108 Train Loss 0.016365508 Test MSE 0.001840148941370614 Test RE 0.0004779482110928019\n",
      "109 Train Loss 0.016277883 Test MSE 0.001666506091626668 Test RE 0.00045483911422284853\n",
      "110 Train Loss 0.016210295 Test MSE 0.0015172638655553121 Test RE 0.00043399518362795427\n",
      "111 Train Loss 0.016157823 Test MSE 0.0015939811565349523 Test RE 0.0004448319203140669\n",
      "112 Train Loss 0.01612606 Test MSE 0.0017687081695286313 Test RE 0.0004685785922072664\n",
      "113 Train Loss 0.016076446 Test MSE 0.0017877271783454526 Test RE 0.00047109118047630597\n",
      "114 Train Loss 0.01593122 Test MSE 0.0018722970894056062 Test RE 0.0004821051085416722\n",
      "115 Train Loss 0.015762173 Test MSE 0.0033108833082373095 Test RE 0.0006411007567579129\n",
      "116 Train Loss 0.015673976 Test MSE 0.004376194469300729 Test RE 0.0007370596877114136\n",
      "117 Train Loss 0.015530073 Test MSE 0.002605853445512209 Test RE 0.0005687603933675948\n",
      "118 Train Loss 0.0153615875 Test MSE 0.002901878217402527 Test RE 0.0006001971703710809\n",
      "119 Train Loss 0.015145303 Test MSE 0.0029273915083291692 Test RE 0.0006028298608344555\n",
      "120 Train Loss 0.015054587 Test MSE 0.0017422211553451408 Test RE 0.00046505679451300556\n",
      "121 Train Loss 0.014931586 Test MSE 0.0018403103607436443 Test RE 0.0004779691736374627\n",
      "122 Train Loss 0.014829439 Test MSE 0.0020983824932810265 Test RE 0.0005103835631129832\n",
      "123 Train Loss 0.01473453 Test MSE 0.0013331471116073397 Test RE 0.0004068116583568619\n",
      "124 Train Loss 0.014686893 Test MSE 0.0014779100315625846 Test RE 0.0004283298595898162\n",
      "125 Train Loss 0.014620242 Test MSE 0.0016996227400554588 Test RE 0.0004593361425149368\n",
      "126 Train Loss 0.014560364 Test MSE 0.0015762133281101425 Test RE 0.00044234574094324596\n",
      "127 Train Loss 0.014497365 Test MSE 0.0014187590075537132 Test RE 0.00041967071834224736\n",
      "128 Train Loss 0.01446282 Test MSE 0.0013812148645140898 Test RE 0.00041408068598055404\n",
      "129 Train Loss 0.014413696 Test MSE 0.0013727177962683443 Test RE 0.00041280503367187993\n",
      "130 Train Loss 0.014337643 Test MSE 0.0019603552741530345 Test RE 0.0004933120739546163\n",
      "131 Train Loss 0.014272099 Test MSE 0.0021912118786381976 Test RE 0.000521550707818543\n",
      "132 Train Loss 0.014209595 Test MSE 0.0018514655353345883 Test RE 0.00047941560742183073\n",
      "133 Train Loss 0.014090877 Test MSE 0.0014874293138035725 Test RE 0.000429707090995354\n",
      "134 Train Loss 0.013993881 Test MSE 0.001451303911752916 Test RE 0.0004244568387892222\n",
      "135 Train Loss 0.013851926 Test MSE 0.0015715487770459494 Test RE 0.0004416907302739774\n",
      "136 Train Loss 0.013767904 Test MSE 0.0017062770709876346 Test RE 0.00046023445621446194\n",
      "137 Train Loss 0.013738122 Test MSE 0.001465644717767915 Test RE 0.0004265487817436888\n",
      "138 Train Loss 0.013635371 Test MSE 0.0011767388276749276 Test RE 0.00038220326628243524\n",
      "139 Train Loss 0.013511952 Test MSE 0.0012176245963000842 Test RE 0.0003887863945519071\n",
      "140 Train Loss 0.013204518 Test MSE 0.0021468186474956847 Test RE 0.0005162404513805489\n",
      "141 Train Loss 0.012887652 Test MSE 0.0021115321530315915 Test RE 0.0005119802426310393\n",
      "142 Train Loss 0.01263788 Test MSE 0.0013119063185230626 Test RE 0.0004035578171177022\n",
      "143 Train Loss 0.012249174 Test MSE 0.003683075895398582 Test RE 0.0006761758980130991\n",
      "144 Train Loss 0.012055406 Test MSE 0.004178726494809984 Test RE 0.0007202384884755915\n",
      "145 Train Loss 0.011904185 Test MSE 0.0019057974435710175 Test RE 0.00048639905486398113\n",
      "146 Train Loss 0.011820883 Test MSE 0.0014267716725773834 Test RE 0.0004208541280724417\n",
      "147 Train Loss 0.011736174 Test MSE 0.0014975633934364708 Test RE 0.0004311684356510646\n",
      "148 Train Loss 0.011668052 Test MSE 0.0010732125569136216 Test RE 0.0003650036648538935\n",
      "149 Train Loss 0.011599732 Test MSE 0.0018720833409841887 Test RE 0.0004820775882972143\n",
      "150 Train Loss 0.011555259 Test MSE 0.00215900189383865 Test RE 0.0005177032173635514\n",
      "151 Train Loss 0.011501436 Test MSE 0.0012594508337510518 Test RE 0.0003954075535497121\n",
      "152 Train Loss 0.011417498 Test MSE 0.001502949272746694 Test RE 0.00043194307296779956\n",
      "153 Train Loss 0.011313125 Test MSE 0.0015098836842175095 Test RE 0.0004329383906589543\n",
      "154 Train Loss 0.011163311 Test MSE 0.0014689359759302455 Test RE 0.000427027443074518\n",
      "155 Train Loss 0.011057894 Test MSE 0.0020819715916108508 Test RE 0.0005083838573419253\n",
      "156 Train Loss 0.010885384 Test MSE 0.001873311500993394 Test RE 0.00048223569326218623\n",
      "157 Train Loss 0.010718709 Test MSE 0.0012929165031325867 Test RE 0.0004006264251841085\n",
      "158 Train Loss 0.01060975 Test MSE 0.0014034426363767 Test RE 0.00041739927025488314\n",
      "159 Train Loss 0.010487621 Test MSE 0.0013196036606546975 Test RE 0.00040473998212232323\n",
      "160 Train Loss 0.010404191 Test MSE 0.0017437107917607484 Test RE 0.0004652555687946851\n",
      "161 Train Loss 0.010348786 Test MSE 0.001623274450970245 Test RE 0.000448900747913584\n",
      "162 Train Loss 0.010280969 Test MSE 0.0010880068247538807 Test RE 0.0003675108473586589\n",
      "163 Train Loss 0.010167892 Test MSE 0.0012909918882912164 Test RE 0.0004003281310872303\n",
      "164 Train Loss 0.010122148 Test MSE 0.0011060255307890107 Test RE 0.0003705415623850515\n",
      "165 Train Loss 0.01003984 Test MSE 0.0022571358022361585 Test RE 0.0005293381523044016\n",
      "166 Train Loss 0.009969368 Test MSE 0.002762831790572488 Test RE 0.0005856411363415374\n",
      "167 Train Loss 0.009845857 Test MSE 0.0026350524487998087 Test RE 0.0005719380420897862\n",
      "168 Train Loss 0.009791172 Test MSE 0.0020963414209007416 Test RE 0.0005101352806286736\n",
      "169 Train Loss 0.009739589 Test MSE 0.0011076081052645375 Test RE 0.0003708065653094684\n",
      "170 Train Loss 0.009665267 Test MSE 0.0009535459158455716 Test RE 0.00034405284825909097\n",
      "171 Train Loss 0.009554105 Test MSE 0.0018135836862515405 Test RE 0.00047448572662698836\n",
      "172 Train Loss 0.009462389 Test MSE 0.0011988057436452546 Test RE 0.00038577027432564875\n",
      "173 Train Loss 0.009385226 Test MSE 0.0017414868057623743 Test RE 0.00046495877301419\n",
      "174 Train Loss 0.009276906 Test MSE 0.0014950251886181628 Test RE 0.0004308028892180162\n",
      "175 Train Loss 0.009176076 Test MSE 0.0009411458293721841 Test RE 0.00034180846452746264\n",
      "176 Train Loss 0.009145206 Test MSE 0.001040044605204931 Test RE 0.0003593191264303316\n",
      "177 Train Loss 0.009087701 Test MSE 0.001174303239563292 Test RE 0.0003818075234495385\n",
      "178 Train Loss 0.0090437485 Test MSE 0.00104534812138615 Test RE 0.000360234102375644\n",
      "179 Train Loss 0.009010462 Test MSE 0.0009625450488914206 Test RE 0.0003456725429666232\n",
      "180 Train Loss 0.008978217 Test MSE 0.0008786652436279055 Test RE 0.00033026768042942805\n",
      "181 Train Loss 0.008876752 Test MSE 0.0009140319474077902 Test RE 0.0003368488275890793\n",
      "182 Train Loss 0.008771706 Test MSE 0.001091747953361013 Test RE 0.00036814215105660355\n",
      "183 Train Loss 0.00869936 Test MSE 0.0009428636267156814 Test RE 0.0003421202599875363\n",
      "184 Train Loss 0.0086366 Test MSE 0.0007905040686546444 Test RE 0.0003132610556573535\n",
      "185 Train Loss 0.008596113 Test MSE 0.0008059026824505481 Test RE 0.00031629742260011825\n",
      "186 Train Loss 0.008526223 Test MSE 0.0007917919083118062 Test RE 0.00031351612444795055\n",
      "187 Train Loss 0.008475883 Test MSE 0.0007514546190589338 Test RE 0.00030542580802937237\n",
      "188 Train Loss 0.008429345 Test MSE 0.0007586941673790462 Test RE 0.0003068935246230083\n",
      "189 Train Loss 0.00838257 Test MSE 0.0008498186186773556 Test RE 0.00032480108643802974\n",
      "190 Train Loss 0.008337827 Test MSE 0.001117835410754598 Test RE 0.0003725145872361212\n",
      "191 Train Loss 0.0082862275 Test MSE 0.001518266916535054 Test RE 0.00043413861529507236\n",
      "192 Train Loss 0.008176904 Test MSE 0.0010624978558088354 Test RE 0.0003631770390058918\n",
      "193 Train Loss 0.00804818 Test MSE 0.0027479512936945754 Test RE 0.0005840618876523329\n",
      "194 Train Loss 0.00797054 Test MSE 0.002287305795613364 Test RE 0.0005328641067644362\n",
      "195 Train Loss 0.007885429 Test MSE 0.0010975898158355065 Test RE 0.00036912578766853997\n",
      "196 Train Loss 0.007817005 Test MSE 0.000835365524009008 Test RE 0.00032202725214377397\n",
      "197 Train Loss 0.00769897 Test MSE 0.003283404389886877 Test RE 0.0006384347822925553\n",
      "198 Train Loss 0.007523936 Test MSE 0.003116171967592564 Test RE 0.0006219637316444123\n",
      "199 Train Loss 0.0072466293 Test MSE 0.004570974681357721 Test RE 0.0007532840299124329\n",
      "Training time: 30.96\n",
      "Training time: 30.96\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 166.91374 Test MSE 5351.736644371612 Test RE 0.8150827824054204\n",
      "1 Train Loss 104.7643 Test MSE 1482.6319021000481 Test RE 0.42901356333636453\n",
      "2 Train Loss 58.66524 Test MSE 462.23525018254276 Test RE 0.23954432319767816\n",
      "3 Train Loss 46.53814 Test MSE 304.3366135480488 Test RE 0.19437099337915742\n",
      "4 Train Loss 36.601463 Test MSE 182.73968904260508 Test RE 0.15061589024743113\n",
      "5 Train Loss 29.695696 Test MSE 125.2734766525243 Test RE 0.12470501459353571\n",
      "6 Train Loss 23.853207 Test MSE 284.6256891091035 Test RE 0.18797123720074563\n",
      "7 Train Loss 19.637764 Test MSE 64.35598914747936 Test RE 0.08938174666657267\n",
      "8 Train Loss 16.78374 Test MSE 43.43538961472855 Test RE 0.07343045395813214\n",
      "9 Train Loss 13.589016 Test MSE 32.6653376051867 Test RE 0.06367924826452213\n",
      "10 Train Loss 12.299433 Test MSE 24.22341037372229 Test RE 0.05483678637470855\n",
      "11 Train Loss 10.923785 Test MSE 18.818278515112162 Test RE 0.04833306075740143\n",
      "12 Train Loss 8.654172 Test MSE 23.823065956274103 Test RE 0.05438174998461138\n",
      "13 Train Loss 7.6528153 Test MSE 16.614329555669567 Test RE 0.04541462816314028\n",
      "14 Train Loss 6.5550213 Test MSE 9.852853046770417 Test RE 0.03497319867807411\n",
      "15 Train Loss 5.4237804 Test MSE 13.421633792946432 Test RE 0.04081850034241801\n",
      "16 Train Loss 4.4094925 Test MSE 11.034396067559095 Test RE 0.037010813972358274\n",
      "17 Train Loss 3.5907152 Test MSE 13.747244595600828 Test RE 0.04131066463569219\n",
      "18 Train Loss 2.6435986 Test MSE 4.112809896313727 Test RE 0.022595589192458746\n",
      "19 Train Loss 2.114031 Test MSE 4.286283074135869 Test RE 0.02306719446939811\n",
      "20 Train Loss 1.5376209 Test MSE 4.653834658736491 Test RE 0.024035868933687518\n",
      "21 Train Loss 1.2383338 Test MSE 0.7615721191385348 Test RE 0.009723214586191917\n",
      "22 Train Loss 0.9142367 Test MSE 5.761112113361484 Test RE 0.026742839370392107\n",
      "23 Train Loss 0.6830314 Test MSE 1.5477060196239603 Test RE 0.013861128453235954\n",
      "24 Train Loss 0.5973603 Test MSE 0.7949871292314699 Test RE 0.009934234356036005\n",
      "25 Train Loss 0.40562922 Test MSE 0.6220059868199305 Test RE 0.008787222732741011\n",
      "26 Train Loss 0.27815536 Test MSE 0.19999577915291242 Test RE 0.004982700316543321\n",
      "27 Train Loss 0.21550448 Test MSE 0.09242321961089958 Test RE 0.0033872316523814038\n",
      "28 Train Loss 0.17785846 Test MSE 0.24002950325042316 Test RE 0.005458667805246186\n",
      "29 Train Loss 0.15065065 Test MSE 0.04623490453612023 Test RE 0.002395738074392688\n",
      "30 Train Loss 0.12860416 Test MSE 0.3756047493169496 Test RE 0.006828414719888327\n",
      "31 Train Loss 0.09031629 Test MSE 0.06799613216496152 Test RE 0.0029053366125147673\n",
      "32 Train Loss 0.07500151 Test MSE 0.03323796148700342 Test RE 0.0020312881851755724\n",
      "33 Train Loss 0.07006718 Test MSE 0.009290066239306139 Test RE 0.0010738997904596002\n",
      "34 Train Loss 0.06387645 Test MSE 0.008770680561045609 Test RE 0.0010434484523123874\n",
      "35 Train Loss 0.054067478 Test MSE 0.04304746275184799 Test RE 0.0023116822152768643\n",
      "36 Train Loss 0.04823828 Test MSE 0.017362977276076563 Test RE 0.001468136549999417\n",
      "37 Train Loss 0.044543058 Test MSE 0.017139465384431174 Test RE 0.0014586563533625408\n",
      "38 Train Loss 0.03905939 Test MSE 0.04039922024422204 Test RE 0.002239447284037523\n",
      "39 Train Loss 0.034738038 Test MSE 0.02191810904730173 Test RE 0.0016495135749825664\n",
      "40 Train Loss 0.031233205 Test MSE 0.04720633350032815 Test RE 0.002420775344717616\n",
      "41 Train Loss 0.027870985 Test MSE 0.01915966305993937 Test RE 0.0015422269428590446\n",
      "42 Train Loss 0.024707722 Test MSE 0.004189228888849796 Test RE 0.0007211430082601578\n",
      "43 Train Loss 0.022015583 Test MSE 0.005201884435409218 Test RE 0.0008035903303377405\n",
      "44 Train Loss 0.020127434 Test MSE 0.013836071083356842 Test RE 0.001310571575969325\n",
      "45 Train Loss 0.018860748 Test MSE 0.006656042883651153 Test RE 0.0009089969123652221\n",
      "46 Train Loss 0.018145945 Test MSE 0.0020066800412507435 Test RE 0.000499106720743071\n",
      "47 Train Loss 0.017720493 Test MSE 0.006994215229634312 Test RE 0.0009318024512817211\n",
      "48 Train Loss 0.017275115 Test MSE 0.009332995859082944 Test RE 0.0010763781889693588\n",
      "49 Train Loss 0.016335495 Test MSE 0.004262275859626989 Test RE 0.0007274030697736622\n",
      "50 Train Loss 0.015140021 Test MSE 0.0034385404257079514 Test RE 0.0006533432666544467\n",
      "51 Train Loss 0.014744312 Test MSE 0.005345689654026442 Test RE 0.0008146221664309122\n",
      "52 Train Loss 0.014501414 Test MSE 0.004563816273846802 Test RE 0.0007526939558848648\n",
      "53 Train Loss 0.0138774235 Test MSE 0.0035357029439574213 Test RE 0.0006625096940945668\n",
      "54 Train Loss 0.012853404 Test MSE 0.0022012446138751428 Test RE 0.0005227433365149334\n",
      "55 Train Loss 0.012077104 Test MSE 0.002563336991541001 Test RE 0.0005641014353539541\n",
      "56 Train Loss 0.011527197 Test MSE 0.00135902435663191 Test RE 0.0004107409209138188\n",
      "57 Train Loss 0.010518938 Test MSE 0.003267288592822835 Test RE 0.0006368660535627348\n",
      "58 Train Loss 0.009370799 Test MSE 0.009109563924676843 Test RE 0.0010634158919594734\n",
      "59 Train Loss 0.008124073 Test MSE 0.0020902430968688483 Test RE 0.0005093927403189114\n",
      "60 Train Loss 0.0077904616 Test MSE 0.0012950340728323652 Test RE 0.00040095436875092874\n",
      "61 Train Loss 0.007559393 Test MSE 0.0015750138032755313 Test RE 0.0004421773926364256\n",
      "62 Train Loss 0.0072437157 Test MSE 0.0007815519980744138 Test RE 0.00031148224141895495\n",
      "63 Train Loss 0.006548713 Test MSE 0.0009490267409053126 Test RE 0.00034323658887183646\n",
      "64 Train Loss 0.006234872 Test MSE 0.0006158409242463305 Test RE 0.00027649585500992763\n",
      "65 Train Loss 0.005965999 Test MSE 0.0026873704733291 Test RE 0.0005775879484086444\n",
      "66 Train Loss 0.0056902184 Test MSE 0.004202321964079316 Test RE 0.0007222690643782354\n",
      "67 Train Loss 0.005510729 Test MSE 0.0011184656301402953 Test RE 0.0003726195815993312\n",
      "68 Train Loss 0.0054160208 Test MSE 0.0004186881686435256 Test RE 0.00022798154721203736\n",
      "69 Train Loss 0.005380997 Test MSE 0.0005488655480036515 Test RE 0.0002610281455320743\n",
      "70 Train Loss 0.0053443247 Test MSE 0.00040726279710312437 Test RE 0.00022484939440085925\n",
      "71 Train Loss 0.005283093 Test MSE 0.00041160614981140836 Test RE 0.00022604519503403232\n",
      "72 Train Loss 0.005167455 Test MSE 0.0007395427532590794 Test RE 0.00030299537212390143\n",
      "73 Train Loss 0.005051861 Test MSE 0.0009880781943141497 Test RE 0.0003502273111468622\n",
      "74 Train Loss 0.0049779215 Test MSE 0.0009981862712888485 Test RE 0.0003520141721235626\n",
      "75 Train Loss 0.0048268535 Test MSE 0.0008866835739973384 Test RE 0.00033177120024055435\n",
      "76 Train Loss 0.004780199 Test MSE 0.0006975024587225374 Test RE 0.0002942572851301811\n",
      "77 Train Loss 0.0046526245 Test MSE 0.00032608939917940145 Test RE 0.0002011975521617576\n",
      "78 Train Loss 0.0044585457 Test MSE 0.000503175518988132 Test RE 0.0002499275339234157\n",
      "79 Train Loss 0.0043534273 Test MSE 0.0010113053508051556 Test RE 0.0003543198675218917\n",
      "80 Train Loss 0.0042791017 Test MSE 0.00028101568615371394 Test RE 0.0001867753821601677\n",
      "81 Train Loss 0.004184791 Test MSE 0.0005821648027465379 Test RE 0.000268829747875371\n",
      "82 Train Loss 0.004105894 Test MSE 0.000366208128386361 Test RE 0.0002132152884291911\n",
      "83 Train Loss 0.004013633 Test MSE 0.00031403539095939333 Test RE 0.00019744386766197493\n",
      "84 Train Loss 0.003954788 Test MSE 0.0005479902080078833 Test RE 0.00026081991644151867\n",
      "85 Train Loss 0.0038763597 Test MSE 0.00034137796786936123 Test RE 0.00020586006082739676\n",
      "86 Train Loss 0.0037700764 Test MSE 0.0003975424423714809 Test RE 0.00022214989030494602\n",
      "87 Train Loss 0.003688795 Test MSE 0.0005076926334214306 Test RE 0.0002510468539502404\n",
      "88 Train Loss 0.0036422755 Test MSE 0.0004469906066602376 Test RE 0.0002355610875345219\n",
      "89 Train Loss 0.0036022172 Test MSE 0.00040157354609978373 Test RE 0.00022327335595782841\n",
      "90 Train Loss 0.0035409553 Test MSE 0.0004094983380163802 Test RE 0.0002254656698283416\n",
      "91 Train Loss 0.0034316606 Test MSE 0.0008495412033901555 Test RE 0.00032474806798246653\n",
      "92 Train Loss 0.0032975632 Test MSE 0.00027312897052427307 Test RE 0.00018413580137454413\n",
      "93 Train Loss 0.0031599551 Test MSE 0.00042854857707141285 Test RE 0.0002306504897315955\n",
      "94 Train Loss 0.0030916866 Test MSE 0.0002028556859120118 Test RE 0.00015868940981488434\n",
      "95 Train Loss 0.003056521 Test MSE 0.0001558340815725166 Test RE 0.00013908669107970037\n",
      "96 Train Loss 0.0030178644 Test MSE 0.0001563471543595361 Test RE 0.0001393154695142519\n",
      "97 Train Loss 0.0029577045 Test MSE 0.0001840522563746828 Test RE 0.0001511558380097573\n",
      "98 Train Loss 0.0029149207 Test MSE 0.00017602189708050043 Test RE 0.0001478215327052541\n",
      "99 Train Loss 0.0028867866 Test MSE 0.00018436708060560444 Test RE 0.00015128505997360602\n",
      "100 Train Loss 0.0028485165 Test MSE 0.00018060632504740903 Test RE 0.00014973413919425188\n",
      "101 Train Loss 0.0028063601 Test MSE 0.00014663207278733522 Test RE 0.00013491767212444556\n",
      "102 Train Loss 0.0028057839 Test MSE 0.00014775712647498214 Test RE 0.00013543426981311502\n",
      "103 Train Loss 0.0027443108 Test MSE 0.0003035268190430123 Test RE 0.00019411222496323863\n",
      "104 Train Loss 0.002650943 Test MSE 0.00023428656316812742 Test RE 0.00017054070074502836\n",
      "105 Train Loss 0.002588953 Test MSE 0.00020937584611634738 Test RE 0.00016121952678323538\n",
      "106 Train Loss 0.0025077725 Test MSE 0.0010760097504749044 Test RE 0.00036547902338949855\n",
      "107 Train Loss 0.0023961756 Test MSE 0.0005683838070093563 Test RE 0.00026562882466455004\n",
      "108 Train Loss 0.002314987 Test MSE 0.00023481506220691842 Test RE 0.00017073294357735413\n",
      "109 Train Loss 0.0021885773 Test MSE 0.0006489280542901298 Test RE 0.0002838262944909341\n",
      "110 Train Loss 0.002045941 Test MSE 0.00015305991085193865 Test RE 0.0001378431154972745\n",
      "111 Train Loss 0.00196711 Test MSE 0.0004117876552576501 Test RE 0.000226095028973524\n",
      "112 Train Loss 0.0019258528 Test MSE 0.00019836870759806817 Test RE 0.00015692456529754726\n",
      "113 Train Loss 0.0018876042 Test MSE 0.00025979094595204194 Test RE 0.00017958346960044895\n",
      "114 Train Loss 0.0018103595 Test MSE 0.0005391497635490099 Test RE 0.0002587075254224363\n",
      "115 Train Loss 0.0017223454 Test MSE 0.0002178602835167911 Test RE 0.00016445359987340318\n",
      "116 Train Loss 0.0016564694 Test MSE 0.0007880274472905632 Test RE 0.00031276995278466585\n",
      "117 Train Loss 0.0016131924 Test MSE 0.0007928133472192198 Test RE 0.0003137182825853569\n",
      "118 Train Loss 0.0015624855 Test MSE 0.00024957293727308487 Test RE 0.00017601638510293338\n",
      "119 Train Loss 0.0015171462 Test MSE 0.0003043685238030138 Test RE 0.00019438118319054548\n",
      "120 Train Loss 0.0014904104 Test MSE 0.00024695579378536316 Test RE 0.00017509105603542236\n",
      "121 Train Loss 0.0014639142 Test MSE 8.081455227406573e-05 Test RE 0.00010016111210968791\n",
      "122 Train Loss 0.0014471426 Test MSE 7.959959216738295e-05 Test RE 9.940535340659144e-05\n",
      "123 Train Loss 0.0014351238 Test MSE 0.0001651348154553483 Test RE 0.0001431771373769493\n",
      "124 Train Loss 0.0014101319 Test MSE 0.0002990548250563459 Test RE 0.00019267694835654287\n",
      "125 Train Loss 0.0013743293 Test MSE 0.00015460203377919723 Test RE 0.00013853577982511045\n",
      "126 Train Loss 0.0013525026 Test MSE 6.81898086955611e-05 Test RE 9.200556324718865e-05\n",
      "127 Train Loss 0.0013128894 Test MSE 0.0002475650467562779 Test RE 0.0001753069024272241\n",
      "128 Train Loss 0.0012843655 Test MSE 9.83949689137384e-05 Test RE 0.00011051998028578976\n",
      "129 Train Loss 0.0012524135 Test MSE 0.00020120799439207184 Test RE 0.00015804361990339567\n",
      "130 Train Loss 0.0012155692 Test MSE 0.0002577715177591901 Test RE 0.0001788841314666258\n",
      "131 Train Loss 0.0011925652 Test MSE 9.240419861968867e-05 Test RE 0.00010710264710806247\n",
      "132 Train Loss 0.0011817602 Test MSE 5.686440552615735e-05 Test RE 8.401843814593724e-05\n",
      "133 Train Loss 0.0011810033 Test MSE 5.569271015406886e-05 Test RE 8.314832951974997e-05\n",
      "134 Train Loss 0.0011640642 Test MSE 6.306712896158055e-05 Test RE 8.848219398450986e-05\n",
      "135 Train Loss 0.0011467102 Test MSE 6.22808072463425e-05 Test RE 8.792886532330597e-05\n",
      "136 Train Loss 0.0011320217 Test MSE 6.10817139719474e-05 Test RE 8.707830360470428e-05\n",
      "137 Train Loss 0.0011077091 Test MSE 0.00018878014701177132 Test RE 0.00015308495551394864\n",
      "138 Train Loss 0.0010957298 Test MSE 0.00013874932641823613 Test RE 0.00013124107953983813\n",
      "139 Train Loss 0.001094987 Test MSE 0.00012794048010715455 Test RE 0.00012602547424587\n",
      "140 Train Loss 0.0010917435 Test MSE 9.262343220587697e-05 Test RE 0.0001072296250327642\n",
      "141 Train Loss 0.0010819782 Test MSE 5.049308908387822e-05 Test RE 7.917176425409773e-05\n",
      "142 Train Loss 0.001081372 Test MSE 5.103165386858053e-05 Test RE 7.95928716660626e-05\n",
      "143 Train Loss 0.0010690544 Test MSE 5.511650238351666e-05 Test RE 8.271707664919373e-05\n",
      "144 Train Loss 0.0010534198 Test MSE 9.486848926030382e-05 Test RE 0.00010852138922296626\n",
      "145 Train Loss 0.0010258137 Test MSE 0.0003593845355544769 Test RE 0.0002112195169204288\n",
      "146 Train Loss 0.0010036419 Test MSE 0.0002356089141804087 Test RE 0.00017102130311040246\n",
      "147 Train Loss 0.0009855884 Test MSE 0.00013969963192072367 Test RE 0.0001316897530526785\n",
      "148 Train Loss 0.0009588329 Test MSE 5.5939510983036246e-05 Test RE 8.333236071677387e-05\n",
      "149 Train Loss 0.00092675904 Test MSE 0.00042626348279370586 Test RE 0.00023003473388395166\n",
      "150 Train Loss 0.0008883312 Test MSE 0.0003559341731509315 Test RE 0.00021020313825903826\n",
      "151 Train Loss 0.0008596432 Test MSE 8.676157491957169e-05 Test RE 0.00010378105129909667\n",
      "152 Train Loss 0.00083003764 Test MSE 0.00011055003166065625 Test RE 0.00011714770583412542\n",
      "153 Train Loss 0.0008198869 Test MSE 8.791887922928003e-05 Test RE 0.0001044709211278902\n",
      "154 Train Loss 0.0008195405 Test MSE 8.741733966561991e-05 Test RE 0.00010417251401003703\n",
      "155 Train Loss 0.00081772957 Test MSE 9.498888663925826e-05 Test RE 0.00010859022950989246\n",
      "156 Train Loss 0.00081717124 Test MSE 9.240408061881694e-05 Test RE 0.00010710257872258894\n",
      "157 Train Loss 0.00081037404 Test MSE 4.276052747746716e-05 Test RE 7.285777081754194e-05\n",
      "158 Train Loss 0.00080973847 Test MSE 4.544728474403976e-05 Test RE 7.511182652348319e-05\n",
      "159 Train Loss 0.0008089524 Test MSE 5.163216368208663e-05 Test RE 8.00598025672485e-05\n",
      "160 Train Loss 0.00080824445 Test MSE 5.789548224752278e-05 Test RE 8.477673571083564e-05\n",
      "161 Train Loss 0.00080740167 Test MSE 6.575250703050374e-05 Test RE 9.03463289520146e-05\n",
      "162 Train Loss 0.00080461655 Test MSE 9.120272639327105e-05 Test RE 0.00010640407557111614\n",
      "163 Train Loss 0.0008037969 Test MSE 9.23622086319623e-05 Test RE 0.00010707830974068276\n",
      "164 Train Loss 0.000803738 Test MSE 9.221136352315155e-05 Test RE 0.0001069908343552602\n",
      "165 Train Loss 0.00080337666 Test MSE 8.869726156302518e-05 Test RE 0.00010493236425741297\n",
      "166 Train Loss 0.00080239295 Test MSE 7.678238414435712e-05 Test RE 9.763041804177214e-05\n",
      "167 Train Loss 0.00080070226 Test MSE 6.344916164047365e-05 Test RE 8.874978229549578e-05\n",
      "168 Train Loss 0.0008004241 Test MSE 6.293331464753023e-05 Test RE 8.838827444579867e-05\n",
      "169 Train Loss 0.00079823454 Test MSE 7.169574621529832e-05 Test RE 9.434112416795182e-05\n",
      "170 Train Loss 0.00079765526 Test MSE 7.14545596654954e-05 Test RE 9.418230736348555e-05\n",
      "171 Train Loss 0.00079579133 Test MSE 6.50696909907453e-05 Test RE 8.987599771105225e-05\n",
      "172 Train Loss 0.00079486094 Test MSE 6.178268749145988e-05 Test RE 8.75765334036414e-05\n",
      "173 Train Loss 0.00078700983 Test MSE 3.9709628967870905e-05 Test RE 7.021053251075796e-05\n",
      "174 Train Loss 0.00078633614 Test MSE 3.983398461352407e-05 Test RE 7.032038308602501e-05\n",
      "175 Train Loss 0.00077566074 Test MSE 4.0144703827747075e-05 Test RE 7.059411229657003e-05\n",
      "176 Train Loss 0.0007704187 Test MSE 3.961250464218451e-05 Test RE 7.012461726062579e-05\n",
      "177 Train Loss 0.000756097 Test MSE 8.1597945407097e-05 Test RE 0.00010064540785681402\n",
      "178 Train Loss 0.00074652355 Test MSE 0.00013925288128972322 Test RE 0.00013147901666610203\n",
      "179 Train Loss 0.0007395566 Test MSE 0.00016092665939618158 Test RE 0.00014134106206831532\n",
      "180 Train Loss 0.000738923 Test MSE 0.0001508438355449142 Test RE 0.00013684159756721797\n",
      "181 Train Loss 0.00073521904 Test MSE 0.00010043410619608947 Test RE 0.00011165931566511717\n",
      "182 Train Loss 0.00073022477 Test MSE 5.876004526867206e-05 Test RE 8.540738270701285e-05\n",
      "183 Train Loss 0.00072378793 Test MSE 4.3977080030363564e-05 Test RE 7.388691728954759e-05\n",
      "184 Train Loss 0.00072073477 Test MSE 4.269916632148727e-05 Test RE 7.280547677101264e-05\n",
      "185 Train Loss 0.0007204441 Test MSE 4.250932263234659e-05 Test RE 7.264344717214988e-05\n",
      "186 Train Loss 0.0007196744 Test MSE 4.265558538227844e-05 Test RE 7.276831280087284e-05\n",
      "187 Train Loss 0.0007189958 Test MSE 4.2612666406667856e-05 Test RE 7.27316947657859e-05\n",
      "188 Train Loss 0.0007180653 Test MSE 3.901818078795515e-05 Test RE 6.959657389921204e-05\n",
      "189 Train Loss 0.0007172248 Test MSE 3.907577643415817e-05 Test RE 6.964792151988409e-05\n",
      "190 Train Loss 0.00071652053 Test MSE 3.949312171421274e-05 Test RE 7.001886783492868e-05\n",
      "191 Train Loss 0.0007106491 Test MSE 9.302802375559034e-05 Test RE 0.00010746356651673672\n",
      "192 Train Loss 0.0007023997 Test MSE 0.00014934821032729643 Test RE 0.0001361615114792378\n",
      "193 Train Loss 0.000684524 Test MSE 9.124434346834381e-05 Test RE 0.00010642834963355952\n",
      "194 Train Loss 0.00067460956 Test MSE 5.5110047397361697e-05 Test RE 8.271223278941089e-05\n",
      "195 Train Loss 0.0006745199 Test MSE 5.5219119647559256e-05 Test RE 8.279404318840613e-05\n",
      "196 Train Loss 0.00066624786 Test MSE 6.072175240084614e-05 Test RE 8.682134323801376e-05\n",
      "197 Train Loss 0.00065209164 Test MSE 3.8110630596998046e-05 Test RE 6.878241494365046e-05\n",
      "198 Train Loss 0.0006361618 Test MSE 3.4771277312009954e-05 Test RE 6.569989487785084e-05\n",
      "199 Train Loss 0.00062814826 Test MSE 3.987649090691747e-05 Test RE 7.035789203589935e-05\n",
      "Training time: 29.85\n",
      "Training time: 29.85\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.01, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1636190690>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(y_true)\n",
    "plt.plot(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "aefc6a9f-3ad4-417a-b9f7-438009e620af"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29355/2016390461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_stan_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
