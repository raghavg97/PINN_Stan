{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "3df10486-4078-44cd-95da-12a75fb13c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "515a82ba-2a23-4124-c9e1-230f67f43912"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "d35a8c58-7c75-4550-d489-9565724f04e6"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "7b44eee8-32ab-4621-ca04-81e30b53601d"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = extent*np.sin(x)/2 + np.square(x)/2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "level = \"medium\"\n",
    "label = \"1D_FODE_tanhAW_\" +level\n",
    "extent = 20.0\n",
    "\n",
    "x = np.linspace(extent,-1.0*extent,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.m_lambda = nn.Sigmoid()    \n",
    "        self.lambdas_bc1 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc1.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+1,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "             \n",
    "      \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y,lambda_ind):\n",
    "        m = self.m_lambda(self.lambdas_bc1)\n",
    "        u_pred = self.forward(x)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - y))/2.0\n",
    "                \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat,lambda_ind):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - extent*torch.cos(g)/2.0 - g\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        #loss_f  = torch.sum(m*(torch.square(f)))/2.0\n",
    "        loss_f = (N_f+1)*self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "        \n",
    "        # loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = False\n",
    "        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = True        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return -1.0*loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(20):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "   \n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fAcpqTqePPt9"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    x_coll_np_array = colloc_pts(N_f,123)\n",
    "    x_coll = torch.from_numpy(x_coll_np_array).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(i)        \n",
    "    \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "3645d237-1d2a-45c9-8d9f-de486f1ca919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "0\n",
      "0 Train Loss 14994072000.0 Test MSE 12585.136919664375 Test RE 1.2499231873854764\n",
      "1 Train Loss 9261689000.0 Test MSE 17850.582989340866 Test RE 1.4886087059694229\n",
      "2 Train Loss 8696210000.0 Test MSE 24826.181149323886 Test RE 1.7555342721901297\n",
      "3 Train Loss 5676323000.0 Test MSE 36440.460528003445 Test RE 2.1268961287949764\n",
      "4 Train Loss 4353945600.0 Test MSE 41218.789556577954 Test RE 2.262048797241448\n",
      "5 Train Loss 3566938400.0 Test MSE 51596.997723972025 Test RE 2.5308509469321905\n",
      "6 Train Loss 3206091500.0 Test MSE 61522.422667091974 Test RE 2.763573801002991\n",
      "7 Train Loss 2965290000.0 Test MSE 73153.50363629007 Test RE 3.013505010831017\n",
      "8 Train Loss 2783950800.0 Test MSE 82087.75138257038 Test RE 3.192225275783213\n",
      "9 Train Loss 2606916400.0 Test MSE 90668.04478784521 Test RE 3.354914661761909\n",
      "10 Train Loss 2556056000.0 Test MSE 89861.77216209071 Test RE 3.339964431505351\n",
      "11 Train Loss 2152283400.0 Test MSE 86162.98245811902 Test RE 3.2705042210810626\n",
      "12 Train Loss 1459848000.0 Test MSE 100695.96634779236 Test RE 3.5355777274389033\n",
      "13 Train Loss 1263309200.0 Test MSE 106297.18189975224 Test RE 3.6325803368951166\n",
      "14 Train Loss 1094599800.0 Test MSE 114198.23131699726 Test RE 3.765165244873809\n",
      "15 Train Loss 981022300.0 Test MSE 121672.90276514103 Test RE 3.8864339113627464\n",
      "16 Train Loss 896984900.0 Test MSE 120237.69389729634 Test RE 3.863444443003809\n",
      "17 Train Loss 801709630.0 Test MSE 115753.98096988868 Test RE 3.7907253570860786\n",
      "18 Train Loss 691051700.0 Test MSE 129656.60345867972 Test RE 4.011914533275062\n",
      "19 Train Loss 623204000.0 Test MSE 138281.5335594167 Test RE 4.143205227754623\n",
      "20 Train Loss 542233000.0 Test MSE 140174.59809655466 Test RE 4.171468919686722\n",
      "21 Train Loss 486453900.0 Test MSE 142122.8398460934 Test RE 4.200357840445218\n",
      "22 Train Loss 456364600.0 Test MSE 138257.773146681 Test RE 4.142849256523615\n",
      "23 Train Loss 443856320.0 Test MSE 133562.39932555627 Test RE 4.071893948927759\n",
      "24 Train Loss 428512670.0 Test MSE 127759.21563390076 Test RE 3.9824512733377024\n",
      "25 Train Loss 391645220.0 Test MSE 123115.95963728148 Test RE 3.909412791589745\n",
      "26 Train Loss 322524220.0 Test MSE 124021.47638615686 Test RE 3.9237633004984285\n",
      "27 Train Loss 274637200.0 Test MSE 124072.85796610547 Test RE 3.924576015708989\n",
      "28 Train Loss 241004850.0 Test MSE 122319.01984360481 Test RE 3.8967392728710237\n",
      "29 Train Loss 204138830.0 Test MSE 123508.73491779968 Test RE 3.9156439007490267\n",
      "30 Train Loss 162406270.0 Test MSE 124828.43779185682 Test RE 3.9365078339649084\n",
      "31 Train Loss 133315464.0 Test MSE 121478.84390802574 Test RE 3.883333394043685\n",
      "32 Train Loss 112988580.0 Test MSE 119736.3466387735 Test RE 3.8553814533235684\n",
      "33 Train Loss 102122910.0 Test MSE 119706.18169843452 Test RE 3.8548957834315463\n",
      "34 Train Loss 89105100.0 Test MSE 120743.3865581476 Test RE 3.87156030718834\n",
      "35 Train Loss 77751896.0 Test MSE 120370.64749220017 Test RE 3.865579866987074\n",
      "36 Train Loss 65899080.0 Test MSE 124334.38946406118 Test RE 3.928710118632712\n",
      "37 Train Loss 54911496.0 Test MSE 129060.23873736043 Test RE 4.002677356894212\n",
      "38 Train Loss 43970204.0 Test MSE 131697.96569728895 Test RE 4.043373734750896\n",
      "39 Train Loss 36811850.0 Test MSE 133534.1541172679 Test RE 4.071463372753346\n",
      "40 Train Loss 27900596.0 Test MSE 136440.70445018684 Test RE 4.115535278837821\n",
      "41 Train Loss 22417594.0 Test MSE 139385.95084590898 Test RE 4.159717654204046\n",
      "42 Train Loss 18738800.0 Test MSE 141417.0016870396 Test RE 4.18991453986033\n",
      "43 Train Loss 16781812.0 Test MSE 142296.10146142173 Test RE 4.202917383715736\n",
      "44 Train Loss 13739060.0 Test MSE 145846.84566986305 Test RE 4.255032413346304\n",
      "45 Train Loss 11348772.0 Test MSE 149042.95382898676 Test RE 4.301402437757653\n",
      "46 Train Loss 10053356.0 Test MSE 151020.1477033846 Test RE 4.329839496225787\n",
      "47 Train Loss 8395971.0 Test MSE 150709.58991092205 Test RE 4.325385264720197\n",
      "48 Train Loss 6449756.0 Test MSE 149395.52661161943 Test RE 4.306487084710682\n",
      "49 Train Loss 5670588.0 Test MSE 148241.41357004803 Test RE 4.289820557957237\n",
      "50 Train Loss 4942417.5 Test MSE 145669.21071791556 Test RE 4.252440404034506\n",
      "51 Train Loss 4485640.0 Test MSE 144975.46793373415 Test RE 4.242302295068967\n",
      "52 Train Loss 4097864.2 Test MSE 145506.71537639704 Test RE 4.25006792406914\n",
      "53 Train Loss 3735379.0 Test MSE 144705.19603201235 Test RE 4.238346074053063\n",
      "54 Train Loss 3378431.5 Test MSE 145192.79585581325 Test RE 4.245480851725975\n",
      "55 Train Loss 3150362.8 Test MSE 146103.17679607458 Test RE 4.258769958821638\n",
      "56 Train Loss 2989597.2 Test MSE 145003.35800503206 Test RE 4.2427103379787905\n",
      "57 Train Loss 2872888.8 Test MSE 144386.78230500617 Test RE 4.23368041303626\n",
      "58 Train Loss 2751798.5 Test MSE 143563.0907748964 Test RE 4.221587081664219\n",
      "59 Train Loss 2630402.0 Test MSE 141367.7771587643 Test RE 4.189185262209784\n",
      "60 Train Loss 2504281.8 Test MSE 138888.53598013427 Test RE 4.152288804138498\n",
      "61 Train Loss 2460873.2 Test MSE 138405.04409567165 Test RE 4.145055132331432\n",
      "62 Train Loss 2413294.5 Test MSE 138282.37062874818 Test RE 4.14321776791343\n",
      "63 Train Loss 2354525.8 Test MSE 137670.0330585743 Test RE 4.134034157799111\n",
      "64 Train Loss 2293230.5 Test MSE 137717.8439716369 Test RE 4.134751942148589\n",
      "65 Train Loss 2232015.0 Test MSE 137234.01545240587 Test RE 4.127482473693469\n",
      "66 Train Loss 2194893.5 Test MSE 136479.84235853876 Test RE 4.116125505537107\n",
      "67 Train Loss 2172443.2 Test MSE 136926.52791677433 Test RE 4.122855846964196\n",
      "68 Train Loss 2145080.0 Test MSE 138266.8273887614 Test RE 4.142984908014004\n",
      "69 Train Loss 2131343.0 Test MSE 138177.02126208835 Test RE 4.141639227819198\n",
      "70 Train Loss 2111399.0 Test MSE 137184.41010108165 Test RE 4.126736435173278\n",
      "71 Train Loss 2089794.4 Test MSE 136685.18551031483 Test RE 4.11922083627158\n",
      "72 Train Loss 2077172.1 Test MSE 136669.8300147776 Test RE 4.118989448892196\n",
      "73 Train Loss 2058925.1 Test MSE 135790.59455140933 Test RE 4.1057187620154085\n",
      "74 Train Loss 2023697.6 Test MSE 135631.3713249797 Test RE 4.103310946007963\n",
      "75 Train Loss 1993715.2 Test MSE 136468.18456987612 Test RE 4.115949706880114\n",
      "76 Train Loss 1973262.4 Test MSE 136434.00074454217 Test RE 4.115434173833695\n",
      "77 Train Loss 1928679.9 Test MSE 137626.71671782882 Test RE 4.133383742759026\n",
      "78 Train Loss 1906475.0 Test MSE 137822.6269730473 Test RE 4.13632461177236\n",
      "79 Train Loss 1876896.2 Test MSE 137241.76956314276 Test RE 4.1275990792740185\n",
      "80 Train Loss 1833691.5 Test MSE 137547.9802103363 Test RE 4.1322012152364\n",
      "81 Train Loss 1805404.2 Test MSE 137907.52210711257 Test RE 4.13759834955192\n",
      "82 Train Loss 1765584.2 Test MSE 138563.15516739746 Test RE 4.147422069344285\n",
      "83 Train Loss 1724355.2 Test MSE 139646.59796458372 Test RE 4.163605104928812\n",
      "84 Train Loss 1691761.1 Test MSE 140181.9257296935 Test RE 4.17157795012036\n",
      "85 Train Loss 1655729.8 Test MSE 140921.1574892469 Test RE 4.182562633478449\n",
      "86 Train Loss 1621041.5 Test MSE 141482.03864905992 Test RE 4.190877889339126\n",
      "87 Train Loss 1602067.9 Test MSE 141262.8522796025 Test RE 4.187630341575354\n",
      "88 Train Loss 1587732.9 Test MSE 142236.99361474317 Test RE 4.202044375962002\n",
      "89 Train Loss 1576765.4 Test MSE 142287.63276702518 Test RE 4.202792314398565\n",
      "90 Train Loss 1563300.5 Test MSE 141265.55087482714 Test RE 4.187670340290833\n",
      "91 Train Loss 1549517.1 Test MSE 140448.54386196364 Test RE 4.175543118861094\n",
      "92 Train Loss 1533663.1 Test MSE 139942.3936174895 Test RE 4.1680123902665125\n",
      "93 Train Loss 1508680.9 Test MSE 139909.58817970837 Test RE 4.167523826787482\n",
      "94 Train Loss 1496317.8 Test MSE 140201.84946956206 Test RE 4.171874388054551\n",
      "95 Train Loss 1489518.6 Test MSE 139968.2059781889 Test RE 4.168396767279889\n",
      "96 Train Loss 1484165.8 Test MSE 139735.5580155077 Test RE 4.164931079057744\n",
      "97 Train Loss 1476420.2 Test MSE 139383.96124264156 Test RE 4.1596879660707735\n",
      "98 Train Loss 1465610.0 Test MSE 140000.50731219616 Test RE 4.168877722965528\n",
      "99 Train Loss 1450137.9 Test MSE 142913.8974563132 Test RE 4.212031243111883\n",
      "100 Train Loss 1446055.8 Test MSE 143058.5628274323 Test RE 4.214162529680596\n",
      "101 Train Loss 1442250.6 Test MSE 142972.13699802046 Test RE 4.212889388425851\n",
      "102 Train Loss 1438255.0 Test MSE 143820.59094126875 Test RE 4.225371384297377\n",
      "103 Train Loss 1436484.5 Test MSE 143952.5426127238 Test RE 4.227309274374253\n",
      "104 Train Loss 1433356.9 Test MSE 144263.841749043 Test RE 4.23187760997547\n",
      "105 Train Loss 1431246.1 Test MSE 144607.83115571717 Test RE 4.236919948760217\n",
      "106 Train Loss 1427786.5 Test MSE 145193.9900532984 Test RE 4.245498311036203\n",
      "107 Train Loss 1424818.0 Test MSE 145687.13295950924 Test RE 4.25270199301357\n",
      "108 Train Loss 1422364.4 Test MSE 145934.75798761245 Test RE 4.256314626229194\n",
      "109 Train Loss 1420056.0 Test MSE 146035.74625211098 Test RE 4.257787076987067\n",
      "110 Train Loss 1418419.6 Test MSE 145560.49260123467 Test RE 4.250853234024739\n",
      "111 Train Loss 1417177.4 Test MSE 145513.8783849345 Test RE 4.250172534011334\n",
      "112 Train Loss 1414184.2 Test MSE 145782.18521052142 Test RE 4.2540890852596\n",
      "113 Train Loss 1410848.5 Test MSE 146294.50971572532 Test RE 4.261557633629537\n",
      "114 Train Loss 1410033.2 Test MSE 146365.11206905375 Test RE 4.262585832521316\n",
      "115 Train Loss 1409733.1 Test MSE 146520.9248097471 Test RE 4.264854093402109\n",
      "116 Train Loss 1409087.9 Test MSE 146731.95127253095 Test RE 4.267924212066733\n",
      "117 Train Loss 1408365.2 Test MSE 146389.1900357512 Test RE 4.26293642897705\n",
      "118 Train Loss 1407875.2 Test MSE 146241.81795048917 Test RE 4.260790109210003\n",
      "119 Train Loss 1407389.8 Test MSE 146208.61281062503 Test RE 4.2603063619296595\n",
      "120 Train Loss 1406822.4 Test MSE 145982.48523118487 Test RE 4.257010572696102\n",
      "121 Train Loss 1406207.9 Test MSE 146243.1005026034 Test RE 4.260808792899313\n",
      "122 Train Loss 1405947.1 Test MSE 146356.37084056894 Test RE 4.262458545381291\n",
      "123 Train Loss 1405534.8 Test MSE 146311.93020890016 Test RE 4.261811355474262\n",
      "124 Train Loss 1405060.5 Test MSE 146455.31130076433 Test RE 4.263899064700128\n",
      "125 Train Loss 1404842.6 Test MSE 146399.6140966763 Test RE 4.263088203565069\n",
      "126 Train Loss 1404659.8 Test MSE 146210.24739313775 Test RE 4.260330176541811\n",
      "127 Train Loss 1404509.5 Test MSE 146113.53893093814 Test RE 4.25892097937848\n",
      "128 Train Loss 1404235.1 Test MSE 145743.13637312382 Test RE 4.253519302476649\n",
      "129 Train Loss 1404084.2 Test MSE 145704.66056248036 Test RE 4.252957806374516\n",
      "130 Train Loss 1403657.8 Test MSE 145542.67316790123 Test RE 4.250593032534443\n",
      "131 Train Loss 1403333.0 Test MSE 145461.15720344454 Test RE 4.249402523549836\n",
      "132 Train Loss 1403151.8 Test MSE 145496.89991738065 Test RE 4.249924573052804\n",
      "133 Train Loss 1402588.9 Test MSE 144932.34515846684 Test RE 4.241671314335033\n",
      "134 Train Loss 1402316.8 Test MSE 144605.1974836209 Test RE 4.236881366103381\n",
      "135 Train Loss 1402083.0 Test MSE 144450.90515386764 Test RE 4.234620407371986\n",
      "136 Train Loss 1401841.2 Test MSE 144174.51250481332 Test RE 4.230567202038436\n",
      "137 Train Loss 1401635.9 Test MSE 144375.61188888742 Test RE 4.23351664151269\n",
      "138 Train Loss 1401431.0 Test MSE 144668.78292280997 Test RE 4.237812779213844\n",
      "139 Train Loss 1401204.1 Test MSE 144537.35904903026 Test RE 4.235887428395855\n",
      "140 Train Loss 1401051.2 Test MSE 144452.4237559975 Test RE 4.23464266644562\n",
      "141 Train Loss 1400938.0 Test MSE 144510.21527566042 Test RE 4.235489664911771\n",
      "142 Train Loss 1400856.0 Test MSE 144378.36431889914 Test RE 4.2335569959831245\n",
      "143 Train Loss 1400740.8 Test MSE 144308.6619130692 Test RE 4.232534942805999\n",
      "144 Train Loss 1400530.9 Test MSE 143912.07269378644 Test RE 4.22671501293832\n",
      "145 Train Loss 1400355.5 Test MSE 143864.84414331595 Test RE 4.226021401901935\n",
      "146 Train Loss 1400129.2 Test MSE 143743.68881591214 Test RE 4.2242415617649\n",
      "147 Train Loss 1399983.0 Test MSE 143632.0982812507 Test RE 4.2226015701247865\n",
      "148 Train Loss 1399762.9 Test MSE 143042.62830542287 Test RE 4.213927826723132\n",
      "149 Train Loss 1399390.4 Test MSE 142850.24491213303 Test RE 4.211093138528425\n",
      "150 Train Loss 1398921.0 Test MSE 142926.26080510888 Test RE 4.212213428631055\n",
      "151 Train Loss 1398519.2 Test MSE 142611.8488590056 Test RE 4.207577823669305\n",
      "152 Train Loss 1398131.9 Test MSE 142229.70936464967 Test RE 4.201936776900553\n",
      "153 Train Loss 1397751.0 Test MSE 141891.38384433184 Test RE 4.196936173045204\n",
      "154 Train Loss 1397340.5 Test MSE 141640.13004430395 Test RE 4.193218669703155\n",
      "155 Train Loss 1397069.1 Test MSE 141755.15987573558 Test RE 4.194921038039306\n",
      "156 Train Loss 1396702.9 Test MSE 142075.89912104057 Test RE 4.199664130251628\n",
      "157 Train Loss 1396215.1 Test MSE 141572.51165654208 Test RE 4.192217637954574\n",
      "158 Train Loss 1395997.6 Test MSE 141200.57377361617 Test RE 4.186707140223913\n",
      "159 Train Loss 1395883.5 Test MSE 141141.66904738703 Test RE 4.185833763590255\n",
      "160 Train Loss 1395579.6 Test MSE 140821.1202555879 Test RE 4.181077809322338\n",
      "161 Train Loss 1395234.5 Test MSE 140232.4658999736 Test RE 4.172329877515\n",
      "162 Train Loss 1394869.1 Test MSE 139197.45254843446 Test RE 4.156904009915197\n",
      "163 Train Loss 1394639.9 Test MSE 138689.94964679325 Test RE 4.149319218607945\n",
      "164 Train Loss 1394524.0 Test MSE 138639.15823215092 Test RE 4.148559361486616\n",
      "165 Train Loss 1394174.6 Test MSE 137938.5468660091 Test RE 4.138063736683776\n",
      "166 Train Loss 1393582.9 Test MSE 137394.12414391237 Test RE 4.129889505138058\n",
      "167 Train Loss 1393250.4 Test MSE 137317.76105586786 Test RE 4.128741657839151\n",
      "168 Train Loss 1392790.2 Test MSE 136920.65554498328 Test RE 4.122767437499894\n",
      "169 Train Loss 1392468.2 Test MSE 136327.15395162953 Test RE 4.113822379614865\n",
      "170 Train Loss 1392059.4 Test MSE 135742.26068247348 Test RE 4.104987993676154\n",
      "171 Train Loss 1391684.9 Test MSE 135373.87016153752 Test RE 4.099413951858052\n",
      "172 Train Loss 1391402.8 Test MSE 134960.58611321892 Test RE 4.093151600616093\n",
      "173 Train Loss 1391106.6 Test MSE 134417.518846027 Test RE 4.084908092774391\n",
      "174 Train Loss 1390779.5 Test MSE 133637.51549586107 Test RE 4.0730388148778545\n",
      "175 Train Loss 1390575.1 Test MSE 133164.19405682522 Test RE 4.065819410016018\n",
      "176 Train Loss 1390136.2 Test MSE 133043.16680348382 Test RE 4.06397136513961\n",
      "177 Train Loss 1389954.4 Test MSE 133001.97198881322 Test RE 4.063342142637651\n",
      "178 Train Loss 1389850.1 Test MSE 133024.3851491173 Test RE 4.0636845003521795\n",
      "179 Train Loss 1389733.8 Test MSE 132992.61789694987 Test RE 4.06319925173376\n",
      "180 Train Loss 1389616.8 Test MSE 132678.623577319 Test RE 4.0583998296091455\n",
      "181 Train Loss 1389481.8 Test MSE 132435.23971443553 Test RE 4.054675784260682\n",
      "182 Train Loss 1389235.2 Test MSE 132409.95355825062 Test RE 4.0542886816292985\n",
      "183 Train Loss 1389059.8 Test MSE 132290.513361096 Test RE 4.052459686607227\n",
      "184 Train Loss 1388963.4 Test MSE 132089.52767132776 Test RE 4.049380115831019\n",
      "185 Train Loss 1388784.1 Test MSE 131852.43579438468 Test RE 4.045744300207291\n",
      "186 Train Loss 1388678.0 Test MSE 131653.10764105976 Test RE 4.042685063012717\n",
      "187 Train Loss 1388698.0 Test MSE 131463.49327391724 Test RE 4.03977276040849\n",
      "188 Train Loss 1388717.0 Test MSE 131291.51836099158 Test RE 4.037129566821907\n",
      "189 Train Loss 1388653.1 Test MSE 131284.94945091382 Test RE 4.037028570683127\n",
      "190 Train Loss 1388597.2 Test MSE 131199.11278273194 Test RE 4.035708610767481\n",
      "191 Train Loss 1388549.5 Test MSE 130967.25099753724 Test RE 4.032140978468548\n",
      "192 Train Loss 1388478.4 Test MSE 130809.31737371965 Test RE 4.029709062358934\n",
      "193 Train Loss 1388354.0 Test MSE 130593.22656394195 Test RE 4.026379242157972\n",
      "194 Train Loss 1388286.2 Test MSE 130411.62856719585 Test RE 4.023578803003165\n",
      "195 Train Loss 1388215.5 Test MSE 130152.57466254199 Test RE 4.019580532479691\n",
      "196 Train Loss 1388085.2 Test MSE 129816.12961798118 Test RE 4.014381852946231\n",
      "197 Train Loss 1387961.5 Test MSE 129662.58272748918 Test RE 4.012007039318869\n",
      "198 Train Loss 1387914.5 Test MSE 129548.89328784957 Test RE 4.010247769799292\n",
      "199 Train Loss 1387811.6 Test MSE 129390.0762793418 Test RE 4.00778888785124\n",
      "Training time: 38.32\n",
      "Training time: 38.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "1\n",
      "0 Train Loss 13627689000.0 Test MSE 12494.1913470384 Test RE 1.2453987593984541\n",
      "1 Train Loss 5881724000.0 Test MSE 44697.703039859 Test RE 2.355575102073989\n",
      "2 Train Loss 2237823700.0 Test MSE 66805.82000719942 Test RE 2.8797945263239386\n",
      "3 Train Loss 1466704000.0 Test MSE 79910.40249216236 Test RE 3.149604421169453\n",
      "4 Train Loss 1032406000.0 Test MSE 90664.73928735113 Test RE 3.3548535058542543\n",
      "5 Train Loss 522994000.0 Test MSE 111837.47949831314 Test RE 3.726044503138518\n",
      "6 Train Loss 273920450.0 Test MSE 152165.53821154227 Test RE 4.346228002148137\n",
      "7 Train Loss 143878770.0 Test MSE 150734.46036806036 Test RE 4.325742142710577\n",
      "8 Train Loss 90268100.0 Test MSE 145196.81855043402 Test RE 4.245539663716589\n",
      "9 Train Loss 52240000.0 Test MSE 144636.09571779124 Test RE 4.237333995616452\n",
      "10 Train Loss 31250240.0 Test MSE 150077.02315251227 Test RE 4.316298344633339\n",
      "11 Train Loss 19214664.0 Test MSE 162000.9408234912 Test RE 4.484490627774731\n",
      "12 Train Loss 12432135.0 Test MSE 161737.77620238782 Test RE 4.48084670156662\n",
      "13 Train Loss 8786055.0 Test MSE 162029.77174153054 Test RE 4.484889657030816\n",
      "14 Train Loss 4573023.5 Test MSE 174545.86525173453 Test RE 4.654886918194612\n",
      "15 Train Loss 3322179.0 Test MSE 172241.96015705238 Test RE 4.624063953096007\n",
      "16 Train Loss 2737788.0 Test MSE 169280.51590746 Test RE 4.584139649210384\n",
      "17 Train Loss 2404007.0 Test MSE 174062.92481788903 Test RE 4.648442794622039\n",
      "18 Train Loss 1988135.0 Test MSE 181199.1439541565 Test RE 4.742773920904319\n",
      "19 Train Loss 990603.0 Test MSE 178908.90642151542 Test RE 4.712705843275406\n",
      "20 Train Loss 336906.38 Test MSE 174937.14666951174 Test RE 4.660101453486044\n",
      "21 Train Loss 203976.28 Test MSE 177644.6672319959 Test RE 4.6960254268379735\n",
      "22 Train Loss 156427.22 Test MSE 179133.61044230053 Test RE 4.715664420562406\n",
      "23 Train Loss 142953.6 Test MSE 176458.74279342243 Test RE 4.680324260025924\n",
      "24 Train Loss 135347.17 Test MSE 175058.75707609992 Test RE 4.661720944791631\n",
      "25 Train Loss 126005.86 Test MSE 175345.0596166614 Test RE 4.665531429109409\n",
      "26 Train Loss 117209.914 Test MSE 174685.8269164777 Test RE 4.656752832260433\n",
      "27 Train Loss 108976.05 Test MSE 174215.39340202016 Test RE 4.650478226223368\n",
      "28 Train Loss 101486.95 Test MSE 174786.83703205612 Test RE 4.658098995174445\n",
      "29 Train Loss 99433.016 Test MSE 175303.85820900518 Test RE 4.664983259243339\n",
      "30 Train Loss 98063.07 Test MSE 175930.11199565063 Test RE 4.673308400968099\n",
      "31 Train Loss 96135.57 Test MSE 176403.9668956149 Test RE 4.679597776152804\n",
      "32 Train Loss 93879.28 Test MSE 176205.86963391036 Test RE 4.676969502192427\n",
      "33 Train Loss 92771.31 Test MSE 175425.6573479716 Test RE 4.666603566666067\n",
      "34 Train Loss 91781.99 Test MSE 175171.00230722423 Test RE 4.663215220497688\n",
      "35 Train Loss 91216.95 Test MSE 174794.39788566864 Test RE 4.658199743103599\n",
      "36 Train Loss 90694.125 Test MSE 174365.913934023 Test RE 4.652486778141478\n",
      "37 Train Loss 90002.94 Test MSE 174058.49148008742 Test RE 4.648383596925561\n",
      "38 Train Loss 89426.64 Test MSE 173052.4252143717 Test RE 4.634930190533813\n",
      "39 Train Loss 88982.96 Test MSE 173059.64161010436 Test RE 4.6350268292885675\n",
      "40 Train Loss 88724.25 Test MSE 173127.6487265885 Test RE 4.6359374513296325\n",
      "41 Train Loss 87966.625 Test MSE 171837.46931762763 Test RE 4.618631215044107\n",
      "42 Train Loss 87382.66 Test MSE 171005.84400528856 Test RE 4.607441486995472\n",
      "43 Train Loss 86462.13 Test MSE 169855.02024630175 Test RE 4.591911888686992\n",
      "44 Train Loss 85174.79 Test MSE 169098.9355466199 Test RE 4.581680378972821\n",
      "45 Train Loss 84695.5 Test MSE 168102.26031425007 Test RE 4.5681581295691425\n",
      "46 Train Loss 84442.63 Test MSE 167611.8193339768 Test RE 4.561489425518724\n",
      "47 Train Loss 84116.45 Test MSE 167474.33569280859 Test RE 4.559618260252584\n",
      "48 Train Loss 83559.72 Test MSE 166044.49613718104 Test RE 4.540112293520283\n",
      "49 Train Loss 83114.26 Test MSE 164926.84246658866 Test RE 4.524806634668083\n",
      "50 Train Loss 82879.28 Test MSE 164664.90806061015 Test RE 4.521212090945882\n",
      "51 Train Loss 82416.68 Test MSE 163889.77402956143 Test RE 4.510558092459531\n",
      "52 Train Loss 82211.695 Test MSE 163436.62524286422 Test RE 4.504318018120692\n",
      "53 Train Loss 81896.734 Test MSE 162818.55461799394 Test RE 4.495792928664394\n",
      "54 Train Loss 81570.64 Test MSE 161843.73136724942 Test RE 4.482314172963133\n",
      "55 Train Loss 81037.94 Test MSE 160351.00481453736 Test RE 4.461595518024234\n",
      "56 Train Loss 80672.125 Test MSE 159674.5649346297 Test RE 4.452174963639152\n",
      "57 Train Loss 80288.12 Test MSE 159272.5850217107 Test RE 4.44656726805638\n",
      "58 Train Loss 80032.22 Test MSE 159303.02093880178 Test RE 4.446992102273427\n",
      "59 Train Loss 79702.43 Test MSE 158782.23741730166 Test RE 4.439717236865387\n",
      "60 Train Loss 79330.75 Test MSE 157982.58011586702 Test RE 4.428523498859976\n",
      "61 Train Loss 78863.26 Test MSE 156559.01773319897 Test RE 4.408525896635216\n",
      "62 Train Loss 78202.4 Test MSE 154847.52928615332 Test RE 4.384362883124012\n",
      "63 Train Loss 76996.0 Test MSE 151942.90680761661 Test RE 4.343047383738593\n",
      "64 Train Loss 76048.16 Test MSE 149386.02408757352 Test RE 4.30635012228363\n",
      "65 Train Loss 75222.695 Test MSE 147513.2329298916 Test RE 4.279271515901187\n",
      "66 Train Loss 74158.19 Test MSE 145501.26877098097 Test RE 4.24998837908202\n",
      "67 Train Loss 73646.19 Test MSE 144265.77696541054 Test RE 4.231905993980958\n",
      "68 Train Loss 72994.58 Test MSE 143295.6718890036 Test RE 4.217653416258459\n",
      "69 Train Loss 72681.8 Test MSE 143049.05276362892 Test RE 4.214022455504638\n",
      "70 Train Loss 72212.01 Test MSE 142612.34146307482 Test RE 4.207585090485437\n",
      "71 Train Loss 71896.484 Test MSE 142554.94242841392 Test RE 4.206738264657081\n",
      "72 Train Loss 71703.53 Test MSE 142431.6064102206 Test RE 4.204918073051707\n",
      "73 Train Loss 71572.81 Test MSE 141926.7861657729 Test RE 4.197459714407367\n",
      "74 Train Loss 71417.16 Test MSE 141677.2393905292 Test RE 4.1937679399294\n",
      "75 Train Loss 71120.94 Test MSE 141189.44375891454 Test RE 4.1865421301638355\n",
      "76 Train Loss 70879.11 Test MSE 140460.920087019 Test RE 4.1757270877423345\n",
      "77 Train Loss 70681.14 Test MSE 140268.70166147556 Test RE 4.1728689031393715\n",
      "78 Train Loss 70570.07 Test MSE 140444.23984208493 Test RE 4.175479138994232\n",
      "79 Train Loss 70374.61 Test MSE 140150.9202352894 Test RE 4.171116588968143\n",
      "80 Train Loss 70085.65 Test MSE 139423.16235884553 Test RE 4.160272871773711\n",
      "81 Train Loss 69589.45 Test MSE 138167.93749475898 Test RE 4.141503089751169\n",
      "82 Train Loss 69309.42 Test MSE 137051.34892703113 Test RE 4.12473459833793\n",
      "83 Train Loss 69054.664 Test MSE 136556.3558624557 Test RE 4.117279137646857\n",
      "84 Train Loss 68826.89 Test MSE 135749.40382398514 Test RE 4.105096000291952\n",
      "85 Train Loss 68620.664 Test MSE 134894.17406413297 Test RE 4.0921443879086965\n",
      "86 Train Loss 68178.15 Test MSE 132380.3324204438 Test RE 4.053835168288021\n",
      "87 Train Loss 66775.766 Test MSE 130521.68176710191 Test RE 4.02527617595312\n",
      "88 Train Loss 66210.445 Test MSE 130259.40527593253 Test RE 4.021229851266401\n",
      "89 Train Loss 65803.93 Test MSE 129927.52262774434 Test RE 4.016103819974805\n",
      "90 Train Loss 65314.61 Test MSE 129189.95378708758 Test RE 4.004688344563779\n",
      "91 Train Loss 65029.113 Test MSE 128599.18168352237 Test RE 3.995521342814856\n",
      "92 Train Loss 64764.65 Test MSE 127860.6388601946 Test RE 3.9840317187234287\n",
      "93 Train Loss 64292.836 Test MSE 126269.32334298258 Test RE 3.9591620588978214\n",
      "94 Train Loss 62966.65 Test MSE 122846.2636734571 Test RE 3.905128493682357\n",
      "95 Train Loss 61415.777 Test MSE 119175.08515895903 Test RE 3.846334831713832\n",
      "96 Train Loss 60117.273 Test MSE 116252.67106185136 Test RE 3.7988821629593166\n",
      "97 Train Loss 58552.945 Test MSE 113799.96632455688 Test RE 3.7585940257900563\n",
      "98 Train Loss 57648.96 Test MSE 111714.55451764789 Test RE 3.723996218977344\n",
      "99 Train Loss 56936.953 Test MSE 110516.95175676256 Test RE 3.703981435110062\n",
      "100 Train Loss 56550.336 Test MSE 109909.42276950128 Test RE 3.693786722382004\n",
      "101 Train Loss 55745.57 Test MSE 108350.67027176938 Test RE 3.667500261519224\n",
      "102 Train Loss 55054.95 Test MSE 107363.0404764014 Test RE 3.6507471375906624\n",
      "103 Train Loss 53967.7 Test MSE 105410.82213217573 Test RE 3.617403484785189\n",
      "104 Train Loss 52878.45 Test MSE 103335.80416931376 Test RE 3.5816221245083755\n",
      "105 Train Loss 52412.293 Test MSE 102617.20554680262 Test RE 3.5691470736651763\n",
      "106 Train Loss 51825.24 Test MSE 101475.96714403266 Test RE 3.5492447772212112\n",
      "107 Train Loss 51301.65 Test MSE 100534.78039141503 Test RE 3.5327468607350347\n",
      "108 Train Loss 50852.25 Test MSE 99212.96320029191 Test RE 3.509445988066161\n",
      "109 Train Loss 49849.684 Test MSE 97775.79382864211 Test RE 3.4839348709038096\n",
      "110 Train Loss 49033.92 Test MSE 96029.54653733801 Test RE 3.4526836752811914\n",
      "111 Train Loss 48315.523 Test MSE 94453.60132480414 Test RE 3.424235402342768\n",
      "112 Train Loss 47575.336 Test MSE 93328.83336021048 Test RE 3.4037861818476696\n",
      "113 Train Loss 46665.465 Test MSE 91244.68060313196 Test RE 3.365566142173289\n",
      "114 Train Loss 46007.586 Test MSE 89824.18721289272 Test RE 3.3392658835061364\n",
      "115 Train Loss 45544.855 Test MSE 88890.40252138779 Test RE 3.3218635462028514\n",
      "116 Train Loss 45034.684 Test MSE 88195.77984431651 Test RE 3.308858951864521\n",
      "117 Train Loss 44535.52 Test MSE 86773.88722526249 Test RE 3.2820778552143057\n",
      "118 Train Loss 43767.523 Test MSE 84423.27164903829 Test RE 3.2373185926307646\n",
      "119 Train Loss 42586.82 Test MSE 83431.06567590906 Test RE 3.2182386614171627\n",
      "120 Train Loss 42105.555 Test MSE 82973.04108266297 Test RE 3.2093926693942407\n",
      "121 Train Loss 41426.176 Test MSE 81334.6338960752 Test RE 3.1775479320096025\n",
      "122 Train Loss 40732.277 Test MSE 79939.43881208163 Test RE 3.1501765908284964\n",
      "123 Train Loss 40365.508 Test MSE 78974.77987028926 Test RE 3.131111724230356\n",
      "124 Train Loss 39815.926 Test MSE 77083.04847028505 Test RE 3.093383703241558\n",
      "125 Train Loss 39100.66 Test MSE 75814.90996230766 Test RE 3.0678326423939764\n",
      "126 Train Loss 38557.656 Test MSE 74343.26961539149 Test RE 3.037911941833111\n",
      "127 Train Loss 37662.496 Test MSE 72245.5776128041 Test RE 2.9947459494507607\n",
      "128 Train Loss 36970.055 Test MSE 70931.24698972638 Test RE 2.967379894609318\n",
      "129 Train Loss 36517.645 Test MSE 70624.56914964717 Test RE 2.9609580743460073\n",
      "130 Train Loss 35981.7 Test MSE 69313.22112909424 Test RE 2.933339924540535\n",
      "131 Train Loss 35308.246 Test MSE 67300.849688058 Test RE 2.8904444413935475\n",
      "132 Train Loss 34896.113 Test MSE 67432.1817454643 Test RE 2.8932632988381934\n",
      "133 Train Loss 34544.695 Test MSE 67298.02660222026 Test RE 2.8903838176541803\n",
      "134 Train Loss 34091.76 Test MSE 66463.0929020519 Test RE 2.872398069291699\n",
      "135 Train Loss 33878.33 Test MSE 66430.81739013009 Test RE 2.871700544004048\n",
      "136 Train Loss 33737.156 Test MSE 66466.84444929425 Test RE 2.872479135222625\n",
      "137 Train Loss 33284.984 Test MSE 65377.21163189138 Test RE 2.8488366590674903\n",
      "138 Train Loss 33122.77 Test MSE 64983.83185037982 Test RE 2.840252891098014\n",
      "139 Train Loss 32960.61 Test MSE 64810.23486599472 Test RE 2.836456646313616\n",
      "140 Train Loss 32729.578 Test MSE 64035.58068483175 Test RE 2.819454097640529\n",
      "141 Train Loss 31893.646 Test MSE 61696.4248961398 Test RE 2.767479112709756\n",
      "142 Train Loss 30686.879 Test MSE 59986.380496403195 Test RE 2.728856392644469\n",
      "143 Train Loss 29989.707 Test MSE 58030.54915582342 Test RE 2.684001118955536\n",
      "144 Train Loss 29280.436 Test MSE 56966.63251417985 Test RE 2.6592834219111254\n",
      "145 Train Loss 28943.404 Test MSE 56479.75931402161 Test RE 2.6478950699657275\n",
      "146 Train Loss 28596.943 Test MSE 55486.25971017752 Test RE 2.6245030240031966\n",
      "147 Train Loss 28127.766 Test MSE 54757.069440450265 Test RE 2.607200620549553\n",
      "148 Train Loss 27819.562 Test MSE 54433.11669108367 Test RE 2.5994768440306255\n",
      "149 Train Loss 27590.652 Test MSE 54349.118256882764 Test RE 2.59747037902905\n",
      "150 Train Loss 27195.059 Test MSE 53746.2448970437 Test RE 2.583023847022401\n",
      "151 Train Loss 27030.707 Test MSE 53593.11985769189 Test RE 2.5793416572453065\n",
      "152 Train Loss 26910.557 Test MSE 53462.557578235755 Test RE 2.576197876085295\n",
      "153 Train Loss 26712.89 Test MSE 52906.19468884225 Test RE 2.5627581022276362\n",
      "154 Train Loss 26477.205 Test MSE 52277.53718838404 Test RE 2.5474866229380866\n",
      "155 Train Loss 26149.555 Test MSE 51755.80484085274 Test RE 2.5347427272300984\n",
      "156 Train Loss 25948.525 Test MSE 51320.98429144934 Test RE 2.5240725915929416\n",
      "157 Train Loss 25785.621 Test MSE 50986.99695982954 Test RE 2.515846090315626\n",
      "158 Train Loss 25632.926 Test MSE 50875.59961149649 Test RE 2.513096253631744\n",
      "159 Train Loss 25551.527 Test MSE 50754.33873452693 Test RE 2.510099511970626\n",
      "160 Train Loss 25502.754 Test MSE 50711.740675037894 Test RE 2.509045929006033\n",
      "161 Train Loss 25396.277 Test MSE 50354.306907727805 Test RE 2.5001879841744716\n",
      "162 Train Loss 25169.969 Test MSE 49730.4449366264 Test RE 2.484651740702858\n",
      "163 Train Loss 24961.63 Test MSE 49560.21098324958 Test RE 2.4803954477772963\n",
      "164 Train Loss 24865.459 Test MSE 49440.85884226112 Test RE 2.477406972251365\n",
      "165 Train Loss 24735.389 Test MSE 49139.08400394149 Test RE 2.469834658361957\n",
      "166 Train Loss 24567.852 Test MSE 48894.670664039375 Test RE 2.463684634802262\n",
      "167 Train Loss 24472.4 Test MSE 48821.429850391 Test RE 2.461838729226862\n",
      "168 Train Loss 24400.57 Test MSE 48662.9179880574 Test RE 2.4578389702554233\n",
      "169 Train Loss 24343.057 Test MSE 48579.36742146076 Test RE 2.4557281015787993\n",
      "170 Train Loss 24242.285 Test MSE 48448.30863334154 Test RE 2.4524132980953133\n",
      "171 Train Loss 24159.97 Test MSE 48152.05499741466 Test RE 2.4449037435927976\n",
      "172 Train Loss 24095.088 Test MSE 47977.22628820429 Test RE 2.4404612742695497\n",
      "173 Train Loss 23984.197 Test MSE 47786.69970707153 Test RE 2.4356106886751476\n",
      "174 Train Loss 23830.094 Test MSE 47361.356375950774 Test RE 2.4247469300579874\n",
      "175 Train Loss 23711.021 Test MSE 47084.33779355746 Test RE 2.4176453065816372\n",
      "176 Train Loss 23526.086 Test MSE 46614.96927889954 Test RE 2.4055647628777503\n",
      "177 Train Loss 23341.453 Test MSE 46289.93999796586 Test RE 2.397163527079898\n",
      "178 Train Loss 23249.238 Test MSE 46102.04209332069 Test RE 2.3922933542438116\n",
      "179 Train Loss 23122.219 Test MSE 45746.66945080402 Test RE 2.3830551486171556\n",
      "180 Train Loss 23001.57 Test MSE 45592.030768936704 Test RE 2.379023986763717\n",
      "181 Train Loss 22854.688 Test MSE 45493.79342495002 Test RE 2.3764595580351027\n",
      "182 Train Loss 22667.818 Test MSE 45006.62734943963 Test RE 2.363701262254144\n",
      "183 Train Loss 22596.117 Test MSE 44714.30194412613 Test RE 2.356012443744758\n",
      "184 Train Loss 22489.52 Test MSE 44432.9045711992 Test RE 2.3485872795010865\n",
      "185 Train Loss 22392.484 Test MSE 44089.80692363509 Test RE 2.3395021618382157\n",
      "186 Train Loss 22259.43 Test MSE 43862.76082426324 Test RE 2.333470603955969\n",
      "187 Train Loss 22160.812 Test MSE 43779.56893770537 Test RE 2.3312566763299403\n",
      "188 Train Loss 22079.086 Test MSE 43479.83910271214 Test RE 2.3232626819533815\n",
      "189 Train Loss 21966.057 Test MSE 43144.87732884865 Test RE 2.3142963564480095\n",
      "190 Train Loss 21852.672 Test MSE 42980.41383816481 Test RE 2.309881224322054\n",
      "191 Train Loss 21658.576 Test MSE 42584.47789768709 Test RE 2.299217285613484\n",
      "192 Train Loss 21517.932 Test MSE 42220.8384907406 Test RE 2.2893794456345598\n",
      "193 Train Loss 21419.844 Test MSE 41848.488837000325 Test RE 2.279261960207066\n",
      "194 Train Loss 21348.252 Test MSE 41639.890231618716 Test RE 2.2735742418977605\n",
      "195 Train Loss 21247.86 Test MSE 41546.04170634995 Test RE 2.271010691068353\n",
      "196 Train Loss 21064.781 Test MSE 41188.137792186135 Test RE 2.261207570673918\n",
      "197 Train Loss 20951.709 Test MSE 41058.60657004397 Test RE 2.257649172076687\n",
      "198 Train Loss 20846.994 Test MSE 41088.1707976169 Test RE 2.258461835349489\n",
      "199 Train Loss 20792.176 Test MSE 41001.75336023468 Test RE 2.256085564879276\n",
      "Training time: 38.12\n",
      "Training time: 38.12\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "2\n",
      "0 Train Loss 11613983000.0 Test MSE 12228.700939842518 Test RE 1.232095905839037\n",
      "1 Train Loss 4640752000.0 Test MSE 14298.579370679608 Test RE 1.3322962257353157\n",
      "2 Train Loss 3213563400.0 Test MSE 15003.940248745528 Test RE 1.3647622944372366\n",
      "3 Train Loss 2116567600.0 Test MSE 15286.353457013965 Test RE 1.3775466057019838\n",
      "4 Train Loss 1456038000.0 Test MSE 16296.15966358633 Test RE 1.422318922372145\n",
      "5 Train Loss 1379310600.0 Test MSE 18323.81067774616 Test RE 1.5082115083782683\n",
      "6 Train Loss 1320210000.0 Test MSE 19119.35403879684 Test RE 1.5406037830605805\n",
      "7 Train Loss 1273071700.0 Test MSE 18982.805770019455 Test RE 1.5350925160416196\n",
      "8 Train Loss 663832500.0 Test MSE 18133.291188815336 Test RE 1.500350302430641\n",
      "9 Train Loss 238737810.0 Test MSE 18703.32509508911 Test RE 1.523750158125315\n",
      "10 Train Loss 146194050.0 Test MSE 19515.024867235355 Test RE 1.5564633788219486\n",
      "11 Train Loss 125319496.0 Test MSE 20486.42420200634 Test RE 1.594730986699374\n",
      "12 Train Loss 115714856.0 Test MSE 20759.555982471167 Test RE 1.6053265281900087\n",
      "13 Train Loss 111649870.0 Test MSE 20673.727036629305 Test RE 1.6020045353450485\n",
      "14 Train Loss 108374280.0 Test MSE 20617.042137129003 Test RE 1.5998067749074807\n",
      "15 Train Loss 106126220.0 Test MSE 20900.1391193995 Test RE 1.6107529701088554\n",
      "16 Train Loss 104762420.0 Test MSE 21575.21030124096 Test RE 1.636559770165188\n",
      "17 Train Loss 104278376.0 Test MSE 21817.876496048208 Test RE 1.645737601239214\n",
      "18 Train Loss 103963690.0 Test MSE 21719.51609121944 Test RE 1.6420237138499978\n",
      "19 Train Loss 103754950.0 Test MSE 21609.697356243116 Test RE 1.6378672334469697\n",
      "20 Train Loss 103385830.0 Test MSE 22019.206227210474 Test RE 1.6533133853633935\n",
      "21 Train Loss 103148710.0 Test MSE 22333.863462215268 Test RE 1.6650845102514087\n",
      "22 Train Loss 103017016.0 Test MSE 22591.102215120056 Test RE 1.6746461780934834\n",
      "23 Train Loss 102975176.0 Test MSE 23031.71755105664 Test RE 1.6908984082993055\n",
      "24 Train Loss 103076800.0 Test MSE 23391.80876520926 Test RE 1.704065385478227\n",
      "25 Train Loss 103158940.0 Test MSE 23773.012964838344 Test RE 1.7178944078358662\n",
      "26 Train Loss 103226470.0 Test MSE 24255.57267617317 Test RE 1.735242271332016\n",
      "27 Train Loss 103306530.0 Test MSE 24512.664480154326 Test RE 1.7444141978410952\n",
      "28 Train Loss 103491650.0 Test MSE 24651.61390413474 Test RE 1.7493512953452413\n",
      "29 Train Loss 103679016.0 Test MSE 25010.662853847323 Test RE 1.7620448290321733\n",
      "30 Train Loss 103878616.0 Test MSE 25096.964325559475 Test RE 1.765082255679584\n",
      "31 Train Loss 104124190.0 Test MSE 25023.691058058725 Test RE 1.7625036991400713\n",
      "32 Train Loss 104331110.0 Test MSE 25102.86662925212 Test RE 1.765289799486937\n",
      "33 Train Loss 104553496.0 Test MSE 25138.60089436728 Test RE 1.766545809434241\n",
      "34 Train Loss 104726710.0 Test MSE 25024.58218560156 Test RE 1.762535081433042\n",
      "35 Train Loss 104840810.0 Test MSE 24856.480371885766 Test RE 1.756605220311546\n",
      "36 Train Loss 104978776.0 Test MSE 24838.946092678587 Test RE 1.7559855380450264\n",
      "37 Train Loss 105194880.0 Test MSE 24891.076988507255 Test RE 1.7578272651162874\n",
      "38 Train Loss 105373256.0 Test MSE 24986.99739140474 Test RE 1.7612109951823023\n",
      "39 Train Loss 105517010.0 Test MSE 25076.758336590196 Test RE 1.7643715638666893\n",
      "40 Train Loss 105715720.0 Test MSE 25143.104362621885 Test RE 1.7667040367504039\n",
      "41 Train Loss 105933960.0 Test MSE 25237.121182251263 Test RE 1.7700040451536405\n",
      "42 Train Loss 106114936.0 Test MSE 25263.328532096646 Test RE 1.7709228321765678\n",
      "43 Train Loss 106236800.0 Test MSE 25245.093184546124 Test RE 1.7702835810417792\n",
      "44 Train Loss 106397710.0 Test MSE 25228.35877200095 Test RE 1.7696967429073305\n",
      "45 Train Loss 106662136.0 Test MSE 25221.31024120922 Test RE 1.769449508565266\n",
      "46 Train Loss 106917490.0 Test MSE 25197.154204906754 Test RE 1.7686019489926748\n",
      "47 Train Loss 107140776.0 Test MSE 25215.55017071318 Test RE 1.769247442617833\n",
      "48 Train Loss 107372640.0 Test MSE 25335.35290686064 Test RE 1.7734454378120466\n",
      "49 Train Loss 107585210.0 Test MSE 25463.155593134365 Test RE 1.7779128312289019\n",
      "50 Train Loss 107914860.0 Test MSE 25491.130478383006 Test RE 1.7788892077426524\n",
      "51 Train Loss 108185710.0 Test MSE 25492.678791673923 Test RE 1.7789432311604017\n",
      "52 Train Loss 108436850.0 Test MSE 25515.907032178842 Test RE 1.779753509143198\n",
      "53 Train Loss 108698000.0 Test MSE 25559.97732728935 Test RE 1.7812898140092825\n",
      "54 Train Loss 108932184.0 Test MSE 25563.023656121633 Test RE 1.7813959610683152\n",
      "55 Train Loss 109148850.0 Test MSE 25540.622472774303 Test RE 1.7806152607040402\n",
      "56 Train Loss 109321950.0 Test MSE 25567.202920007883 Test RE 1.7815415741155451\n",
      "57 Train Loss 109475240.0 Test MSE 25586.896575135805 Test RE 1.782227576253166\n",
      "58 Train Loss 109624930.0 Test MSE 25527.879084807704 Test RE 1.7801709899698217\n",
      "59 Train Loss 109803510.0 Test MSE 25466.029975027686 Test RE 1.7780131773195285\n",
      "60 Train Loss 110034720.0 Test MSE 25424.528896597138 Test RE 1.7765638043326706\n",
      "61 Train Loss 110294024.0 Test MSE 25379.125484195625 Test RE 1.7749767915823014\n",
      "62 Train Loss 110530776.0 Test MSE 25372.6837469336 Test RE 1.7747515147149417\n",
      "63 Train Loss 110724450.0 Test MSE 25321.788859705357 Test RE 1.7729706404379515\n",
      "64 Train Loss 110896040.0 Test MSE 25304.232664383726 Test RE 1.7723559126216826\n",
      "65 Train Loss 111113704.0 Test MSE 25295.372390329845 Test RE 1.7720455903486132\n",
      "66 Train Loss 111325950.0 Test MSE 25288.47885907724 Test RE 1.7718041136895673\n",
      "67 Train Loss 111531240.0 Test MSE 25282.793499091214 Test RE 1.7716049338475617\n",
      "68 Train Loss 111721224.0 Test MSE 25159.871013590397 Test RE 1.7672930008828074\n",
      "69 Train Loss 111902800.0 Test MSE 24980.234203226293 Test RE 1.760972627055981\n",
      "70 Train Loss 112112010.0 Test MSE 24885.20524578259 Test RE 1.7576199193627\n",
      "71 Train Loss 112355016.0 Test MSE 24851.670732515515 Test RE 1.7564352636999507\n",
      "72 Train Loss 112596830.0 Test MSE 24819.432067136444 Test RE 1.7552956319745725\n",
      "73 Train Loss 112761990.0 Test MSE 24663.657900997077 Test RE 1.7497785819478373\n",
      "74 Train Loss 112909540.0 Test MSE 24528.288907453152 Test RE 1.7449700560392933\n",
      "75 Train Loss 113065460.0 Test MSE 24328.717322438708 Test RE 1.7378566837117149\n",
      "76 Train Loss 113278160.0 Test MSE 24328.717322438708 Test RE 1.7378566837117149\n",
      "77 Train Loss 113507864.0 Test MSE 24283.234657260095 Test RE 1.7362314576522753\n",
      "78 Train Loss 113743304.0 Test MSE 24264.65745016149 Test RE 1.7355672030133493\n",
      "79 Train Loss 113923896.0 Test MSE 24255.115236096357 Test RE 1.7352259086365027\n",
      "80 Train Loss 114124360.0 Test MSE 24225.59048202405 Test RE 1.734169477489974\n",
      "81 Train Loss 114407704.0 Test MSE 24221.763355346673 Test RE 1.7340324911816805\n",
      "82 Train Loss 114596696.0 Test MSE 24212.772797480997 Test RE 1.7337106449530009\n",
      "83 Train Loss 114757070.0 Test MSE 24184.25619987656 Test RE 1.7326894051579904\n",
      "84 Train Loss 114974280.0 Test MSE 24124.099169254114 Test RE 1.7305330777470096\n",
      "85 Train Loss 115158056.0 Test MSE 24021.841090565835 Test RE 1.7268614605809036\n",
      "86 Train Loss 115300090.0 Test MSE 23957.825078327875 Test RE 1.724558961543768\n",
      "87 Train Loss 115481510.0 Test MSE 23777.360287084015 Test RE 1.7180514745735087\n",
      "88 Train Loss 115698000.0 Test MSE 23681.933980963142 Test RE 1.7146004579680927\n",
      "89 Train Loss 115888390.0 Test MSE 23664.70468249547 Test RE 1.7139766335356506\n",
      "90 Train Loss 116123510.0 Test MSE 23645.803707899155 Test RE 1.7132920211465053\n",
      "91 Train Loss 116296400.0 Test MSE 23624.495078441494 Test RE 1.7125198728649853\n",
      "92 Train Loss 116531896.0 Test MSE 23422.984431186862 Test RE 1.7052005624270876\n",
      "93 Train Loss 116750230.0 Test MSE 23399.649966488014 Test RE 1.7043509726304318\n",
      "94 Train Loss 116989830.0 Test MSE 23399.649966488014 Test RE 1.7043509726304318\n",
      "95 Train Loss 117213370.0 Test MSE 23361.36276355859 Test RE 1.702956046926849\n",
      "96 Train Loss 117381864.0 Test MSE 23166.048278835824 Test RE 1.6958222569443495\n",
      "97 Train Loss 117585820.0 Test MSE 22940.843235508586 Test RE 1.6875592933370585\n",
      "98 Train Loss 117805040.0 Test MSE 22717.024420384765 Test RE 1.6793069091368502\n",
      "99 Train Loss 117999576.0 Test MSE 22670.468327021263 Test RE 1.677585247327658\n",
      "100 Train Loss 118298630.0 Test MSE 22465.29062185219 Test RE 1.6699765506763111\n",
      "101 Train Loss 118511590.0 Test MSE 22207.73675360154 Test RE 1.6603762122300083\n",
      "102 Train Loss 118642664.0 Test MSE 22132.67026520604 Test RE 1.6575676388676477\n",
      "103 Train Loss 118833360.0 Test MSE 21966.94931534003 Test RE 1.6513503633501845\n",
      "104 Train Loss 119030960.0 Test MSE 21966.94931534003 Test RE 1.6513503633501845\n",
      "105 Train Loss 119196170.0 Test MSE 21927.060458765554 Test RE 1.6498503733677827\n",
      "106 Train Loss 119353580.0 Test MSE 21858.812164480896 Test RE 1.647280780705921\n",
      "107 Train Loss 119477460.0 Test MSE 21650.71283116522 Test RE 1.6394208428155561\n",
      "108 Train Loss 119634500.0 Test MSE 21455.519828303735 Test RE 1.6320139735596622\n",
      "109 Train Loss 119775090.0 Test MSE 21259.670927951258 Test RE 1.6245482748704827\n",
      "110 Train Loss 119961704.0 Test MSE 21248.43101539845 Test RE 1.6241187716406877\n",
      "111 Train Loss 120132690.0 Test MSE 21251.147160173787 Test RE 1.624222572263629\n",
      "112 Train Loss 120279680.0 Test MSE 21251.147160173787 Test RE 1.624222572263629\n",
      "113 Train Loss 120426220.0 Test MSE 21260.909712114273 Test RE 1.624595604750763\n",
      "114 Train Loss 120628830.0 Test MSE 21263.609060211795 Test RE 1.6246987332134886\n",
      "115 Train Loss 120856376.0 Test MSE 21242.81734854638 Test RE 1.6239042178234628\n",
      "116 Train Loss 121097640.0 Test MSE 20994.527811506287 Test RE 1.614386094164332\n",
      "117 Train Loss 121326750.0 Test MSE 20994.527811506287 Test RE 1.614386094164332\n",
      "118 Train Loss 121581224.0 Test MSE 20841.635860688537 Test RE 1.6084969962527258\n",
      "119 Train Loss 121750610.0 Test MSE 20627.716712672835 Test RE 1.6002208752491796\n",
      "120 Train Loss 121901970.0 Test MSE 20319.873838811327 Test RE 1.588235342260043\n",
      "121 Train Loss 122051720.0 Test MSE 20035.237453903927 Test RE 1.5770722838925237\n",
      "122 Train Loss 122223864.0 Test MSE 19919.293469642867 Test RE 1.572502401602448\n",
      "123 Train Loss 122397980.0 Test MSE 19755.68620944042 Test RE 1.5660312065094089\n",
      "124 Train Loss 122576330.0 Test MSE 19457.02934662801 Test RE 1.5541488783320885\n",
      "125 Train Loss 122741150.0 Test MSE 19382.624498546862 Test RE 1.5511744527155102\n",
      "126 Train Loss 122894024.0 Test MSE 19378.3729722994 Test RE 1.5510043204255977\n",
      "127 Train Loss 123084690.0 Test MSE 19368.215201873132 Test RE 1.5505977638234378\n",
      "128 Train Loss 123284910.0 Test MSE 19369.158350503443 Test RE 1.5506355170789297\n",
      "129 Train Loss 123463656.0 Test MSE 19359.759238056977 Test RE 1.5502592393419803\n",
      "130 Train Loss 123666056.0 Test MSE 19348.970279167563 Test RE 1.549827208811536\n",
      "131 Train Loss 123849110.0 Test MSE 19341.82763286177 Test RE 1.549541124094282\n",
      "132 Train Loss 124052180.0 Test MSE 19320.81596124036 Test RE 1.5486992363208096\n",
      "133 Train Loss 124245140.0 Test MSE 19295.511133132124 Test RE 1.5476847240900748\n",
      "134 Train Loss 124378850.0 Test MSE 19240.214997868472 Test RE 1.545465493287556\n",
      "135 Train Loss 124507250.0 Test MSE 19184.624230292906 Test RE 1.543231220946441\n",
      "136 Train Loss 124644040.0 Test MSE 19107.26752801349 Test RE 1.5401167512308944\n",
      "137 Train Loss 124810540.0 Test MSE 19023.026921399938 Test RE 1.5367179479757431\n",
      "138 Train Loss 124972860.0 Test MSE 18986.141426260936 Test RE 1.5352273832490544\n",
      "139 Train Loss 125144800.0 Test MSE 18808.763073048347 Test RE 1.5280391105791304\n",
      "140 Train Loss 125326320.0 Test MSE 18623.34032913757 Test RE 1.5204885086582425\n",
      "141 Train Loss 125492950.0 Test MSE 18214.35807194096 Test RE 1.5037003034234167\n",
      "142 Train Loss 125645620.0 Test MSE 17926.075624732406 Test RE 1.4917531530800225\n",
      "143 Train Loss 125787850.0 Test MSE 17714.992943882095 Test RE 1.4829443175404446\n",
      "144 Train Loss 125943680.0 Test MSE 17563.313427244015 Test RE 1.4765820279873179\n",
      "145 Train Loss 126120160.0 Test MSE 17220.10269336722 Test RE 1.462083651078079\n",
      "146 Train Loss 126285140.0 Test MSE 16872.211274388792 Test RE 1.447239325038674\n",
      "147 Train Loss 126432460.0 Test MSE 16541.95995051183 Test RE 1.4330054260967613\n",
      "148 Train Loss 126559064.0 Test MSE 16273.11711964373 Test RE 1.421312997031907\n",
      "149 Train Loss 126745100.0 Test MSE 15999.3830418528 Test RE 1.409308172364397\n",
      "150 Train Loss 126948860.0 Test MSE 15660.364078556502 Test RE 1.3942969571900736\n",
      "151 Train Loss 127080740.0 Test MSE 15409.545389068193 Test RE 1.3830862555914594\n",
      "152 Train Loss 127258600.0 Test MSE 15366.662208488506 Test RE 1.381160421380459\n",
      "153 Train Loss 127423910.0 Test MSE 15173.996033853411 Test RE 1.3724746620199315\n",
      "154 Train Loss 127598450.0 Test MSE 15086.995664743035 Test RE 1.3685344524616057\n",
      "155 Train Loss 127785660.0 Test MSE 14770.877637591077 Test RE 1.3541210913532344\n",
      "156 Train Loss 127943660.0 Test MSE 14585.692342078066 Test RE 1.3456058814482026\n",
      "157 Train Loss 128067416.0 Test MSE 14114.407282457307 Test RE 1.3236881316694566\n",
      "158 Train Loss 128176970.0 Test MSE 13806.12500169265 Test RE 1.309152540826169\n",
      "159 Train Loss 128344456.0 Test MSE 13492.240964728708 Test RE 1.2941851039275358\n",
      "160 Train Loss 128520390.0 Test MSE 13124.801120084057 Test RE 1.2764409190482362\n",
      "161 Train Loss 128635480.0 Test MSE 13083.427787465684 Test RE 1.2744274676175356\n",
      "162 Train Loss 128790520.0 Test MSE 13019.40638457898 Test RE 1.2713055528978454\n",
      "163 Train Loss 128944800.0 Test MSE 12897.795602762524 Test RE 1.2653541606689251\n",
      "164 Train Loss 129101656.0 Test MSE 12884.249884473827 Test RE 1.2646895264096831\n",
      "165 Train Loss 129248776.0 Test MSE 12869.272833934314 Test RE 1.2639542555627548\n",
      "166 Train Loss 129406630.0 Test MSE 12844.237939865889 Test RE 1.2627242573633133\n",
      "167 Train Loss 129542984.0 Test MSE 12834.620956450417 Test RE 1.262251443318355\n",
      "168 Train Loss 129667990.0 Test MSE 12798.504660351016 Test RE 1.2604742204850545\n",
      "169 Train Loss 129837160.0 Test MSE 12655.863331190827 Test RE 1.2534304486911787\n",
      "170 Train Loss 129989230.0 Test MSE 12626.57008722946 Test RE 1.251979014131181\n",
      "171 Train Loss 130132070.0 Test MSE 12469.700895744916 Test RE 1.2441775783799085\n",
      "172 Train Loss 130291336.0 Test MSE 12418.05516169995 Test RE 1.2415984012325068\n",
      "173 Train Loss 130416690.0 Test MSE 12082.121106981778 Test RE 1.2246893597778115\n",
      "174 Train Loss 130575064.0 Test MSE 11574.676966617199 Test RE 1.198695271028913\n",
      "175 Train Loss 130721840.0 Test MSE 11543.870454625581 Test RE 1.1970990180957914\n",
      "176 Train Loss 130885704.0 Test MSE 11450.589885940199 Test RE 1.1922526117769736\n",
      "177 Train Loss 131028290.0 Test MSE 11295.113832648361 Test RE 1.1841307478372225\n",
      "178 Train Loss 131188584.0 Test MSE 11239.026035116376 Test RE 1.1811870882580044\n",
      "179 Train Loss 131335420.0 Test MSE 11209.815816670041 Test RE 1.1796511375408893\n",
      "180 Train Loss 131452370.0 Test MSE 11186.696234004372 Test RE 1.178434029284311\n",
      "181 Train Loss 131571790.0 Test MSE 11143.685318193453 Test RE 1.1761664098316857\n",
      "182 Train Loss 131739040.0 Test MSE 10899.440839170555 Test RE 1.1632055390059728\n",
      "183 Train Loss 131864730.0 Test MSE 10489.029697559308 Test RE 1.141095545104354\n",
      "184 Train Loss 132006510.0 Test MSE 10414.3890999412 Test RE 1.137028242415495\n",
      "185 Train Loss 132142744.0 Test MSE 10383.932331215008 Test RE 1.1353644116382724\n",
      "186 Train Loss 132282504.0 Test MSE 10316.46298421292 Test RE 1.1316698993238337\n",
      "187 Train Loss 132441370.0 Test MSE 9970.370430594125 Test RE 1.1125255645641077\n",
      "188 Train Loss 132541976.0 Test MSE 9812.399765218115 Test RE 1.103676941189856\n",
      "189 Train Loss 132637860.0 Test MSE 9721.051000685367 Test RE 1.098527575515593\n",
      "190 Train Loss 132753000.0 Test MSE 9699.15771636126 Test RE 1.0972898527069805\n",
      "191 Train Loss 132891450.0 Test MSE 9567.341780222288 Test RE 1.0898080128841918\n",
      "192 Train Loss 133041416.0 Test MSE 9385.201243543272 Test RE 1.0793844249413493\n",
      "193 Train Loss 133169544.0 Test MSE 9338.653529804724 Test RE 1.0767043902678095\n",
      "194 Train Loss 133269064.0 Test MSE 9143.152701285844 Test RE 1.0653746011839977\n",
      "195 Train Loss 133388376.0 Test MSE 9085.222286680051 Test RE 1.0619941663952495\n",
      "196 Train Loss 133550680.0 Test MSE 9080.229561292887 Test RE 1.06170232026743\n",
      "197 Train Loss 133712696.0 Test MSE 9077.319079373436 Test RE 1.0615321531455455\n",
      "198 Train Loss 133866520.0 Test MSE 9073.466055434099 Test RE 1.0613068364587552\n",
      "199 Train Loss 133985384.0 Test MSE 8794.923877441484 Test RE 1.0448895716243585\n",
      "Training time: 35.91\n",
      "Training time: 35.91\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "3\n",
      "0 Train Loss 13656823000.0 Test MSE 10856.532890957371 Test RE 1.1609136791869976\n",
      "1 Train Loss 9345696000.0 Test MSE 12967.637605611962 Test RE 1.2687755032072812\n",
      "2 Train Loss 4827394000.0 Test MSE 19119.179069224625 Test RE 1.5405967336748885\n",
      "3 Train Loss 3185686500.0 Test MSE 24916.011712407242 Test RE 1.7587074995490013\n",
      "4 Train Loss 1921409200.0 Test MSE 30618.701007830798 Test RE 1.949609952951561\n",
      "5 Train Loss 807240500.0 Test MSE 29621.53809025694 Test RE 1.9176005917588632\n",
      "6 Train Loss 560677100.0 Test MSE 33031.13159263993 Test RE 2.024958273066037\n",
      "7 Train Loss 488377380.0 Test MSE 35853.202076322086 Test RE 2.1096884576122634\n",
      "8 Train Loss 475850460.0 Test MSE 36826.28961033208 Test RE 2.13812619601745\n",
      "9 Train Loss 465075460.0 Test MSE 39540.88811673743 Test RE 2.215529628020666\n",
      "10 Train Loss 461591400.0 Test MSE 39850.741404289976 Test RE 2.2241934382664432\n",
      "11 Train Loss 454610100.0 Test MSE 38645.878543532526 Test RE 2.190311809084035\n",
      "12 Train Loss 426523230.0 Test MSE 38030.1379789018 Test RE 2.172792747072506\n",
      "13 Train Loss 228738000.0 Test MSE 39092.37834421532 Test RE 2.202928486979404\n",
      "14 Train Loss 160876940.0 Test MSE 41760.25000771367 Test RE 2.2768577451310423\n",
      "15 Train Loss 148240910.0 Test MSE 41886.058495142905 Test RE 2.280284839225456\n",
      "16 Train Loss 139054850.0 Test MSE 42269.25677895663 Test RE 2.290691784236834\n",
      "17 Train Loss 134927920.0 Test MSE 44045.52096108403 Test RE 2.338326911338767\n",
      "18 Train Loss 132652376.0 Test MSE 45006.03810640432 Test RE 2.3636857889877927\n",
      "19 Train Loss 130721780.0 Test MSE 45882.53372428547 Test RE 2.3865912750913023\n",
      "20 Train Loss 128699680.0 Test MSE 47376.148792116495 Test RE 2.425125562219381\n",
      "21 Train Loss 122894300.0 Test MSE 48750.43772895301 Test RE 2.460048176028515\n",
      "22 Train Loss 56522280.0 Test MSE 49446.624076319866 Test RE 2.477551411638513\n",
      "23 Train Loss 27576936.0 Test MSE 51115.349510095526 Test RE 2.519010743198049\n",
      "24 Train Loss 9781000.0 Test MSE 53135.75661271521 Test RE 2.5683120348825397\n",
      "25 Train Loss 5463380.0 Test MSE 52580.10028273599 Test RE 2.5548479434169784\n",
      "26 Train Loss 3504927.2 Test MSE 51681.22707678514 Test RE 2.5329158443070643\n",
      "27 Train Loss 3002491.0 Test MSE 51735.17545060858 Test RE 2.53423751425267\n",
      "28 Train Loss 2299579.8 Test MSE 53041.49982506028 Test RE 2.566033077088055\n",
      "29 Train Loss 1954594.8 Test MSE 53986.40786576691 Test RE 2.588788484365301\n",
      "30 Train Loss 1680606.0 Test MSE 55485.775919082786 Test RE 2.6244915823060504\n",
      "31 Train Loss 1441455.1 Test MSE 56697.226825059864 Test RE 2.652987849505823\n",
      "32 Train Loss 1210221.5 Test MSE 57094.30900046487 Test RE 2.6622618139931853\n",
      "33 Train Loss 1035243.56 Test MSE 57341.84521823688 Test RE 2.6680267797531245\n",
      "34 Train Loss 777723.44 Test MSE 57212.1639599213 Test RE 2.6650081385663698\n",
      "35 Train Loss 436982.03 Test MSE 56165.01084824814 Test RE 2.6405067127699096\n",
      "36 Train Loss 235225.72 Test MSE 56075.95363626556 Test RE 2.638412442204901\n",
      "37 Train Loss 170644.86 Test MSE 55837.59195667388 Test RE 2.632798929495957\n",
      "38 Train Loss 130116.414 Test MSE 55093.65412225819 Test RE 2.615201408317596\n",
      "39 Train Loss 97100.72 Test MSE 54715.19130905764 Test RE 2.6062034381278836\n",
      "40 Train Loss 77200.81 Test MSE 54671.31851253154 Test RE 2.605158350278685\n",
      "41 Train Loss 64227.406 Test MSE 54432.44830881518 Test RE 2.5994608845406217\n",
      "42 Train Loss 55759.125 Test MSE 54465.16379041127 Test RE 2.600241942925265\n",
      "43 Train Loss 51119.67 Test MSE 54727.337430522486 Test RE 2.6064926951530922\n",
      "44 Train Loss 46973.63 Test MSE 54837.207010577215 Test RE 2.609107756340632\n",
      "45 Train Loss 43503.008 Test MSE 54810.992423812604 Test RE 2.608484047899843\n",
      "46 Train Loss 40034.027 Test MSE 54844.9393520998 Test RE 2.6092916989793298\n",
      "47 Train Loss 37551.062 Test MSE 54895.08075017691 Test RE 2.6104841849743505\n",
      "48 Train Loss 36235.523 Test MSE 54987.13989241525 Test RE 2.6126721612209702\n",
      "49 Train Loss 34915.883 Test MSE 55039.234471761105 Test RE 2.613909485404061\n",
      "50 Train Loss 33966.164 Test MSE 54990.77608677149 Test RE 2.6127585452985365\n",
      "51 Train Loss 32834.098 Test MSE 55077.059343501904 Test RE 2.614807515790485\n",
      "52 Train Loss 31714.793 Test MSE 55080.831361542965 Test RE 2.6148970533618017\n",
      "53 Train Loss 30653.273 Test MSE 54960.20674420302 Test RE 2.6120322288164246\n",
      "54 Train Loss 30103.037 Test MSE 54970.99023191343 Test RE 2.612288463623592\n",
      "55 Train Loss 29807.117 Test MSE 55021.37374161302 Test RE 2.6134853323380933\n",
      "56 Train Loss 29372.02 Test MSE 55040.93309199536 Test RE 2.6139498203154057\n",
      "57 Train Loss 29079.207 Test MSE 55143.629888340045 Test RE 2.6163872714667007\n",
      "58 Train Loss 28740.047 Test MSE 55230.17112319324 Test RE 2.6184395177427917\n",
      "59 Train Loss 28498.367 Test MSE 55178.341109468434 Test RE 2.6172106096295376\n",
      "60 Train Loss 28207.756 Test MSE 55104.9606942355 Test RE 2.6154697463550307\n",
      "61 Train Loss 27890.71 Test MSE 54980.45114300648 Test RE 2.6125132509659585\n",
      "62 Train Loss 27746.975 Test MSE 54876.57945913026 Test RE 2.6100442421131937\n",
      "63 Train Loss 27647.502 Test MSE 54801.93923709235 Test RE 2.608268616037448\n",
      "64 Train Loss 27590.963 Test MSE 54725.813527272796 Test RE 2.6064564055168145\n",
      "65 Train Loss 27562.361 Test MSE 54661.659907859066 Test RE 2.6049282176723363\n",
      "66 Train Loss 27531.889 Test MSE 54591.32230147984 Test RE 2.6032516915997195\n",
      "67 Train Loss 27512.367 Test MSE 54559.324434353126 Test RE 2.6024886517261043\n",
      "68 Train Loss 27456.426 Test MSE 54451.456500379514 Test RE 2.5999147198821717\n",
      "69 Train Loss 27417.73 Test MSE 54335.97627293414 Test RE 2.5971563170999303\n",
      "70 Train Loss 27356.994 Test MSE 54179.587777301786 Test RE 2.5934160875833965\n",
      "71 Train Loss 27270.828 Test MSE 54053.39156422739 Test RE 2.590394007670077\n",
      "72 Train Loss 27189.717 Test MSE 53981.99040039141 Test RE 2.5886825677291267\n",
      "73 Train Loss 27131.959 Test MSE 53840.019775311805 Test RE 2.5852762572471586\n",
      "74 Train Loss 27035.793 Test MSE 53641.75979696971 Test RE 2.580511868728281\n",
      "75 Train Loss 26916.646 Test MSE 53421.80389470587 Test RE 2.5752157909277744\n",
      "76 Train Loss 26785.557 Test MSE 53091.27920561774 Test RE 2.5672369041068404\n",
      "77 Train Loss 26654.346 Test MSE 52710.924560451625 Test RE 2.5580243208916293\n",
      "78 Train Loss 26473.387 Test MSE 52207.59803223008 Test RE 2.545781983576926\n",
      "79 Train Loss 26280.043 Test MSE 51715.14083995836 Test RE 2.5337467709832007\n",
      "80 Train Loss 26031.492 Test MSE 51055.282504061564 Test RE 2.5175302298783335\n",
      "81 Train Loss 25721.85 Test MSE 50089.93334573314 Test RE 2.493616019293762\n",
      "82 Train Loss 25568.781 Test MSE 49606.01285776713 Test RE 2.4815413319759068\n",
      "83 Train Loss 25357.521 Test MSE 49113.70828279849 Test RE 2.4691968572082774\n",
      "84 Train Loss 25007.621 Test MSE 48241.330947992195 Test RE 2.4471691716051702\n",
      "85 Train Loss 24374.262 Test MSE 46934.430823034214 Test RE 2.413793592505818\n",
      "86 Train Loss 24167.322 Test MSE 46490.620515644994 Test RE 2.4023541122933914\n",
      "87 Train Loss 24008.004 Test MSE 45994.484682788345 Test RE 2.389501079549124\n",
      "88 Train Loss 23667.607 Test MSE 45412.62107202413 Test RE 2.3743385111510475\n",
      "89 Train Loss 23453.412 Test MSE 45083.11116017908 Test RE 2.365708834803111\n",
      "90 Train Loss 23148.059 Test MSE 44509.45501254947 Test RE 2.350609519844624\n",
      "91 Train Loss 22744.3 Test MSE 43908.71342087308 Test RE 2.334692608243488\n",
      "92 Train Loss 22124.445 Test MSE 42704.8751842974 Test RE 2.302465231252992\n",
      "93 Train Loss 21630.4 Test MSE 41613.093361481246 Test RE 2.2728425579479863\n",
      "94 Train Loss 21278.254 Test MSE 41026.94030522224 Test RE 2.256778403288024\n",
      "95 Train Loss 20804.957 Test MSE 40366.86716361077 Test RE 2.238550391355021\n",
      "96 Train Loss 20435.572 Test MSE 39634.29993788962 Test RE 2.218145079800147\n",
      "97 Train Loss 20068.062 Test MSE 39096.69887684248 Test RE 2.2030502186549987\n",
      "98 Train Loss 19687.0 Test MSE 38277.88672715106 Test RE 2.179858627597995\n",
      "99 Train Loss 19246.826 Test MSE 36946.62942944963 Test RE 2.141616798308333\n",
      "100 Train Loss 18728.537 Test MSE 36098.87271121637 Test RE 2.1169040430724966\n",
      "101 Train Loss 18174.23 Test MSE 34957.73392363895 Test RE 2.083176120470753\n",
      "102 Train Loss 17754.45 Test MSE 34034.1001270016 Test RE 2.055471637124113\n",
      "103 Train Loss 17317.223 Test MSE 33221.69028463092 Test RE 2.0307909292424435\n",
      "104 Train Loss 16662.732 Test MSE 32067.900750567747 Test RE 1.9952146167586395\n",
      "105 Train Loss 16131.501 Test MSE 31158.109740866465 Test RE 1.9667080872883236\n",
      "106 Train Loss 15801.51 Test MSE 30357.265345433076 Test RE 1.9412688047825735\n",
      "107 Train Loss 15472.1045 Test MSE 29555.42782739454 Test RE 1.9154595163720811\n",
      "108 Train Loss 15139.906 Test MSE 28906.645142530157 Test RE 1.8943193604696797\n",
      "109 Train Loss 14909.314 Test MSE 28488.301663062954 Test RE 1.8805618955504364\n",
      "110 Train Loss 14570.416 Test MSE 27925.758907917203 Test RE 1.8619021117159689\n",
      "111 Train Loss 14125.305 Test MSE 27186.220244447766 Test RE 1.8370829556083799\n",
      "112 Train Loss 13516.45 Test MSE 26119.633545409517 Test RE 1.8006856023959337\n",
      "113 Train Loss 13005.621 Test MSE 24925.729773462663 Test RE 1.7590504428909537\n",
      "114 Train Loss 12438.933 Test MSE 23639.94734285494 Test RE 1.713079842170388\n",
      "115 Train Loss 11892.054 Test MSE 22664.94553759058 Test RE 1.6773808952545013\n",
      "116 Train Loss 11611.183 Test MSE 22360.292223483455 Test RE 1.666069407247206\n",
      "117 Train Loss 11312.158 Test MSE 21569.050316581677 Test RE 1.6363261246383256\n",
      "118 Train Loss 10915.34 Test MSE 20817.70187815109 Test RE 1.607573153276246\n",
      "119 Train Loss 10394.382 Test MSE 19693.35955338558 Test RE 1.5635589412532473\n",
      "120 Train Loss 9822.362 Test MSE 18240.069856208396 Test RE 1.5047612571763544\n",
      "121 Train Loss 9469.984 Test MSE 17583.673058973716 Test RE 1.4774376170883199\n",
      "122 Train Loss 9108.02 Test MSE 16951.708407719725 Test RE 1.450644811479797\n",
      "123 Train Loss 8600.203 Test MSE 15977.910340948132 Test RE 1.4083621417232406\n",
      "124 Train Loss 8049.218 Test MSE 14823.731563457679 Test RE 1.3565416213784158\n",
      "125 Train Loss 7701.8037 Test MSE 14222.467268685454 Test RE 1.3287455523259275\n",
      "126 Train Loss 7240.1924 Test MSE 13509.318677751344 Test RE 1.295003897948008\n",
      "127 Train Loss 6906.3545 Test MSE 12793.603168589263 Test RE 1.2602328330805326\n",
      "128 Train Loss 6330.2417 Test MSE 11920.074980917223 Test RE 1.2164488326706464\n",
      "129 Train Loss 5814.3516 Test MSE 11130.151151553333 Test RE 1.1754519572982016\n",
      "130 Train Loss 5487.7 Test MSE 10263.401144036852 Test RE 1.1287558240157103\n",
      "131 Train Loss 5234.406 Test MSE 9590.9849964853 Test RE 1.0911537716074453\n",
      "132 Train Loss 4915.7563 Test MSE 8919.257973011521 Test RE 1.0522494683305064\n",
      "133 Train Loss 4608.9424 Test MSE 8380.942714538996 Test RE 1.0200014483458084\n",
      "134 Train Loss 4295.3315 Test MSE 7708.109628967296 Test RE 0.9782014308655249\n",
      "135 Train Loss 4023.5225 Test MSE 7205.43854416219 Test RE 0.9457678822819026\n",
      "136 Train Loss 3698.2444 Test MSE 6480.609860186153 Test RE 0.896937725495874\n",
      "137 Train Loss 3519.1858 Test MSE 6239.1384231528655 Test RE 0.8800688773052673\n",
      "138 Train Loss 3324.4207 Test MSE 5875.161220114095 Test RE 0.8540125377989507\n",
      "139 Train Loss 3040.6904 Test MSE 5088.294044489358 Test RE 0.7947681463612261\n",
      "140 Train Loss 2757.5818 Test MSE 4547.440113992394 Test RE 0.7513423114362812\n",
      "141 Train Loss 2603.7222 Test MSE 4389.225563269119 Test RE 0.738156251714084\n",
      "142 Train Loss 2407.5647 Test MSE 4100.098597596657 Test RE 0.7134302217904442\n",
      "143 Train Loss 2252.701 Test MSE 3730.3534806252205 Test RE 0.6805019055453682\n",
      "144 Train Loss 1955.7979 Test MSE 3056.006757703208 Test RE 0.6159302123972596\n",
      "145 Train Loss 1764.0996 Test MSE 2657.546573382883 Test RE 0.574374028915155\n",
      "146 Train Loss 1584.2712 Test MSE 2345.945030090957 Test RE 0.5396513503661152\n",
      "147 Train Loss 1471.9517 Test MSE 2141.1344840501106 Test RE 0.5155565696650916\n",
      "148 Train Loss 1315.0845 Test MSE 1948.0337179952153 Test RE 0.49175930598418816\n",
      "149 Train Loss 1245.1621 Test MSE 1862.9040279182748 Test RE 0.4808942598756296\n",
      "150 Train Loss 1169.3928 Test MSE 1727.9538310222365 Test RE 0.46314866820936385\n",
      "151 Train Loss 1067.3276 Test MSE 1496.7485484919061 Test RE 0.4310511173371263\n",
      "152 Train Loss 962.15375 Test MSE 1303.5823122833888 Test RE 0.40227549869057505\n",
      "153 Train Loss 883.90063 Test MSE 1228.2927177293855 Test RE 0.39048584087501836\n",
      "154 Train Loss 802.1251 Test MSE 1205.5867877054916 Test RE 0.386859790463608\n",
      "155 Train Loss 709.72516 Test MSE 1087.1507040917852 Test RE 0.36736622714986733\n",
      "156 Train Loss 612.9624 Test MSE 896.4907719000721 Test RE 0.33360093876498337\n",
      "157 Train Loss 549.07385 Test MSE 770.074759791353 Test RE 0.30918669492750994\n",
      "158 Train Loss 500.2484 Test MSE 731.856474590528 Test RE 0.30141670087377825\n",
      "159 Train Loss 458.87997 Test MSE 703.2428156754792 Test RE 0.29546565414980897\n",
      "160 Train Loss 402.18546 Test MSE 600.6025456907846 Test RE 0.27305361881766\n",
      "161 Train Loss 338.14938 Test MSE 475.59238182799334 Test RE 0.2429807101517115\n",
      "162 Train Loss 294.13757 Test MSE 391.27432156639134 Test RE 0.22039159395484864\n",
      "163 Train Loss 262.55542 Test MSE 335.68783146540716 Test RE 0.2041371990824646\n",
      "164 Train Loss 220.30396 Test MSE 291.4603253018146 Test RE 0.1902146992738762\n",
      "165 Train Loss 192.24683 Test MSE 231.14812529312363 Test RE 0.16939459158043735\n",
      "166 Train Loss 158.18068 Test MSE 175.66282863245507 Test RE 0.14767068458509783\n",
      "167 Train Loss 127.54446 Test MSE 146.12206673746067 Test RE 0.13468283685418286\n",
      "168 Train Loss 103.404396 Test MSE 120.76056884437476 Test RE 0.12243819749692385\n",
      "169 Train Loss 94.894104 Test MSE 106.61730295093446 Test RE 0.11504511920743346\n",
      "170 Train Loss 89.069305 Test MSE 98.90086439686925 Test RE 0.11080373400103613\n",
      "171 Train Loss 75.53916 Test MSE 63.01079465210364 Test RE 0.0884426670788903\n",
      "172 Train Loss 67.97815 Test MSE 44.66580818218106 Test RE 0.07446324369486468\n",
      "173 Train Loss 56.657745 Test MSE 29.638423395928054 Test RE 0.06065713607404265\n",
      "174 Train Loss 46.54629 Test MSE 28.01767173232875 Test RE 0.05897532917976984\n",
      "175 Train Loss 37.041096 Test MSE 18.696400643352693 Test RE 0.04817629031959816\n",
      "176 Train Loss 31.201788 Test MSE 14.491547971383733 Test RE 0.04241424510373156\n",
      "177 Train Loss 27.140533 Test MSE 9.411717722427793 Test RE 0.034181317550390174\n",
      "178 Train Loss 24.07281 Test MSE 6.325293996163387 Test RE 0.028021714916454853\n",
      "179 Train Loss 21.090895 Test MSE 4.736964648686846 Test RE 0.024249591340886206\n",
      "180 Train Loss 16.62043 Test MSE 1.4953916499616697 Test RE 0.01362485308367454\n",
      "181 Train Loss 13.083163 Test MSE 0.27179922191864125 Test RE 0.005808693442595564\n",
      "182 Train Loss 11.96647 Test MSE 0.14346501366721087 Test RE 0.004220144817544634\n",
      "183 Train Loss 10.515669 Test MSE 0.053805745930776666 Test RE 0.002584453249910338\n",
      "184 Train Loss 9.228591 Test MSE 0.1265986006701698 Test RE 0.003964320926744953\n",
      "185 Train Loss 8.608291 Test MSE 0.1942579246749368 Test RE 0.004910703632837299\n",
      "186 Train Loss 7.8780437 Test MSE 0.03709346724456713 Test RE 0.0021458683160255226\n",
      "187 Train Loss 7.3727713 Test MSE 0.0013108457275014568 Test RE 0.0004033946589977198\n",
      "188 Train Loss 7.006546 Test MSE 0.005954532404074885 Test RE 0.0008597618766886366\n",
      "189 Train Loss 6.675432 Test MSE 0.09056431166005136 Test RE 0.0033529949369559653\n",
      "190 Train Loss 6.5807247 Test MSE 0.0658289642938718 Test RE 0.0028586623608641945\n",
      "191 Train Loss 6.4509597 Test MSE 0.0523141787722211 Test RE 0.002548379239529354\n",
      "192 Train Loss 6.2949877 Test MSE 0.08802525353782353 Test RE 0.0033056585691278624\n",
      "193 Train Loss 6.021224 Test MSE 0.05018488571734733 Test RE 0.002495978396677334\n",
      "194 Train Loss 5.6860676 Test MSE 0.03653409986048977 Test RE 0.002129627067856441\n",
      "195 Train Loss 5.2276373 Test MSE 0.04947563855437423 Test RE 0.0024782781985555062\n",
      "196 Train Loss 5.035011 Test MSE 0.032098235672601824 Test RE 0.001996158089385825\n",
      "197 Train Loss 4.905416 Test MSE 0.04309304925762458 Test RE 0.0023129059067954305\n",
      "198 Train Loss 4.752153 Test MSE 0.08143181214976092 Test RE 0.0031794456253077317\n",
      "199 Train Loss 4.5880156 Test MSE 0.03410636531725036 Test RE 0.002057652689114234\n",
      "Training time: 47.07\n",
      "Training time: 47.07\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "4\n",
      "0 Train Loss 12750712000.0 Test MSE 12537.085417463655 Test RE 1.2475347300116921\n",
      "1 Train Loss 8711247000.0 Test MSE 18761.909038707607 Test RE 1.5261346940233596\n",
      "2 Train Loss 4269965300.0 Test MSE 33461.756650944975 Test RE 2.03811516569848\n",
      "3 Train Loss 1777815300.0 Test MSE 37591.31937321253 Test RE 2.160220766699173\n",
      "4 Train Loss 1131856600.0 Test MSE 43766.90715086996 Test RE 2.3309195326532706\n",
      "5 Train Loss 911214850.0 Test MSE 44516.17913437775 Test RE 2.3507870685010928\n",
      "6 Train Loss 713192770.0 Test MSE 52846.29261734959 Test RE 2.561306873330151\n",
      "7 Train Loss 615529540.0 Test MSE 59473.791132562685 Test RE 2.717172208921253\n",
      "8 Train Loss 544245300.0 Test MSE 63594.5991312763 Test RE 2.809729229192485\n",
      "9 Train Loss 490399140.0 Test MSE 67368.52617438462 Test RE 2.891897364890035\n",
      "10 Train Loss 448170560.0 Test MSE 74954.96840937495 Test RE 3.0503843567186877\n",
      "11 Train Loss 405808540.0 Test MSE 82457.09712223448 Test RE 3.199398767098278\n",
      "12 Train Loss 344066100.0 Test MSE 89833.17458334895 Test RE 3.3394329346636327\n",
      "13 Train Loss 309940130.0 Test MSE 88505.6870174542 Test RE 3.314667279054393\n",
      "14 Train Loss 267230020.0 Test MSE 92205.78980693557 Test RE 3.383244998480186\n",
      "15 Train Loss 232900850.0 Test MSE 94315.66496835569 Test RE 3.4217341789047873\n",
      "16 Train Loss 191036860.0 Test MSE 89606.21468970821 Test RE 3.3352117955089082\n",
      "17 Train Loss 173170200.0 Test MSE 89259.89584688707 Test RE 3.3287604299993787\n",
      "18 Train Loss 163772580.0 Test MSE 91154.18213557488 Test RE 3.363896707113007\n",
      "19 Train Loss 158670460.0 Test MSE 91419.91708540737 Test RE 3.368796396661914\n",
      "20 Train Loss 149055650.0 Test MSE 92254.66904051392 Test RE 3.3841416260810564\n",
      "21 Train Loss 145331650.0 Test MSE 91456.48542746653 Test RE 3.369470095490872\n",
      "22 Train Loss 143654780.0 Test MSE 91260.62773845054 Test RE 3.3658602348999676\n",
      "23 Train Loss 142548240.0 Test MSE 92398.23499949803 Test RE 3.3867737891568166\n",
      "24 Train Loss 141301400.0 Test MSE 92380.99489108035 Test RE 3.3864578141213593\n",
      "25 Train Loss 140364910.0 Test MSE 92302.63913966938 Test RE 3.3850213458484544\n",
      "26 Train Loss 139369920.0 Test MSE 93444.14615669652 Test RE 3.4058883132321114\n",
      "27 Train Loss 138314300.0 Test MSE 94630.50858875264 Test RE 3.427440620180522\n",
      "28 Train Loss 136480770.0 Test MSE 96775.98766753614 Test RE 3.466076617100052\n",
      "29 Train Loss 135443550.0 Test MSE 97139.43523470123 Test RE 3.472579038863422\n",
      "30 Train Loss 134255410.0 Test MSE 98853.1167169887 Test RE 3.5030758074916077\n",
      "31 Train Loss 132688180.0 Test MSE 100401.55374188085 Test RE 3.5304053225358745\n",
      "32 Train Loss 130387090.0 Test MSE 102620.0922724348 Test RE 3.5691972751680776\n",
      "33 Train Loss 103890460.0 Test MSE 107443.62293122383 Test RE 3.65211693386579\n",
      "34 Train Loss 92062110.0 Test MSE 108366.67864077463 Test RE 3.667771180605186\n",
      "35 Train Loss 83436470.0 Test MSE 114697.26265551314 Test RE 3.7733829170191115\n",
      "36 Train Loss 71769600.0 Test MSE 121837.55575733962 Test RE 3.889062666882199\n",
      "37 Train Loss 63181016.0 Test MSE 125284.00523647793 Test RE 3.943684530239207\n",
      "38 Train Loss 60108210.0 Test MSE 124702.68111170549 Test RE 3.9345244441659486\n",
      "39 Train Loss 57296024.0 Test MSE 124780.0839749084 Test RE 3.9357453329703866\n",
      "40 Train Loss 52723556.0 Test MSE 125327.04470354968 Test RE 3.9443618693187457\n",
      "41 Train Loss 47565676.0 Test MSE 125984.01844118958 Test RE 3.954686676050099\n",
      "42 Train Loss 43155056.0 Test MSE 130200.30356646697 Test RE 4.0203174851991825\n",
      "43 Train Loss 37390770.0 Test MSE 131195.34651924993 Test RE 4.035650684876347\n",
      "44 Train Loss 34482956.0 Test MSE 130462.63103046661 Test RE 4.0243655134071705\n",
      "45 Train Loss 31708234.0 Test MSE 129101.83848063838 Test RE 4.0033223926407775\n",
      "46 Train Loss 28894518.0 Test MSE 127389.90797566052 Test RE 3.9766911638123488\n",
      "47 Train Loss 27292390.0 Test MSE 126009.37076790557 Test RE 3.9550845656651914\n",
      "48 Train Loss 25914268.0 Test MSE 126058.22430374696 Test RE 3.9558511798475933\n",
      "49 Train Loss 25343106.0 Test MSE 126277.74859488764 Test RE 3.9592941431617743\n",
      "50 Train Loss 24767998.0 Test MSE 124808.77714522489 Test RE 3.936197819121646\n",
      "51 Train Loss 24211416.0 Test MSE 123253.2575571877 Test RE 3.9115920569072133\n",
      "52 Train Loss 23866244.0 Test MSE 122840.2547153599 Test RE 3.9050329838974367\n",
      "53 Train Loss 23580872.0 Test MSE 123834.36752710507 Test RE 3.920802329494619\n",
      "54 Train Loss 23366600.0 Test MSE 124002.06383649855 Test RE 3.9234562035623903\n",
      "55 Train Loss 23131478.0 Test MSE 124807.60068811083 Test RE 3.9361792676265304\n",
      "56 Train Loss 22969010.0 Test MSE 125561.55166716929 Test RE 3.9480504109075207\n",
      "57 Train Loss 22834722.0 Test MSE 125410.79190316031 Test RE 3.9456795182591535\n",
      "58 Train Loss 22753242.0 Test MSE 124236.55325107601 Test RE 3.927164103227761\n",
      "59 Train Loss 22727784.0 Test MSE 124156.13386655175 Test RE 3.925892853999348\n",
      "60 Train Loss 22657952.0 Test MSE 124841.53617608405 Test RE 3.936714359579161\n",
      "61 Train Loss 22546026.0 Test MSE 125045.97812943817 Test RE 3.939936445625446\n",
      "62 Train Loss 22482426.0 Test MSE 125499.30505852484 Test RE 3.9470716749376047\n",
      "63 Train Loss 22404140.0 Test MSE 125545.54879848985 Test RE 3.9477988126091623\n",
      "64 Train Loss 22326154.0 Test MSE 125242.79372594933 Test RE 3.9430358498115563\n",
      "65 Train Loss 22237192.0 Test MSE 125287.29762482461 Test RE 3.9437363487281796\n",
      "66 Train Loss 22144336.0 Test MSE 125301.95681538327 Test RE 3.9439670596333416\n",
      "67 Train Loss 22103240.0 Test MSE 125069.56860424395 Test RE 3.940308071285228\n",
      "68 Train Loss 22044394.0 Test MSE 124577.12835009852 Test RE 3.932543272601427\n",
      "69 Train Loss 21990002.0 Test MSE 124237.34364562009 Test RE 3.927176595542071\n",
      "70 Train Loss 21937196.0 Test MSE 122862.42329186639 Test RE 3.9053853322270227\n",
      "71 Train Loss 21898788.0 Test MSE 122247.20313747857 Test RE 3.895595165974997\n",
      "72 Train Loss 21871064.0 Test MSE 122878.09425276108 Test RE 3.9056343879910678\n",
      "73 Train Loss 21843276.0 Test MSE 124115.63112537017 Test RE 3.9252524410470526\n",
      "74 Train Loss 21824448.0 Test MSE 124563.91965877947 Test RE 3.9323347867909755\n",
      "75 Train Loss 21791138.0 Test MSE 125488.71147117318 Test RE 3.9469050822581555\n",
      "76 Train Loss 21747040.0 Test MSE 126195.53832073383 Test RE 3.9580051288487867\n",
      "77 Train Loss 21716870.0 Test MSE 126678.69888211686 Test RE 3.965574830051823\n",
      "78 Train Loss 21710952.0 Test MSE 127334.27953708691 Test RE 3.9758228011644974\n",
      "79 Train Loss 21690222.0 Test MSE 127975.8499036479 Test RE 3.985826254960836\n",
      "80 Train Loss 21575302.0 Test MSE 126444.76626697922 Test RE 3.9619116018572704\n",
      "81 Train Loss 20568616.0 Test MSE 128895.22645583338 Test RE 4.000117691016315\n",
      "82 Train Loss 13519422.0 Test MSE 126293.14789080415 Test RE 3.959535549443255\n",
      "83 Train Loss 8271195.5 Test MSE 125896.4301915226 Test RE 3.953311722648972\n",
      "84 Train Loss 5812490.5 Test MSE 125004.00862501326 Test RE 3.939275204617518\n",
      "85 Train Loss 4491875.0 Test MSE 121114.75146920785 Test RE 3.87750952673771\n",
      "86 Train Loss 3531972.2 Test MSE 118638.0810778295 Test RE 3.8376592367090336\n",
      "87 Train Loss 2972218.5 Test MSE 119615.47578021274 Test RE 3.8534350062191773\n",
      "88 Train Loss 2437409.0 Test MSE 122452.21020085156 Test RE 3.898860230386048\n",
      "89 Train Loss 2156649.8 Test MSE 124686.54474013281 Test RE 3.93426987464985\n",
      "90 Train Loss 1921644.1 Test MSE 125475.90512742498 Test RE 3.946703682816483\n",
      "91 Train Loss 1751001.5 Test MSE 125564.57021253655 Test RE 3.9480978669061737\n",
      "92 Train Loss 1601800.5 Test MSE 125493.84368455954 Test RE 3.946985791318574\n",
      "93 Train Loss 1390718.1 Test MSE 125853.73882578 Test RE 3.9526413835994565\n",
      "94 Train Loss 1206278.8 Test MSE 127364.60626571307 Test RE 3.9762962263933637\n",
      "95 Train Loss 1053499.5 Test MSE 128710.83753428973 Test RE 3.9972555161379635\n",
      "96 Train Loss 948220.0 Test MSE 130031.79658196487 Test RE 4.017715068417884\n",
      "97 Train Loss 881966.6 Test MSE 130413.12004282992 Test RE 4.023601811122358\n",
      "98 Train Loss 781386.25 Test MSE 128875.45201903934 Test RE 3.999810840597418\n",
      "99 Train Loss 696821.5 Test MSE 128364.86182763052 Test RE 3.9918795745047975\n",
      "100 Train Loss 614774.7 Test MSE 128421.16026932008 Test RE 3.9927548606241303\n",
      "101 Train Loss 552028.56 Test MSE 126912.0378053939 Test RE 3.969225393468799\n",
      "102 Train Loss 486380.38 Test MSE 127508.85461296406 Test RE 3.978547290740977\n",
      "103 Train Loss 342061.3 Test MSE 129570.27488766563 Test RE 4.0105786949546225\n",
      "104 Train Loss 268962.34 Test MSE 130372.63692829122 Test RE 4.02297725518853\n",
      "105 Train Loss 247846.27 Test MSE 129821.41504054728 Test RE 4.014463574256723\n",
      "106 Train Loss 222713.55 Test MSE 128796.62808901629 Test RE 3.9985874538766972\n",
      "107 Train Loss 189758.8 Test MSE 126659.81959311156 Test RE 3.9652793185586646\n",
      "108 Train Loss 174553.12 Test MSE 126039.2292212081 Test RE 3.9555531249170914\n",
      "109 Train Loss 168005.67 Test MSE 126442.36373652272 Test RE 3.96187396226654\n",
      "110 Train Loss 163506.14 Test MSE 126350.32434044957 Test RE 3.960431744411147\n",
      "111 Train Loss 157296.31 Test MSE 125927.29011839375 Test RE 3.953796213890106\n",
      "112 Train Loss 146144.47 Test MSE 125872.05633837548 Test RE 3.952929018773112\n",
      "113 Train Loss 138660.38 Test MSE 125934.77444188057 Test RE 3.9539137064930525\n",
      "114 Train Loss 132367.03 Test MSE 125359.64825818824 Test RE 3.9448748944816177\n",
      "115 Train Loss 127552.31 Test MSE 125184.00388963573 Test RE 3.9421102969911708\n",
      "116 Train Loss 119520.35 Test MSE 125212.45509231153 Test RE 3.942558243228721\n",
      "117 Train Loss 114586.17 Test MSE 124818.43321702654 Test RE 3.9363500819453034\n",
      "118 Train Loss 111576.05 Test MSE 124529.53120118384 Test RE 3.931791947970646\n",
      "119 Train Loss 108709.016 Test MSE 124309.2808768641 Test RE 3.9283134088339438\n",
      "120 Train Loss 105448.25 Test MSE 123857.46653891711 Test RE 3.9211679890370377\n",
      "121 Train Loss 102301.26 Test MSE 123985.53831867699 Test RE 3.923194759100368\n",
      "122 Train Loss 98424.89 Test MSE 123822.54483933335 Test RE 3.920615162034084\n",
      "123 Train Loss 94877.72 Test MSE 123556.01529659414 Test RE 3.9163933028567484\n",
      "124 Train Loss 92012.77 Test MSE 123801.79958541668 Test RE 3.920286717952588\n",
      "125 Train Loss 89722.77 Test MSE 123649.1491350964 Test RE 3.9178690708777517\n",
      "126 Train Loss 87551.664 Test MSE 123380.56578885276 Test RE 3.9136116763222226\n",
      "127 Train Loss 86018.45 Test MSE 123271.63550713715 Test RE 3.9118836693357593\n",
      "128 Train Loss 84516.62 Test MSE 123336.24953581319 Test RE 3.91290876099304\n",
      "129 Train Loss 83968.1 Test MSE 123358.97935241075 Test RE 3.913269302202563\n",
      "130 Train Loss 82807.91 Test MSE 122775.04830229361 Test RE 3.9039964059669137\n",
      "131 Train Loss 80888.07 Test MSE 122062.11225981107 Test RE 3.8926449460819663\n",
      "132 Train Loss 79780.984 Test MSE 122271.19465836477 Test RE 3.8959774105797833\n",
      "133 Train Loss 78206.92 Test MSE 122817.80469689486 Test RE 3.904676129913901\n",
      "134 Train Loss 76818.445 Test MSE 122836.43663406163 Test RE 3.904972295936084\n",
      "135 Train Loss 75702.64 Test MSE 122975.43326065113 Test RE 3.907181023783738\n",
      "136 Train Loss 74691.75 Test MSE 122889.8955540902 Test RE 3.905821933468399\n",
      "137 Train Loss 73520.0 Test MSE 122912.98718121214 Test RE 3.906188877987359\n",
      "138 Train Loss 72875.37 Test MSE 123051.70250305276 Test RE 3.9083924508897727\n",
      "139 Train Loss 72152.16 Test MSE 123296.02275932637 Test RE 3.9122706009089505\n",
      "140 Train Loss 71563.51 Test MSE 123699.15639062843 Test RE 3.9186612399951266\n",
      "141 Train Loss 70914.61 Test MSE 124025.08150036544 Test RE 3.9238203289762428\n",
      "142 Train Loss 70238.67 Test MSE 124331.11539165587 Test RE 3.928658391326521\n",
      "143 Train Loss 69599.57 Test MSE 125013.73070944166 Test RE 3.9394283885910872\n",
      "144 Train Loss 68844.74 Test MSE 125476.11422223858 Test RE 3.9467069712363845\n",
      "145 Train Loss 68408.93 Test MSE 125430.44174948607 Test RE 3.9459886182905093\n",
      "146 Train Loss 68070.95 Test MSE 125789.65660623224 Test RE 3.951634952298396\n",
      "147 Train Loss 67436.555 Test MSE 125993.95866780521 Test RE 3.9548426867364737\n",
      "148 Train Loss 67006.99 Test MSE 125901.28666543897 Test RE 3.953387971713723\n",
      "149 Train Loss 66724.14 Test MSE 126047.87672064806 Test RE 3.9556888170224576\n",
      "150 Train Loss 66349.33 Test MSE 126129.71541535282 Test RE 3.9569727572537134\n",
      "151 Train Loss 65913.414 Test MSE 126143.41173162029 Test RE 3.9571875935394134\n",
      "152 Train Loss 65625.98 Test MSE 126050.13002382347 Test RE 3.955724173930362\n",
      "153 Train Loss 65484.51 Test MSE 125829.93565248454 Test RE 3.952267577235792\n",
      "154 Train Loss 65342.332 Test MSE 125592.89303656833 Test RE 3.9485431158165856\n",
      "155 Train Loss 65089.21 Test MSE 125418.17074401751 Test RE 3.9457955932498052\n",
      "156 Train Loss 64639.78 Test MSE 124742.6314080168 Test RE 3.9351546344267017\n",
      "157 Train Loss 64431.484 Test MSE 124396.18728580013 Test RE 3.929686339162759\n",
      "158 Train Loss 64346.727 Test MSE 124258.87371599252 Test RE 3.9275168665230646\n",
      "159 Train Loss 64232.418 Test MSE 123850.84318254469 Test RE 3.921063144166033\n",
      "160 Train Loss 64023.008 Test MSE 123644.2039402482 Test RE 3.917790724929557\n",
      "161 Train Loss 63891.46 Test MSE 123233.5630458544 Test RE 3.911279529785088\n",
      "162 Train Loss 63716.07 Test MSE 122689.82989121485 Test RE 3.9026412848497474\n",
      "163 Train Loss 63565.277 Test MSE 122310.51455114491 Test RE 3.896603793191104\n",
      "164 Train Loss 63384.97 Test MSE 121698.82278751273 Test RE 3.8868478535274766\n",
      "165 Train Loss 63267.445 Test MSE 121731.78507329087 Test RE 3.887374196809352\n",
      "166 Train Loss 63066.965 Test MSE 121561.47450563525 Test RE 3.884653902211324\n",
      "167 Train Loss 62840.52 Test MSE 121200.53669247068 Test RE 3.8788824979758227\n",
      "168 Train Loss 62617.32 Test MSE 120679.16534019442 Test RE 3.870530563881158\n",
      "169 Train Loss 62458.69 Test MSE 120395.82484012742 Test RE 3.865984118200134\n",
      "170 Train Loss 62100.953 Test MSE 119849.17153693996 Test RE 3.857197445758668\n",
      "171 Train Loss 61834.76 Test MSE 119310.73945677071 Test RE 3.84852330699047\n",
      "172 Train Loss 61712.02 Test MSE 118751.83437676137 Test RE 3.8394986199892345\n",
      "173 Train Loss 61536.133 Test MSE 118495.4404571466 Test RE 3.8353515007423598\n",
      "174 Train Loss 61372.176 Test MSE 118071.4671450522 Test RE 3.8284839630402723\n",
      "175 Train Loss 61251.07 Test MSE 117636.8266425941 Test RE 3.821430826341477\n",
      "176 Train Loss 60963.734 Test MSE 116934.87864882867 Test RE 3.810012381318314\n",
      "177 Train Loss 60635.473 Test MSE 116087.52168495863 Test RE 3.7961828443502017\n",
      "178 Train Loss 60334.242 Test MSE 115596.38423045921 Test RE 3.7881439796295173\n",
      "179 Train Loss 60197.312 Test MSE 115196.8213444009 Test RE 3.7815913873017992\n",
      "180 Train Loss 60120.707 Test MSE 115090.75491477447 Test RE 3.7798500533748793\n",
      "181 Train Loss 59995.78 Test MSE 115038.55255110883 Test RE 3.7789927321814085\n",
      "182 Train Loss 59895.65 Test MSE 114835.63191024044 Test RE 3.775658310499058\n",
      "183 Train Loss 59699.527 Test MSE 114366.72298976096 Test RE 3.7679418428667244\n",
      "184 Train Loss 59466.555 Test MSE 113836.4136165658 Test RE 3.759195869584544\n",
      "185 Train Loss 59224.54 Test MSE 113394.7178578118 Test RE 3.7518957685101872\n",
      "186 Train Loss 58960.684 Test MSE 112928.0808161936 Test RE 3.744167990411218\n",
      "187 Train Loss 58732.125 Test MSE 112334.31281343737 Test RE 3.7343117298772266\n",
      "188 Train Loss 58559.285 Test MSE 111980.52681160037 Test RE 3.7284266666634567\n",
      "189 Train Loss 58323.016 Test MSE 111309.3891796206 Test RE 3.7172370069104255\n",
      "190 Train Loss 57895.96 Test MSE 110219.01097379379 Test RE 3.6989853151141148\n",
      "191 Train Loss 57509.17 Test MSE 109644.93043610745 Test RE 3.6893395752810876\n",
      "192 Train Loss 57281.96 Test MSE 109173.7102505213 Test RE 3.681403215742086\n",
      "193 Train Loss 57051.176 Test MSE 108552.85805955429 Test RE 3.670920536423684\n",
      "194 Train Loss 56814.812 Test MSE 107720.38220415833 Test RE 3.656817571956184\n",
      "195 Train Loss 56460.547 Test MSE 107161.49561178556 Test RE 3.647318886258377\n",
      "196 Train Loss 56211.68 Test MSE 106546.95869297188 Test RE 3.636845745742046\n",
      "197 Train Loss 55933.945 Test MSE 105735.04600705123 Test RE 3.6229624400708245\n",
      "198 Train Loss 55727.344 Test MSE 105317.38900233347 Test RE 3.615799948018353\n",
      "199 Train Loss 55525.047 Test MSE 104767.24505629276 Test RE 3.6063436985549275\n",
      "Training time: 48.27\n",
      "Training time: 48.27\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "5\n",
      "0 Train Loss 12894140000.0 Test MSE 16043.252180703112 Test RE 1.411238959749631\n",
      "1 Train Loss 9486452000.0 Test MSE 19835.5923407389 Test RE 1.5691950858796644\n",
      "2 Train Loss 6935543000.0 Test MSE 30385.783405952312 Test RE 1.9421804189373175\n",
      "3 Train Loss 3633435400.0 Test MSE 31912.234851094323 Test RE 1.990366081220146\n",
      "4 Train Loss 2357264600.0 Test MSE 43140.64794241788 Test RE 2.314182921272164\n",
      "5 Train Loss 1739783200.0 Test MSE 46314.589702093355 Test RE 2.397801694977849\n",
      "6 Train Loss 1464140500.0 Test MSE 49785.41137675549 Test RE 2.4860244887669767\n",
      "7 Train Loss 1287785500.0 Test MSE 54047.487273595965 Test RE 2.590252528511165\n",
      "8 Train Loss 1102998900.0 Test MSE 58850.25554150674 Test RE 2.7028909792732865\n",
      "9 Train Loss 950003000.0 Test MSE 65951.68260040342 Test RE 2.8613256783325602\n",
      "10 Train Loss 845941400.0 Test MSE 70508.64649133736 Test RE 2.9585270287498453\n",
      "11 Train Loss 738143700.0 Test MSE 79184.48993707102 Test RE 3.135266153820491\n",
      "12 Train Loss 614067650.0 Test MSE 87501.34110084109 Test RE 3.2958065083426127\n",
      "13 Train Loss 561515200.0 Test MSE 91241.4805764782 Test RE 3.365507125057344\n",
      "14 Train Loss 518070900.0 Test MSE 97410.66605397614 Test RE 3.4774236928583764\n",
      "15 Train Loss 495051100.0 Test MSE 101218.70962065765 Test RE 3.544742975584861\n",
      "16 Train Loss 478036400.0 Test MSE 102083.4204324957 Test RE 3.559852133536354\n",
      "17 Train Loss 466371650.0 Test MSE 107009.9027493689 Test RE 3.6447381869877207\n",
      "18 Train Loss 459319970.0 Test MSE 108961.7044902962 Test RE 3.677826997807951\n",
      "19 Train Loss 454447200.0 Test MSE 106860.84691077714 Test RE 3.6421988947957966\n",
      "20 Train Loss 447006880.0 Test MSE 107485.00279993174 Test RE 3.6528201378683303\n",
      "21 Train Loss 437700260.0 Test MSE 108754.51527703668 Test RE 3.674328664762295\n",
      "22 Train Loss 433603780.0 Test MSE 109907.56628560658 Test RE 3.693755526310069\n",
      "23 Train Loss 425534820.0 Test MSE 112909.38348674607 Test RE 3.7438580194994135\n",
      "24 Train Loss 419829000.0 Test MSE 113162.12538413043 Test RE 3.7480458948567583\n",
      "25 Train Loss 414339650.0 Test MSE 115048.65993602524 Test RE 3.779158741290323\n",
      "26 Train Loss 409643260.0 Test MSE 116506.15887550598 Test RE 3.803021620400201\n",
      "27 Train Loss 406605380.0 Test MSE 117505.91815151028 Test RE 3.8193039541318705\n",
      "28 Train Loss 402860670.0 Test MSE 120034.3608836129 Test RE 3.8601763405749154\n",
      "29 Train Loss 401734140.0 Test MSE 120297.90516183939 Test RE 3.864411667809094\n",
      "30 Train Loss 399422430.0 Test MSE 121896.77205853623 Test RE 3.890007646158634\n",
      "31 Train Loss 398770980.0 Test MSE 124739.00906949944 Test RE 3.935097498524057\n",
      "32 Train Loss 396103600.0 Test MSE 125723.00971315315 Test RE 3.9505879700069237\n",
      "33 Train Loss 380516220.0 Test MSE 128147.12480053896 Test RE 3.988492553808134\n",
      "34 Train Loss 376925150.0 Test MSE 127669.80292630628 Test RE 3.981057463613284\n",
      "35 Train Loss 374890020.0 Test MSE 120104.92455689926 Test RE 3.861310799908856\n",
      "36 Train Loss 330882750.0 Test MSE 119254.70396142178 Test RE 3.847619451906641\n",
      "37 Train Loss 262582720.0 Test MSE 114228.59246273876 Test RE 3.765665721686813\n",
      "38 Train Loss 211527760.0 Test MSE 117552.8283287153 Test RE 3.820066240620897\n",
      "39 Train Loss 163754400.0 Test MSE 120888.63719595046 Test RE 3.873888292182384\n",
      "40 Train Loss 141266180.0 Test MSE 119818.27382864384 Test RE 3.856700211434113\n",
      "41 Train Loss 114024750.0 Test MSE 118367.06339731102 Test RE 3.833273342387575\n",
      "42 Train Loss 82952824.0 Test MSE 122600.43094894734 Test RE 3.90121918009119\n",
      "43 Train Loss 70589930.0 Test MSE 130403.39735377017 Test RE 4.02345182253817\n",
      "44 Train Loss 59980764.0 Test MSE 140283.30185674728 Test RE 4.17308606888917\n",
      "45 Train Loss 48862596.0 Test MSE 143150.1062626286 Test RE 4.215510639193929\n",
      "46 Train Loss 35759420.0 Test MSE 144728.26654856323 Test RE 4.238683922792825\n",
      "47 Train Loss 30210172.0 Test MSE 143172.0414888046 Test RE 4.2158336031082575\n",
      "48 Train Loss 21812758.0 Test MSE 141026.82218786416 Test RE 4.184130412277852\n",
      "49 Train Loss 18395120.0 Test MSE 140289.38577372584 Test RE 4.173176558753567\n",
      "50 Train Loss 16203576.0 Test MSE 142489.18780095654 Test RE 4.2057679566218376\n",
      "51 Train Loss 14744502.0 Test MSE 143487.96119845856 Test RE 4.220482314577492\n",
      "52 Train Loss 13235503.0 Test MSE 144421.50202595902 Test RE 4.23418940480373\n",
      "53 Train Loss 11229383.0 Test MSE 144677.42055042702 Test RE 4.237939289237613\n",
      "54 Train Loss 8186548.0 Test MSE 141206.98262855763 Test RE 4.186802152919572\n",
      "55 Train Loss 5874601.0 Test MSE 138307.32540365323 Test RE 4.14359159865588\n",
      "56 Train Loss 5066019.0 Test MSE 139366.88446467972 Test RE 4.159433143912645\n",
      "57 Train Loss 4558537.0 Test MSE 139686.98357906536 Test RE 4.164207115996955\n",
      "58 Train Loss 4190398.0 Test MSE 139347.74516842555 Test RE 4.159147526002576\n",
      "59 Train Loss 3939147.5 Test MSE 139849.5306992568 Test RE 4.166629256766551\n",
      "60 Train Loss 3534142.8 Test MSE 140045.90122421083 Test RE 4.169553528841863\n",
      "61 Train Loss 3134594.0 Test MSE 138549.57785346222 Test RE 4.147218868750007\n",
      "62 Train Loss 2800534.8 Test MSE 137199.02255808114 Test RE 4.126956212890584\n",
      "63 Train Loss 2436391.5 Test MSE 135770.94678425332 Test RE 4.105421719629203\n",
      "64 Train Loss 2294403.8 Test MSE 134948.93703736845 Test RE 4.092974947328706\n",
      "65 Train Loss 2041740.4 Test MSE 135485.8929674724 Test RE 4.101109747689133\n",
      "66 Train Loss 1865036.6 Test MSE 137187.00215822627 Test RE 4.126775421695602\n",
      "67 Train Loss 1784754.9 Test MSE 138483.4016031528 Test RE 4.146228320177038\n",
      "68 Train Loss 1730062.9 Test MSE 140032.26968241483 Test RE 4.169350599567048\n",
      "69 Train Loss 1702315.6 Test MSE 140686.8367833646 Test RE 4.179083848633893\n",
      "70 Train Loss 1680701.4 Test MSE 140859.01109192532 Test RE 4.181640274239828\n",
      "71 Train Loss 1668041.5 Test MSE 140680.3500643683 Test RE 4.178987503959112\n",
      "72 Train Loss 1653835.0 Test MSE 140513.90873073827 Test RE 4.1765146564506015\n",
      "73 Train Loss 1630014.8 Test MSE 139937.91620363283 Test RE 4.167945712595716\n",
      "74 Train Loss 1612331.4 Test MSE 138825.86102448803 Test RE 4.151351815779603\n",
      "75 Train Loss 1599824.2 Test MSE 137917.51035383015 Test RE 4.137748184036013\n",
      "76 Train Loss 1588878.2 Test MSE 137032.1552511353 Test RE 4.124445759184955\n",
      "77 Train Loss 1581280.9 Test MSE 136901.38493017846 Test RE 4.122477302097956\n",
      "78 Train Loss 1576174.8 Test MSE 137389.77040626694 Test RE 4.129824070760987\n",
      "79 Train Loss 1568171.8 Test MSE 137653.773135458 Test RE 4.133790019474429\n",
      "80 Train Loss 1563113.6 Test MSE 137210.7879015473 Test RE 4.127133160275151\n",
      "81 Train Loss 1550141.6 Test MSE 137399.47469204193 Test RE 4.129969919630219\n",
      "82 Train Loss 1540383.9 Test MSE 137848.02917496985 Test RE 4.136705778908315\n",
      "83 Train Loss 1538333.9 Test MSE 137841.14307779924 Test RE 4.1366024545667\n",
      "84 Train Loss 1535321.2 Test MSE 137978.43936195964 Test RE 4.138662067465918\n",
      "85 Train Loss 1526549.2 Test MSE 138566.85977741537 Test RE 4.1474775114949525\n",
      "86 Train Loss 1517594.8 Test MSE 138571.75383688646 Test RE 4.147550753475053\n",
      "87 Train Loss 1508780.1 Test MSE 138366.71706799683 Test RE 4.144481169714278\n",
      "88 Train Loss 1505361.1 Test MSE 138274.81714589344 Test RE 4.143104607603787\n",
      "89 Train Loss 1504556.6 Test MSE 138406.01190008206 Test RE 4.145069624562013\n",
      "90 Train Loss 1500125.5 Test MSE 138579.60925668993 Test RE 4.147668310948699\n",
      "91 Train Loss 1495352.1 Test MSE 138508.07133926134 Test RE 4.146597612822754\n",
      "92 Train Loss 1490509.9 Test MSE 137991.44537100338 Test RE 4.138857120432622\n",
      "93 Train Loss 1483854.1 Test MSE 137351.46487395433 Test RE 4.129248314148902\n",
      "94 Train Loss 1470505.0 Test MSE 137908.93892730362 Test RE 4.137619603714629\n",
      "95 Train Loss 1463512.9 Test MSE 138507.75006617396 Test RE 4.146592803749257\n",
      "96 Train Loss 1460632.4 Test MSE 137907.38938600404 Test RE 4.137596358555477\n",
      "97 Train Loss 1459785.8 Test MSE 137709.72264754013 Test RE 4.134630025646405\n",
      "98 Train Loss 1459402.8 Test MSE 137910.32315509437 Test RE 4.137640368843111\n",
      "99 Train Loss 1458714.6 Test MSE 137646.27805748492 Test RE 4.133677478061383\n",
      "100 Train Loss 1457901.6 Test MSE 137214.9056952576 Test RE 4.127195088916932\n",
      "101 Train Loss 1457502.2 Test MSE 137147.58386145416 Test RE 4.126182500564099\n",
      "102 Train Loss 1456825.2 Test MSE 136902.33322499652 Test RE 4.122491579956049\n",
      "103 Train Loss 1455280.0 Test MSE 137094.30567475356 Test RE 4.125380966663004\n",
      "104 Train Loss 1453060.0 Test MSE 136707.79994590144 Test RE 4.119561582787692\n",
      "105 Train Loss 1452053.8 Test MSE 136279.1035448977 Test RE 4.11309732868457\n",
      "106 Train Loss 1451677.8 Test MSE 136367.06526290055 Test RE 4.114424519487705\n",
      "107 Train Loss 1450596.1 Test MSE 136464.06728744623 Test RE 4.115887616747948\n",
      "108 Train Loss 1448095.4 Test MSE 136158.3054050839 Test RE 4.111273994639413\n",
      "109 Train Loss 1445856.1 Test MSE 135397.32438097024 Test RE 4.099769058755937\n",
      "110 Train Loss 1444375.9 Test MSE 134982.27137350186 Test RE 4.093480428072988\n",
      "111 Train Loss 1443021.1 Test MSE 135165.9546648163 Test RE 4.096264676431285\n",
      "112 Train Loss 1442395.0 Test MSE 135236.1487409437 Test RE 4.097328169803657\n",
      "113 Train Loss 1442084.5 Test MSE 135105.53300530717 Test RE 4.0953490213909935\n",
      "114 Train Loss 1441486.5 Test MSE 135263.97404471008 Test RE 4.097749667820879\n",
      "115 Train Loss 1439731.9 Test MSE 135664.12889673954 Test RE 4.103806430151915\n",
      "116 Train Loss 1438999.4 Test MSE 135445.84836550977 Test RE 4.100503634976871\n",
      "117 Train Loss 1438369.5 Test MSE 135447.4114350495 Test RE 4.100527295184679\n",
      "118 Train Loss 1437871.1 Test MSE 135725.15656372596 Test RE 4.104729362305023\n",
      "119 Train Loss 1437154.9 Test MSE 135999.821054181 Test RE 4.108880595640175\n",
      "120 Train Loss 1436019.0 Test MSE 136105.2781845076 Test RE 4.110473343285315\n",
      "121 Train Loss 1435663.4 Test MSE 136155.26080198432 Test RE 4.111228028778261\n",
      "122 Train Loss 1435117.2 Test MSE 136374.28179394477 Test RE 4.114533385508266\n",
      "123 Train Loss 1434696.2 Test MSE 136413.0413485993 Test RE 4.115118049088856\n",
      "124 Train Loss 1433621.1 Test MSE 136228.08524788963 Test RE 4.112327354202382\n",
      "125 Train Loss 1433081.0 Test MSE 136280.8239756776 Test RE 4.113123291127557\n",
      "126 Train Loss 1432246.9 Test MSE 136436.48373454946 Test RE 4.115471622473263\n",
      "127 Train Loss 1431403.8 Test MSE 136206.360573543 Test RE 4.111999438947308\n",
      "128 Train Loss 1431276.2 Test MSE 136213.77098337482 Test RE 4.112111295642165\n",
      "129 Train Loss 1431185.5 Test MSE 136245.87144748715 Test RE 4.11259580210629\n",
      "130 Train Loss 1430679.4 Test MSE 136284.58620185842 Test RE 4.11318006504806\n",
      "131 Train Loss 1429759.8 Test MSE 136669.7708285246 Test RE 4.118988557007067\n",
      "132 Train Loss 1429481.1 Test MSE 136648.25784309334 Test RE 4.118664362316403\n",
      "133 Train Loss 1429404.5 Test MSE 136471.3400510407 Test RE 4.115997292061739\n",
      "134 Train Loss 1429250.6 Test MSE 136494.5884607722 Test RE 4.116347864997074\n",
      "135 Train Loss 1429166.1 Test MSE 136474.16309413523 Test RE 4.116039863555644\n",
      "136 Train Loss 1428941.5 Test MSE 136322.2907329885 Test RE 4.1137490024646475\n",
      "137 Train Loss 1428170.8 Test MSE 136453.16026658268 Test RE 4.115723130337544\n",
      "138 Train Loss 1427286.8 Test MSE 136812.3757061634 Test RE 4.121136927899407\n",
      "139 Train Loss 1427131.2 Test MSE 136898.0552758812 Test RE 4.122427169267912\n",
      "140 Train Loss 1427053.8 Test MSE 136942.4992591564 Test RE 4.1230962884012765\n",
      "141 Train Loss 1427043.6 Test MSE 136935.0872077939 Test RE 4.122984705017141\n",
      "142 Train Loss 1426915.2 Test MSE 136997.67453255848 Test RE 4.123926819668483\n",
      "143 Train Loss 1426220.8 Test MSE 137452.25116871423 Test RE 4.130763024316329\n",
      "144 Train Loss 1425408.6 Test MSE 137705.2448849682 Test RE 4.13456280438663\n",
      "145 Train Loss 1425334.6 Test MSE 137407.2614079324 Test RE 4.1300869449931525\n",
      "146 Train Loss 1425358.9 Test MSE 137392.34012022923 Test RE 4.1298626923298345\n",
      "147 Train Loss 1425229.9 Test MSE 137306.04995761978 Test RE 4.12856559492569\n",
      "148 Train Loss 1424808.0 Test MSE 137039.14203928097 Test RE 4.124550903343929\n",
      "149 Train Loss 1423203.1 Test MSE 135628.42238082032 Test RE 4.103266337962995\n",
      "150 Train Loss 1421903.4 Test MSE 134931.02210388557 Test RE 4.092703260091858\n",
      "151 Train Loss 1421230.1 Test MSE 134986.4681687229 Test RE 4.093544063709696\n",
      "152 Train Loss 1421046.4 Test MSE 134539.68129171545 Test RE 4.0867639112621506\n",
      "153 Train Loss 1420939.5 Test MSE 134254.57866678876 Test RE 4.082431490421056\n",
      "154 Train Loss 1420885.9 Test MSE 134250.8232878974 Test RE 4.082374392985689\n",
      "155 Train Loss 1420851.6 Test MSE 134040.2525071373 Test RE 4.0791715593413675\n",
      "156 Train Loss 1420554.5 Test MSE 133653.6384605088 Test RE 4.073284507414083\n",
      "157 Train Loss 1419887.9 Test MSE 132764.8617754791 Test RE 4.059718550841132\n",
      "158 Train Loss 1419746.0 Test MSE 132483.4751617473 Test RE 4.0554141122744705\n",
      "159 Train Loss 1419646.1 Test MSE 132178.2324346515 Test RE 4.05073956887114\n",
      "160 Train Loss 1419563.6 Test MSE 132088.14856416112 Test RE 4.049358976593815\n",
      "161 Train Loss 1419614.6 Test MSE 132156.55233560654 Test RE 4.050407350642575\n",
      "162 Train Loss 1419693.6 Test MSE 132188.51094287282 Test RE 4.05089706360308\n",
      "163 Train Loss 1419805.0 Test MSE 132112.02426710562 Test RE 4.049724932679049\n",
      "164 Train Loss 1419907.1 Test MSE 132063.46235846728 Test RE 4.048980562706079\n",
      "165 Train Loss 1420004.2 Test MSE 132002.6652526647 Test RE 4.048048455394784\n",
      "166 Train Loss 1419942.9 Test MSE 131521.39706069647 Test RE 4.040662332040822\n",
      "167 Train Loss 1419484.5 Test MSE 130486.16185483761 Test RE 4.024728423336858\n",
      "168 Train Loss 1418827.9 Test MSE 129559.31677883134 Test RE 4.01040909862123\n",
      "169 Train Loss 1418327.6 Test MSE 128693.36941978002 Test RE 3.996984261255104\n",
      "170 Train Loss 1418073.2 Test MSE 128037.52112728881 Test RE 3.9867865188535556\n",
      "171 Train Loss 1417886.2 Test MSE 127802.5331570869 Test RE 3.9831263530349417\n",
      "172 Train Loss 1417716.8 Test MSE 127230.6009055692 Test RE 3.974203866301684\n",
      "173 Train Loss 1417675.5 Test MSE 126683.40079651265 Test RE 3.9656484241940517\n",
      "174 Train Loss 1417701.0 Test MSE 126551.50848076424 Test RE 3.9635835334589884\n",
      "175 Train Loss 1417727.6 Test MSE 126256.52385613103 Test RE 3.9589613904946304\n",
      "176 Train Loss 1417680.8 Test MSE 125563.79330976823 Test RE 3.94808565289986\n",
      "177 Train Loss 1417650.8 Test MSE 125146.27062077846 Test RE 3.941516131946842\n",
      "178 Train Loss 1417662.0 Test MSE 125080.94733971758 Test RE 3.940487310343915\n",
      "179 Train Loss 1417643.2 Test MSE 124766.20246726031 Test RE 3.935526405410937\n",
      "180 Train Loss 1417558.1 Test MSE 124259.27507582502 Test RE 3.9275232095156882\n",
      "181 Train Loss 1417344.0 Test MSE 123956.73587281241 Test RE 3.922739043984583\n",
      "182 Train Loss 1417144.8 Test MSE 123771.78182190878 Test RE 3.9198114204444208\n",
      "183 Train Loss 1416916.1 Test MSE 123567.03136657787 Test RE 3.9165678888572772\n",
      "184 Train Loss 1416743.6 Test MSE 123593.5864949954 Test RE 3.916988710555862\n",
      "185 Train Loss 1416342.0 Test MSE 123046.9744777371 Test RE 3.9083173639320816\n",
      "186 Train Loss 1416150.2 Test MSE 122607.90209533663 Test RE 3.901338046457885\n",
      "187 Train Loss 1416039.2 Test MSE 122163.00438020799 Test RE 3.8942533734359976\n",
      "188 Train Loss 1415964.1 Test MSE 121917.02217035861 Test RE 3.8903307466690897\n",
      "189 Train Loss 1415909.6 Test MSE 121595.06583456369 Test RE 3.8851905922855816\n",
      "190 Train Loss 1415908.4 Test MSE 121345.09683738089 Test RE 3.881195048438849\n",
      "191 Train Loss 1415904.9 Test MSE 121216.04865762129 Test RE 3.879130711253989\n",
      "192 Train Loss 1415970.0 Test MSE 120968.76683139677 Test RE 3.8751719605336543\n",
      "193 Train Loss 1416022.5 Test MSE 120848.19454222274 Test RE 3.873240243565433\n",
      "194 Train Loss 1416108.8 Test MSE 120791.74448743649 Test RE 3.8723355127839274\n",
      "195 Train Loss 1416072.9 Test MSE 120501.38286222501 Test RE 3.8676785134526566\n",
      "196 Train Loss 1415950.8 Test MSE 120239.78455396886 Test RE 3.863478031059815\n",
      "197 Train Loss 1415838.4 Test MSE 120445.5799965324 Test RE 3.86678287004953\n",
      "198 Train Loss 1415717.8 Test MSE 120220.86271090074 Test RE 3.86317402601875\n",
      "199 Train Loss 1415708.0 Test MSE 120006.15556837275 Test RE 3.8597227875820668\n",
      "Training time: 49.59\n",
      "Training time: 49.59\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "6\n",
      "0 Train Loss 13847734000.0 Test MSE 12731.351198907978 Test RE 1.2571630316261446\n",
      "1 Train Loss 7616815600.0 Test MSE 18650.0283994233 Test RE 1.5215775822625777\n",
      "2 Train Loss 1944174500.0 Test MSE 24207.23934125238 Test RE 1.733512527202993\n",
      "3 Train Loss 722813760.0 Test MSE 28373.767306983482 Test RE 1.8767777831801928\n",
      "4 Train Loss 241111730.0 Test MSE 28109.374238185483 Test RE 1.8680132015992779\n",
      "5 Train Loss 61044540.0 Test MSE 30723.46377432362 Test RE 1.9529424281209395\n",
      "6 Train Loss 9671479.0 Test MSE 34229.10337283737 Test RE 2.061351785773906\n",
      "7 Train Loss 2434511.0 Test MSE 34623.05680166018 Test RE 2.0731802192401902\n",
      "8 Train Loss 516751.16 Test MSE 34648.0556876874 Test RE 2.073928533354822\n",
      "9 Train Loss 202614.03 Test MSE 34314.611771098294 Test RE 2.0639249315465102\n",
      "10 Train Loss 100250.81 Test MSE 34273.65808038726 Test RE 2.0626929406576173\n",
      "11 Train Loss 54047.453 Test MSE 34078.66806341928 Test RE 2.056817025398064\n",
      "12 Train Loss 40316.215 Test MSE 34166.25504824508 Test RE 2.0594584842247876\n",
      "13 Train Loss 34045.49 Test MSE 34233.493880929316 Test RE 2.06148398455511\n",
      "14 Train Loss 26991.574 Test MSE 34302.5327784662 Test RE 2.0635616412885445\n",
      "15 Train Loss 24060.844 Test MSE 34262.30108223079 Test RE 2.062351163004144\n",
      "16 Train Loss 21803.527 Test MSE 34207.383768550266 Test RE 2.0606976808566237\n",
      "17 Train Loss 19809.842 Test MSE 34248.19556049359 Test RE 2.0619265923690153\n",
      "18 Train Loss 18438.156 Test MSE 34369.04901490187 Test RE 2.0655614043784998\n",
      "19 Train Loss 17930.58 Test MSE 34448.0412574552 Test RE 2.0679337377416154\n",
      "20 Train Loss 17515.1 Test MSE 34542.826774109315 Test RE 2.0707767951813576\n",
      "21 Train Loss 17298.95 Test MSE 34549.678705459824 Test RE 2.0709821651266154\n",
      "22 Train Loss 17157.346 Test MSE 34390.20965377247 Test RE 2.066197178054848\n",
      "23 Train Loss 17049.965 Test MSE 34198.87679200928 Test RE 2.060441429209627\n",
      "24 Train Loss 16952.285 Test MSE 34101.22941016536 Test RE 2.057497757464402\n",
      "25 Train Loss 16881.355 Test MSE 33967.7507819164 Test RE 2.0534670927259757\n",
      "26 Train Loss 16759.846 Test MSE 33744.766025340294 Test RE 2.0467158979104014\n",
      "27 Train Loss 16640.56 Test MSE 33407.28613241968 Test RE 2.0364556233211304\n",
      "28 Train Loss 16523.207 Test MSE 33075.82920172911 Test RE 2.026327893026482\n",
      "29 Train Loss 16449.451 Test MSE 33011.03660876068 Test RE 2.024342222501071\n",
      "30 Train Loss 16323.809 Test MSE 32809.10828272409 Test RE 2.0181412801544014\n",
      "31 Train Loss 16139.729 Test MSE 32511.50797268135 Test RE 2.008967491640083\n",
      "32 Train Loss 15989.479 Test MSE 32226.38536345389 Test RE 2.0001388724835234\n",
      "33 Train Loss 15884.443 Test MSE 31968.075942275227 Test RE 1.9921067245169009\n",
      "34 Train Loss 15843.358 Test MSE 31814.523991034872 Test RE 1.9873166320213524\n",
      "35 Train Loss 15809.827 Test MSE 31719.880582104073 Test RE 1.9843584467697772\n",
      "36 Train Loss 15719.706 Test MSE 31520.119564623343 Test RE 1.9781001708539219\n",
      "37 Train Loss 15616.366 Test MSE 31249.609240241658 Test RE 1.9695937065912459\n",
      "38 Train Loss 15495.352 Test MSE 30797.518634125106 Test RE 1.9552946666876738\n",
      "39 Train Loss 15431.754 Test MSE 30675.284643190025 Test RE 1.9514105698267172\n",
      "40 Train Loss 15276.952 Test MSE 30452.590670865862 Test RE 1.9443143201619433\n",
      "41 Train Loss 15152.157 Test MSE 30108.68287460932 Test RE 1.933304363064087\n",
      "42 Train Loss 15028.171 Test MSE 29813.320775083037 Test RE 1.9237982652962788\n",
      "43 Train Loss 14904.42 Test MSE 29593.8244049729 Test RE 1.9167033355936796\n",
      "44 Train Loss 14805.289 Test MSE 29278.85048328129 Test RE 1.9064760914860925\n",
      "45 Train Loss 14719.954 Test MSE 29094.05848014519 Test RE 1.900450254156296\n",
      "46 Train Loss 14614.306 Test MSE 28950.461848005223 Test RE 1.8957545218827003\n",
      "47 Train Loss 14452.292 Test MSE 28839.251406561078 Test RE 1.8921098381328623\n",
      "48 Train Loss 14331.184 Test MSE 28553.468270268237 Test RE 1.882711546959747\n",
      "49 Train Loss 14243.53 Test MSE 28216.096521310432 Test RE 1.8715559657542502\n",
      "50 Train Loss 14063.752 Test MSE 27848.368215341197 Test RE 1.8593203758913173\n",
      "51 Train Loss 13833.43 Test MSE 27302.68020908781 Test RE 1.841013586239861\n",
      "52 Train Loss 13630.368 Test MSE 26901.270383746625 Test RE 1.827429987001406\n",
      "53 Train Loss 13468.4375 Test MSE 26637.416570207013 Test RE 1.8184459777464195\n",
      "54 Train Loss 13375.031 Test MSE 26418.916486482016 Test RE 1.8109724910191916\n",
      "55 Train Loss 13268.99 Test MSE 26132.837015798712 Test RE 1.8011406680297393\n",
      "56 Train Loss 13138.378 Test MSE 25882.246040698206 Test RE 1.7924841867848058\n",
      "57 Train Loss 12986.147 Test MSE 25553.537677529348 Test RE 1.7810654083893422\n",
      "58 Train Loss 12887.17 Test MSE 25404.093496088506 Test RE 1.7758496890170752\n",
      "59 Train Loss 12749.023 Test MSE 25145.13814633608 Test RE 1.7667754881747997\n",
      "60 Train Loss 12663.274 Test MSE 24925.308028735806 Test RE 1.7590355612125939\n",
      "61 Train Loss 12478.954 Test MSE 24411.031921783437 Test RE 1.7407941624563097\n",
      "62 Train Loss 12345.831 Test MSE 24128.68355605529 Test RE 1.7306974995576707\n",
      "63 Train Loss 12209.729 Test MSE 23717.288631062813 Test RE 1.7158798402111073\n",
      "64 Train Loss 12112.312 Test MSE 23517.984088993508 Test RE 1.7086550655910742\n",
      "65 Train Loss 12007.218 Test MSE 23344.667620556724 Test RE 1.702347431402431\n",
      "66 Train Loss 11848.829 Test MSE 22969.412974136172 Test RE 1.68860978058442\n",
      "67 Train Loss 11579.618 Test MSE 22549.927655360774 Test RE 1.6731193764016539\n",
      "68 Train Loss 11422.946 Test MSE 22273.70379243864 Test RE 1.6628404184066041\n",
      "69 Train Loss 11285.972 Test MSE 21931.167974246026 Test RE 1.650004896328668\n",
      "70 Train Loss 11120.386 Test MSE 21581.794081132855 Test RE 1.636809453187739\n",
      "71 Train Loss 10960.639 Test MSE 21303.443114983194 Test RE 1.6262198309860258\n",
      "72 Train Loss 10784.164 Test MSE 20931.536452121916 Test RE 1.6119623966918115\n",
      "73 Train Loss 10443.905 Test MSE 20127.819884372588 Test RE 1.5807118937955058\n",
      "74 Train Loss 10277.45 Test MSE 19832.252370949533 Test RE 1.5690629676969785\n",
      "75 Train Loss 10072.542 Test MSE 19659.68768754296 Test RE 1.562221676514453\n",
      "76 Train Loss 9925.057 Test MSE 19373.095037410872 Test RE 1.5507930886235641\n",
      "77 Train Loss 9749.844 Test MSE 18981.84209187437 Test RE 1.5350535504139429\n",
      "78 Train Loss 9635.681 Test MSE 18674.54746016843 Test RE 1.522577457394021\n",
      "79 Train Loss 9540.687 Test MSE 18456.619019501828 Test RE 1.513667290484834\n",
      "80 Train Loss 9412.625 Test MSE 18082.02525425807 Test RE 1.4982279270465853\n",
      "81 Train Loss 9292.718 Test MSE 17809.142447692688 Test RE 1.4868797821662212\n",
      "82 Train Loss 9187.047 Test MSE 17663.66712388987 Test RE 1.480794484787868\n",
      "83 Train Loss 9051.521 Test MSE 17355.13548231457 Test RE 1.4678049788621768\n",
      "84 Train Loss 8915.903 Test MSE 16926.81711469336 Test RE 1.4495793822900893\n",
      "85 Train Loss 8818.456 Test MSE 16716.913104626834 Test RE 1.4405634605701803\n",
      "86 Train Loss 8625.176 Test MSE 16399.14177991628 Test RE 1.4268059529870276\n",
      "87 Train Loss 8473.18 Test MSE 16080.241350016064 Test RE 1.4128648926525968\n",
      "88 Train Loss 8395.175 Test MSE 15974.16321227371 Test RE 1.408196987973566\n",
      "89 Train Loss 8249.306 Test MSE 15644.929914650762 Test RE 1.3936097090354245\n",
      "90 Train Loss 8156.896 Test MSE 15454.813572723966 Test RE 1.3851162923870495\n",
      "91 Train Loss 8035.863 Test MSE 15190.541375192735 Test RE 1.3732227139836495\n",
      "92 Train Loss 7958.8564 Test MSE 14951.95970246737 Test RE 1.3623961613339106\n",
      "93 Train Loss 7865.3584 Test MSE 14627.235248458559 Test RE 1.3475207933573086\n",
      "94 Train Loss 7775.4336 Test MSE 14438.554891129881 Test RE 1.338801580791662\n",
      "95 Train Loss 7683.7163 Test MSE 14344.27585547037 Test RE 1.334423454181534\n",
      "96 Train Loss 7604.4727 Test MSE 14287.812998068119 Test RE 1.331794543072198\n",
      "97 Train Loss 7489.5376 Test MSE 14023.836279988247 Test RE 1.319434297069809\n",
      "98 Train Loss 7331.91 Test MSE 13574.01311063646 Test RE 1.298101000095737\n",
      "99 Train Loss 7161.5503 Test MSE 13357.012454609154 Test RE 1.2876831649705955\n",
      "100 Train Loss 7048.3296 Test MSE 13204.02226494764 Test RE 1.2802874146014098\n",
      "101 Train Loss 6920.715 Test MSE 13092.577519524517 Test RE 1.2748730172117646\n",
      "102 Train Loss 6829.94 Test MSE 12976.417586491949 Test RE 1.269204954585761\n",
      "103 Train Loss 6722.537 Test MSE 12751.251274667411 Test RE 1.2581451689771808\n",
      "104 Train Loss 6625.897 Test MSE 12661.771595085858 Test RE 1.2537229903205174\n",
      "105 Train Loss 6519.898 Test MSE 12469.745682957953 Test RE 1.2441798127236465\n",
      "106 Train Loss 6456.9824 Test MSE 12273.064475522066 Test RE 1.234328794122842\n",
      "107 Train Loss 6299.9805 Test MSE 11946.462575996256 Test RE 1.217794521114572\n",
      "108 Train Loss 6166.756 Test MSE 11682.821996647803 Test RE 1.2042821027293658\n",
      "109 Train Loss 6087.5 Test MSE 11566.523153866756 Test RE 1.1982729845694864\n",
      "110 Train Loss 6003.3374 Test MSE 11367.132780311495 Test RE 1.1878998264289944\n",
      "111 Train Loss 5896.1436 Test MSE 11189.08404396185 Test RE 1.178559791453518\n",
      "112 Train Loss 5775.1025 Test MSE 10919.122146420546 Test RE 1.1642552754333841\n",
      "113 Train Loss 5689.49 Test MSE 10754.021979115714 Test RE 1.1554198175980632\n",
      "114 Train Loss 5621.6216 Test MSE 10623.335386460334 Test RE 1.1483778275076126\n",
      "115 Train Loss 5546.8604 Test MSE 10532.913294924054 Test RE 1.1434800893529284\n",
      "116 Train Loss 5514.9844 Test MSE 10462.057087170313 Test RE 1.1396274332488128\n",
      "117 Train Loss 5474.7188 Test MSE 10420.169233961198 Test RE 1.1373437320665378\n",
      "118 Train Loss 5428.424 Test MSE 10426.688278973199 Test RE 1.1376994477720583\n",
      "119 Train Loss 5401.6094 Test MSE 10378.102367210622 Test RE 1.1350456468984567\n",
      "120 Train Loss 5359.5503 Test MSE 10282.60948801168 Test RE 1.1298115849110368\n",
      "121 Train Loss 5318.367 Test MSE 10216.303116494879 Test RE 1.1261629553432586\n",
      "122 Train Loss 5293.1416 Test MSE 10155.59576996112 Test RE 1.122812025531276\n",
      "123 Train Loss 5269.1934 Test MSE 10096.964362851693 Test RE 1.1195661627104607\n",
      "124 Train Loss 5207.193 Test MSE 9963.527757756958 Test RE 1.112143735468373\n",
      "125 Train Loss 5139.023 Test MSE 9858.82270980034 Test RE 1.1062846354186666\n",
      "126 Train Loss 5103.6367 Test MSE 9823.505092856261 Test RE 1.1043013158774662\n",
      "127 Train Loss 5087.492 Test MSE 9805.443083044353 Test RE 1.1032856357385254\n",
      "128 Train Loss 5062.7725 Test MSE 9777.478273117598 Test RE 1.1017112447432695\n",
      "129 Train Loss 5026.5312 Test MSE 9667.85477152755 Test RE 1.0955177316914502\n",
      "130 Train Loss 4964.087 Test MSE 9508.01213934641 Test RE 1.086423662675575\n",
      "131 Train Loss 4935.445 Test MSE 9467.724707025214 Test RE 1.084119517368436\n",
      "132 Train Loss 4904.991 Test MSE 9419.122457065216 Test RE 1.0813332912713882\n",
      "133 Train Loss 4861.9077 Test MSE 9366.709070203799 Test RE 1.078320515643313\n",
      "134 Train Loss 4818.4077 Test MSE 9332.126299638297 Test RE 1.076328044480029\n",
      "135 Train Loss 4772.727 Test MSE 9254.457695918734 Test RE 1.0718397017643495\n",
      "136 Train Loss 4739.3184 Test MSE 9197.149180871049 Test RE 1.0685158477810492\n",
      "137 Train Loss 4695.493 Test MSE 9145.768445263322 Test RE 1.0655269855968361\n",
      "138 Train Loss 4660.5176 Test MSE 9056.207132194993 Test RE 1.060296983567569\n",
      "139 Train Loss 4612.5186 Test MSE 8948.255885784265 Test RE 1.053958594692261\n",
      "140 Train Loss 4570.0483 Test MSE 8866.831596596754 Test RE 1.049152409679071\n",
      "141 Train Loss 4535.7246 Test MSE 8732.048128283323 Test RE 1.041147864599317\n",
      "142 Train Loss 4484.225 Test MSE 8547.860343074875 Test RE 1.030108714730387\n",
      "143 Train Loss 4423.0874 Test MSE 8448.832866818462 Test RE 1.0241243963545454\n",
      "144 Train Loss 4373.2124 Test MSE 8314.30441412898 Test RE 1.0159382529716574\n",
      "145 Train Loss 4328.0425 Test MSE 8204.06968027436 Test RE 1.0091809003666754\n",
      "146 Train Loss 4293.5977 Test MSE 8131.736700474539 Test RE 1.0047222184920086\n",
      "147 Train Loss 4220.892 Test MSE 7979.243621937737 Test RE 0.9952569406505773\n",
      "148 Train Loss 4138.416 Test MSE 7780.674643177129 Test RE 0.9827950942241336\n",
      "149 Train Loss 4105.485 Test MSE 7690.716395129758 Test RE 0.9770971591375326\n",
      "150 Train Loss 4047.7588 Test MSE 7630.078758585827 Test RE 0.9732375634049766\n",
      "151 Train Loss 3975.6516 Test MSE 7447.247789776835 Test RE 0.9615065662991873\n",
      "152 Train Loss 3912.5127 Test MSE 7258.550882482448 Test RE 0.9492471789465675\n",
      "153 Train Loss 3807.6401 Test MSE 7024.22760596244 Test RE 0.9337995066200098\n",
      "154 Train Loss 3730.132 Test MSE 6892.999271624005 Test RE 0.9250356463586618\n",
      "155 Train Loss 3681.5925 Test MSE 6804.855043704012 Test RE 0.9191021700997097\n",
      "156 Train Loss 3652.1367 Test MSE 6775.218907090495 Test RE 0.917098574202618\n",
      "157 Train Loss 3606.1667 Test MSE 6710.580345773552 Test RE 0.9127133284352975\n",
      "158 Train Loss 3554.6028 Test MSE 6637.194481250189 Test RE 0.9077089634757849\n",
      "159 Train Loss 3516.1753 Test MSE 6551.1642424676 Test RE 0.9018069877744599\n",
      "160 Train Loss 3463.502 Test MSE 6419.0100356224875 Test RE 0.8926647381071237\n",
      "161 Train Loss 3427.668 Test MSE 6357.862975733208 Test RE 0.8884028318931249\n",
      "162 Train Loss 3383.7346 Test MSE 6237.660528430108 Test RE 0.8799646380583479\n",
      "163 Train Loss 3295.257 Test MSE 6068.945780173725 Test RE 0.8679825239109726\n",
      "164 Train Loss 3187.0173 Test MSE 5867.330088527696 Test RE 0.8534431819631779\n",
      "165 Train Loss 3128.357 Test MSE 5745.852094530011 Test RE 0.8445620711926811\n",
      "166 Train Loss 3082.0894 Test MSE 5642.682298972374 Test RE 0.8369454481741778\n",
      "167 Train Loss 3037.4995 Test MSE 5560.521143453132 Test RE 0.8308298674712582\n",
      "168 Train Loss 2954.75 Test MSE 5444.482356859089 Test RE 0.8221151466956791\n",
      "169 Train Loss 2875.4834 Test MSE 5282.285095530181 Test RE 0.8097766896982808\n",
      "170 Train Loss 2835.682 Test MSE 5161.751551988367 Test RE 0.8004844518767563\n",
      "171 Train Loss 2783.6257 Test MSE 5050.121655533886 Test RE 0.7917813582271049\n",
      "172 Train Loss 2696.0078 Test MSE 4893.967436541211 Test RE 0.779443949247778\n",
      "173 Train Loss 2655.5579 Test MSE 4815.957422526985 Test RE 0.7732068122988803\n",
      "174 Train Loss 2607.0188 Test MSE 4717.504831071072 Test RE 0.765262670806992\n",
      "175 Train Loss 2525.9092 Test MSE 4556.293827343128 Test RE 0.7520733749079896\n",
      "176 Train Loss 2462.8452 Test MSE 4442.0442586351255 Test RE 0.7425843514006505\n",
      "177 Train Loss 2429.0513 Test MSE 4428.959876435333 Test RE 0.7414898752477938\n",
      "178 Train Loss 2391.8472 Test MSE 4380.707798613432 Test RE 0.7374396680630453\n",
      "179 Train Loss 2357.274 Test MSE 4315.325391596695 Test RE 0.7319158071611257\n",
      "180 Train Loss 2292.3003 Test MSE 4147.660295287988 Test RE 0.7175562341975714\n",
      "181 Train Loss 2236.7075 Test MSE 4027.6802266084524 Test RE 0.7071016388345869\n",
      "182 Train Loss 2183.4417 Test MSE 3903.0446300157523 Test RE 0.696075120123385\n",
      "183 Train Loss 2131.9324 Test MSE 3806.3526699672298 Test RE 0.6873989502864203\n",
      "184 Train Loss 2082.2922 Test MSE 3698.0837672526313 Test RE 0.6775521450340254\n",
      "185 Train Loss 2032.548 Test MSE 3609.0595447605015 Test RE 0.669347084497717\n",
      "186 Train Loss 2008.1067 Test MSE 3563.81508752739 Test RE 0.6651382652299215\n",
      "187 Train Loss 1977.9238 Test MSE 3517.2951006139665 Test RE 0.660782839572188\n",
      "188 Train Loss 1950.0297 Test MSE 3507.3898643537314 Test RE 0.659851751259821\n",
      "189 Train Loss 1923.8689 Test MSE 3468.329747052484 Test RE 0.6561672382706877\n",
      "190 Train Loss 1897.7091 Test MSE 3407.664305406061 Test RE 0.6504033281789918\n",
      "191 Train Loss 1861.7317 Test MSE 3370.745472147553 Test RE 0.6468704796078184\n",
      "192 Train Loss 1837.9412 Test MSE 3333.2083422650694 Test RE 0.6432585721648161\n",
      "193 Train Loss 1812.0243 Test MSE 3277.6090439514523 Test RE 0.6378711014197093\n",
      "194 Train Loss 1778.7615 Test MSE 3188.1658916364186 Test RE 0.6291074206581565\n",
      "195 Train Loss 1757.8899 Test MSE 3157.410724347983 Test RE 0.6260656730350882\n",
      "196 Train Loss 1740.8627 Test MSE 3113.341350393181 Test RE 0.6216811828571371\n",
      "197 Train Loss 1717.5143 Test MSE 3092.4062258473805 Test RE 0.6195874635008891\n",
      "198 Train Loss 1695.3864 Test MSE 3049.4027378201476 Test RE 0.6152643409520704\n",
      "199 Train Loss 1660.1111 Test MSE 2929.9215012723803 Test RE 0.6030903018966873\n",
      "Training time: 46.87\n",
      "Training time: 46.87\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "7\n",
      "0 Train Loss 12489972000.0 Test MSE 27439.672859735834 Test RE 1.8456264989084517\n",
      "1 Train Loss 2671789300.0 Test MSE 44788.37668479234 Test RE 2.357963148375264\n",
      "2 Train Loss 684964600.0 Test MSE 67714.76441092695 Test RE 2.8993192447946377\n",
      "3 Train Loss 177008140.0 Test MSE 81854.79069230707 Test RE 3.1876923743439\n",
      "4 Train Loss 73501160.0 Test MSE 94137.87419468138 Test RE 3.41850756891787\n",
      "5 Train Loss 40060436.0 Test MSE 97769.1207196396 Test RE 3.4838159811820812\n",
      "6 Train Loss 21475618.0 Test MSE 98660.1812062443 Test RE 3.4996555925092125\n",
      "7 Train Loss 12976154.0 Test MSE 101507.34241233752 Test RE 3.549793428807978\n",
      "8 Train Loss 8628751.0 Test MSE 102940.9026619447 Test RE 3.5747719245267224\n",
      "9 Train Loss 6127204.5 Test MSE 103374.84041166412 Test RE 3.5822985593070094\n",
      "10 Train Loss 4941269.5 Test MSE 105538.83014334712 Test RE 3.6195992561529002\n",
      "11 Train Loss 3716998.5 Test MSE 108031.38314011383 Test RE 3.6620925904234465\n",
      "12 Train Loss 2992126.0 Test MSE 108592.37809290287 Test RE 3.671588698013024\n",
      "13 Train Loss 2354993.0 Test MSE 109456.88058201043 Test RE 3.686174460882059\n",
      "14 Train Loss 1700575.5 Test MSE 110928.50915252289 Test RE 3.7108717101986137\n",
      "15 Train Loss 1101485.6 Test MSE 110887.67665945752 Test RE 3.7101886662678525\n",
      "16 Train Loss 600241.2 Test MSE 110901.02741356115 Test RE 3.7104120108593754\n",
      "17 Train Loss 354327.88 Test MSE 112861.98168309103 Test RE 3.7430720606749106\n",
      "18 Train Loss 243762.6 Test MSE 113852.01698470295 Test RE 3.7594534941237967\n",
      "19 Train Loss 174399.97 Test MSE 113817.17312030583 Test RE 3.7588781687353103\n",
      "20 Train Loss 139336.53 Test MSE 114686.4566036561 Test RE 3.773205160676115\n",
      "21 Train Loss 111869.58 Test MSE 115358.0266646015 Test RE 3.7842364235142796\n",
      "22 Train Loss 94807.77 Test MSE 114772.35196418586 Test RE 3.7746178826550194\n",
      "23 Train Loss 85291.7 Test MSE 114657.13121255145 Test RE 3.7727227244992605\n",
      "24 Train Loss 76985.375 Test MSE 115015.15124529142 Test RE 3.778608348596889\n",
      "25 Train Loss 71153.98 Test MSE 115096.41581956699 Test RE 3.779943010920381\n",
      "26 Train Loss 67071.63 Test MSE 115306.06657808818 Test RE 3.7833840710413456\n",
      "27 Train Loss 64708.86 Test MSE 115403.79449581828 Test RE 3.7849870393300904\n",
      "28 Train Loss 62479.03 Test MSE 115465.80835793274 Test RE 3.78600386092207\n",
      "29 Train Loss 61083.426 Test MSE 115571.23376144872 Test RE 3.787731861287026\n",
      "30 Train Loss 60227.715 Test MSE 115531.1698353497 Test RE 3.7870752767795732\n",
      "31 Train Loss 59807.176 Test MSE 115354.23515743754 Test RE 3.7841742341815503\n",
      "32 Train Loss 59317.082 Test MSE 115268.14556899156 Test RE 3.7827618941525216\n",
      "33 Train Loss 58751.824 Test MSE 115144.74400000069 Test RE 3.780736513456085\n",
      "34 Train Loss 58377.953 Test MSE 115083.88976281318 Test RE 3.77973731785583\n",
      "35 Train Loss 58035.312 Test MSE 114901.79253591478 Test RE 3.7767457950317413\n",
      "36 Train Loss 57772.668 Test MSE 114513.86125701947 Test RE 3.770364882610129\n",
      "37 Train Loss 57563.234 Test MSE 114289.84419148765 Test RE 3.7666752002173736\n",
      "38 Train Loss 57376.375 Test MSE 114002.38286375327 Test RE 3.761935255184486\n",
      "39 Train Loss 56994.67 Test MSE 113274.95198855286 Test RE 3.749913895905021\n",
      "40 Train Loss 56694.73 Test MSE 112825.29296074099 Test RE 3.742463619763768\n",
      "41 Train Loss 56475.395 Test MSE 112412.2460971198 Test RE 3.735606867176393\n",
      "42 Train Loss 56302.438 Test MSE 112231.5729298058 Test RE 3.732603656419232\n",
      "43 Train Loss 56107.23 Test MSE 112050.92711037502 Test RE 3.7295984825012347\n",
      "44 Train Loss 55906.273 Test MSE 111679.900296453 Test RE 3.723418576256956\n",
      "45 Train Loss 55817.27 Test MSE 111483.74168985455 Test RE 3.720147165750499\n",
      "46 Train Loss 55700.254 Test MSE 111253.72501885532 Test RE 3.716307423362443\n",
      "47 Train Loss 55595.375 Test MSE 110963.11910943012 Test RE 3.711450565421061\n",
      "48 Train Loss 55453.113 Test MSE 110570.16358222437 Test RE 3.7048730263626344\n",
      "49 Train Loss 55343.094 Test MSE 110363.30179343431 Test RE 3.7014057462736654\n",
      "50 Train Loss 55077.23 Test MSE 110065.70449236369 Test RE 3.6964119126366715\n",
      "51 Train Loss 54914.566 Test MSE 109943.12422297167 Test RE 3.694352990664179\n",
      "52 Train Loss 54781.305 Test MSE 109872.5455038809 Test RE 3.693166993069477\n",
      "53 Train Loss 54576.805 Test MSE 109444.45907888772 Test RE 3.685965295745784\n",
      "54 Train Loss 54373.59 Test MSE 108855.68264698623 Test RE 3.6760372639834062\n",
      "55 Train Loss 54302.434 Test MSE 108708.4907527849 Test RE 3.6735511010735484\n",
      "56 Train Loss 54156.12 Test MSE 108510.87483827023 Test RE 3.6702105966939906\n",
      "57 Train Loss 54013.805 Test MSE 108126.42300389204 Test RE 3.6637030866374944\n",
      "58 Train Loss 53826.28 Test MSE 107724.6650551413 Test RE 3.6568902668758585\n",
      "59 Train Loss 53614.438 Test MSE 107593.50747726917 Test RE 3.654663409411066\n",
      "60 Train Loss 53476.2 Test MSE 107304.74452381728 Test RE 3.6497558622007342\n",
      "61 Train Loss 53313.67 Test MSE 106980.10455600134 Test RE 3.6442306910820306\n",
      "62 Train Loss 53049.066 Test MSE 106390.47776525958 Test RE 3.6341741251305133\n",
      "63 Train Loss 52864.7 Test MSE 105870.24614600399 Test RE 3.625277985213155\n",
      "64 Train Loss 52696.484 Test MSE 105611.16498087876 Test RE 3.6208394550287966\n",
      "65 Train Loss 52497.473 Test MSE 105218.81746014408 Test RE 3.6141074525246166\n",
      "66 Train Loss 52337.582 Test MSE 104783.98227622443 Test RE 3.60663175498451\n",
      "67 Train Loss 52129.86 Test MSE 104223.06827808585 Test RE 3.5969655599074097\n",
      "68 Train Loss 51793.293 Test MSE 103688.74710194697 Test RE 3.587733417089418\n",
      "69 Train Loss 51544.395 Test MSE 103079.38263271365 Test RE 3.5771755751943095\n",
      "70 Train Loss 51137.965 Test MSE 102242.5423628679 Test RE 3.562625502487632\n",
      "71 Train Loss 50784.93 Test MSE 101225.14656401833 Test RE 3.5448556866987246\n",
      "72 Train Loss 50410.22 Test MSE 100269.7290771986 Test RE 3.5280868954617217\n",
      "73 Train Loss 50056.223 Test MSE 99384.15580412222 Test RE 3.5124724688475992\n",
      "74 Train Loss 49670.035 Test MSE 98236.56797412752 Test RE 3.492134345526642\n",
      "75 Train Loss 49407.92 Test MSE 97643.93111943836 Test RE 3.4815848205127446\n",
      "76 Train Loss 49100.418 Test MSE 97139.25154756293 Test RE 3.472575755601551\n",
      "77 Train Loss 48722.91 Test MSE 96679.71307320998 Test RE 3.4643521286131356\n",
      "78 Train Loss 48492.52 Test MSE 96386.17798547792 Test RE 3.4590889667655467\n",
      "79 Train Loss 48157.742 Test MSE 95913.49607980071 Test RE 3.450596782945903\n",
      "80 Train Loss 47719.105 Test MSE 95274.71653617478 Test RE 3.4390871783567745\n",
      "81 Train Loss 47332.242 Test MSE 94483.56533536536 Test RE 3.4247785033374583\n",
      "82 Train Loss 46984.566 Test MSE 93680.48691903593 Test RE 3.410192712930449\n",
      "83 Train Loss 46565.844 Test MSE 92770.9939555333 Test RE 3.3935984842344458\n",
      "84 Train Loss 46054.19 Test MSE 91678.26730538992 Test RE 3.373553102330628\n",
      "85 Train Loss 45623.914 Test MSE 90668.64804609965 Test RE 3.354925822676434\n",
      "86 Train Loss 45263.582 Test MSE 89800.0136530651 Test RE 3.3388165202675686\n",
      "87 Train Loss 44955.36 Test MSE 89119.96757945175 Test RE 3.3261502411430888\n",
      "88 Train Loss 44709.72 Test MSE 88967.50470472498 Test RE 3.32330390089631\n",
      "89 Train Loss 44500.715 Test MSE 88738.67250617489 Test RE 3.319027235078341\n",
      "90 Train Loss 44375.773 Test MSE 88316.24703977325 Test RE 3.311117976820299\n",
      "91 Train Loss 44258.766 Test MSE 88160.4059827436 Test RE 3.308195321147145\n",
      "92 Train Loss 44089.38 Test MSE 88014.7054038368 Test RE 3.3054605033868203\n",
      "93 Train Loss 43921.582 Test MSE 87633.5757259988 Test RE 3.2982959285109694\n",
      "94 Train Loss 43814.53 Test MSE 87514.4128261756 Test RE 3.2960526775328365\n",
      "95 Train Loss 43738.043 Test MSE 87388.87611934994 Test RE 3.293687786545862\n",
      "96 Train Loss 43614.18 Test MSE 87191.33003375743 Test RE 3.2899629231390075\n",
      "97 Train Loss 43470.4 Test MSE 86839.40959937993 Test RE 3.283316758758307\n",
      "98 Train Loss 43313.457 Test MSE 86446.12333366141 Test RE 3.2758734289737674\n",
      "99 Train Loss 43141.816 Test MSE 86155.77055237081 Test RE 3.2703673463983813\n",
      "100 Train Loss 43070.613 Test MSE 86048.66916112257 Test RE 3.2683339955754627\n",
      "101 Train Loss 42952.223 Test MSE 85715.27017022834 Test RE 3.2619962055874634\n",
      "102 Train Loss 42844.844 Test MSE 85308.21927621188 Test RE 3.2542415860795955\n",
      "103 Train Loss 42693.83 Test MSE 84984.4425293158 Test RE 3.2480601805110116\n",
      "104 Train Loss 42568.152 Test MSE 84811.63354687081 Test RE 3.2447561663589157\n",
      "105 Train Loss 42349.414 Test MSE 84575.75721367657 Test RE 3.2402409008964668\n",
      "106 Train Loss 42074.48 Test MSE 84147.92917998348 Test RE 3.232035101148892\n",
      "107 Train Loss 41865.055 Test MSE 83590.0670221565 Test RE 3.2213038308988273\n",
      "108 Train Loss 41670.805 Test MSE 83125.64297040153 Test RE 3.212342629960216\n",
      "109 Train Loss 41518.4 Test MSE 82743.13440735074 Test RE 3.2049431962560497\n",
      "110 Train Loss 41288.156 Test MSE 82269.20607659196 Test RE 3.19575152969964\n",
      "111 Train Loss 41101.883 Test MSE 81947.75131932063 Test RE 3.1895019556359725\n",
      "112 Train Loss 40936.004 Test MSE 81478.54708243783 Test RE 3.1803578600951474\n",
      "113 Train Loss 40625.71 Test MSE 80745.4099085942 Test RE 3.166017230813757\n",
      "114 Train Loss 40438.0 Test MSE 80490.36590903864 Test RE 3.1610131548384266\n",
      "115 Train Loss 40228.83 Test MSE 80066.29804253764 Test RE 3.1526751732726113\n",
      "116 Train Loss 39864.06 Test MSE 79169.88056993097 Test RE 3.134976915566569\n",
      "117 Train Loss 39562.73 Test MSE 78748.13031657679 Test RE 3.1266155105365505\n",
      "118 Train Loss 39296.082 Test MSE 78310.99363447643 Test RE 3.1179253971847642\n",
      "119 Train Loss 39108.91 Test MSE 77905.39399689568 Test RE 3.1098405096446604\n",
      "120 Train Loss 38764.723 Test MSE 77140.98022775988 Test RE 3.0945459010286918\n",
      "121 Train Loss 38422.344 Test MSE 76280.36470275161 Test RE 3.0772354913527575\n",
      "122 Train Loss 38078.01 Test MSE 75794.02383402454 Test RE 3.0674100371323614\n",
      "123 Train Loss 37728.82 Test MSE 75168.69762327273 Test RE 3.0547302471459443\n",
      "124 Train Loss 37402.246 Test MSE 74247.56610790637 Test RE 3.03595593116495\n",
      "125 Train Loss 37097.43 Test MSE 73662.85229676476 Test RE 3.023977934929926\n",
      "126 Train Loss 36876.074 Test MSE 73245.67512182618 Test RE 3.015402881626431\n",
      "127 Train Loss 36653.25 Test MSE 72563.42519287314 Test RE 3.0013264774889894\n",
      "128 Train Loss 36402.098 Test MSE 72075.11250032332 Test RE 2.9912107766985923\n",
      "129 Train Loss 36042.418 Test MSE 71507.58447239657 Test RE 2.9794109273172937\n",
      "130 Train Loss 35810.84 Test MSE 70961.13625075438 Test RE 2.9680050313054145\n",
      "131 Train Loss 35392.598 Test MSE 70029.79826040175 Test RE 2.948463731514285\n",
      "132 Train Loss 35029.297 Test MSE 68963.74576216012 Test RE 2.9259356698614503\n",
      "133 Train Loss 34751.117 Test MSE 68373.3895702222 Test RE 2.913385185463822\n",
      "134 Train Loss 34475.445 Test MSE 67746.32290156596 Test RE 2.89999478049088\n",
      "135 Train Loss 34195.87 Test MSE 67275.48547802691 Test RE 2.8898997175963737\n",
      "136 Train Loss 33863.043 Test MSE 66753.1015707484 Test RE 2.8786580368115042\n",
      "137 Train Loss 33247.156 Test MSE 65456.23310387394 Test RE 2.8505578343536064\n",
      "138 Train Loss 32802.035 Test MSE 64671.0415940037 Test RE 2.833409072908429\n",
      "139 Train Loss 32506.12 Test MSE 63917.738474644735 Test RE 2.8168586366742847\n",
      "140 Train Loss 32205.848 Test MSE 63188.613379604605 Test RE 2.8007462617180536\n",
      "141 Train Loss 31770.975 Test MSE 62512.991000387774 Test RE 2.785733017852793\n",
      "142 Train Loss 31190.098 Test MSE 61555.32256368635 Test RE 2.764312630329463\n",
      "143 Train Loss 30964.285 Test MSE 61138.944759662714 Test RE 2.75494746574023\n",
      "144 Train Loss 30807.457 Test MSE 60865.383954635836 Test RE 2.7487771714128284\n",
      "145 Train Loss 30553.053 Test MSE 60248.38831135431 Test RE 2.7348094329265082\n",
      "146 Train Loss 30083.87 Test MSE 59159.326585935065 Test RE 2.7099792355375154\n",
      "147 Train Loss 29565.486 Test MSE 57975.290902992725 Test RE 2.6827229255008236\n",
      "148 Train Loss 29124.375 Test MSE 56631.86188068271 Test RE 2.651458123780543\n",
      "149 Train Loss 28659.695 Test MSE 56020.077333961344 Test RE 2.6370976052299477\n",
      "150 Train Loss 28440.98 Test MSE 55913.65475720917 Test RE 2.634591538208035\n",
      "151 Train Loss 28022.547 Test MSE 55035.763174256135 Test RE 2.61382705511016\n",
      "152 Train Loss 27730.604 Test MSE 54568.95272549122 Test RE 2.6027182771226873\n",
      "153 Train Loss 27623.992 Test MSE 54429.01764046729 Test RE 2.5993789662274214\n",
      "154 Train Loss 27518.55 Test MSE 54082.55228295562 Test RE 2.5910926462183688\n",
      "155 Train Loss 27288.982 Test MSE 53337.45775515218 Test RE 2.573182021016963\n",
      "156 Train Loss 27119.898 Test MSE 52946.03272493134 Test RE 2.5637227911296154\n",
      "157 Train Loss 26871.84 Test MSE 52555.738202307475 Test RE 2.554256002530249\n",
      "158 Train Loss 26494.059 Test MSE 51714.06356975069 Test RE 2.533720380799585\n",
      "159 Train Loss 26250.807 Test MSE 50929.258454570765 Test RE 2.5144211942762835\n",
      "160 Train Loss 26040.973 Test MSE 50568.75539389779 Test RE 2.505506217346157\n",
      "161 Train Loss 25755.102 Test MSE 50149.71440580341 Test RE 2.4951036091895347\n",
      "162 Train Loss 25566.42 Test MSE 49758.357663022914 Test RE 2.485348936106699\n",
      "163 Train Loss 25244.06 Test MSE 49316.067842797194 Test RE 2.474278452356231\n",
      "164 Train Loss 25040.104 Test MSE 48890.42956716516 Test RE 2.4635777831592223\n",
      "165 Train Loss 24654.553 Test MSE 47705.90607680591 Test RE 2.433550857419986\n",
      "166 Train Loss 24333.77 Test MSE 47178.48911323355 Test RE 2.4200612991297064\n",
      "167 Train Loss 23858.059 Test MSE 46202.121982052595 Test RE 2.394888582645409\n",
      "168 Train Loss 23451.373 Test MSE 45508.605635707274 Test RE 2.376846399320757\n",
      "169 Train Loss 23056.38 Test MSE 44964.466255990534 Test RE 2.362593874459458\n",
      "170 Train Loss 22790.584 Test MSE 44464.68553769207 Test RE 2.3494270516514284\n",
      "171 Train Loss 22389.367 Test MSE 43869.111544650805 Test RE 2.333639524957455\n",
      "172 Train Loss 22003.396 Test MSE 43147.48225166047 Test RE 2.3143662195783827\n",
      "173 Train Loss 21695.8 Test MSE 42286.60678601781 Test RE 2.2911618592295815\n",
      "174 Train Loss 21330.898 Test MSE 41367.29044053061 Test RE 2.266119928853955\n",
      "175 Train Loss 21011.217 Test MSE 40481.723428943245 Test RE 2.2417328145272992\n",
      "176 Train Loss 20465.404 Test MSE 39024.15742310873 Test RE 2.2010054595137265\n",
      "177 Train Loss 19938.668 Test MSE 38054.24704701179 Test RE 2.1734813549974175\n",
      "178 Train Loss 19518.822 Test MSE 37209.915265058735 Test RE 2.149233953277057\n",
      "179 Train Loss 18905.191 Test MSE 36387.717151539626 Test RE 2.1253563526396375\n",
      "180 Train Loss 18558.807 Test MSE 35829.33633807207 Test RE 2.108986182066993\n",
      "181 Train Loss 18107.07 Test MSE 35152.76416180898 Test RE 2.0889790887886157\n",
      "182 Train Loss 17459.885 Test MSE 34033.63362149017 Test RE 2.0554575498979246\n",
      "183 Train Loss 16923.465 Test MSE 33272.618817238996 Test RE 2.032346924648855\n",
      "184 Train Loss 16509.408 Test MSE 32499.951819585687 Test RE 2.008610418087869\n",
      "185 Train Loss 16073.652 Test MSE 31684.083213064183 Test RE 1.9832384100110263\n",
      "186 Train Loss 15799.527 Test MSE 31099.482781744104 Test RE 1.9648569415565935\n",
      "187 Train Loss 15428.153 Test MSE 30393.06074619221 Test RE 1.9424129793647886\n",
      "188 Train Loss 15001.188 Test MSE 29552.23568682487 Test RE 1.9153560737629332\n",
      "189 Train Loss 14747.248 Test MSE 28908.579431363036 Test RE 1.8943827386215195\n",
      "190 Train Loss 14433.677 Test MSE 28293.594244681364 Test RE 1.8741243910338683\n",
      "191 Train Loss 14182.155 Test MSE 27793.675921458856 Test RE 1.857493689440263\n",
      "192 Train Loss 13857.356 Test MSE 26977.92098693826 Test RE 1.8300316108597345\n",
      "193 Train Loss 13546.806 Test MSE 26337.058009736316 Test RE 1.8081646835870167\n",
      "194 Train Loss 13150.173 Test MSE 25752.262304341868 Test RE 1.787977486339605\n",
      "195 Train Loss 12722.544 Test MSE 24784.39873397948 Test RE 1.7540563696919507\n",
      "196 Train Loss 12364.192 Test MSE 23892.937051141198 Test RE 1.7222219567939931\n",
      "197 Train Loss 11952.341 Test MSE 22810.310616714673 Test RE 1.6827513668164296\n",
      "198 Train Loss 11533.752 Test MSE 22099.152399046816 Test RE 1.6563120474738318\n",
      "199 Train Loss 11173.866 Test MSE 21731.747883506894 Test RE 1.642486018486652\n",
      "Training time: 46.97\n",
      "Training time: 46.97\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "8\n",
      "0 Train Loss 12516361000.0 Test MSE 16666.398434137416 Test RE 1.4383852879454353\n",
      "1 Train Loss 3595291400.0 Test MSE 28354.140352924536 Test RE 1.876128560199549\n",
      "2 Train Loss 1762461000.0 Test MSE 32830.01982876446 Test RE 2.0187843292038976\n",
      "3 Train Loss 938170900.0 Test MSE 34727.63551076081 Test RE 2.076308871924167\n",
      "4 Train Loss 710587650.0 Test MSE 39804.096545234395 Test RE 2.2228913600327314\n",
      "5 Train Loss 600093400.0 Test MSE 45389.71780321772 Test RE 2.373739702162699\n",
      "6 Train Loss 451873570.0 Test MSE 54027.35176210262 Test RE 2.5897699813955755\n",
      "7 Train Loss 316230240.0 Test MSE 57081.24776110375 Test RE 2.6619572790200925\n",
      "8 Train Loss 226692580.0 Test MSE 58574.69468745541 Test RE 2.696555536342369\n",
      "9 Train Loss 184548940.0 Test MSE 60911.703739364624 Test RE 2.749822909926175\n",
      "10 Train Loss 167057820.0 Test MSE 65046.121593759606 Test RE 2.841613816138907\n",
      "11 Train Loss 159773730.0 Test MSE 66282.20789128602 Test RE 2.8684866672563114\n",
      "12 Train Loss 152940590.0 Test MSE 66720.8598837289 Test RE 2.877962758246872\n",
      "13 Train Loss 147190670.0 Test MSE 68604.27097575983 Test RE 2.91829995993863\n",
      "14 Train Loss 142717740.0 Test MSE 71127.37344987376 Test RE 2.9714794980738115\n",
      "15 Train Loss 140491070.0 Test MSE 72312.21950975273 Test RE 2.996126861299328\n",
      "16 Train Loss 138425100.0 Test MSE 73193.48187443333 Test RE 3.0143283354818853\n",
      "17 Train Loss 137247790.0 Test MSE 73797.02712968945 Test RE 3.026730727740714\n",
      "18 Train Loss 135965620.0 Test MSE 74952.98867316684 Test RE 3.050344072556315\n",
      "19 Train Loss 133860330.0 Test MSE 78024.87286913281 Test RE 3.1122242848048147\n",
      "20 Train Loss 133233790.0 Test MSE 78926.34906761281 Test RE 3.1301515094005303\n",
      "21 Train Loss 131287416.0 Test MSE 79903.15330344408 Test RE 3.1494615574498384\n",
      "22 Train Loss 130523300.0 Test MSE 80636.14076074671 Test RE 3.1638742909022595\n",
      "23 Train Loss 129934910.0 Test MSE 81527.83964207041 Test RE 3.181319734613289\n",
      "24 Train Loss 128961540.0 Test MSE 83816.09356538406 Test RE 3.2256560743308706\n",
      "25 Train Loss 128070210.0 Test MSE 85194.64304925154 Test RE 3.2520745757638503\n",
      "26 Train Loss 127622410.0 Test MSE 86064.4536419805 Test RE 3.2686337479730523\n",
      "27 Train Loss 126665790.0 Test MSE 89995.65414927356 Test RE 3.3424515549834704\n",
      "28 Train Loss 126054024.0 Test MSE 91634.37717803083 Test RE 3.3727454768967076\n",
      "29 Train Loss 125916070.0 Test MSE 92019.16653214364 Test RE 3.3798194431628854\n",
      "30 Train Loss 125827830.0 Test MSE 93051.70882719646 Test RE 3.398728934871039\n",
      "31 Train Loss 125905870.0 Test MSE 93327.23578262524 Test RE 3.40375704917996\n",
      "32 Train Loss 125923370.0 Test MSE 94091.50218032843 Test RE 3.417665492281831\n",
      "33 Train Loss 125993576.0 Test MSE 95023.88271404295 Test RE 3.434557078774676\n",
      "34 Train Loss 126043160.0 Test MSE 95177.03543018096 Test RE 3.437323751498961\n",
      "35 Train Loss 125795650.0 Test MSE 95065.31130076031 Test RE 3.4353056976286385\n",
      "36 Train Loss 124018136.0 Test MSE 95209.1330317299 Test RE 3.437903305943487\n",
      "37 Train Loss 118544350.0 Test MSE 92415.34464002338 Test RE 3.3870873438464355\n",
      "38 Train Loss 112665690.0 Test MSE 91262.23667444802 Test RE 3.365889905032423\n",
      "39 Train Loss 110758590.0 Test MSE 93674.11029462011 Test RE 3.4100766488018017\n",
      "40 Train Loss 109590310.0 Test MSE 94641.57055216844 Test RE 3.427640942018435\n",
      "41 Train Loss 108881620.0 Test MSE 93949.7801566592 Test RE 3.4150906529869065\n",
      "42 Train Loss 108404530.0 Test MSE 93270.29934971 Test RE 3.4027186205262723\n",
      "43 Train Loss 108253400.0 Test MSE 94071.5069399143 Test RE 3.417302331568454\n",
      "44 Train Loss 108140290.0 Test MSE 95009.75668590334 Test RE 3.43430178267551\n",
      "45 Train Loss 108255740.0 Test MSE 96230.05630644104 Test RE 3.456286398761541\n",
      "46 Train Loss 108425780.0 Test MSE 96852.69082165453 Test RE 3.467449924452852\n",
      "47 Train Loss 108473016.0 Test MSE 97569.9945065493 Test RE 3.480266431678971\n",
      "48 Train Loss 108639970.0 Test MSE 98001.99520151436 Test RE 3.4879625323108168\n",
      "49 Train Loss 108814350.0 Test MSE 98252.08749043733 Test RE 3.492410180160951\n",
      "50 Train Loss 109083860.0 Test MSE 98345.55775791411 Test RE 3.494071004483461\n",
      "51 Train Loss 109295440.0 Test MSE 98186.56550031844 Test RE 3.4912454831343624\n",
      "52 Train Loss 109208060.0 Test MSE 96830.78166321357 Test RE 3.4670577143558683\n",
      "53 Train Loss 102288136.0 Test MSE 99990.52158957283 Test RE 3.5231713791332306\n",
      "54 Train Loss 68081144.0 Test MSE 102219.22726492216 Test RE 3.562219273844862\n",
      "55 Train Loss 39525690.0 Test MSE 97134.15968621717 Test RE 3.4724847413838305\n",
      "56 Train Loss 22983290.0 Test MSE 92209.93069926808 Test RE 3.383320967115211\n",
      "57 Train Loss 16792184.0 Test MSE 88321.53228842537 Test RE 3.3112170515692676\n",
      "58 Train Loss 13781384.0 Test MSE 86821.0224603307 Test RE 3.282969140101697\n",
      "59 Train Loss 11266603.0 Test MSE 86693.97214385924 Test RE 3.280566179616306\n",
      "60 Train Loss 8419824.0 Test MSE 86651.99525636864 Test RE 3.279771864697553\n",
      "61 Train Loss 6243200.5 Test MSE 87185.81653300153 Test RE 3.289858901903055\n",
      "62 Train Loss 4430353.5 Test MSE 86189.16463889238 Test RE 3.271001084271651\n",
      "63 Train Loss 3507213.8 Test MSE 84654.18621276991 Test RE 3.2417429268404363\n",
      "64 Train Loss 2951550.8 Test MSE 84191.19113348136 Test RE 3.2328658178801692\n",
      "65 Train Loss 2505760.5 Test MSE 83586.42667398724 Test RE 3.2212336862297413\n",
      "66 Train Loss 2056271.8 Test MSE 83090.02647090593 Test RE 3.211654366665951\n",
      "67 Train Loss 1733590.4 Test MSE 82765.04916973262 Test RE 3.2053675874710303\n",
      "68 Train Loss 1575853.5 Test MSE 82649.15494923406 Test RE 3.203122595665615\n",
      "69 Train Loss 1088304.2 Test MSE 82672.3381677614 Test RE 3.203571804640958\n",
      "70 Train Loss 810571.9 Test MSE 84657.31675731274 Test RE 3.2418028667398175\n",
      "71 Train Loss 560094.2 Test MSE 86504.01220142386 Test RE 3.276970093921777\n",
      "72 Train Loss 390989.84 Test MSE 86919.1315882946 Test RE 3.2848235198949456\n",
      "73 Train Loss 277445.84 Test MSE 87425.72925764488 Test RE 3.294382210953046\n",
      "74 Train Loss 231740.66 Test MSE 87762.68286988673 Test RE 3.3007246599527367\n",
      "75 Train Loss 198478.06 Test MSE 87875.60404855204 Test RE 3.3028474409111768\n",
      "76 Train Loss 170184.52 Test MSE 87730.70396291137 Test RE 3.3001232472535325\n",
      "77 Train Loss 144081.58 Test MSE 87812.18086180206 Test RE 3.301655330069505\n",
      "78 Train Loss 130170.734 Test MSE 87628.27860711669 Test RE 3.2981962422347566\n",
      "79 Train Loss 115298.31 Test MSE 87436.95392181393 Test RE 3.294593688440467\n",
      "80 Train Loss 101629.72 Test MSE 87637.2966729218 Test RE 3.2983659510663137\n",
      "81 Train Loss 94292.766 Test MSE 87678.27616305344 Test RE 3.2991370243583047\n",
      "82 Train Loss 90699.305 Test MSE 87742.89773355823 Test RE 3.3003525828565845\n",
      "83 Train Loss 85682.45 Test MSE 87947.48413299794 Test RE 3.30419798880617\n",
      "84 Train Loss 81186.4 Test MSE 88181.1394527872 Test RE 3.3085843071978926\n",
      "85 Train Loss 76075.88 Test MSE 88678.84665587278 Test RE 3.3179082353282614\n",
      "86 Train Loss 73414.03 Test MSE 88905.17051231947 Test RE 3.3221394770792303\n",
      "87 Train Loss 67986.516 Test MSE 88667.13907619524 Test RE 3.3176892092658594\n",
      "88 Train Loss 64818.79 Test MSE 88321.14523299926 Test RE 3.311209796113561\n",
      "89 Train Loss 61153.96 Test MSE 88294.89585275548 Test RE 3.310717707476312\n",
      "90 Train Loss 58107.023 Test MSE 88051.60009028463 Test RE 3.3061532350761347\n",
      "91 Train Loss 55941.395 Test MSE 87847.69290813332 Test RE 3.3023228723409916\n",
      "92 Train Loss 54446.81 Test MSE 87714.67507506131 Test RE 3.299821758040459\n",
      "93 Train Loss 52775.23 Test MSE 87670.11951908334 Test RE 3.2989835626732975\n",
      "94 Train Loss 51558.656 Test MSE 87418.67683002402 Test RE 3.294249333247249\n",
      "95 Train Loss 49708.18 Test MSE 87066.44350711822 Test RE 3.2876059269023408\n",
      "96 Train Loss 48827.613 Test MSE 87011.77477298367 Test RE 3.286573626346413\n",
      "97 Train Loss 48353.355 Test MSE 86730.37357743998 Test RE 3.2812548363718492\n",
      "98 Train Loss 47631.902 Test MSE 86502.81292124702 Test RE 3.276947378103198\n",
      "99 Train Loss 47171.746 Test MSE 86292.47439498744 Test RE 3.2729608730878152\n",
      "100 Train Loss 46782.613 Test MSE 85871.83252848603 Test RE 3.264973929534221\n",
      "101 Train Loss 46483.027 Test MSE 85593.60966996376 Test RE 3.259680416291646\n",
      "102 Train Loss 46225.508 Test MSE 85427.84360099974 Test RE 3.2565224335823535\n",
      "103 Train Loss 46061.516 Test MSE 85203.77425695016 Test RE 3.2522488506459974\n",
      "104 Train Loss 45597.64 Test MSE 84658.01409966766 Test RE 3.2418162184610653\n",
      "105 Train Loss 45215.523 Test MSE 84314.4113808632 Test RE 3.2352307260817055\n",
      "106 Train Loss 44948.97 Test MSE 84324.83620756738 Test RE 3.2354307255638064\n",
      "107 Train Loss 44669.324 Test MSE 84163.7100255105 Test RE 3.2323381499350914\n",
      "108 Train Loss 44135.99 Test MSE 83689.8991183996 Test RE 3.2232268677127256\n",
      "109 Train Loss 43567.375 Test MSE 83412.9077673445 Test RE 3.2178884341801637\n",
      "110 Train Loss 43182.28 Test MSE 82975.13397907911 Test RE 3.209433145698183\n",
      "111 Train Loss 43001.105 Test MSE 82783.39256606615 Test RE 3.2057227740573833\n",
      "112 Train Loss 42774.453 Test MSE 82786.4777876917 Test RE 3.20578250991003\n",
      "113 Train Loss 42514.48 Test MSE 82580.69418764372 Test RE 3.201795699812529\n",
      "114 Train Loss 42324.215 Test MSE 82453.96648501384 Test RE 3.1993380309561763\n",
      "115 Train Loss 42186.633 Test MSE 82282.30634336772 Test RE 3.196005959838811\n",
      "116 Train Loss 42021.906 Test MSE 81937.28546316556 Test RE 3.1892982774751704\n",
      "117 Train Loss 41883.344 Test MSE 81696.69424249338 Test RE 3.1846124909296014\n",
      "118 Train Loss 41643.008 Test MSE 81478.89797551683 Test RE 3.1803647083048925\n",
      "119 Train Loss 41465.6 Test MSE 81279.94911137564 Test RE 3.176479551092326\n",
      "120 Train Loss 41354.574 Test MSE 81016.73393442675 Test RE 3.1713320600167316\n",
      "121 Train Loss 41246.09 Test MSE 80854.50168245389 Test RE 3.1681552461857114\n",
      "122 Train Loss 41009.09 Test MSE 80444.07015809586 Test RE 3.1601039620114366\n",
      "123 Train Loss 40815.527 Test MSE 79894.38887653017 Test RE 3.1492888234495187\n",
      "124 Train Loss 40643.973 Test MSE 79341.55340465314 Test RE 3.1383740340883626\n",
      "125 Train Loss 40346.215 Test MSE 78484.17229630674 Test RE 3.1213710178677627\n",
      "126 Train Loss 40167.086 Test MSE 78037.0135595339 Test RE 3.112466406838494\n",
      "127 Train Loss 39860.54 Test MSE 77787.69892658773 Test RE 3.107490536035208\n",
      "128 Train Loss 39400.945 Test MSE 76903.75372049074 Test RE 3.0897840120069557\n",
      "129 Train Loss 39221.105 Test MSE 76307.84863120427 Test RE 3.07778980768106\n",
      "130 Train Loss 39046.562 Test MSE 75987.46641982054 Test RE 3.0713218865187555\n",
      "131 Train Loss 38849.086 Test MSE 75806.54968096827 Test RE 3.0676634893137416\n",
      "132 Train Loss 38708.46 Test MSE 75565.25034932829 Test RE 3.0627772681124483\n",
      "133 Train Loss 38463.33 Test MSE 74875.12360486797 Test RE 3.0487592327012605\n",
      "134 Train Loss 38090.19 Test MSE 73750.01322007617 Test RE 3.0257664536964133\n",
      "135 Train Loss 37675.85 Test MSE 72774.08872323425 Test RE 3.005679991665317\n",
      "136 Train Loss 37446.832 Test MSE 72359.33474397655 Test RE 2.9971027698902537\n",
      "137 Train Loss 37279.53 Test MSE 72211.38137809496 Test RE 2.9940371091311495\n",
      "138 Train Loss 37084.633 Test MSE 71786.59130947612 Test RE 2.9852177710207703\n",
      "139 Train Loss 36738.527 Test MSE 71051.37178827309 Test RE 2.9698915177435268\n",
      "140 Train Loss 36475.656 Test MSE 70631.843888305 Test RE 2.961110568313507\n",
      "141 Train Loss 36272.35 Test MSE 70189.93472178232 Test RE 2.9518329182705965\n",
      "142 Train Loss 35959.02 Test MSE 69736.8289686688 Test RE 2.9422898263855966\n",
      "143 Train Loss 35743.46 Test MSE 69355.80895791817 Test RE 2.934240945970697\n",
      "144 Train Loss 35380.027 Test MSE 68590.79056966568 Test RE 2.918013229967462\n",
      "145 Train Loss 35097.387 Test MSE 67907.15427790134 Test RE 2.9034350680434513\n",
      "146 Train Loss 34795.793 Test MSE 66948.13447064171 Test RE 2.8828602650527517\n",
      "147 Train Loss 34639.543 Test MSE 66538.09457035003 Test RE 2.874018321179957\n",
      "148 Train Loss 34482.46 Test MSE 66306.0158375387 Test RE 2.869001787732404\n",
      "149 Train Loss 34302.88 Test MSE 66314.39609245444 Test RE 2.869183085058116\n",
      "150 Train Loss 34098.86 Test MSE 66034.45011995854 Test RE 2.8631205572814418\n",
      "151 Train Loss 33888.047 Test MSE 65619.28081903915 Test RE 2.854105914090014\n",
      "152 Train Loss 33498.027 Test MSE 64733.132234366785 Test RE 2.83476892402979\n",
      "153 Train Loss 33170.805 Test MSE 64009.225666298495 Test RE 2.818873838889424\n",
      "154 Train Loss 33013.375 Test MSE 63794.33879035022 Test RE 2.8141382072134915\n",
      "155 Train Loss 32867.95 Test MSE 63702.69237615397 Test RE 2.812116096995613\n",
      "156 Train Loss 32689.697 Test MSE 63368.71295035014 Test RE 2.804734751617108\n",
      "157 Train Loss 32532.4 Test MSE 63065.83508124955 Test RE 2.798023951136073\n",
      "158 Train Loss 32252.865 Test MSE 62600.554849968205 Test RE 2.7876833656360738\n",
      "159 Train Loss 31960.117 Test MSE 61999.60886517677 Test RE 2.7742706490288165\n",
      "160 Train Loss 31804.656 Test MSE 61685.1381285374 Test RE 2.76722595928677\n",
      "161 Train Loss 31635.023 Test MSE 61705.54191917045 Test RE 2.767683583542561\n",
      "162 Train Loss 31345.195 Test MSE 61317.11089411094 Test RE 2.758958667375366\n",
      "163 Train Loss 31040.184 Test MSE 60916.00097880255 Test RE 2.749919906384372\n",
      "164 Train Loss 30943.354 Test MSE 60850.108701176236 Test RE 2.7484322224449107\n",
      "165 Train Loss 30862.227 Test MSE 60749.813845908306 Test RE 2.74616626683237\n",
      "166 Train Loss 30777.236 Test MSE 60695.50528112997 Test RE 2.744938496030782\n",
      "167 Train Loss 30683.566 Test MSE 60587.34413182655 Test RE 2.7424916254734506\n",
      "168 Train Loss 30585.81 Test MSE 60426.00181733735 Test RE 2.738837603966858\n",
      "169 Train Loss 30485.488 Test MSE 60180.64486723576 Test RE 2.733271487076716\n",
      "170 Train Loss 30378.814 Test MSE 59996.010625658426 Test RE 2.7290754272398163\n",
      "171 Train Loss 30286.82 Test MSE 59918.93694712369 Test RE 2.727321914993602\n",
      "172 Train Loss 30174.031 Test MSE 59684.33501803339 Test RE 2.721977506913291\n",
      "173 Train Loss 30080.998 Test MSE 59495.82199089633 Test RE 2.7176754229610642\n",
      "174 Train Loss 29872.99 Test MSE 59138.58903499708 Test RE 2.7095042194965315\n",
      "175 Train Loss 29658.855 Test MSE 58759.0116618649 Test RE 2.700794829269747\n",
      "176 Train Loss 29498.75 Test MSE 58437.804639289716 Test RE 2.6934027453777634\n",
      "177 Train Loss 29419.734 Test MSE 58157.32532195366 Test RE 2.6869313146760745\n",
      "178 Train Loss 29343.611 Test MSE 58071.92834120582 Test RE 2.6849578735455575\n",
      "179 Train Loss 29251.953 Test MSE 57968.28850323419 Test RE 2.682560907636426\n",
      "180 Train Loss 29126.578 Test MSE 57755.49797749871 Test RE 2.677632796333114\n",
      "181 Train Loss 29038.77 Test MSE 57527.58075356269 Test RE 2.6723442790240006\n",
      "182 Train Loss 28970.39 Test MSE 57343.91130978668 Test RE 2.6680748453316716\n",
      "183 Train Loss 28907.502 Test MSE 57191.2086752411 Test RE 2.6645200332840844\n",
      "184 Train Loss 28822.64 Test MSE 57063.455148773704 Test RE 2.6615423716100968\n",
      "185 Train Loss 28684.615 Test MSE 56934.48646745242 Test RE 2.658533004442615\n",
      "186 Train Loss 28615.684 Test MSE 56730.84929838581 Test RE 2.653774367628628\n",
      "187 Train Loss 28503.9 Test MSE 56546.51097273698 Test RE 2.6494593400403206\n",
      "188 Train Loss 28370.79 Test MSE 56312.303414218106 Test RE 2.6439668060482284\n",
      "189 Train Loss 28295.389 Test MSE 56174.8658176044 Test RE 2.64073836029349\n",
      "190 Train Loss 28221.643 Test MSE 56027.465871968925 Test RE 2.6372715040748376\n",
      "191 Train Loss 28156.186 Test MSE 55873.967481751344 Test RE 2.6336563612717723\n",
      "192 Train Loss 28103.01 Test MSE 55756.881574544226 Test RE 2.6308954533328244\n",
      "193 Train Loss 28056.682 Test MSE 55643.191633200775 Test RE 2.628211847572848\n",
      "194 Train Loss 28019.062 Test MSE 55489.187234196164 Test RE 2.6245722591105682\n",
      "195 Train Loss 27967.428 Test MSE 55237.563331951664 Test RE 2.618614742656232\n",
      "196 Train Loss 27901.846 Test MSE 55133.43934057028 Test RE 2.6161455059949112\n",
      "197 Train Loss 27737.355 Test MSE 54946.78617444775 Test RE 2.611713297146953\n",
      "198 Train Loss 27654.184 Test MSE 54738.98784781483 Test RE 2.6067701169411777\n",
      "199 Train Loss 27588.088 Test MSE 54628.62177851768 Test RE 2.604140874554113\n",
      "Training time: 46.10\n",
      "Training time: 46.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "9\n",
      "0 Train Loss 14541293000.0 Test MSE 12301.434700944272 Test RE 1.2357545982765694\n",
      "1 Train Loss 10653176000.0 Test MSE 15223.078578445191 Test RE 1.3746926065025649\n",
      "2 Train Loss 9661179000.0 Test MSE 15402.296675187445 Test RE 1.3827609125789333\n",
      "3 Train Loss 8717847000.0 Test MSE 15380.844638162342 Test RE 1.3817976350024617\n",
      "4 Train Loss 7801720300.0 Test MSE 16427.996533554386 Test RE 1.428060654017311\n",
      "5 Train Loss 4172510500.0 Test MSE 13189.554413495152 Test RE 1.279585806995503\n",
      "6 Train Loss 949744500.0 Test MSE 14633.197762591004 Test RE 1.3477954109803716\n",
      "7 Train Loss 702806200.0 Test MSE 15988.216835585845 Test RE 1.408816298009938\n",
      "8 Train Loss 645339500.0 Test MSE 17528.631515466557 Test RE 1.4751234195646405\n",
      "9 Train Loss 616328600.0 Test MSE 18251.512527440387 Test RE 1.5052331794025877\n",
      "10 Train Loss 597401900.0 Test MSE 18532.8017534011 Test RE 1.5167880289446285\n",
      "11 Train Loss 524034780.0 Test MSE 19197.14288002903 Test RE 1.54373464546759\n",
      "12 Train Loss 353560670.0 Test MSE 17894.20089960531 Test RE 1.4904263044550374\n",
      "13 Train Loss 255415380.0 Test MSE 18733.840510950737 Test RE 1.5249926890405616\n",
      "14 Train Loss 187657860.0 Test MSE 18646.88271154952 Test RE 1.5214492551122343\n",
      "15 Train Loss 167928110.0 Test MSE 18736.467911045598 Test RE 1.5250996245473858\n",
      "16 Train Loss 154928850.0 Test MSE 19688.806523664658 Test RE 1.5633781863692062\n",
      "17 Train Loss 147537330.0 Test MSE 20373.80972000156 Test RE 1.590341804733502\n",
      "18 Train Loss 142644480.0 Test MSE 20988.324178942545 Test RE 1.614147560624163\n",
      "19 Train Loss 139918830.0 Test MSE 20843.058669398015 Test RE 1.6085518994401018\n",
      "20 Train Loss 128786456.0 Test MSE 20578.456078568644 Test RE 1.5983090055491944\n",
      "21 Train Loss 67402590.0 Test MSE 20912.372531440134 Test RE 1.6112243096067187\n",
      "22 Train Loss 30028032.0 Test MSE 20336.604841058856 Test RE 1.5888890692876514\n",
      "23 Train Loss 15688659.0 Test MSE 20793.900960968942 Test RE 1.6066539197763594\n",
      "24 Train Loss 12288176.0 Test MSE 21024.85787986698 Test RE 1.615551797186803\n",
      "25 Train Loss 9681210.0 Test MSE 21141.743597926234 Test RE 1.620036327057633\n",
      "26 Train Loss 7867984.5 Test MSE 21215.810282293307 Test RE 1.6228716138068482\n",
      "27 Train Loss 6615050.5 Test MSE 21314.240961199128 Test RE 1.6266319110203877\n",
      "28 Train Loss 5124143.0 Test MSE 21487.44812377972 Test RE 1.633227834927909\n",
      "29 Train Loss 4400367.5 Test MSE 21652.83153776575 Test RE 1.639501056497555\n",
      "30 Train Loss 3735038.2 Test MSE 21968.605646435077 Test RE 1.65141261895426\n",
      "31 Train Loss 3490903.8 Test MSE 22179.36010293081 Test RE 1.6593150735630517\n",
      "32 Train Loss 3268607.0 Test MSE 22223.216391965467 Test RE 1.6609547841560106\n",
      "33 Train Loss 2974277.0 Test MSE 22409.874825718587 Test RE 1.667915588683368\n",
      "34 Train Loss 2754242.5 Test MSE 22605.915453697224 Test RE 1.675195130241081\n",
      "35 Train Loss 2608154.8 Test MSE 22706.10236753399 Test RE 1.6789031660323028\n",
      "36 Train Loss 2469590.5 Test MSE 22677.835546095084 Test RE 1.6778578075132278\n",
      "37 Train Loss 2338520.0 Test MSE 22706.35213473115 Test RE 1.6789123999769524\n",
      "38 Train Loss 1997741.2 Test MSE 22916.13699128437 Test RE 1.6866503362621506\n",
      "39 Train Loss 1749420.4 Test MSE 23085.949852174814 Test RE 1.692887999242964\n",
      "40 Train Loss 1534226.4 Test MSE 23243.051227568845 Test RE 1.6986383388972017\n",
      "41 Train Loss 1364179.9 Test MSE 23424.298888037298 Test RE 1.7052484081899124\n",
      "42 Train Loss 1297067.4 Test MSE 23504.012908515444 Test RE 1.7081474652240374\n",
      "43 Train Loss 1148158.6 Test MSE 23556.273821740164 Test RE 1.7100454342972677\n",
      "44 Train Loss 991808.5 Test MSE 23711.57750656501 Test RE 1.7156732357895297\n",
      "45 Train Loss 890831.4 Test MSE 23878.560468042888 Test RE 1.721703741041174\n",
      "46 Train Loss 828167.8 Test MSE 23933.852737052788 Test RE 1.7236959436632124\n",
      "47 Train Loss 758014.2 Test MSE 24087.362208507617 Test RE 1.7292149197696822\n",
      "48 Train Loss 650119.0 Test MSE 24262.351943804628 Test RE 1.7354847485983351\n",
      "49 Train Loss 568770.4 Test MSE 24428.98386363081 Test RE 1.7414341372909052\n",
      "50 Train Loss 453629.7 Test MSE 24647.905915331794 Test RE 1.7492197254817072\n",
      "51 Train Loss 404666.72 Test MSE 24826.218960705788 Test RE 1.755535609068144\n",
      "52 Train Loss 361686.7 Test MSE 24951.322316948095 Test RE 1.759953265507627\n",
      "53 Train Loss 331365.75 Test MSE 24918.983889439034 Test RE 1.7588123926237922\n",
      "54 Train Loss 305756.88 Test MSE 24930.122438202263 Test RE 1.759205434911787\n",
      "55 Train Loss 290327.75 Test MSE 25008.487978196503 Test RE 1.7619682154747682\n",
      "56 Train Loss 268336.34 Test MSE 25040.071869949265 Test RE 1.763080482919982\n",
      "57 Train Loss 245894.3 Test MSE 25110.01316529538 Test RE 1.7655410618148892\n",
      "58 Train Loss 232823.22 Test MSE 25139.189498413692 Test RE 1.7665664905757563\n",
      "59 Train Loss 214005.53 Test MSE 25280.72699012915 Test RE 1.771532530608464\n",
      "60 Train Loss 183948.33 Test MSE 25540.92261578094 Test RE 1.7806257232064415\n",
      "61 Train Loss 173361.5 Test MSE 25560.14282209954 Test RE 1.781295580715151\n",
      "62 Train Loss 156083.23 Test MSE 25698.89532912651 Test RE 1.7861238931474261\n",
      "63 Train Loss 143291.88 Test MSE 25794.61307208977 Test RE 1.78944708756884\n",
      "64 Train Loss 130629.34 Test MSE 25849.885174686937 Test RE 1.7913632546341531\n",
      "65 Train Loss 122882.65 Test MSE 25937.685294078135 Test RE 1.794402892516491\n",
      "66 Train Loss 103833.75 Test MSE 26152.962427719514 Test RE 1.8018340814845564\n",
      "67 Train Loss 95343.766 Test MSE 26165.815574168864 Test RE 1.8022767921837262\n",
      "68 Train Loss 88273.35 Test MSE 26168.01391056306 Test RE 1.802352500266268\n",
      "69 Train Loss 83430.94 Test MSE 26181.105906822937 Test RE 1.802803307149456\n",
      "70 Train Loss 78865.12 Test MSE 26198.914134827202 Test RE 1.8034163308114453\n",
      "71 Train Loss 75868.945 Test MSE 26192.788201341576 Test RE 1.8032054775236532\n",
      "72 Train Loss 71508.42 Test MSE 26168.82769176391 Test RE 1.8023805251133975\n",
      "73 Train Loss 64652.418 Test MSE 26219.82587318144 Test RE 1.8041359226197666\n",
      "74 Train Loss 58732.79 Test MSE 26248.98128969948 Test RE 1.8051387080745813\n",
      "75 Train Loss 51978.484 Test MSE 26269.377275117575 Test RE 1.8058398864010525\n",
      "76 Train Loss 45966.016 Test MSE 26379.88866221232 Test RE 1.8096343506344847\n",
      "77 Train Loss 42351.562 Test MSE 26431.029044049257 Test RE 1.8113875912161843\n",
      "78 Train Loss 40466.39 Test MSE 26490.04894762339 Test RE 1.813408857708966\n",
      "79 Train Loss 37551.5 Test MSE 26626.339139250817 Test RE 1.8180678291300105\n",
      "80 Train Loss 34154.137 Test MSE 26732.279709012782 Test RE 1.8216810922058313\n",
      "81 Train Loss 31569.969 Test MSE 26768.267696102168 Test RE 1.8229068871401393\n",
      "82 Train Loss 28646.98 Test MSE 26905.163692249924 Test RE 1.8275622203718522\n",
      "83 Train Loss 26492.016 Test MSE 26992.74318084705 Test RE 1.8305342692867812\n",
      "84 Train Loss 24964.82 Test MSE 27007.33498599642 Test RE 1.8310289798526047\n",
      "85 Train Loss 24267.074 Test MSE 27021.632286506272 Test RE 1.8315135761267252\n",
      "86 Train Loss 23770.41 Test MSE 26997.038531219954 Test RE 1.8306799097877757\n",
      "87 Train Loss 23057.572 Test MSE 27033.11929903462 Test RE 1.8319028269266935\n",
      "88 Train Loss 22016.592 Test MSE 27107.041372384763 Test RE 1.8344057864413583\n",
      "89 Train Loss 21288.248 Test MSE 27146.59633408415 Test RE 1.8357436934235978\n",
      "90 Train Loss 20908.434 Test MSE 27178.455890441637 Test RE 1.8368206024593987\n",
      "91 Train Loss 20552.703 Test MSE 27205.916502868713 Test RE 1.8377483130756773\n",
      "92 Train Loss 20103.328 Test MSE 27232.237087255766 Test RE 1.8386370704177173\n",
      "93 Train Loss 19705.613 Test MSE 27268.144468109436 Test RE 1.8398488491745248\n",
      "94 Train Loss 19397.582 Test MSE 27253.854933619787 Test RE 1.8393667109631502\n",
      "95 Train Loss 19035.535 Test MSE 27262.63728683665 Test RE 1.8396630482524219\n",
      "96 Train Loss 18764.201 Test MSE 27296.460432368312 Test RE 1.8408038752230071\n",
      "97 Train Loss 18581.75 Test MSE 27323.101003514683 Test RE 1.8417019423393055\n",
      "98 Train Loss 18481.352 Test MSE 27316.771061468196 Test RE 1.8414885964633552\n",
      "99 Train Loss 18393.014 Test MSE 27336.043660911073 Test RE 1.8421380878404356\n",
      "100 Train Loss 18142.184 Test MSE 27370.465791272156 Test RE 1.8432975527732682\n",
      "101 Train Loss 17976.287 Test MSE 27372.310273236297 Test RE 1.8433596611757814\n",
      "102 Train Loss 17824.46 Test MSE 27385.974021439146 Test RE 1.8438196892030976\n",
      "103 Train Loss 17743.674 Test MSE 27393.235720404093 Test RE 1.8440641277623644\n",
      "104 Train Loss 17613.25 Test MSE 27376.89992200272 Test RE 1.8435141972506004\n",
      "105 Train Loss 17457.586 Test MSE 27368.501003980964 Test RE 1.843231391082211\n",
      "106 Train Loss 17291.008 Test MSE 27385.586646073865 Test RE 1.8438066487165539\n",
      "107 Train Loss 17137.469 Test MSE 27388.99897550776 Test RE 1.8439215171654348\n",
      "108 Train Loss 16997.145 Test MSE 27373.140419834494 Test RE 1.8433876136418772\n",
      "109 Train Loss 16900.412 Test MSE 27374.71314599141 Test RE 1.8434405688788609\n",
      "110 Train Loss 16819.477 Test MSE 27377.776729492194 Test RE 1.8435437183818757\n",
      "111 Train Loss 16716.357 Test MSE 27385.501751317213 Test RE 1.843803790832951\n",
      "112 Train Loss 16485.262 Test MSE 27434.861925369012 Test RE 1.8454646971037563\n",
      "113 Train Loss 16297.199 Test MSE 27471.964742628155 Test RE 1.8467121756756453\n",
      "114 Train Loss 16204.16 Test MSE 27474.89190949016 Test RE 1.846810557621542\n",
      "115 Train Loss 16141.564 Test MSE 27484.73088121593 Test RE 1.8471412066090194\n",
      "116 Train Loss 16010.8125 Test MSE 27453.227847446786 Test RE 1.8460823052840616\n",
      "117 Train Loss 15941.317 Test MSE 27447.636566847097 Test RE 1.8458943038977154\n",
      "118 Train Loss 15878.505 Test MSE 27436.8322508721 Test RE 1.8455309649946634\n",
      "119 Train Loss 15862.637 Test MSE 27424.6165656704 Test RE 1.8451200769043152\n",
      "120 Train Loss 15841.494 Test MSE 27421.982825406718 Test RE 1.8450314761447615\n",
      "121 Train Loss 15794.986 Test MSE 27414.209649375545 Test RE 1.844769956563385\n",
      "122 Train Loss 15752.299 Test MSE 27404.776126912755 Test RE 1.8444525267265213\n",
      "123 Train Loss 15708.228 Test MSE 27395.820217628872 Test RE 1.8441511175991858\n",
      "124 Train Loss 15675.079 Test MSE 27389.023757643412 Test RE 1.843922351374506\n",
      "125 Train Loss 15639.685 Test MSE 27371.71943504448 Test RE 1.8433397663777984\n",
      "126 Train Loss 15622.807 Test MSE 27373.543572486054 Test RE 1.8434011883332815\n",
      "127 Train Loss 15574.54 Test MSE 27372.92392432431 Test RE 1.843380323905938\n",
      "128 Train Loss 15531.103 Test MSE 27364.78871767739 Test RE 1.843106378121211\n",
      "129 Train Loss 15497.817 Test MSE 27381.124987130017 Test RE 1.843656446136976\n",
      "130 Train Loss 15471.303 Test MSE 27384.103406673526 Test RE 1.8437567165435733\n",
      "131 Train Loss 15417.701 Test MSE 27390.657381634952 Test RE 1.8439773411197193\n",
      "132 Train Loss 15374.786 Test MSE 27388.93663498299 Test RE 1.843919418675078\n",
      "133 Train Loss 15313.82 Test MSE 27382.200339146544 Test RE 1.8436926491821894\n",
      "134 Train Loss 15268.711 Test MSE 27387.276419061433 Test RE 1.8438635320444583\n",
      "135 Train Loss 15246.381 Test MSE 27389.601887361157 Test RE 1.8439418121025084\n",
      "136 Train Loss 15233.414 Test MSE 27385.008854301017 Test RE 1.8437871979385692\n",
      "137 Train Loss 15216.187 Test MSE 27380.907544972943 Test RE 1.8436491255935246\n",
      "138 Train Loss 15200.463 Test MSE 27376.597624472593 Test RE 1.8435040191199756\n",
      "139 Train Loss 15186.975 Test MSE 27373.560230258252 Test RE 1.8434017492207004\n",
      "140 Train Loss 15162.564 Test MSE 27375.353595890265 Test RE 1.843462133021182\n",
      "141 Train Loss 15136.81 Test MSE 27371.31721257217 Test RE 1.8433262225556115\n",
      "142 Train Loss 15106.149 Test MSE 27369.24850242163 Test RE 1.8432565624129809\n",
      "143 Train Loss 15082.326 Test MSE 27366.9302943057 Test RE 1.8431784977410677\n",
      "144 Train Loss 15071.351 Test MSE 27364.880856241612 Test RE 1.84310948103266\n",
      "145 Train Loss 15047.782 Test MSE 27362.043819717765 Test RE 1.843013936957809\n",
      "146 Train Loss 15025.818 Test MSE 27365.925584990662 Test RE 1.843144663555374\n",
      "147 Train Loss 15008.415 Test MSE 27365.298537880437 Test RE 1.8431235470550023\n",
      "148 Train Loss 14983.357 Test MSE 27351.577885831462 Test RE 1.8426614284430087\n",
      "149 Train Loss 14951.649 Test MSE 27349.968149849956 Test RE 1.842607204111609\n",
      "150 Train Loss 14915.342 Test MSE 27343.16730297664 Test RE 1.8423780983868947\n",
      "151 Train Loss 14895.383 Test MSE 27350.160514053397 Test RE 1.8426136840285485\n",
      "152 Train Loss 14852.824 Test MSE 27369.872463313823 Test RE 1.8432775734658444\n",
      "153 Train Loss 14821.149 Test MSE 27378.208494301998 Test RE 1.8435582552482652\n",
      "154 Train Loss 14792.03 Test MSE 27384.54554425816 Test RE 1.843771600924463\n",
      "155 Train Loss 14772.471 Test MSE 27373.952979511225 Test RE 1.8434149735155487\n",
      "156 Train Loss 14741.16 Test MSE 27356.281940055054 Test RE 1.8428198764611317\n",
      "157 Train Loss 14672.389 Test MSE 27348.407854821056 Test RE 1.8425546436872149\n",
      "158 Train Loss 14614.028 Test MSE 27354.19493935908 Test RE 1.842749581094828\n",
      "159 Train Loss 14568.926 Test MSE 27344.19686884308 Test RE 1.8424127840494366\n",
      "160 Train Loss 14536.265 Test MSE 27347.098612188674 Test RE 1.8425105391094097\n",
      "161 Train Loss 14506.092 Test MSE 27334.967390157188 Test RE 1.8421018232864166\n",
      "162 Train Loss 14473.82 Test MSE 27332.395946392367 Test RE 1.8420151765275918\n",
      "163 Train Loss 14454.163 Test MSE 27339.660894401604 Test RE 1.8422599639840396\n",
      "164 Train Loss 14445.651 Test MSE 27343.615854995252 Test RE 1.8423932100053986\n",
      "165 Train Loss 14420.902 Test MSE 27341.865267297788 Test RE 1.8423342323901482\n",
      "166 Train Loss 14392.802 Test MSE 27332.261015914828 Test RE 1.842010629829698\n",
      "167 Train Loss 14367.406 Test MSE 27322.697694387025 Test RE 1.8416883498494794\n",
      "168 Train Loss 14341.647 Test MSE 27314.387611790593 Test RE 1.8414082577012176\n",
      "169 Train Loss 14320.164 Test MSE 27317.630776050257 Test RE 1.8415175739391232\n",
      "170 Train Loss 14305.478 Test MSE 27319.734719438005 Test RE 1.8415884873751631\n",
      "171 Train Loss 14296.012 Test MSE 27315.7212975644 Test RE 1.8414532125809406\n",
      "172 Train Loss 14282.468 Test MSE 27307.93470136006 Test RE 1.841190732333283\n",
      "173 Train Loss 14260.48 Test MSE 27299.506890259432 Test RE 1.8409065950434786\n",
      "174 Train Loss 14254.867 Test MSE 27288.263012618543 Test RE 1.8405274478971987\n",
      "175 Train Loss 14248.111 Test MSE 27285.89614646411 Test RE 1.8404476264628684\n",
      "176 Train Loss 14221.108 Test MSE 27268.248539404096 Test RE 1.8398523601444272\n",
      "177 Train Loss 14195.415 Test MSE 27243.874981501198 Test RE 1.8390299059028803\n",
      "178 Train Loss 14164.867 Test MSE 27220.132576657033 Test RE 1.8382283952964653\n",
      "179 Train Loss 14137.688 Test MSE 27197.78596112386 Test RE 1.8374736852039315\n",
      "180 Train Loss 14106.147 Test MSE 27160.64887513137 Test RE 1.8362187718770568\n",
      "181 Train Loss 14072.984 Test MSE 27150.274647431826 Test RE 1.8358680591401508\n",
      "182 Train Loss 14050.444 Test MSE 27138.93949583254 Test RE 1.8354847846521232\n",
      "183 Train Loss 14044.569 Test MSE 27130.272694673084 Test RE 1.8351916808648545\n",
      "184 Train Loss 14037.273 Test MSE 27126.02628873577 Test RE 1.83504805396699\n",
      "185 Train Loss 14026.233 Test MSE 27122.498500225855 Test RE 1.8349287244370667\n",
      "186 Train Loss 14022.428 Test MSE 27121.504608612053 Test RE 1.8348951040650576\n",
      "187 Train Loss 14011.404 Test MSE 27119.391470994247 Test RE 1.8348236209107591\n",
      "188 Train Loss 13996.87 Test MSE 27112.68646283667 Test RE 1.8345967856613\n",
      "189 Train Loss 13982.166 Test MSE 27113.91780909414 Test RE 1.8346384450933297\n",
      "190 Train Loss 13974.036 Test MSE 27107.853989076422 Test RE 1.8344332822047138\n",
      "191 Train Loss 13962.124 Test MSE 27109.40504471544 Test RE 1.8344857627033253\n",
      "192 Train Loss 13955.201 Test MSE 27108.982027469723 Test RE 1.834471449918875\n",
      "193 Train Loss 13950.219 Test MSE 27108.28934238537 Test RE 1.8344480126845462\n",
      "194 Train Loss 13946.962 Test MSE 27104.041702883325 Test RE 1.8343042858154945\n",
      "195 Train Loss 13945.764 Test MSE 27103.332193181992 Test RE 1.8342802771230364\n",
      "196 Train Loss 13945.123 Test MSE 27103.556609368505 Test RE 1.8342878710478303\n",
      "197 Train Loss 13945.219 Test MSE 27103.556609368505 Test RE 1.8342878710478303\n",
      "198 Train Loss 13942.236 Test MSE 27102.110159417964 Test RE 1.8342389246855817\n",
      "199 Train Loss 13936.567 Test MSE 27098.647983028364 Test RE 1.8341217629316746\n",
      "Training time: 47.64\n",
      "Training time: 47.64\n"
     ]
    }
   ],
   "source": [
    " \n",
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "  optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=5e-3)\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "e199619a-d416-48f4-91f7-2c23d1e79435"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17674/2488343543.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_tanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tanh_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
