{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "3df10486-4078-44cd-95da-12a75fb13c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "515a82ba-2a23-4124-c9e1-230f67f43912"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "d35a8c58-7c75-4550-d489-9565724f04e6"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "7b44eee8-32ab-4621-ca04-81e30b53601d"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = extent*np.sin(x)/2 + np.square(x)/2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "level = \"high\"\n",
    "label = \"1D_FODE_tanhAW_\" +level\n",
    "extent = 100.0\n",
    "\n",
    "x = np.linspace(extent,-1.0*extent,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.m_lambda = nn.Sigmoid()    \n",
    "        self.lambdas_bc1 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc1.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+1,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "             \n",
    "      \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y,lambda_ind):\n",
    "        m = self.m_lambda(self.lambdas_bc1)\n",
    "        u_pred = self.forward(x)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - y))/2.0\n",
    "                \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat,lambda_ind):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - extent*torch.cos(g)/2.0 - g\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        #loss_f  = torch.sum(m*(torch.square(f)))/2.0\n",
    "        loss_f = self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "        \n",
    "        # loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = False\n",
    "        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = True        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return -1.0*loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(20):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "   \n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fAcpqTqePPt9"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    x_coll_np_array = colloc_pts(N_f,123)\n",
    "    x_coll = torch.from_numpy(x_coll_np_array).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(i)        \n",
    "    \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "3645d237-1d2a-45c9-8d9f-de486f1ca919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "0\n",
      "0 Train Loss 12908963000000.0 Test MSE 5369424.954850821 Test RE 1.035739912220272\n",
      "1 Train Loss 12649413000000.0 Test MSE 5988743.915963928 Test RE 1.093842252172117\n",
      "2 Train Loss 12307497000000.0 Test MSE 6369152.643598883 Test RE 1.1280481882017828\n",
      "3 Train Loss 12128197000000.0 Test MSE 6649513.6647303235 Test RE 1.1526083595369017\n",
      "4 Train Loss 12105653000000.0 Test MSE 6701655.303060145 Test RE 1.1571185790608276\n",
      "5 Train Loss 12044192000000.0 Test MSE 7007415.429313757 Test RE 1.1832206926966933\n",
      "6 Train Loss 11785004000000.0 Test MSE 7392702.477883688 Test RE 1.2153138234259209\n",
      "7 Train Loss 11183093000000.0 Test MSE 7903449.701414634 Test RE 1.256594546304804\n",
      "8 Train Loss 10774931000000.0 Test MSE 7903197.853706439 Test RE 1.2565745251121772\n",
      "9 Train Loss 10779147000000.0 Test MSE 7822143.956189703 Test RE 1.2501143078490686\n",
      "10 Train Loss 10799684000000.0 Test MSE 7962755.336864404 Test RE 1.2613003306083777\n",
      "11 Train Loss 10822046000000.0 Test MSE 7907817.5637262035 Test RE 1.2569417284878888\n",
      "12 Train Loss 10901925000000.0 Test MSE 7899819.6280070115 Test RE 1.2563059347162038\n",
      "13 Train Loss 10960827000000.0 Test MSE 7883136.783729413 Test RE 1.2549786997830459\n",
      "14 Train Loss 11032079000000.0 Test MSE 7890879.103067238 Test RE 1.2555948289695154\n",
      "15 Train Loss 11061010000000.0 Test MSE 7898182.287607547 Test RE 1.256175735094318\n",
      "16 Train Loss 11090459000000.0 Test MSE 7988642.597837927 Test RE 1.2633489378144325\n",
      "17 Train Loss 11131493000000.0 Test MSE 8137158.8073650515 Test RE 1.275038268200222\n",
      "18 Train Loss 11185611000000.0 Test MSE 8170044.8575596865 Test RE 1.277612182094769\n",
      "19 Train Loss 11204235000000.0 Test MSE 8390837.15828306 Test RE 1.294760584619561\n",
      "20 Train Loss 11203553000000.0 Test MSE 8556935.285090629 Test RE 1.3075127948248224\n",
      "21 Train Loss 11232393000000.0 Test MSE 8781236.893888755 Test RE 1.3245387562383921\n",
      "22 Train Loss 11217121000000.0 Test MSE 8867557.916819006 Test RE 1.331033053682204\n",
      "23 Train Loss 11130751000000.0 Test MSE 8762473.63847398 Test RE 1.3231228993701758\n",
      "24 Train Loss 11121268000000.0 Test MSE 8831965.843277395 Test RE 1.32835915747666\n",
      "25 Train Loss 11085126000000.0 Test MSE 8900676.357909068 Test RE 1.333516299796467\n",
      "26 Train Loss 10956721000000.0 Test MSE 9291063.512997817 Test RE 1.3624467525944333\n",
      "27 Train Loss 10779382000000.0 Test MSE 9820470.63933563 Test RE 1.4007253046814605\n",
      "28 Train Loss 10649394000000.0 Test MSE 10127482.212593421 Test RE 1.4224518295297395\n",
      "29 Train Loss 10549294000000.0 Test MSE 10442332.744776404 Test RE 1.444393706970025\n",
      "30 Train Loss 10512142000000.0 Test MSE 10587480.927844264 Test RE 1.4543975830496847\n",
      "31 Train Loss 10482808000000.0 Test MSE 10843502.860404924 Test RE 1.4718773521456059\n",
      "32 Train Loss 10449699000000.0 Test MSE 10952932.983890356 Test RE 1.4792856313496452\n",
      "33 Train Loss 10451186000000.0 Test MSE 11098912.901064193 Test RE 1.4891109092618342\n",
      "34 Train Loss 10455590000000.0 Test MSE 11179003.346038202 Test RE 1.49447400994971\n",
      "35 Train Loss 10399480000000.0 Test MSE 11308776.216414742 Test RE 1.5031233755893212\n",
      "36 Train Loss 10293582000000.0 Test MSE 11565967.513619443 Test RE 1.5201197715081853\n",
      "37 Train Loss 10095176000000.0 Test MSE 11397523.227275437 Test RE 1.509009822193863\n",
      "38 Train Loss 9800452000000.0 Test MSE 11503633.964015003 Test RE 1.5160179757212686\n",
      "39 Train Loss 9714910000000.0 Test MSE 11324692.4342715 Test RE 1.5041807679529557\n",
      "40 Train Loss 9699912000000.0 Test MSE 11305540.375302738 Test RE 1.5029083118059787\n",
      "41 Train Loss 9684765000000.0 Test MSE 11443365.213120745 Test RE 1.5120414715640744\n",
      "42 Train Loss 9670386000000.0 Test MSE 11621789.666737016 Test RE 1.5237837199367719\n",
      "43 Train Loss 9638325000000.0 Test MSE 11801079.445836209 Test RE 1.535492468850836\n",
      "44 Train Loss 9546065000000.0 Test MSE 12308286.195181027 Test RE 1.5681428299472653\n",
      "45 Train Loss 9510384000000.0 Test MSE 12367957.777002785 Test RE 1.5719394764792127\n",
      "46 Train Loss 9471595000000.0 Test MSE 12477263.697066905 Test RE 1.5788704641218179\n",
      "47 Train Loss 9435603000000.0 Test MSE 12599137.641580578 Test RE 1.5865626783294153\n",
      "48 Train Loss 9436014000000.0 Test MSE 12610814.54659648 Test RE 1.5872977227306913\n",
      "49 Train Loss 9415777000000.0 Test MSE 12608388.090541806 Test RE 1.5871450088262338\n",
      "50 Train Loss 9409602000000.0 Test MSE 12643618.8436437 Test RE 1.589360887146276\n",
      "51 Train Loss 9412675000000.0 Test MSE 12699511.941193404 Test RE 1.5928700226288055\n",
      "52 Train Loss 9407134000000.0 Test MSE 12704883.665441504 Test RE 1.5932068684043983\n",
      "53 Train Loss 9399978000000.0 Test MSE 12624855.649860086 Test RE 1.5881811395315952\n",
      "54 Train Loss 9393532000000.0 Test MSE 12818186.562914804 Test RE 1.6002952559533945\n",
      "55 Train Loss 9392240000000.0 Test MSE 13153569.202107958 Test RE 1.621095613573756\n",
      "56 Train Loss 9384158000000.0 Test MSE 13411379.777943714 Test RE 1.6369052970420934\n",
      "57 Train Loss 9375407000000.0 Test MSE 13597755.203001997 Test RE 1.6482399367565814\n",
      "58 Train Loss 9312316000000.0 Test MSE 13792123.026875466 Test RE 1.6599782009855988\n",
      "59 Train Loss 9202605000000.0 Test MSE 13737449.72554053 Test RE 1.6566847773122568\n",
      "60 Train Loss 9178219000000.0 Test MSE 13828043.968666608 Test RE 1.6621384632642258\n",
      "61 Train Loss 9139350000000.0 Test MSE 13929073.840101711 Test RE 1.6681993359453142\n",
      "62 Train Loss 9096059000000.0 Test MSE 14016707.908696473 Test RE 1.6734388040793653\n",
      "63 Train Loss 9055272000000.0 Test MSE 14275355.318732537 Test RE 1.688808036812212\n",
      "64 Train Loss 9023599000000.0 Test MSE 14264635.802987888 Test RE 1.6881738457996365\n",
      "65 Train Loss 9003789000000.0 Test MSE 14269996.918880466 Test RE 1.688491051424475\n",
      "66 Train Loss 9003947000000.0 Test MSE 14281177.501647802 Test RE 1.6891523906561703\n",
      "67 Train Loss 8999722000000.0 Test MSE 14269465.941720173 Test RE 1.6884596373138445\n",
      "68 Train Loss 9002472000000.0 Test MSE 14403406.04869253 Test RE 1.696365477918733\n",
      "69 Train Loss 9001950000000.0 Test MSE 14385331.821819082 Test RE 1.6953007950553651\n",
      "70 Train Loss 9002070000000.0 Test MSE 14257536.369460711 Test RE 1.6877536959351478\n",
      "71 Train Loss 8972884000000.0 Test MSE 14270190.764906103 Test RE 1.688502519757999\n",
      "72 Train Loss 8958536000000.0 Test MSE 14284252.54099832 Test RE 1.6893342359729857\n",
      "73 Train Loss 8958116000000.0 Test MSE 14168156.347045269 Test RE 1.6824551428072925\n",
      "74 Train Loss 8960903000000.0 Test MSE 14243942.982355133 Test RE 1.6869489369733799\n",
      "75 Train Loss 8964920000000.0 Test MSE 14351764.17802215 Test RE 1.6933216787423577\n",
      "76 Train Loss 8960348000000.0 Test MSE 14310748.940332418 Test RE 1.6909003149918802\n",
      "77 Train Loss 8946213000000.0 Test MSE 14373447.977241114 Test RE 1.6946003991948952\n",
      "78 Train Loss 8943118000000.0 Test MSE 14309601.467844421 Test RE 1.6908325232748\n",
      "79 Train Loss 8934662000000.0 Test MSE 14322940.610615479 Test RE 1.6916204209239771\n",
      "80 Train Loss 8926970000000.0 Test MSE 14446119.191546557 Test RE 1.6988788925431522\n",
      "81 Train Loss 8925283000000.0 Test MSE 14554875.700787986 Test RE 1.7052618419257655\n",
      "82 Train Loss 8921092000000.0 Test MSE 14552102.072982157 Test RE 1.705099353866449\n",
      "83 Train Loss 8923524000000.0 Test MSE 14566044.267208045 Test RE 1.7059159759207088\n",
      "84 Train Loss 8922337000000.0 Test MSE 14727135.136545861 Test RE 1.715323192377235\n",
      "85 Train Loss 8901960000000.0 Test MSE 14991767.59768619 Test RE 1.7306659300963096\n",
      "86 Train Loss 8897942000000.0 Test MSE 14884592.140293533 Test RE 1.7244686085496472\n",
      "87 Train Loss 8900158000000.0 Test MSE 14979302.446637826 Test RE 1.7299462851889869\n",
      "88 Train Loss 8888560000000.0 Test MSE 15269725.185319724 Test RE 1.746636108437214\n",
      "89 Train Loss 8891340000000.0 Test MSE 15262990.177282227 Test RE 1.7462508721112788\n",
      "90 Train Loss 8897355000000.0 Test MSE 15244037.154440228 Test RE 1.7451663200921577\n",
      "91 Train Loss 8902572000000.0 Test MSE 15298762.515964948 Test RE 1.7482960453699363\n",
      "92 Train Loss 8904805000000.0 Test MSE 15298589.71174839 Test RE 1.7482861715719333\n",
      "93 Train Loss 8906788000000.0 Test MSE 15354336.255694564 Test RE 1.751468565852955\n",
      "94 Train Loss 8904354000000.0 Test MSE 15505408.208216283 Test RE 1.7600638612405481\n",
      "95 Train Loss 8907713000000.0 Test MSE 15561247.747178845 Test RE 1.7632302670316236\n",
      "96 Train Loss 8911785000000.0 Test MSE 15597445.032293048 Test RE 1.7652798159247292\n",
      "97 Train Loss 8913260000000.0 Test MSE 15626066.127094513 Test RE 1.7668987055659278\n",
      "98 Train Loss 8916511000000.0 Test MSE 15611791.509936152 Test RE 1.7660914785543744\n",
      "99 Train Loss 8913234000000.0 Test MSE 15567003.044632116 Test RE 1.7635563005447148\n",
      "100 Train Loss 8911726000000.0 Test MSE 15452921.775002653 Test RE 1.7570823923645544\n",
      "101 Train Loss 8913374000000.0 Test MSE 15386580.072165452 Test RE 1.7533066268587174\n",
      "102 Train Loss 8916059000000.0 Test MSE 15391350.630460095 Test RE 1.7535784092562983\n",
      "103 Train Loss 8919116000000.0 Test MSE 15355828.186408637 Test RE 1.7515536560265657\n",
      "104 Train Loss 8922285000000.0 Test MSE 15308460.89802578 Test RE 1.7488501083751646\n",
      "105 Train Loss 8922702000000.0 Test MSE 15303462.449782409 Test RE 1.7485645718183398\n",
      "106 Train Loss 8924916000000.0 Test MSE 15245887.658687476 Test RE 1.7452722414978106\n",
      "107 Train Loss 8929404000000.0 Test MSE 15190181.581531262 Test RE 1.742080848393537\n",
      "108 Train Loss 8933831000000.0 Test MSE 15218675.805992078 Test RE 1.7437140081272804\n",
      "109 Train Loss 8936332000000.0 Test MSE 15259824.511252353 Test RE 1.7460697695333673\n",
      "110 Train Loss 8934542000000.0 Test MSE 15216295.522311188 Test RE 1.743577639616813\n",
      "111 Train Loss 8933279000000.0 Test MSE 15194803.678363146 Test RE 1.7423458700424148\n",
      "112 Train Loss 8936617000000.0 Test MSE 15201864.763482703 Test RE 1.7427506605152783\n",
      "113 Train Loss 8938490000000.0 Test MSE 15243352.398599619 Test RE 1.7451271235775276\n",
      "114 Train Loss 8938611000000.0 Test MSE 15285819.424975092 Test RE 1.7475563403507748\n",
      "115 Train Loss 8936601000000.0 Test MSE 15300386.121821813 Test RE 1.7483888132780778\n",
      "116 Train Loss 8935604000000.0 Test MSE 15371675.991414547 Test RE 1.7524572583022389\n",
      "117 Train Loss 8939483000000.0 Test MSE 15424234.704515453 Test RE 1.7554506957072336\n",
      "118 Train Loss 8945054000000.0 Test MSE 15428989.769729285 Test RE 1.7557212647001408\n",
      "119 Train Loss 8950637000000.0 Test MSE 15429004.041405646 Test RE 1.7557220767131192\n",
      "120 Train Loss 8956044000000.0 Test MSE 15428994.953927722 Test RE 1.7557215596645812\n",
      "121 Train Loss 8960328000000.0 Test MSE 15397924.424694404 Test RE 1.753952854400251\n",
      "122 Train Loss 8964356000000.0 Test MSE 15391849.708881235 Test RE 1.753606839706309\n",
      "123 Train Loss 8968774000000.0 Test MSE 15386152.688294442 Test RE 1.7532822764126383\n",
      "124 Train Loss 8972916000000.0 Test MSE 15362556.745983362 Test RE 1.751937358625941\n",
      "125 Train Loss 8975699000000.0 Test MSE 15325229.943951154 Test RE 1.7498077004291346\n",
      "126 Train Loss 8977264000000.0 Test MSE 15281770.512563232 Test RE 1.747324878399807\n",
      "127 Train Loss 8977736000000.0 Test MSE 15226161.124307392 Test RE 1.7441427790013726\n",
      "128 Train Loss 8978327000000.0 Test MSE 15214154.774487786 Test RE 1.7434549852124548\n",
      "129 Train Loss 8976794000000.0 Test MSE 15252216.147787787 Test RE 1.7456344306555847\n",
      "130 Train Loss 8972196000000.0 Test MSE 15226898.898028826 Test RE 1.7441850341407705\n",
      "131 Train Loss 8961897000000.0 Test MSE 15170932.329901919 Test RE 1.7409767015208144\n",
      "132 Train Loss 8959446000000.0 Test MSE 15238925.640514813 Test RE 1.744873707650784\n",
      "133 Train Loss 8955121000000.0 Test MSE 15303844.307825632 Test RE 1.748586387119383\n",
      "134 Train Loss 8951004000000.0 Test MSE 15089565.499519043 Test RE 1.7363017018719313\n",
      "135 Train Loss 8921639000000.0 Test MSE 14658121.320451383 Test RE 1.711299327163752\n",
      "136 Train Loss 8882217000000.0 Test MSE 14664617.190315772 Test RE 1.7116784735147408\n",
      "137 Train Loss 8837069000000.0 Test MSE 14792676.463501377 Test RE 1.719135873925427\n",
      "138 Train Loss 8798675000000.0 Test MSE 14954957.931204809 Test RE 1.728539950367359\n",
      "139 Train Loss 8772334000000.0 Test MSE 14895371.301351078 Test RE 1.7250929105317978\n",
      "140 Train Loss 8742904000000.0 Test MSE 15073920.355364425 Test RE 1.7354013533731587\n",
      "141 Train Loss 8727926700000.0 Test MSE 15182918.855920492 Test RE 1.7416643369841571\n",
      "142 Train Loss 8714368600000.0 Test MSE 15155088.270748636 Test RE 1.7400673525429546\n",
      "143 Train Loss 8706616000000.0 Test MSE 15299674.85435174 Test RE 1.7483481742206446\n",
      "144 Train Loss 8700091000000.0 Test MSE 15340227.511234991 Test RE 1.7506636889367766\n",
      "145 Train Loss 8688126400000.0 Test MSE 15270332.979730926 Test RE 1.7466708695441933\n",
      "146 Train Loss 8680106000000.0 Test MSE 15390437.630681006 Test RE 1.7535263982118463\n",
      "147 Train Loss 8675263500000.0 Test MSE 15317466.34065697 Test RE 1.7493644270233084\n",
      "148 Train Loss 8676138500000.0 Test MSE 15243707.051753769 Test RE 1.7451474245986778\n",
      "149 Train Loss 8665706000000.0 Test MSE 15215772.308878992 Test RE 1.7435476628369198\n",
      "150 Train Loss 8653219000000.0 Test MSE 15337664.2682132 Test RE 1.750517421094637\n",
      "151 Train Loss 8646344300000.0 Test MSE 15452288.093894897 Test RE 1.7570463654776027\n",
      "152 Train Loss 8613086000000.0 Test MSE 15526342.235738602 Test RE 1.761251601573359\n",
      "153 Train Loss 8602287300000.0 Test MSE 15590227.431281146 Test RE 1.7648713336489164\n",
      "154 Train Loss 8603304000000.0 Test MSE 15594028.922640238 Test RE 1.7650864919658174\n",
      "155 Train Loss 8603972400000.0 Test MSE 15591401.436574304 Test RE 1.7649377832658706\n",
      "156 Train Loss 8601576400000.0 Test MSE 15595980.932386117 Test RE 1.765196962397655\n",
      "157 Train Loss 8599845000000.0 Test MSE 15590445.263101852 Test RE 1.7648836632892215\n",
      "158 Train Loss 8594252600000.0 Test MSE 15581123.61125534 Test RE 1.7643559653737044\n",
      "159 Train Loss 8565502000000.0 Test MSE 15485266.698772889 Test RE 1.7589203291780837\n",
      "160 Train Loss 8546705500000.0 Test MSE 15469086.952438392 Test RE 1.7580011870351326\n",
      "161 Train Loss 8540013000000.0 Test MSE 15487787.397347027 Test RE 1.7590634822679363\n",
      "162 Train Loss 8537914700000.0 Test MSE 15544184.873078778 Test RE 1.762263312828584\n",
      "163 Train Loss 8536975700000.0 Test MSE 15646190.96633374 Test RE 1.7680361355018377\n",
      "164 Train Loss 8533618000000.0 Test MSE 15706600.597835131 Test RE 1.7714460234369778\n",
      "165 Train Loss 8519805300000.0 Test MSE 15829144.127979308 Test RE 1.778343043460595\n",
      "166 Train Loss 8510330400000.0 Test MSE 15914789.555709168 Test RE 1.783147519502404\n",
      "167 Train Loss 8502227000000.0 Test MSE 15961806.452701868 Test RE 1.7857795440658375\n",
      "168 Train Loss 8502554700000.0 Test MSE 15997368.131285086 Test RE 1.7877677271220431\n",
      "169 Train Loss 8500590000000.0 Test MSE 16044396.829944791 Test RE 1.7903936180879438\n",
      "170 Train Loss 8489445400000.0 Test MSE 16125000.54709238 Test RE 1.7948852667083335\n",
      "171 Train Loss 8469350000000.0 Test MSE 16324805.144107636 Test RE 1.8059712267729073\n",
      "172 Train Loss 8450501000000.0 Test MSE 16421295.465647796 Test RE 1.811300601460757\n",
      "173 Train Loss 8422787000000.0 Test MSE 16444129.138835382 Test RE 1.8125594631272872\n",
      "174 Train Loss 8414146600000.0 Test MSE 16445126.446323272 Test RE 1.8126144265644586\n",
      "175 Train Loss 8408817700000.0 Test MSE 16483447.51325736 Test RE 1.8147251096507544\n",
      "176 Train Loss 8401595000000.0 Test MSE 16511288.08665102 Test RE 1.8162570001186333\n",
      "177 Train Loss 8397356000000.0 Test MSE 16446540.954993557 Test RE 1.8126923798626289\n",
      "178 Train Loss 8396898600000.0 Test MSE 16467632.81331026 Test RE 1.813854350662962\n",
      "179 Train Loss 8395711600000.0 Test MSE 16450526.60722475 Test RE 1.8129120103548142\n",
      "180 Train Loss 8387313000000.0 Test MSE 16350437.92870979 Test RE 1.8073885151768316\n",
      "181 Train Loss 8377158000000.0 Test MSE 16347446.680926234 Test RE 1.8072231803337013\n",
      "182 Train Loss 8366127000000.0 Test MSE 16361849.367108874 Test RE 1.8080191192429709\n",
      "183 Train Loss 8358862000000.0 Test MSE 16316334.924966315 Test RE 1.8055026466990847\n",
      "184 Train Loss 8352736000000.0 Test MSE 16322036.323839648 Test RE 1.8058180665441168\n",
      "185 Train Loss 8339380000000.0 Test MSE 16449298.570944736 Test RE 1.8128443419048983\n",
      "186 Train Loss 8323752500000.0 Test MSE 16580125.279059913 Test RE 1.8200391396884856\n",
      "187 Train Loss 8296176000000.0 Test MSE 16755567.733774452 Test RE 1.8296431646604536\n",
      "188 Train Loss 8282017000000.0 Test MSE 16897049.49274265 Test RE 1.8373515570331\n",
      "189 Train Loss 8278740300000.0 Test MSE 16935523.75012561 Test RE 1.8394421747508296\n",
      "190 Train Loss 8276557700000.0 Test MSE 16975023.999849718 Test RE 1.8415860738328274\n",
      "191 Train Loss 8271954400000.0 Test MSE 17090539.69652332 Test RE 1.8478414821303968\n",
      "192 Train Loss 8266941700000.0 Test MSE 17109747.65793676 Test RE 1.8488795798571551\n",
      "193 Train Loss 8252002000000.0 Test MSE 17059316.407326292 Test RE 1.8461527682083816\n",
      "194 Train Loss 8243288500000.0 Test MSE 17227560.94991074 Test RE 1.8552341127252485\n",
      "195 Train Loss 8228461600000.0 Test MSE 17275876.098166388 Test RE 1.857833817723267\n",
      "196 Train Loss 8216752700000.0 Test MSE 17157232.741534773 Test RE 1.8514434216280042\n",
      "197 Train Loss 8210506300000.0 Test MSE 17222651.862695936 Test RE 1.8549697643653575\n",
      "198 Train Loss 8191328000000.0 Test MSE 17216189.017756347 Test RE 1.8546216905922464\n",
      "199 Train Loss 8184869700000.0 Test MSE 17157982.357021917 Test RE 1.8514838668304539\n",
      "Training time: 47.81\n",
      "Training time: 47.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "1\n",
      "0 Train Loss 13012356000000.0 Test MSE 5096323.655177376 Test RE 1.009056128079391\n",
      "1 Train Loss 13284412000000.0 Test MSE 5217514.0633590445 Test RE 1.0209832984499185\n",
      "2 Train Loss 12902406000000.0 Test MSE 5366360.206478028 Test RE 1.0354442813734819\n",
      "3 Train Loss 12698098000000.0 Test MSE 5363650.901611208 Test RE 1.0351828669050764\n",
      "4 Train Loss 12664071000000.0 Test MSE 5433618.521525037 Test RE 1.041912854086803\n",
      "5 Train Loss 12682679000000.0 Test MSE 5504850.400109353 Test RE 1.0487200814662914\n",
      "6 Train Loss 12620574000000.0 Test MSE 5584310.223149768 Test RE 1.0562618442772607\n",
      "7 Train Loss 12514967000000.0 Test MSE 5732243.512042848 Test RE 1.0701610480144534\n",
      "8 Train Loss 12508503000000.0 Test MSE 5845572.496075527 Test RE 1.0806880499088938\n",
      "9 Train Loss 12608920000000.0 Test MSE 5892995.134382488 Test RE 1.0850627760415021\n",
      "10 Train Loss 12692807000000.0 Test MSE 5940392.65198455 Test RE 1.0894176314992394\n",
      "11 Train Loss 12724491000000.0 Test MSE 6114308.890450707 Test RE 1.1052499686214787\n",
      "12 Train Loss 12845021000000.0 Test MSE 6204332.715038364 Test RE 1.113356793183712\n",
      "13 Train Loss 12935197000000.0 Test MSE 6218502.580126499 Test RE 1.1146274470087227\n",
      "14 Train Loss 13035350000000.0 Test MSE 6272593.677467361 Test RE 1.1194646950596965\n",
      "15 Train Loss 13145943000000.0 Test MSE 6294996.104732303 Test RE 1.121461984822556\n",
      "16 Train Loss 13241272000000.0 Test MSE 6297437.003442201 Test RE 1.1216793884366356\n",
      "17 Train Loss 13343683000000.0 Test MSE 6313384.654476221 Test RE 1.1230987627600424\n",
      "18 Train Loss 13422035000000.0 Test MSE 6329334.94285788 Test RE 1.12451657982364\n",
      "19 Train Loss 13500899000000.0 Test MSE 6340228.99043613 Test RE 1.1254839223928874\n",
      "20 Train Loss 13594692000000.0 Test MSE 6374561.086734073 Test RE 1.1285270344619809\n",
      "21 Train Loss 13698012000000.0 Test MSE 6382417.225300158 Test RE 1.1292222301548074\n",
      "22 Train Loss 13796603000000.0 Test MSE 6395942.052874141 Test RE 1.1304180508808475\n",
      "23 Train Loss 13901077000000.0 Test MSE 6406466.231203374 Test RE 1.131347689644328\n",
      "24 Train Loss 14005341000000.0 Test MSE 6407204.840823674 Test RE 1.1314129050199861\n",
      "25 Train Loss 14105614000000.0 Test MSE 6412108.09086441 Test RE 1.1318457411444343\n",
      "26 Train Loss 14200213000000.0 Test MSE 6436749.877592648 Test RE 1.1340185021367302\n",
      "27 Train Loss 14286286000000.0 Test MSE 6491871.885871632 Test RE 1.1388638139380196\n",
      "28 Train Loss 14368360000000.0 Test MSE 6490116.749739704 Test RE 1.1387098524778179\n",
      "29 Train Loss 14448189000000.0 Test MSE 6503786.412657852 Test RE 1.1399084127500283\n",
      "30 Train Loss 14528320000000.0 Test MSE 6523138.718978458 Test RE 1.141603077190837\n",
      "31 Train Loss 14609489000000.0 Test MSE 6555559.834189813 Test RE 1.1444365421352605\n",
      "32 Train Loss 14685825000000.0 Test MSE 6569601.028223807 Test RE 1.1456615069414537\n",
      "33 Train Loss 14761833000000.0 Test MSE 6582141.950696481 Test RE 1.1467544806163616\n",
      "34 Train Loss 14831219000000.0 Test MSE 6596165.753006268 Test RE 1.1479754586378443\n",
      "35 Train Loss 14902700000000.0 Test MSE 6584511.201886171 Test RE 1.1469608499858917\n",
      "36 Train Loss 14973184000000.0 Test MSE 6581320.9771127775 Test RE 1.1466829625223445\n",
      "37 Train Loss 15045376000000.0 Test MSE 6573358.941340497 Test RE 1.1459891281233008\n",
      "38 Train Loss 15116090000000.0 Test MSE 6568333.6426713215 Test RE 1.145550993009865\n",
      "39 Train Loss 15184213000000.0 Test MSE 6566222.588663588 Test RE 1.1453668889060773\n",
      "40 Train Loss 15249054000000.0 Test MSE 6574250.109412337 Test RE 1.146066807915251\n",
      "41 Train Loss 15309593000000.0 Test MSE 6568231.506323108 Test RE 1.1455420864243955\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 48.42\n",
      "Training time: 48.42\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "2\n",
      "0 Train Loss 11833885000000.0 Test MSE 5425681.455400822 Test RE 1.0411515977195316\n",
      "1 Train Loss 11607553000000.0 Test MSE 5568765.262830345 Test RE 1.0547906696429719\n",
      "2 Train Loss 11336696000000.0 Test MSE 5961403.6328309085 Test RE 1.091342548754621\n",
      "3 Train Loss 11174651000000.0 Test MSE 5996543.366097395 Test RE 1.0945543040077343\n",
      "4 Train Loss 11052352000000.0 Test MSE 6286024.721363811 Test RE 1.120662567916819\n",
      "5 Train Loss 11124369000000.0 Test MSE 6318522.204155174 Test RE 1.1235556335705654\n",
      "6 Train Loss 11229434000000.0 Test MSE 6332030.450749374 Test RE 1.1247560062948856\n",
      "7 Train Loss 11332084000000.0 Test MSE 6326725.1937358845 Test RE 1.1242847222304146\n",
      "8 Train Loss 11452637000000.0 Test MSE 6358507.233296054 Test RE 1.1271050833301082\n",
      "9 Train Loss 11569985000000.0 Test MSE 6363248.98398821 Test RE 1.1275252649152232\n",
      "10 Train Loss 11705440000000.0 Test MSE 6370258.912661103 Test RE 1.128146150261831\n",
      "11 Train Loss 11846430000000.0 Test MSE 6369940.720456174 Test RE 1.1281179746571468\n",
      "12 Train Loss 11986348000000.0 Test MSE 6376231.052890302 Test RE 1.128674846877261\n",
      "13 Train Loss 12120883000000.0 Test MSE 6373963.719611149 Test RE 1.1284741554689808\n",
      "14 Train Loss 12255430000000.0 Test MSE 6377689.69052174 Test RE 1.1288039382988422\n",
      "15 Train Loss 12384917000000.0 Test MSE 6383246.675491893 Test RE 1.1292956038573814\n",
      "16 Train Loss 12506675000000.0 Test MSE 6386129.210884424 Test RE 1.1295505577465923\n",
      "17 Train Loss 12621954000000.0 Test MSE 6384724.795693484 Test RE 1.1294263475140702\n",
      "18 Train Loss 12736753000000.0 Test MSE 6392336.052428126 Test RE 1.130099343904477\n",
      "19 Train Loss 12850427000000.0 Test MSE 6389595.915137265 Test RE 1.129857103880623\n",
      "20 Train Loss 12957640000000.0 Test MSE 6375343.989248874 Test RE 1.1285963333136975\n",
      "21 Train Loss 13061240000000.0 Test MSE 6367053.35072728 Test RE 1.1278622687538904\n",
      "22 Train Loss 13157008000000.0 Test MSE 6358208.226664875 Test RE 1.1270785821527367\n",
      "23 Train Loss 13256943000000.0 Test MSE 6361528.786727807 Test RE 1.1273728508751164\n",
      "24 Train Loss 13355904000000.0 Test MSE 6360180.917303197 Test RE 1.1272534116691277\n",
      "25 Train Loss 13453632000000.0 Test MSE 6354250.310879883 Test RE 1.1267277304362597\n",
      "26 Train Loss 13547119000000.0 Test MSE 6355340.472858114 Test RE 1.1268243793713986\n",
      "27 Train Loss 13626974000000.0 Test MSE 6369982.567554694 Test RE 1.1281216802164735\n",
      "28 Train Loss 13700994000000.0 Test MSE 6395312.119218278 Test RE 1.1303623823097642\n",
      "29 Train Loss 13769055000000.0 Test MSE 6408045.727250317 Test RE 1.1314871463284164\n",
      "30 Train Loss 13843078000000.0 Test MSE 6401852.619723553 Test RE 1.1309402467139216\n",
      "31 Train Loss 13914871000000.0 Test MSE 6394495.687269067 Test RE 1.130290228410063\n",
      "32 Train Loss 13976029000000.0 Test MSE 6388136.063326807 Test RE 1.1297280254393847\n",
      "33 Train Loss 14034154000000.0 Test MSE 6410113.866919238 Test RE 1.13166972029658\n",
      "34 Train Loss 14092537000000.0 Test MSE 6413153.504652532 Test RE 1.1319380040088918\n",
      "35 Train Loss 14156626000000.0 Test MSE 6393659.791484003 Test RE 1.1302163495803204\n",
      "36 Train Loss 14219795000000.0 Test MSE 6386270.996238969 Test RE 1.1295630968631751\n",
      "37 Train Loss 14280678000000.0 Test MSE 6393120.441113849 Test RE 1.1301686777082962\n",
      "38 Train Loss 14337383000000.0 Test MSE 6400136.295741104 Test RE 1.1307886351336902\n",
      "39 Train Loss 14398276000000.0 Test MSE 6393547.191377658 Test RE 1.1302063972958996\n",
      "40 Train Loss 14458887000000.0 Test MSE 6381276.984457465 Test RE 1.1291213559268405\n",
      "41 Train Loss 14513177000000.0 Test MSE 6395785.474439107 Test RE 1.1304042139690085\n",
      "42 Train Loss 14513425000000.0 Test MSE 6363194.344110898 Test RE 1.1275204239939778\n",
      "43 Train Loss 14442879000000.0 Test MSE 6477136.167548085 Test RE 1.1375705420801856\n",
      "44 Train Loss 14444594000000.0 Test MSE 6533510.961527619 Test RE 1.1425103306821132\n",
      "45 Train Loss 14478770000000.0 Test MSE 6526656.405942683 Test RE 1.141910847820018\n",
      "46 Train Loss 14520844000000.0 Test MSE 6538621.387810081 Test RE 1.142957071612152\n",
      "47 Train Loss 14561011000000.0 Test MSE 6537104.237257327 Test RE 1.1428244642589642\n",
      "48 Train Loss 14596004000000.0 Test MSE 6540083.169722079 Test RE 1.1430848248881016\n",
      "49 Train Loss 14623289000000.0 Test MSE 6549786.772468752 Test RE 1.1439325151588773\n",
      "50 Train Loss 14646917000000.0 Test MSE 6571547.833297128 Test RE 1.1458312443740493\n",
      "51 Train Loss 14653827000000.0 Test MSE 6571796.512017277 Test RE 1.1458529242856121\n",
      "52 Train Loss 14664499000000.0 Test MSE 6545131.227869198 Test RE 1.1435258931023087\n",
      "53 Train Loss 14700360000000.0 Test MSE 6554939.226091295 Test RE 1.1443823695386686\n",
      "54 Train Loss 14735137000000.0 Test MSE 6550774.081121506 Test RE 1.1440187295618118\n",
      "55 Train Loss 14772073000000.0 Test MSE 6548043.897983725 Test RE 1.1437803068877297\n",
      "56 Train Loss 14807359000000.0 Test MSE 6550265.543254475 Test RE 1.1439743235016524\n",
      "57 Train Loss 14833567000000.0 Test MSE 6541030.594802194 Test RE 1.1431676180258166\n",
      "58 Train Loss 14859038000000.0 Test MSE 6540780.718416006 Test RE 1.1431457825277878\n",
      "59 Train Loss 14880750000000.0 Test MSE 6548181.1995056905 Test RE 1.1437922984042923\n",
      "60 Train Loss 14901136000000.0 Test MSE 6563332.7144216 Test RE 1.1451148162345814\n",
      "61 Train Loss 14928987000000.0 Test MSE 6553534.024485894 Test RE 1.1442597006565274\n",
      "62 Train Loss 14956459000000.0 Test MSE 6547568.174463565 Test RE 1.1437387576032776\n",
      "63 Train Loss 14976473000000.0 Test MSE 6560720.34996696 Test RE 1.1448869019188033\n",
      "64 Train Loss 14995228000000.0 Test MSE 6605706.837045339 Test RE 1.1488054084038233\n",
      "65 Train Loss 14998968000000.0 Test MSE 6617846.502240373 Test RE 1.1498605348176338\n",
      "66 Train Loss 15000695000000.0 Test MSE 6568704.513775018 Test RE 1.1455833334623489\n",
      "67 Train Loss 14992762000000.0 Test MSE 6525712.91377821 Test RE 1.141828307633294\n",
      "68 Train Loss 14962382000000.0 Test MSE 6520942.630105747 Test RE 1.1414108941815428\n",
      "69 Train Loss 14980438000000.0 Test MSE 6530596.996408111 Test RE 1.1422555207822849\n",
      "70 Train Loss 15000646000000.0 Test MSE 6525131.616007238 Test RE 1.141777450582064\n",
      "71 Train Loss 15008434000000.0 Test MSE 6512776.326801675 Test RE 1.1406959647483947\n",
      "72 Train Loss 15021830000000.0 Test MSE 6501514.151010233 Test RE 1.1397092674922842\n",
      "73 Train Loss 15033950000000.0 Test MSE 6484595.752670806 Test RE 1.1382254119367416\n",
      "74 Train Loss 15043608000000.0 Test MSE 6497520.156877839 Test RE 1.1393591420193396\n",
      "75 Train Loss 15060953000000.0 Test MSE 6496628.62614807 Test RE 1.1392809730788027\n",
      "76 Train Loss 15075729000000.0 Test MSE 6510707.875564777 Test RE 1.1405148084156573\n",
      "77 Train Loss 15091086000000.0 Test MSE 6525694.354936176 Test RE 1.1418266839772626\n",
      "78 Train Loss 15106061000000.0 Test MSE 6518690.884260959 Test RE 1.1412138069429087\n",
      "79 Train Loss 15121472000000.0 Test MSE 6525219.644856838 Test RE 1.1417851522670568\n",
      "80 Train Loss 15133144000000.0 Test MSE 6530669.195403476 Test RE 1.1422618348660631\n",
      "81 Train Loss 15118972000000.0 Test MSE 6495823.399872285 Test RE 1.1392103666569364\n",
      "82 Train Loss 15112785000000.0 Test MSE 6491434.912728228 Test RE 1.1388254843711345\n",
      "83 Train Loss 15086895000000.0 Test MSE 6534930.190455224 Test RE 1.1426344137114837\n",
      "84 Train Loss 15087861000000.0 Test MSE 6573411.274077417 Test RE 1.1459936899180183\n",
      "85 Train Loss 15099914000000.0 Test MSE 6601699.004592824 Test RE 1.1484568523526597\n",
      "86 Train Loss 15113071000000.0 Test MSE 6608093.885738704 Test RE 1.1490129567271974\n",
      "87 Train Loss 15120646000000.0 Test MSE 6621515.561036037 Test RE 1.1501792428225297\n",
      "88 Train Loss 15121232000000.0 Test MSE 6625144.5274046315 Test RE 1.1504943814049473\n",
      "89 Train Loss 15114746000000.0 Test MSE 6643978.561966876 Test RE 1.1521285399410917\n",
      "90 Train Loss 15120897000000.0 Test MSE 6673966.02257191 Test RE 1.1547256669561823\n",
      "91 Train Loss 15132906000000.0 Test MSE 6672888.645769614 Test RE 1.154632459644809\n",
      "92 Train Loss 15143656000000.0 Test MSE 6677713.008741264 Test RE 1.1550497721408077\n",
      "93 Train Loss 15144578000000.0 Test MSE 6676600.443897646 Test RE 1.1549535474829238\n",
      "94 Train Loss 15148970000000.0 Test MSE 6678337.5458892 Test RE 1.1551037842422256\n",
      "95 Train Loss 15158567000000.0 Test MSE 6689751.025024198 Test RE 1.156090416379385\n",
      "96 Train Loss 15168084000000.0 Test MSE 6698589.141123499 Test RE 1.1568538445520598\n",
      "97 Train Loss 15170253000000.0 Test MSE 6739207.072237334 Test RE 1.1603559248142274\n",
      "98 Train Loss 15150730000000.0 Test MSE 6826089.647508984 Test RE 1.1678116877241258\n",
      "99 Train Loss 15141516000000.0 Test MSE 6861339.484735673 Test RE 1.1708230871271117\n",
      "100 Train Loss 15145948000000.0 Test MSE 6857504.931318247 Test RE 1.170495876156019\n",
      "101 Train Loss 15153755000000.0 Test MSE 6836307.547152086 Test RE 1.1686854032145277\n",
      "102 Train Loss 15158022000000.0 Test MSE 6833989.551071304 Test RE 1.168487252543179\n",
      "103 Train Loss 15160722000000.0 Test MSE 6837463.822598416 Test RE 1.1687842332548632\n",
      "104 Train Loss 15165225000000.0 Test MSE 6828477.606080191 Test RE 1.1680159365858944\n",
      "105 Train Loss 15173128000000.0 Test MSE 6841062.038547091 Test RE 1.169091729208926\n",
      "106 Train Loss 15180841000000.0 Test MSE 6859202.105585514 Test RE 1.1706407110925878\n",
      "107 Train Loss 15188966000000.0 Test MSE 6852868.640140261 Test RE 1.1701001289189645\n",
      "108 Train Loss 15197274000000.0 Test MSE 6847291.009068709 Test RE 1.169623852800503\n",
      "109 Train Loss 15205425000000.0 Test MSE 6851774.977812013 Test RE 1.1700067559419112\n",
      "110 Train Loss 15213465000000.0 Test MSE 6857312.885706801 Test RE 1.170479486071556\n",
      "111 Train Loss 15219140000000.0 Test MSE 6864879.683960407 Test RE 1.1711250990354534\n",
      "112 Train Loss 15223407000000.0 Test MSE 6892479.7370114885 Test RE 1.1734769750300034\n",
      "113 Train Loss 15223515000000.0 Test MSE 6916885.642528996 Test RE 1.175552748995518\n",
      "114 Train Loss 15216098000000.0 Test MSE 6974589.45313067 Test RE 1.1804460628710327\n",
      "115 Train Loss 15211628000000.0 Test MSE 7019224.577303829 Test RE 1.1842172759935665\n",
      "116 Train Loss 15213565000000.0 Test MSE 7037229.063319536 Test RE 1.1857350767342378\n",
      "117 Train Loss 15213780000000.0 Test MSE 6998139.020643574 Test RE 1.182437260222445\n",
      "118 Train Loss 15214913000000.0 Test MSE 7000280.916895654 Test RE 1.1826181986239046\n",
      "119 Train Loss 15215409000000.0 Test MSE 7020968.873618747 Test RE 1.1843644074534148\n",
      "120 Train Loss 15204807000000.0 Test MSE 7049194.203234582 Test RE 1.1867426793342668\n",
      "121 Train Loss 15189000000000.0 Test MSE 7176135.417152542 Test RE 1.1973803766001836\n",
      "122 Train Loss 15182300000000.0 Test MSE 7212682.880750023 Test RE 1.2004255838431492\n",
      "123 Train Loss 15165371000000.0 Test MSE 7254874.827088937 Test RE 1.2039315218780018\n",
      "124 Train Loss 15151137000000.0 Test MSE 7272136.347001729 Test RE 1.2053629277707767\n",
      "125 Train Loss 15136054000000.0 Test MSE 7324228.079346064 Test RE 1.2096723492181618\n",
      "126 Train Loss 15118431000000.0 Test MSE 7422691.258534818 Test RE 1.217776312186556\n",
      "127 Train Loss 15109154000000.0 Test MSE 7515931.051906134 Test RE 1.2254009631827447\n",
      "128 Train Loss 15107490000000.0 Test MSE 7538136.55418602 Test RE 1.2272098259272952\n",
      "129 Train Loss 15107129000000.0 Test MSE 7513578.555393579 Test RE 1.2252091721003064\n",
      "130 Train Loss 15104321000000.0 Test MSE 7517474.854774556 Test RE 1.225526807896735\n",
      "131 Train Loss 15102689000000.0 Test MSE 7501588.255081547 Test RE 1.2242311766364353\n",
      "132 Train Loss 15100971000000.0 Test MSE 7499547.572276156 Test RE 1.224064649405587\n",
      "133 Train Loss 15100541000000.0 Test MSE 7518328.552237155 Test RE 1.2255963923945383\n",
      "134 Train Loss 15104275000000.0 Test MSE 7518831.687906005 Test RE 1.2256374009072746\n",
      "135 Train Loss 15107766000000.0 Test MSE 7518970.463597978 Test RE 1.2256487117000048\n",
      "136 Train Loss 15110960000000.0 Test MSE 7532849.658519902 Test RE 1.226779396717092\n",
      "137 Train Loss 15115809000000.0 Test MSE 7523670.819499474 Test RE 1.2260317485212717\n",
      "138 Train Loss 15119065000000.0 Test MSE 7512918.613130461 Test RE 1.2251553638474593\n",
      "139 Train Loss 15116042000000.0 Test MSE 7514202.7672420535 Test RE 1.2252600649067653\n",
      "140 Train Loss 15112628000000.0 Test MSE 7458171.428563049 Test RE 1.2206833037554605\n",
      "141 Train Loss 15114454000000.0 Test MSE 7458236.096523793 Test RE 1.2206885958655715\n",
      "142 Train Loss 15117475000000.0 Test MSE 7475921.462680073 Test RE 1.2221350197791117\n",
      "143 Train Loss 15120688000000.0 Test MSE 7460876.266256323 Test RE 1.2209046348778356\n",
      "144 Train Loss 15123862000000.0 Test MSE 7445551.149012729 Test RE 1.2196500821932905\n",
      "145 Train Loss 15125686000000.0 Test MSE 7450912.918640354 Test RE 1.2200891568661654\n",
      "146 Train Loss 15123536000000.0 Test MSE 7450825.543399048 Test RE 1.2200820029846076\n",
      "147 Train Loss 15123314000000.0 Test MSE 7470704.198353282 Test RE 1.2217084961518576\n",
      "148 Train Loss 15124562000000.0 Test MSE 7493279.985520315 Test RE 1.2235530495461076\n",
      "149 Train Loss 15127441000000.0 Test MSE 7480065.652791435 Test RE 1.2224737110150876\n",
      "150 Train Loss 15131014000000.0 Test MSE 7477014.339754434 Test RE 1.222224346194721\n",
      "151 Train Loss 15134612000000.0 Test MSE 7491291.570351039 Test RE 1.2233906978876352\n",
      "152 Train Loss 15137761000000.0 Test MSE 7471564.116081061 Test RE 1.2217788066966437\n",
      "153 Train Loss 15139856000000.0 Test MSE 7444953.4385981625 Test RE 1.2196011259660897\n",
      "154 Train Loss 15143328000000.0 Test MSE 7450873.240469022 Test RE 1.2200859082057942\n",
      "155 Train Loss 15145900000000.0 Test MSE 7430995.832975685 Test RE 1.2184573513544954\n",
      "156 Train Loss 15148722000000.0 Test MSE 7428872.569013246 Test RE 1.2182832635557375\n",
      "157 Train Loss 15151187000000.0 Test MSE 7436597.368992009 Test RE 1.2189165056286326\n",
      "158 Train Loss 15151471000000.0 Test MSE 7422661.878139085 Test RE 1.2177739020913405\n",
      "159 Train Loss 15150593000000.0 Test MSE 7383089.713867721 Test RE 1.2145234274099959\n",
      "160 Train Loss 15151936000000.0 Test MSE 7387748.323454867 Test RE 1.214906539254661\n",
      "161 Train Loss 15154416000000.0 Test MSE 7392787.085132929 Test RE 1.2153207778560278\n",
      "162 Train Loss 15156511000000.0 Test MSE 7390764.181574097 Test RE 1.215154491115935\n",
      "163 Train Loss 15159968000000.0 Test MSE 7391947.929440699 Test RE 1.2152518003324535\n",
      "164 Train Loss 15163466000000.0 Test MSE 7391947.929440699 Test RE 1.2152518003324535\n",
      "165 Train Loss 15166461000000.0 Test MSE 7396020.409654852 Test RE 1.2155865163972093\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 44.50\n",
      "Training time: 44.50\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "3\n",
      "0 Train Loss 12443664000000.0 Test MSE 5261807.173916914 Test RE 1.0253078631528412\n",
      "1 Train Loss 12169740000000.0 Test MSE 5366968.582556626 Test RE 1.035502973080194\n",
      "2 Train Loss 11893570000000.0 Test MSE 5538389.573334768 Test RE 1.0519099767342273\n",
      "3 Train Loss 11712906000000.0 Test MSE 5639364.428426538 Test RE 1.0614557743148385\n",
      "4 Train Loss 11518534000000.0 Test MSE 5779658.622173108 Test RE 1.0745779317333473\n",
      "5 Train Loss 11562311000000.0 Test MSE 5814155.097806589 Test RE 1.0777800242419184\n",
      "6 Train Loss 11546030000000.0 Test MSE 5920203.170372611 Test RE 1.0875647659863787\n",
      "7 Train Loss 11569861000000.0 Test MSE 5984335.455240625 Test RE 1.09343957604936\n",
      "8 Train Loss 11561387000000.0 Test MSE 6056400.51122575 Test RE 1.1000036275872471\n",
      "9 Train Loss 11583447000000.0 Test MSE 6115338.233775531 Test RE 1.1053429990701953\n",
      "10 Train Loss 11636806000000.0 Test MSE 6146661.188815901 Test RE 1.1081701842885694\n",
      "11 Train Loss 11715834000000.0 Test MSE 6205169.052160605 Test RE 1.113431830281343\n",
      "12 Train Loss 11824051000000.0 Test MSE 6223520.669991522 Test RE 1.115077086828281\n",
      "13 Train Loss 11914675000000.0 Test MSE 6227049.348075586 Test RE 1.1153931611778058\n",
      "14 Train Loss 12020333000000.0 Test MSE 6275679.6263895035 Test RE 1.1197400345968218\n",
      "15 Train Loss 12132246000000.0 Test MSE 6273746.573654773 Test RE 1.1195675685556319\n",
      "16 Train Loss 12240813000000.0 Test MSE 6292846.828454614 Test RE 1.121270520255073\n",
      "17 Train Loss 12344108000000.0 Test MSE 6305872.8051139265 Test RE 1.122430416049543\n",
      "18 Train Loss 12445003000000.0 Test MSE 6320416.945154009 Test RE 1.1237240817680814\n",
      "19 Train Loss 12539623000000.0 Test MSE 6327646.610101793 Test RE 1.1243665889620953\n",
      "20 Train Loss 12620015000000.0 Test MSE 6343605.825460499 Test RE 1.1257836014437725\n",
      "21 Train Loss 12694495000000.0 Test MSE 6347232.140911206 Test RE 1.1261053319638854\n",
      "22 Train Loss 12761696000000.0 Test MSE 6356600.970655772 Test RE 1.1269361191935372\n",
      "23 Train Loss 12830052000000.0 Test MSE 6383349.23107521 Test RE 1.1293046756597178\n",
      "24 Train Loss 12899907000000.0 Test MSE 6391154.355414152 Test RE 1.1299948831304678\n",
      "25 Train Loss 12978371000000.0 Test MSE 6401275.574094502 Test RE 1.1308892756223092\n",
      "26 Train Loss 13046825000000.0 Test MSE 6375993.089341562 Test RE 1.1286537853774763\n",
      "27 Train Loss 13113974000000.0 Test MSE 6390619.336580612 Test RE 1.1299475848519676\n",
      "28 Train Loss 13166035000000.0 Test MSE 6410947.834281163 Test RE 1.131743334036996\n",
      "29 Train Loss 13182965000000.0 Test MSE 6428616.569799475 Test RE 1.1333018176977423\n",
      "30 Train Loss 13220309000000.0 Test MSE 6440361.879938262 Test RE 1.1343366365288587\n",
      "31 Train Loss 13283136000000.0 Test MSE 6435014.716640441 Test RE 1.1338656425967213\n",
      "32 Train Loss 13346929000000.0 Test MSE 6447845.287788882 Test RE 1.1349954690144264\n",
      "33 Train Loss 13408450000000.0 Test MSE 6471098.049873006 Test RE 1.137040185285858\n",
      "34 Train Loss 13459440000000.0 Test MSE 6467600.05628907 Test RE 1.1367328265590821\n",
      "35 Train Loss 13518318000000.0 Test MSE 6479419.8822630355 Test RE 1.1377710672481063\n",
      "36 Train Loss 13572028000000.0 Test MSE 6487741.703510373 Test RE 1.1385014790296464\n",
      "37 Train Loss 13624428000000.0 Test MSE 6482922.409772895 Test RE 1.1380785435450598\n",
      "38 Train Loss 13660620000000.0 Test MSE 6489353.212548217 Test RE 1.1386428680976064\n",
      "39 Train Loss 13705859000000.0 Test MSE 6500021.781526444 Test RE 1.1395784545071896\n",
      "40 Train Loss 13752861000000.0 Test MSE 6507088.252304018 Test RE 1.1401977301547062\n",
      "41 Train Loss 13802680000000.0 Test MSE 6504511.042839729 Test RE 1.139971913377313\n",
      "42 Train Loss 13849913000000.0 Test MSE 6501806.195598038 Test RE 1.1397348647745311\n",
      "43 Train Loss 13893725000000.0 Test MSE 6507579.103104871 Test RE 1.1402407337527292\n",
      "44 Train Loss 13936409000000.0 Test MSE 6517132.208129304 Test RE 1.1410773616736873\n",
      "45 Train Loss 13980295000000.0 Test MSE 6514118.360386307 Test RE 1.1408134855539134\n",
      "46 Train Loss 14019441000000.0 Test MSE 6506147.435290068 Test RE 1.140115300336692\n",
      "47 Train Loss 14054477000000.0 Test MSE 6503308.394856805 Test RE 1.1398665212662904\n",
      "48 Train Loss 14081627000000.0 Test MSE 6503958.276386136 Test RE 1.1399234737931707\n",
      "49 Train Loss 14106641000000.0 Test MSE 6539400.6830243105 Test RE 1.1430251803452882\n",
      "50 Train Loss 14128918000000.0 Test MSE 6544345.166942978 Test RE 1.1434572231259859\n",
      "51 Train Loss 14146359000000.0 Test MSE 6535576.586399615 Test RE 1.1426909235720328\n",
      "52 Train Loss 14170433000000.0 Test MSE 6537024.619663283 Test RE 1.1428175048157607\n",
      "53 Train Loss 14184752000000.0 Test MSE 6544102.329608695 Test RE 1.1434360081172446\n",
      "54 Train Loss 14204952000000.0 Test MSE 6562833.354227714 Test RE 1.1450712533330396\n",
      "55 Train Loss 14210990000000.0 Test MSE 6594572.881729976 Test RE 1.1478368411736248\n",
      "56 Train Loss 14228521000000.0 Test MSE 6621907.040722319 Test RE 1.150213242980795\n",
      "57 Train Loss 14251671000000.0 Test MSE 6627211.63317901 Test RE 1.150673849799467\n",
      "58 Train Loss 14255556000000.0 Test MSE 6629890.9817333575 Test RE 1.150906432138279\n",
      "59 Train Loss 14261328000000.0 Test MSE 6611388.955750502 Test RE 1.1492993938107214\n",
      "60 Train Loss 14280135000000.0 Test MSE 6615530.082594759 Test RE 1.1496592765494515\n",
      "61 Train Loss 14293242000000.0 Test MSE 6624909.780277037 Test RE 1.1504739986312098\n",
      "62 Train Loss 14259775000000.0 Test MSE 6616672.357180335 Test RE 1.1497585255797564\n",
      "63 Train Loss 14148553000000.0 Test MSE 6686962.866625284 Test RE 1.1558494732470408\n",
      "64 Train Loss 14117588000000.0 Test MSE 6709919.799309235 Test RE 1.157831839834599\n",
      "65 Train Loss 14106840000000.0 Test MSE 6736555.954938628 Test RE 1.160127667822734\n",
      "66 Train Loss 14098526000000.0 Test MSE 6760176.230589109 Test RE 1.1621597564182056\n",
      "67 Train Loss 14079105000000.0 Test MSE 6759568.334034533 Test RE 1.1621075026920835\n",
      "68 Train Loss 14087805000000.0 Test MSE 6760631.9611588605 Test RE 1.162198928681178\n",
      "69 Train Loss 14103443000000.0 Test MSE 6763342.28883188 Test RE 1.1624318672805367\n",
      "70 Train Loss 14118078000000.0 Test MSE 6762049.665962639 Test RE 1.162320778875385\n",
      "71 Train Loss 14131613000000.0 Test MSE 6760193.050808043 Test RE 1.1621612022214447\n",
      "72 Train Loss 14137226000000.0 Test MSE 6764037.780534955 Test RE 1.162491633657986\n",
      "73 Train Loss 14141376000000.0 Test MSE 6770862.646135783 Test RE 1.1630779586556388\n",
      "74 Train Loss 14144542000000.0 Test MSE 6788715.098123694 Test RE 1.1646102689206566\n",
      "75 Train Loss 14152272000000.0 Test MSE 6796736.942948965 Test RE 1.1652981431743632\n",
      "76 Train Loss 14162185000000.0 Test MSE 6802056.816420443 Test RE 1.1657540991833535\n",
      "77 Train Loss 14174484000000.0 Test MSE 6799873.278788925 Test RE 1.1655669739999561\n",
      "78 Train Loss 14179721000000.0 Test MSE 6816227.229657945 Test RE 1.1669677484722494\n",
      "79 Train Loss 14190058000000.0 Test MSE 6835279.117593042 Test RE 1.1685974934904713\n",
      "80 Train Loss 14197801000000.0 Test MSE 6855213.969230253 Test RE 1.1703003396084408\n",
      "81 Train Loss 14202225000000.0 Test MSE 6866570.229153863 Test RE 1.171269290785667\n",
      "82 Train Loss 14210207000000.0 Test MSE 6885674.743855409 Test RE 1.1728975409875488\n",
      "83 Train Loss 14221084000000.0 Test MSE 6905612.539059044 Test RE 1.1745944035474793\n",
      "84 Train Loss 14230936000000.0 Test MSE 6930532.64403972 Test RE 1.176711859155674\n",
      "85 Train Loss 14238986000000.0 Test MSE 6961552.94079494 Test RE 1.1793423350093666\n",
      "86 Train Loss 14243365000000.0 Test MSE 6996252.436300679 Test RE 1.182277866562221\n",
      "87 Train Loss 14243912000000.0 Test MSE 7028069.0044198055 Test RE 1.1849631152276332\n",
      "88 Train Loss 14238188000000.0 Test MSE 7051894.345873881 Test RE 1.1869699441551547\n",
      "89 Train Loss 14239810000000.0 Test MSE 7058145.6756638335 Test RE 1.187495937348867\n",
      "90 Train Loss 14239025000000.0 Test MSE 7041217.377320117 Test RE 1.1860710338210894\n",
      "91 Train Loss 14240865000000.0 Test MSE 7041734.200517963 Test RE 1.186114561647588\n",
      "92 Train Loss 14244001000000.0 Test MSE 7044785.930048574 Test RE 1.1863715515198334\n",
      "93 Train Loss 14253226000000.0 Test MSE 7051366.211288708 Test RE 1.1869254957001776\n",
      "94 Train Loss 14261426000000.0 Test MSE 7064385.938030411 Test RE 1.1880207670543588\n",
      "95 Train Loss 14271283000000.0 Test MSE 7066742.18648909 Test RE 1.1882188761857184\n",
      "96 Train Loss 14280455000000.0 Test MSE 7069682.712095179 Test RE 1.1884660639665703\n",
      "97 Train Loss 14289537000000.0 Test MSE 7075996.291770436 Test RE 1.1889966253082043\n",
      "98 Train Loss 14296005000000.0 Test MSE 7087340.480953432 Test RE 1.1899493392710094\n",
      "99 Train Loss 14299309000000.0 Test MSE 7097019.959051978 Test RE 1.1907616438722826\n",
      "100 Train Loss 14302837000000.0 Test MSE 7114960.0254629515 Test RE 1.1922657160062382\n",
      "101 Train Loss 14306019000000.0 Test MSE 7114132.579944052 Test RE 1.192196385777759\n",
      "102 Train Loss 14308763000000.0 Test MSE 7109072.24302832 Test RE 1.1917723011468935\n",
      "103 Train Loss 14305460000000.0 Test MSE 7121916.099701044 Test RE 1.1928483941043182\n",
      "104 Train Loss 14307283000000.0 Test MSE 7117013.394854632 Test RE 1.1924377468616845\n",
      "105 Train Loss 14315183000000.0 Test MSE 7118307.9562685955 Test RE 1.1925461921897051\n",
      "106 Train Loss 14323461000000.0 Test MSE 7118413.121615995 Test RE 1.1925550014511297\n",
      "107 Train Loss 14330789000000.0 Test MSE 7114625.980981246 Test RE 1.1922377274823046\n",
      "108 Train Loss 14333752000000.0 Test MSE 7107176.534649292 Test RE 1.1916133912854265\n",
      "109 Train Loss 14340418000000.0 Test MSE 7103728.803491795 Test RE 1.1913243270582248\n",
      "110 Train Loss 14349577000000.0 Test MSE 7098378.425358643 Test RE 1.1908756024197493\n",
      "111 Train Loss 14358882000000.0 Test MSE 7098357.546606855 Test RE 1.1908738510328194\n",
      "112 Train Loss 14368206000000.0 Test MSE 7098357.546606855 Test RE 1.1908738510328194\n",
      "113 Train Loss 14377353000000.0 Test MSE 7098357.546606855 Test RE 1.1908738510328194\n",
      "114 Train Loss 14386494000000.0 Test MSE 7098357.546606855 Test RE 1.1908738510328194\n",
      "115 Train Loss 14395906000000.0 Test MSE 7099478.831772797 Test RE 1.1909679049351736\n",
      "116 Train Loss 14404901000000.0 Test MSE 7099478.831772797 Test RE 1.1909679049351736\n",
      "117 Train Loss 14413719000000.0 Test MSE 7099478.831772797 Test RE 1.1909679049351736\n",
      "118 Train Loss 14422278000000.0 Test MSE 7099478.831772797 Test RE 1.1909679049351736\n",
      "119 Train Loss 14429991000000.0 Test MSE 7099478.831772797 Test RE 1.1909679049351736\n",
      "120 Train Loss 14438041000000.0 Test MSE 7099478.831772797 Test RE 1.1909679049351736\n",
      "121 Train Loss 14445319000000.0 Test MSE 7099693.419941335 Test RE 1.190985903840192\n",
      "122 Train Loss 14452602000000.0 Test MSE 7098311.354430404 Test RE 1.190869976253042\n",
      "123 Train Loss 14460245000000.0 Test MSE 7098311.354430404 Test RE 1.190869976253042\n",
      "124 Train Loss 14467872000000.0 Test MSE 7098311.354430404 Test RE 1.190869976253042\n",
      "125 Train Loss 14471919000000.0 Test MSE 7096495.665425272 Test RE 1.1907176591945543\n",
      "126 Train Loss 14476060000000.0 Test MSE 7090115.029453653 Test RE 1.1901822368651256\n",
      "127 Train Loss 14481323000000.0 Test MSE 7089736.107318933 Test RE 1.190150432556043\n",
      "128 Train Loss 14487828000000.0 Test MSE 7087846.613023538 Test RE 1.1899918277595916\n",
      "129 Train Loss 14491766000000.0 Test MSE 7086573.249115729 Test RE 1.1898849292338325\n",
      "130 Train Loss 14497976000000.0 Test MSE 7087449.020379776 Test RE 1.1899584510051302\n",
      "131 Train Loss 14500387000000.0 Test MSE 7090822.033015603 Test RE 1.1902415759662108\n",
      "132 Train Loss 14502359000000.0 Test MSE 7091748.07066539 Test RE 1.190319294214019\n",
      "133 Train Loss 14503990000000.0 Test MSE 7090497.052621337 Test RE 1.1902143005948322\n",
      "134 Train Loss 14506220000000.0 Test MSE 7095231.904760473 Test RE 1.1906116315711133\n",
      "135 Train Loss 14511843000000.0 Test MSE 7094354.761921094 Test RE 1.190538035051686\n",
      "136 Train Loss 14517069000000.0 Test MSE 7096382.942800254 Test RE 1.1907082023189683\n",
      "137 Train Loss 14522346000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "138 Train Loss 14528452000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "139 Train Loss 14534620000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "140 Train Loss 14540184000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "141 Train Loss 14545383000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "142 Train Loss 14550451000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "143 Train Loss 14555886000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "144 Train Loss 14561359000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "145 Train Loss 14566223000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "146 Train Loss 14571093000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "147 Train Loss 14576324000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "148 Train Loss 14581121000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "149 Train Loss 14585427000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "150 Train Loss 14589977000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "151 Train Loss 14594654000000.0 Test MSE 7097434.082108627 Test RE 1.1907963848382894\n",
      "152 Train Loss 14599152000000.0 Test MSE 7097434.589199173 Test RE 1.1907964273777176\n",
      "153 Train Loss 14603798000000.0 Test MSE 7097434.589199173 Test RE 1.1907964273777176\n",
      "154 Train Loss 14608420000000.0 Test MSE 7097434.589199173 Test RE 1.1907964273777176\n",
      "155 Train Loss 14612906000000.0 Test MSE 7097434.589199173 Test RE 1.1907964273777176\n",
      "156 Train Loss 14617528000000.0 Test MSE 7097434.825501514 Test RE 1.1907964472009351\n",
      "157 Train Loss 14621795000000.0 Test MSE 7097434.825501514 Test RE 1.1907964472009351\n",
      "158 Train Loss 14625882000000.0 Test MSE 7097434.825501514 Test RE 1.1907964472009351\n",
      "159 Train Loss 14629880000000.0 Test MSE 7095937.410755265 Test RE 1.1906708236296084\n",
      "160 Train Loss 14633120000000.0 Test MSE 7096064.102437775 Test RE 1.1906814527690668\n",
      "161 Train Loss 14636191000000.0 Test MSE 7097329.057654349 Test RE 1.190787574386393\n",
      "162 Train Loss 14637400000000.0 Test MSE 7102516.698128155 Test RE 1.1912226852128256\n",
      "163 Train Loss 14639276000000.0 Test MSE 7106110.157730337 Test RE 1.1915239917412344\n",
      "164 Train Loss 14642932000000.0 Test MSE 7106110.157730337 Test RE 1.1915239917412344\n",
      "165 Train Loss 14646501000000.0 Test MSE 7106110.157730337 Test RE 1.1915239917412344\n",
      "166 Train Loss 14649657000000.0 Test MSE 7107281.550157571 Test RE 1.1916221948815586\n",
      "167 Train Loss 14652976000000.0 Test MSE 7106623.957080982 Test RE 1.1915670668677003\n",
      "168 Train Loss 14654764000000.0 Test MSE 7109682.62415031 Test RE 1.1918234625133723\n",
      "169 Train Loss 14657717000000.0 Test MSE 7111261.354113779 Test RE 1.1919557794584688\n",
      "170 Train Loss 14658416000000.0 Test MSE 7117570.211879078 Test RE 1.192484392597395\n",
      "171 Train Loss 14659372000000.0 Test MSE 7120085.216881056 Test RE 1.1926950571384605\n",
      "172 Train Loss 14658378000000.0 Test MSE 7121099.81593985 Test RE 1.1927800325382691\n",
      "173 Train Loss 14659296000000.0 Test MSE 7121280.611013002 Test RE 1.1927951739763947\n",
      "174 Train Loss 14661828000000.0 Test MSE 7120628.519290439 Test RE 1.1927405609293238\n",
      "175 Train Loss 14664432000000.0 Test MSE 7119850.100468552 Test RE 1.1926753646425465\n",
      "176 Train Loss 14665515000000.0 Test MSE 7121667.846810585 Test RE 1.192827604008072\n",
      "177 Train Loss 14662300000000.0 Test MSE 7125129.344724868 Test RE 1.1931174566627416\n",
      "178 Train Loss 14662419000000.0 Test MSE 7128162.410865817 Test RE 1.1933713762021985\n",
      "179 Train Loss 14662846000000.0 Test MSE 7127347.022035625 Test RE 1.1933031195139383\n",
      "180 Train Loss 14665577000000.0 Test MSE 7126558.998301238 Test RE 1.193237149864142\n",
      "181 Train Loss 14668064000000.0 Test MSE 7126378.156022824 Test RE 1.1932220100825248\n",
      "182 Train Loss 14670651000000.0 Test MSE 7126378.156022824 Test RE 1.1932220100825248\n",
      "183 Train Loss 14669427000000.0 Test MSE 7119368.435309927 Test RE 1.1926350211046353\n",
      "184 Train Loss 14668955000000.0 Test MSE 7121660.577450349 Test RE 1.192826995225414\n",
      "185 Train Loss 14667082000000.0 Test MSE 7120743.276616605 Test RE 1.1927501721009879\n",
      "186 Train Loss 14665427000000.0 Test MSE 7116437.552038324 Test RE 1.19238950537594\n",
      "187 Train Loss 14665896000000.0 Test MSE 7121652.5399364475 Test RE 1.1928263221122999\n",
      "188 Train Loss 14667665000000.0 Test MSE 7122965.284433948 Test RE 1.1929362547458169\n",
      "189 Train Loss 14668804000000.0 Test MSE 7123155.498912378 Test RE 1.1929521829599496\n",
      "190 Train Loss 14670686000000.0 Test MSE 7123672.798520884 Test RE 1.1929954996117944\n",
      "191 Train Loss 14670309000000.0 Test MSE 7134152.981207968 Test RE 1.1938727307853925\n",
      "192 Train Loss 14670306000000.0 Test MSE 7138154.3158140965 Test RE 1.194207487738473\n",
      "193 Train Loss 14671719000000.0 Test MSE 7138650.118991751 Test RE 1.1942489607551632\n",
      "194 Train Loss 14673294000000.0 Test MSE 7143193.972610388 Test RE 1.194628978625208\n",
      "195 Train Loss 14675691000000.0 Test MSE 7143193.972610388 Test RE 1.194628978625208\n",
      "196 Train Loss 14677982000000.0 Test MSE 7143768.840566769 Test RE 1.1946770481656455\n",
      "197 Train Loss 14680297000000.0 Test MSE 7143768.840566769 Test RE 1.1946770481656455\n",
      "198 Train Loss 14682666000000.0 Test MSE 7143768.840566769 Test RE 1.1946770481656455\n",
      "199 Train Loss 14685091000000.0 Test MSE 7143768.840566769 Test RE 1.1946770481656455\n",
      "Training time: 37.46\n",
      "Training time: 37.46\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 38.69\n",
      "Training time: 38.69\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "5\n",
      "0 Train Loss 12664449000000.0 Test MSE 5264145.462122979 Test RE 1.0255356555169706\n",
      "1 Train Loss 12876942000000.0 Test MSE 5547359.015177518 Test RE 1.05276141814178\n",
      "2 Train Loss 12994545000000.0 Test MSE 5738940.381822749 Test RE 1.0707859898196035\n",
      "3 Train Loss 13113582000000.0 Test MSE 5759933.059567288 Test RE 1.0727426355463332\n",
      "4 Train Loss 13232741000000.0 Test MSE 5842539.649929852 Test RE 1.0804076679769414\n",
      "5 Train Loss 13218699000000.0 Test MSE 6011153.150655911 Test RE 1.095886861226812\n",
      "6 Train Loss 13184770000000.0 Test MSE 6268666.983957238 Test RE 1.1191142433052068\n",
      "7 Train Loss 13056852000000.0 Test MSE 6457117.605545405 Test RE 1.1358112654059676\n",
      "8 Train Loss 12886263000000.0 Test MSE 6626127.267668897 Test RE 1.150579707485202\n",
      "9 Train Loss 12908690000000.0 Test MSE 6891678.12838844 Test RE 1.1734087342335102\n",
      "10 Train Loss 12790299000000.0 Test MSE 7282922.16661942 Test RE 1.2062564760661303\n",
      "11 Train Loss 12703975000000.0 Test MSE 7863498.677282666 Test RE 1.2534145525410716\n",
      "12 Train Loss 12681124000000.0 Test MSE 7807651.845365176 Test RE 1.2489557256088708\n",
      "13 Train Loss 12718755000000.0 Test MSE 7905633.03419014 Test RE 1.2567681018223609\n",
      "14 Train Loss 12819756000000.0 Test MSE 7951300.256114295 Test RE 1.2603927617748665\n",
      "15 Train Loss 12907958000000.0 Test MSE 8078068.037193384 Test RE 1.270400268855948\n",
      "16 Train Loss 12986317000000.0 Test MSE 8087257.172076474 Test RE 1.2711226297830671\n",
      "17 Train Loss 13018024000000.0 Test MSE 8192419.566332965 Test RE 1.2793604378273646\n",
      "18 Train Loss 13021382000000.0 Test MSE 8666716.815066312 Test RE 1.3158734572425905\n",
      "19 Train Loss 13097366000000.0 Test MSE 8692384.80537015 Test RE 1.317820610736745\n",
      "20 Train Loss 13160770000000.0 Test MSE 8750870.307033258 Test RE 1.322246564669663\n",
      "21 Train Loss 13234529000000.0 Test MSE 8949230.716447186 Test RE 1.3371486061358344\n",
      "22 Train Loss 13324076000000.0 Test MSE 9126823.066562964 Test RE 1.350350903426755\n",
      "23 Train Loss 13386616000000.0 Test MSE 9408370.701007068 Test RE 1.3710207698550216\n",
      "24 Train Loss 13457376000000.0 Test MSE 9748909.433708236 Test RE 1.3956124709892654\n",
      "25 Train Loss 13541500000000.0 Test MSE 9947277.5952296 Test RE 1.4097397401947038\n",
      "26 Train Loss 13630951000000.0 Test MSE 9950304.418216214 Test RE 1.4099542063166708\n",
      "27 Train Loss 13709226000000.0 Test MSE 9935887.08903876 Test RE 1.4089323711144344\n",
      "28 Train Loss 13745059000000.0 Test MSE 10296514.932965193 Test RE 1.4342734212110666\n",
      "29 Train Loss 13820423000000.0 Test MSE 10428341.629395375 Test RE 1.4434257502322532\n",
      "30 Train Loss 13891133000000.0 Test MSE 10425964.002484296 Test RE 1.4432611927374386\n",
      "31 Train Loss 13957115000000.0 Test MSE 10400051.644036641 Test RE 1.4414665592911389\n",
      "32 Train Loss 13994841000000.0 Test MSE 10849965.405301215 Test RE 1.4723158938583636\n",
      "33 Train Loss 14018932000000.0 Test MSE 11025508.529144935 Test RE 1.4841785081247398\n",
      "34 Train Loss 14077609000000.0 Test MSE 10983079.019176316 Test RE 1.4813199702044006\n",
      "35 Train Loss 14142594000000.0 Test MSE 11078243.631758401 Test RE 1.487723693336509\n",
      "36 Train Loss 14206782000000.0 Test MSE 11032061.176819721 Test RE 1.4846194789097171\n",
      "37 Train Loss 14274232000000.0 Test MSE 11017189.837505845 Test RE 1.4836184998030555\n",
      "38 Train Loss 14333965000000.0 Test MSE 10980059.513158808 Test RE 1.4811163314059592\n",
      "39 Train Loss 14388948000000.0 Test MSE 10841440.600326128 Test RE 1.471737381774509\n",
      "40 Train Loss 14442437000000.0 Test MSE 10807360.523607733 Test RE 1.469422357236361\n",
      "41 Train Loss 14495929000000.0 Test MSE 10856641.391444243 Test RE 1.4727687823530393\n",
      "42 Train Loss 14549278000000.0 Test MSE 10883584.35058584 Test RE 1.474595137265258\n",
      "43 Train Loss 14600810000000.0 Test MSE 10846620.445623383 Test RE 1.472088924612622\n",
      "44 Train Loss 14646968000000.0 Test MSE 10696663.332933297 Test RE 1.4618775178831007\n",
      "45 Train Loss 14685740000000.0 Test MSE 10494039.679616047 Test RE 1.44796536798282\n",
      "46 Train Loss 14713326000000.0 Test MSE 10585661.223103004 Test RE 1.454272591660128\n",
      "47 Train Loss 14732523000000.0 Test MSE 10530323.496956298 Test RE 1.450466424665506\n",
      "48 Train Loss 14761792000000.0 Test MSE 10593855.436813904 Test RE 1.4548353489092178\n",
      "49 Train Loss 14791217000000.0 Test MSE 10485511.071843743 Test RE 1.447376860641856\n",
      "50 Train Loss 14828348000000.0 Test MSE 10466774.981407803 Test RE 1.446083156001599\n",
      "51 Train Loss 14865704000000.0 Test MSE 10462959.14796495 Test RE 1.4458195353787389\n",
      "52 Train Loss 14904515000000.0 Test MSE 10462343.470063847 Test RE 1.4457769961592053\n",
      "53 Train Loss 14943261000000.0 Test MSE 10521202.130942712 Test RE 1.4498380915757163\n",
      "54 Train Loss 14978944000000.0 Test MSE 10543345.787201034 Test RE 1.4513630048521302\n",
      "55 Train Loss 15009851000000.0 Test MSE 10576522.67847454 Test RE 1.4536447232529568\n",
      "56 Train Loss 15041803000000.0 Test MSE 10561140.909448339 Test RE 1.4525872980692183\n",
      "57 Train Loss 15074335000000.0 Test MSE 10507053.104763623 Test RE 1.4488628846415164\n",
      "58 Train Loss 15105767000000.0 Test MSE 10504915.257181192 Test RE 1.448715478629663\n",
      "59 Train Loss 15138217000000.0 Test MSE 10455308.569637116 Test RE 1.4452908427680342\n",
      "60 Train Loss 15169799000000.0 Test MSE 10488709.015788749 Test RE 1.4475975593360726\n",
      "61 Train Loss 15203180000000.0 Test MSE 10478010.80478081 Test RE 1.4468591150172654\n",
      "62 Train Loss 15232953000000.0 Test MSE 10444491.990327079 Test RE 1.4445430337322147\n",
      "63 Train Loss 15260580000000.0 Test MSE 10419095.970912255 Test RE 1.4427857452680657\n",
      "64 Train Loss 15272088000000.0 Test MSE 10338433.998158261 Test RE 1.4371900552491954\n",
      "65 Train Loss 15281342000000.0 Test MSE 10426943.471037159 Test RE 1.4433289848261215\n",
      "66 Train Loss 15290930000000.0 Test MSE 10376881.960549342 Test RE 1.439859983315068\n",
      "67 Train Loss 15293615000000.0 Test MSE 10469514.674792359 Test RE 1.446272400790164\n",
      "68 Train Loss 15309535000000.0 Test MSE 10524338.71827113 Test RE 1.450054188788329\n",
      "69 Train Loss 15333910000000.0 Test MSE 10475084.875592878 Test RE 1.4466570870286157\n",
      "70 Train Loss 15353713000000.0 Test MSE 10448550.572358862 Test RE 1.4448236709794322\n",
      "71 Train Loss 15369300000000.0 Test MSE 10554070.548770986 Test RE 1.4521009852886209\n",
      "72 Train Loss 15374573000000.0 Test MSE 10516207.51727807 Test RE 1.4494939179412412\n",
      "73 Train Loss 15394804000000.0 Test MSE 10603413.017871635 Test RE 1.4554914637734957\n",
      "74 Train Loss 15404821000000.0 Test MSE 10998242.373997878 Test RE 1.4823421805006973\n",
      "75 Train Loss 15387993000000.0 Test MSE 11301838.47825028 Test RE 1.5026622347863017\n",
      "76 Train Loss 15366545000000.0 Test MSE 11090984.811453635 Test RE 1.4885789691590043\n",
      "77 Train Loss 15331850000000.0 Test MSE 11455438.370566675 Test RE 1.5128388900681806\n",
      "78 Train Loss 15339891000000.0 Test MSE 11639880.633833535 Test RE 1.5249692517004552\n",
      "79 Train Loss 15339164000000.0 Test MSE 11845412.388855066 Test RE 1.538373946266824\n",
      "80 Train Loss 15342354000000.0 Test MSE 11889167.053375846 Test RE 1.5412125554163014\n",
      "81 Train Loss 15342567000000.0 Test MSE 12077692.369731978 Test RE 1.553383920765126\n",
      "82 Train Loss 15350253000000.0 Test MSE 12128551.089231106 Test RE 1.5566511062311366\n",
      "83 Train Loss 15360308000000.0 Test MSE 11972456.12869309 Test RE 1.5466015846090284\n",
      "84 Train Loss 15357239000000.0 Test MSE 12183309.4250091 Test RE 1.560161155755176\n",
      "85 Train Loss 15367241000000.0 Test MSE 12196047.506554352 Test RE 1.5609765445565618\n",
      "86 Train Loss 15377768000000.0 Test MSE 12376402.845204819 Test RE 1.5724760594371447\n",
      "87 Train Loss 15389754000000.0 Test MSE 12474211.47934576 Test RE 1.5786773387990194\n",
      "88 Train Loss 15402574000000.0 Test MSE 12441395.393896012 Test RE 1.5765994468566815\n",
      "89 Train Loss 15415017000000.0 Test MSE 12497597.98834905 Test RE 1.5801564889592512\n",
      "90 Train Loss 15425148000000.0 Test MSE 12472857.070094865 Test RE 1.5785916326508769\n",
      "91 Train Loss 15433613000000.0 Test MSE 12492666.833956877 Test RE 1.5798447184739086\n",
      "92 Train Loss 15445624000000.0 Test MSE 12568706.063394288 Test RE 1.5846454520352447\n",
      "93 Train Loss 15459337000000.0 Test MSE 12571667.36704942 Test RE 1.584832119619486\n",
      "94 Train Loss 15473476000000.0 Test MSE 12568455.135680674 Test RE 1.5846296336430086\n",
      "95 Train Loss 15487210000000.0 Test MSE 12566194.0164293 Test RE 1.5844870863809277\n",
      "96 Train Loss 15500986000000.0 Test MSE 12568168.33078261 Test RE 1.5846115533726652\n",
      "97 Train Loss 15514014000000.0 Test MSE 12568168.33078261 Test RE 1.5846115533726652\n",
      "98 Train Loss 15526388000000.0 Test MSE 12568168.33078261 Test RE 1.5846115533726652\n",
      "99 Train Loss 15539213000000.0 Test MSE 12568168.33078261 Test RE 1.5846115533726652\n",
      "100 Train Loss 15551045000000.0 Test MSE 12568168.33078261 Test RE 1.5846115533726652\n",
      "101 Train Loss 15562707000000.0 Test MSE 12568166.976675957 Test RE 1.5846114680088692\n",
      "102 Train Loss 15573932000000.0 Test MSE 12568166.976675957 Test RE 1.5846114680088692\n",
      "103 Train Loss 15585228000000.0 Test MSE 12568166.976675957 Test RE 1.5846114680088692\n",
      "104 Train Loss 15596246000000.0 Test MSE 12568166.976675957 Test RE 1.5846114680088692\n",
      "105 Train Loss 15606884000000.0 Test MSE 12568166.976675957 Test RE 1.5846114680088692\n",
      "106 Train Loss 15618275000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "107 Train Loss 15629706000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "108 Train Loss 15640662000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "109 Train Loss 15651269000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "110 Train Loss 15661622000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "111 Train Loss 15671618000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "112 Train Loss 15681400000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "113 Train Loss 15690679000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "114 Train Loss 15699966000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "115 Train Loss 15709557000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "116 Train Loss 15718692000000.0 Test MSE 12568163.46039651 Test RE 1.584611246340223\n",
      "117 Train Loss 15727545000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "118 Train Loss 15735949000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "119 Train Loss 15744006000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "120 Train Loss 15752257000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "121 Train Loss 15760783000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "122 Train Loss 15769075000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "123 Train Loss 15776869000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "124 Train Loss 15784563000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "125 Train Loss 15791943000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "126 Train Loss 15798890000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "127 Train Loss 15806257000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "128 Train Loss 15813369000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "129 Train Loss 15820357000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "130 Train Loss 15826958000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "131 Train Loss 15833389000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "132 Train Loss 15839941000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "133 Train Loss 15846711000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "134 Train Loss 15853163000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "135 Train Loss 15859556000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "136 Train Loss 15865785000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "137 Train Loss 15872032000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "138 Train Loss 15878604000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "139 Train Loss 15884846000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "140 Train Loss 15890820000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "141 Train Loss 15896318000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "142 Train Loss 15901948000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "143 Train Loss 15907377000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "144 Train Loss 15912573000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "145 Train Loss 15918360000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "146 Train Loss 15924060000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "147 Train Loss 15929417000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "148 Train Loss 15934473000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "149 Train Loss 15939205000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "150 Train Loss 15944235000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "151 Train Loss 15949215000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "152 Train Loss 15954121000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "153 Train Loss 15958978000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "154 Train Loss 15963976000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "155 Train Loss 15968570000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "156 Train Loss 15973339000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "157 Train Loss 15977730000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "158 Train Loss 15982080000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "159 Train Loss 15986285000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "160 Train Loss 15990395000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "161 Train Loss 15994455000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "162 Train Loss 15998705000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "163 Train Loss 16002970000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "164 Train Loss 16006890000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "165 Train Loss 16010624000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "166 Train Loss 16014373000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "167 Train Loss 16018008000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "168 Train Loss 16021747000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "169 Train Loss 16025440000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "170 Train Loss 16029064000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "171 Train Loss 16032682000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "172 Train Loss 16036290000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "173 Train Loss 16039756000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "174 Train Loss 16043257000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "175 Train Loss 16046796000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "176 Train Loss 16050272000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "177 Train Loss 16053607000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "178 Train Loss 16056772000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "179 Train Loss 16059916000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "180 Train Loss 16062898000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "181 Train Loss 16065823000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "182 Train Loss 16068875000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "183 Train Loss 16071840000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "184 Train Loss 16074733000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "185 Train Loss 16077771000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "186 Train Loss 16080604000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "187 Train Loss 16083296000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "188 Train Loss 16085960000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "189 Train Loss 16088657000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "190 Train Loss 16091230000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "191 Train Loss 16093909000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "192 Train Loss 16096427000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "193 Train Loss 16098824000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "194 Train Loss 16101483000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "195 Train Loss 16104000000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "196 Train Loss 16106446000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "197 Train Loss 16108845000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "198 Train Loss 16111220000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "199 Train Loss 16113422000000.0 Test MSE 12568281.648157949 Test RE 1.5846186969600473\n",
      "Training time: 34.17\n",
      "Training time: 34.17\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "6\n",
      "0 Train Loss 12246688000000.0 Test MSE 5236676.171778327 Test RE 1.0228564378626077\n",
      "1 Train Loss 12463530000000.0 Test MSE 5351261.308329199 Test RE 1.0339865819533371\n",
      "2 Train Loss 12509779000000.0 Test MSE 5538416.313932894 Test RE 1.051912516160863\n",
      "3 Train Loss 12557793000000.0 Test MSE 5618053.548252457 Test RE 1.059448281443009\n",
      "4 Train Loss 12576855000000.0 Test MSE 5654638.290081548 Test RE 1.0628922452190057\n",
      "5 Train Loss 12494564000000.0 Test MSE 5821193.21554772 Test RE 1.0784321609985523\n",
      "6 Train Loss 12275667000000.0 Test MSE 5970379.0201737 Test RE 1.0921637930952948\n",
      "7 Train Loss 12097649000000.0 Test MSE 6096001.689618885 Test RE 1.1035940822250765\n",
      "8 Train Loss 11735242000000.0 Test MSE 6274530.432120388 Test RE 1.1196375072354927\n",
      "9 Train Loss 11630792000000.0 Test MSE 6421466.350783864 Test RE 1.132671385737018\n",
      "10 Train Loss 11644134000000.0 Test MSE 6456551.269844606 Test RE 1.135761454901242\n",
      "11 Train Loss 11478580000000.0 Test MSE 6567753.70642652 Test RE 1.1455004199694145\n",
      "12 Train Loss 11149080000000.0 Test MSE 6573019.010027123 Test RE 1.1459594961931576\n",
      "13 Train Loss 10880903000000.0 Test MSE 6644981.035434805 Test RE 1.1522154558393265\n",
      "14 Train Loss 10866690000000.0 Test MSE 6759404.115877922 Test RE 1.1620933863830583\n",
      "15 Train Loss 10908275000000.0 Test MSE 6807868.914659869 Test RE 1.166252039056655\n",
      "16 Train Loss 10936414000000.0 Test MSE 6824678.209595694 Test RE 1.1676909466417174\n",
      "17 Train Loss 10990288000000.0 Test MSE 6855246.045189414 Test RE 1.1703030775581353\n",
      "18 Train Loss 11045330000000.0 Test MSE 6866104.126078196 Test RE 1.1712295372067012\n",
      "19 Train Loss 11081138000000.0 Test MSE 6878766.037542998 Test RE 1.1723089829635176\n",
      "20 Train Loss 11055481000000.0 Test MSE 6937870.977993138 Test RE 1.1773346698717408\n",
      "21 Train Loss 11057712000000.0 Test MSE 7002119.438393545 Test RE 1.1827734871235038\n",
      "22 Train Loss 11094247000000.0 Test MSE 7051990.036964498 Test RE 1.1869779974566936\n",
      "23 Train Loss 11134610000000.0 Test MSE 7099896.080485443 Test RE 1.1910029020480444\n",
      "24 Train Loss 11179957000000.0 Test MSE 7124831.702255273 Test RE 1.193092535982856\n",
      "25 Train Loss 11229630000000.0 Test MSE 7146714.854449704 Test RE 1.1949233587944041\n",
      "26 Train Loss 11294133000000.0 Test MSE 7162780.518388822 Test RE 1.1962656860643281\n",
      "27 Train Loss 11347246000000.0 Test MSE 7193321.19959509 Test RE 1.1988132936175442\n",
      "28 Train Loss 11397046000000.0 Test MSE 7207647.027747599 Test RE 1.200006445609562\n",
      "29 Train Loss 11446239000000.0 Test MSE 7219392.089917694 Test RE 1.2009837696394727\n",
      "30 Train Loss 11489638000000.0 Test MSE 7235884.403649923 Test RE 1.2023547785668411\n",
      "31 Train Loss 11534933000000.0 Test MSE 7239136.084732913 Test RE 1.2026249069253199\n",
      "32 Train Loss 11585254000000.0 Test MSE 7237124.017353514 Test RE 1.2024577647084211\n",
      "33 Train Loss 11636267000000.0 Test MSE 7236402.770693891 Test RE 1.2023978451704267\n",
      "34 Train Loss 11678431000000.0 Test MSE 7249829.360570668 Test RE 1.2035128066817236\n",
      "35 Train Loss 11710667000000.0 Test MSE 7271800.958024832 Test RE 1.2053351319429\n",
      "36 Train Loss 11753099000000.0 Test MSE 7269961.953066116 Test RE 1.2051827104548698\n",
      "37 Train Loss 11790157000000.0 Test MSE 7298550.029598074 Test RE 1.207549989060536\n",
      "38 Train Loss 11825720000000.0 Test MSE 7312035.27853292 Test RE 1.2086650459689228\n",
      "39 Train Loss 11866568000000.0 Test MSE 7314639.807357064 Test RE 1.208880288548674\n",
      "40 Train Loss 11903697000000.0 Test MSE 7338480.002520581 Test RE 1.2108487038194196\n",
      "41 Train Loss 11904750000000.0 Test MSE 7362609.923614416 Test RE 1.2128377877404537\n",
      "42 Train Loss 11909212000000.0 Test MSE 7415409.485063883 Test RE 1.217178837038541\n",
      "43 Train Loss 11911470000000.0 Test MSE 7431447.978070969 Test RE 1.2184944198139303\n",
      "44 Train Loss 11909556000000.0 Test MSE 7461067.2541714385 Test RE 1.2209202614966275\n",
      "45 Train Loss 11925503000000.0 Test MSE 7492847.7043687925 Test RE 1.2235177561531827\n",
      "46 Train Loss 11943615000000.0 Test MSE 7499753.435241859 Test RE 1.2240814496090162\n",
      "47 Train Loss 11960213000000.0 Test MSE 7515721.138171012 Test RE 1.2253838508457966\n",
      "48 Train Loss 11977330000000.0 Test MSE 7562870.344635655 Test RE 1.229221509620388\n",
      "49 Train Loss 11996182000000.0 Test MSE 7591200.704824306 Test RE 1.2315216770636859\n",
      "50 Train Loss 12016185000000.0 Test MSE 7606970.685176804 Test RE 1.2328001965971558\n",
      "51 Train Loss 11995755000000.0 Test MSE 7669200.895947641 Test RE 1.2378324990438883\n",
      "52 Train Loss 11975468000000.0 Test MSE 7689141.288334806 Test RE 1.2394406745668938\n",
      "53 Train Loss 11923533000000.0 Test MSE 7665009.723769766 Test RE 1.2374942188114697\n",
      "54 Train Loss 11916263000000.0 Test MSE 7655278.297420545 Test RE 1.236708413639722\n",
      "55 Train Loss 11923869000000.0 Test MSE 7661768.0904517975 Test RE 1.237232514857133\n",
      "56 Train Loss 11940454000000.0 Test MSE 7683085.011286319 Test RE 1.2389524617420742\n",
      "57 Train Loss 11955825000000.0 Test MSE 7696826.602487175 Test RE 1.240059931750594\n",
      "58 Train Loss 11969992000000.0 Test MSE 7725675.5457621 Test RE 1.2423817301412106\n",
      "59 Train Loss 11984164000000.0 Test MSE 7741249.281067265 Test RE 1.2436333218645608\n",
      "60 Train Loss 12000662000000.0 Test MSE 7739999.109416922 Test RE 1.243532897640882\n",
      "61 Train Loss 12010294000000.0 Test MSE 7728594.578863644 Test RE 1.2426164158252173\n",
      "62 Train Loss 12025702000000.0 Test MSE 7720737.121339511 Test RE 1.2419845875853286\n",
      "63 Train Loss 12039613000000.0 Test MSE 7708899.050330052 Test RE 1.2410320657157894\n",
      "64 Train Loss 12040839000000.0 Test MSE 7723205.979866021 Test RE 1.2421831462918085\n",
      "65 Train Loss 12016836000000.0 Test MSE 7732068.260830014 Test RE 1.242895636664337\n",
      "66 Train Loss 11842277000000.0 Test MSE 7746033.7059669895 Test RE 1.2440175719351527\n",
      "67 Train Loss 11780364000000.0 Test MSE 7762398.898439981 Test RE 1.2453310083419398\n",
      "68 Train Loss 11753135000000.0 Test MSE 7799398.566121359 Test RE 1.24829543067702\n",
      "69 Train Loss 11727708000000.0 Test MSE 7817197.257311678 Test RE 1.2497189611855601\n",
      "70 Train Loss 11703236000000.0 Test MSE 7855255.886731415 Test RE 1.2527574441108251\n",
      "71 Train Loss 11678533000000.0 Test MSE 7847584.721457838 Test RE 1.2521455953747878\n",
      "72 Train Loss 11670841000000.0 Test MSE 7803905.432722208 Test RE 1.2486560410781102\n",
      "73 Train Loss 11664394000000.0 Test MSE 7827562.363674112 Test RE 1.2505472106530238\n",
      "74 Train Loss 11660233000000.0 Test MSE 7832489.234625384 Test RE 1.2509407121817255\n",
      "75 Train Loss 11646313000000.0 Test MSE 7863463.7923293365 Test RE 1.253411772267401\n",
      "76 Train Loss 11637491000000.0 Test MSE 7918837.607992793 Test RE 1.2578172375004786\n",
      "77 Train Loss 11631155000000.0 Test MSE 7937823.374742868 Test RE 1.2593241713008725\n",
      "78 Train Loss 11638916000000.0 Test MSE 7951866.774126001 Test RE 1.2604376615057495\n",
      "79 Train Loss 11642881000000.0 Test MSE 7949309.606442129 Test RE 1.2602349789335563\n",
      "80 Train Loss 11647323000000.0 Test MSE 7946589.619320564 Test RE 1.2600193554169086\n",
      "81 Train Loss 11656082000000.0 Test MSE 7958264.489492497 Test RE 1.2609446053661864\n",
      "82 Train Loss 11656026000000.0 Test MSE 7967845.524348041 Test RE 1.261703409013777\n",
      "83 Train Loss 11661893000000.0 Test MSE 7978991.197397421 Test RE 1.2625855558436263\n",
      "84 Train Loss 11667165000000.0 Test MSE 7991371.567105594 Test RE 1.263564703258932\n",
      "85 Train Loss 11668271000000.0 Test MSE 7978813.766453509 Test RE 1.2625715175406116\n",
      "86 Train Loss 11660638000000.0 Test MSE 7962426.599154335 Test RE 1.2612742943153832\n",
      "87 Train Loss 11661791000000.0 Test MSE 7962570.454179965 Test RE 1.261285687816106\n",
      "88 Train Loss 11655703000000.0 Test MSE 7953389.499977423 Test RE 1.2605583381463092\n",
      "89 Train Loss 11660761000000.0 Test MSE 7951301.835372562 Test RE 1.2603928869421677\n",
      "90 Train Loss 11659768000000.0 Test MSE 7971975.907990057 Test RE 1.2620303884913469\n",
      "91 Train Loss 11662070000000.0 Test MSE 7985887.737676991 Test RE 1.263131087928524\n",
      "92 Train Loss 11663425000000.0 Test MSE 8003465.4278124785 Test RE 1.2645204589877002\n",
      "93 Train Loss 11667636000000.0 Test MSE 8017682.214427766 Test RE 1.2656430627728688\n",
      "94 Train Loss 11673629000000.0 Test MSE 8013481.359717608 Test RE 1.2653114532695018\n",
      "95 Train Loss 11679200000000.0 Test MSE 8016009.8057166515 Test RE 1.2655110556155973\n",
      "96 Train Loss 11685107000000.0 Test MSE 8026409.0918938415 Test RE 1.2663316725069589\n",
      "97 Train Loss 11687008000000.0 Test MSE 8039710.0868683765 Test RE 1.2673804913959106\n",
      "98 Train Loss 11686337000000.0 Test MSE 8058383.573920343 Test RE 1.268851482677801\n",
      "99 Train Loss 11687022000000.0 Test MSE 8064915.961878275 Test RE 1.2693656646469258\n",
      "100 Train Loss 11685781000000.0 Test MSE 8076803.901624551 Test RE 1.2703008625998315\n",
      "101 Train Loss 11677578000000.0 Test MSE 8086389.390962506 Test RE 1.2710544307769087\n",
      "102 Train Loss 11671559000000.0 Test MSE 8105661.638348242 Test RE 1.2725681779063867\n",
      "103 Train Loss 11664719000000.0 Test MSE 8108925.183156645 Test RE 1.2728243362254492\n",
      "104 Train Loss 11663067000000.0 Test MSE 8109023.263297958 Test RE 1.2728320338185612\n",
      "105 Train Loss 11663348000000.0 Test MSE 8114163.908338178 Test RE 1.2732354203177814\n",
      "106 Train Loss 11659102000000.0 Test MSE 8123181.810353116 Test RE 1.2739427466657471\n",
      "107 Train Loss 11661002000000.0 Test MSE 8121258.740380514 Test RE 1.2737919420849444\n",
      "108 Train Loss 11659665000000.0 Test MSE 8111431.094325902 Test RE 1.273020992272111\n",
      "109 Train Loss 11654847000000.0 Test MSE 8117776.3027736675 Test RE 1.2735188085264333\n",
      "110 Train Loss 11624276000000.0 Test MSE 8116481.827188896 Test RE 1.273417265648932\n",
      "111 Train Loss 11604380000000.0 Test MSE 8120162.263642849 Test RE 1.273705949850021\n",
      "112 Train Loss 11573227000000.0 Test MSE 8131260.974090084 Test RE 1.2745761089614012\n",
      "113 Train Loss 11548111000000.0 Test MSE 8148868.370152521 Test RE 1.2759553434144695\n",
      "114 Train Loss 11512972000000.0 Test MSE 8149650.305188848 Test RE 1.276016559905484\n",
      "115 Train Loss 11505140000000.0 Test MSE 8126936.151486684 Test RE 1.2742371056539532\n",
      "116 Train Loss 11491850000000.0 Test MSE 8115983.552682245 Test RE 1.2733781772167858\n",
      "117 Train Loss 11468005000000.0 Test MSE 8116837.08306897 Test RE 1.2734451338824178\n",
      "118 Train Loss 11458534000000.0 Test MSE 8129374.499161916 Test RE 1.2744282478020101\n",
      "119 Train Loss 11447213000000.0 Test MSE 8151418.365481895 Test RE 1.2761549677989665\n",
      "120 Train Loss 11447128000000.0 Test MSE 8148590.556045779 Test RE 1.275933593068337\n",
      "121 Train Loss 11447845000000.0 Test MSE 8150232.953111153 Test RE 1.2760621726055248\n",
      "122 Train Loss 11442196000000.0 Test MSE 8163538.574925365 Test RE 1.2771033622724075\n",
      "123 Train Loss 11435246000000.0 Test MSE 8176943.4940669695 Test RE 1.2781514644629306\n",
      "124 Train Loss 11431358000000.0 Test MSE 8193066.601064435 Test RE 1.2794109585731972\n",
      "125 Train Loss 11422381000000.0 Test MSE 8222543.872106401 Test RE 1.2817104445031107\n",
      "126 Train Loss 11417498000000.0 Test MSE 8227872.008055393 Test RE 1.2821256457873145\n",
      "127 Train Loss 11409286000000.0 Test MSE 8238026.348986032 Test RE 1.2829165627175132\n",
      "128 Train Loss 11404578000000.0 Test MSE 8238144.975200481 Test RE 1.2829257995772276\n",
      "129 Train Loss 11401477000000.0 Test MSE 8234577.779832642 Test RE 1.2826480099472868\n",
      "130 Train Loss 11394475000000.0 Test MSE 8241836.310233823 Test RE 1.2832131930735784\n",
      "131 Train Loss 11379705000000.0 Test MSE 8268524.297693322 Test RE 1.2852891078158613\n",
      "132 Train Loss 11370307000000.0 Test MSE 8275566.094716739 Test RE 1.285836292447382\n",
      "133 Train Loss 11357974000000.0 Test MSE 8308757.724181065 Test RE 1.2884123272869825\n",
      "134 Train Loss 11350050000000.0 Test MSE 8331596.812469599 Test RE 1.2901819040383442\n",
      "135 Train Loss 11344382000000.0 Test MSE 8345472.411948675 Test RE 1.2912558038174762\n",
      "136 Train Loss 11344633000000.0 Test MSE 8347574.015192742 Test RE 1.2914183791894007\n",
      "137 Train Loss 11332632000000.0 Test MSE 8358130.938785501 Test RE 1.2922347299887826\n",
      "138 Train Loss 11312755000000.0 Test MSE 8380701.751211924 Test RE 1.2939783688184006\n",
      "139 Train Loss 11288289000000.0 Test MSE 8423207.074641729 Test RE 1.2972556246133136\n",
      "140 Train Loss 11278914000000.0 Test MSE 8448130.866541462 Test RE 1.2991734598708566\n",
      "141 Train Loss 11276365000000.0 Test MSE 8464755.055309497 Test RE 1.3004510851028785\n",
      "142 Train Loss 11274576000000.0 Test MSE 8471295.141394168 Test RE 1.3009533689738926\n",
      "143 Train Loss 11263648000000.0 Test MSE 8439050.714503655 Test RE 1.2984750885839602\n",
      "144 Train Loss 11257316000000.0 Test MSE 8436053.473516207 Test RE 1.2982444827664552\n",
      "145 Train Loss 11252894000000.0 Test MSE 8437755.514348917 Test RE 1.298375441732298\n",
      "146 Train Loss 11247826000000.0 Test MSE 8445566.979517099 Test RE 1.2989763046190113\n",
      "147 Train Loss 11247977000000.0 Test MSE 8457656.752759226 Test RE 1.2999057101205058\n",
      "148 Train Loss 11245944000000.0 Test MSE 8458515.799521454 Test RE 1.2999717243529454\n",
      "149 Train Loss 11242908000000.0 Test MSE 8454733.507982044 Test RE 1.2996810455891747\n",
      "150 Train Loss 11238004000000.0 Test MSE 8470839.55335322 Test RE 1.300918385730131\n",
      "151 Train Loss 11229683000000.0 Test MSE 8493915.487996385 Test RE 1.30268913646195\n",
      "152 Train Loss 11224690000000.0 Test MSE 8502570.139104003 Test RE 1.3033526378552809\n",
      "153 Train Loss 11216403000000.0 Test MSE 8507896.750889953 Test RE 1.3037608301099672\n",
      "154 Train Loss 11210806000000.0 Test MSE 8505823.442531792 Test RE 1.3036019622366057\n",
      "155 Train Loss 11208638000000.0 Test MSE 8518023.11651435 Test RE 1.304536487896471\n",
      "156 Train Loss 11208245000000.0 Test MSE 8519844.098189032 Test RE 1.3046759222499005\n",
      "157 Train Loss 11203375000000.0 Test MSE 8514823.233394166 Test RE 1.3042914335957505\n",
      "158 Train Loss 11197942000000.0 Test MSE 8515912.484400027 Test RE 1.3043748560746724\n",
      "159 Train Loss 11191898000000.0 Test MSE 8525464.336588915 Test RE 1.3051061754781055\n",
      "160 Train Loss 11182810000000.0 Test MSE 8534347.325976556 Test RE 1.3057859171066022\n",
      "161 Train Loss 11176264000000.0 Test MSE 8548614.229556955 Test RE 1.306876905192956\n",
      "162 Train Loss 11167913000000.0 Test MSE 8562890.173971098 Test RE 1.3079676737623394\n",
      "163 Train Loss 11156862000000.0 Test MSE 8568500.371324724 Test RE 1.308396077905904\n",
      "164 Train Loss 11148015000000.0 Test MSE 8568342.439511444 Test RE 1.3083840198847632\n",
      "165 Train Loss 11141045000000.0 Test MSE 8578669.975114638 Test RE 1.3091722886445802\n",
      "166 Train Loss 11129825000000.0 Test MSE 8580986.87916183 Test RE 1.3093490655697373\n",
      "167 Train Loss 11121359000000.0 Test MSE 8580476.034090705 Test RE 1.3093100907710837\n",
      "168 Train Loss 11117094000000.0 Test MSE 8583370.021052135 Test RE 1.3095308714640677\n",
      "169 Train Loss 11107467000000.0 Test MSE 8624882.201383451 Test RE 1.3126937266263734\n",
      "170 Train Loss 11100977000000.0 Test MSE 8637162.627690455 Test RE 1.313627925149876\n",
      "171 Train Loss 11092772000000.0 Test MSE 8637133.416942429 Test RE 1.3136257038135175\n",
      "172 Train Loss 11070270000000.0 Test MSE 8676328.410634873 Test RE 1.3166029225677425\n",
      "173 Train Loss 11064101000000.0 Test MSE 8685362.822910218 Test RE 1.3172882146776241\n",
      "174 Train Loss 11060197000000.0 Test MSE 8680857.951557536 Test RE 1.3169465488346024\n",
      "175 Train Loss 11052005000000.0 Test MSE 8700668.389030267 Test RE 1.318448383093766\n",
      "176 Train Loss 11034032000000.0 Test MSE 8725390.551326867 Test RE 1.3203201803181401\n",
      "177 Train Loss 11015594000000.0 Test MSE 8731482.386783455 Test RE 1.3207810061028826\n",
      "178 Train Loss 11005490000000.0 Test MSE 8741880.789063076 Test RE 1.3215672371836613\n",
      "179 Train Loss 10983654000000.0 Test MSE 8775863.473578796 Test RE 1.3241334379056993\n",
      "180 Train Loss 10953039000000.0 Test MSE 8769958.211445572 Test RE 1.3236878594777846\n",
      "181 Train Loss 10946020000000.0 Test MSE 8778874.492577177 Test RE 1.3243605750428105\n",
      "182 Train Loss 10934857000000.0 Test MSE 8800932.68980159 Test RE 1.3260233553526664\n",
      "183 Train Loss 10927797000000.0 Test MSE 8801682.137699291 Test RE 1.3260798132472624\n",
      "184 Train Loss 10921942000000.0 Test MSE 8810210.799429731 Test RE 1.3267221306808508\n",
      "185 Train Loss 10909389000000.0 Test MSE 8813547.26425965 Test RE 1.3269733245976987\n",
      "186 Train Loss 10902489000000.0 Test MSE 8822231.056313172 Test RE 1.3276268823018211\n",
      "187 Train Loss 10889195000000.0 Test MSE 8834875.89219266 Test RE 1.328577980320154\n",
      "188 Train Loss 10875487000000.0 Test MSE 8844256.224246627 Test RE 1.3292830947179555\n",
      "189 Train Loss 10868422000000.0 Test MSE 8847425.808779849 Test RE 1.3295212661148543\n",
      "190 Train Loss 10864514000000.0 Test MSE 8845747.024969274 Test RE 1.3293951229454066\n",
      "191 Train Loss 10848621000000.0 Test MSE 8848448.48684525 Test RE 1.3295981038853373\n",
      "192 Train Loss 10837171000000.0 Test MSE 8851726.02728743 Test RE 1.3298443282994776\n",
      "193 Train Loss 10807270000000.0 Test MSE 8861751.097679049 Test RE 1.3305971764078717\n",
      "194 Train Loss 10785253000000.0 Test MSE 8853785.894773033 Test RE 1.3299990520075113\n",
      "195 Train Loss 10760935000000.0 Test MSE 8865118.980986757 Test RE 1.3308499972259724\n",
      "196 Train Loss 10745410000000.0 Test MSE 8876236.671906192 Test RE 1.3316842411307133\n",
      "197 Train Loss 10735115000000.0 Test MSE 8872142.5642655 Test RE 1.3313770902707818\n",
      "198 Train Loss 10731991000000.0 Test MSE 8862545.51150567 Test RE 1.3306568159246137\n",
      "199 Train Loss 10730546000000.0 Test MSE 8857760.847432675 Test RE 1.3302975734589098\n",
      "Training time: 46.74\n",
      "Training time: 46.74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "7\n",
      "0 Train Loss 12565998000000.0 Test MSE 5247319.392355869 Test RE 1.0238953564835733\n",
      "1 Train Loss 11862957000000.0 Test MSE 5410382.172998642 Test RE 1.0396826470147675\n",
      "2 Train Loss 11613241000000.0 Test MSE 5649735.141982359 Test RE 1.06243132729253\n",
      "3 Train Loss 11464871000000.0 Test MSE 5731498.794959788 Test RE 1.0700915295877653\n",
      "4 Train Loss 11205672000000.0 Test MSE 5850707.826108299 Test RE 1.081162637426902\n",
      "5 Train Loss 10859744000000.0 Test MSE 6196102.843484233 Test RE 1.11261813002823\n",
      "6 Train Loss 10689959000000.0 Test MSE 6401682.857563451 Test RE 1.1309252516692498\n",
      "7 Train Loss 10590200000000.0 Test MSE 6644839.791502748 Test RE 1.1522032101831305\n",
      "8 Train Loss 10626171000000.0 Test MSE 6785634.930932507 Test RE 1.1643460361994684\n",
      "9 Train Loss 10705912000000.0 Test MSE 6804583.04816452 Test RE 1.1659705551544681\n",
      "10 Train Loss 10773568000000.0 Test MSE 6910318.794867509 Test RE 1.1749945852970756\n",
      "11 Train Loss 10845345000000.0 Test MSE 7040444.900469182 Test RE 1.1860059713824231\n",
      "12 Train Loss 10920808000000.0 Test MSE 7214838.100969504 Test RE 1.2006049199025002\n",
      "13 Train Loss 10970787000000.0 Test MSE 7419301.921837927 Test RE 1.2174982509635386\n",
      "14 Train Loss 10992334000000.0 Test MSE 7547451.846275339 Test RE 1.2279678573286483\n",
      "15 Train Loss 10933025000000.0 Test MSE 7799784.399711253 Test RE 1.2483263066700059\n",
      "16 Train Loss 10825996000000.0 Test MSE 8117576.881859334 Test RE 1.2735031658285136\n",
      "17 Train Loss 10784292000000.0 Test MSE 8151727.887299261 Test RE 1.2761791963465376\n",
      "18 Train Loss 10821723000000.0 Test MSE 8179843.986827852 Test RE 1.278378134756913\n",
      "19 Train Loss 10854393000000.0 Test MSE 8181169.324162569 Test RE 1.2784816950234197\n",
      "20 Train Loss 10881725000000.0 Test MSE 8187840.663911374 Test RE 1.2790028581566064\n",
      "21 Train Loss 10903787000000.0 Test MSE 8043545.053073047 Test RE 1.2676827275360734\n",
      "22 Train Loss 10942433000000.0 Test MSE 7904252.117410578 Test RE 1.2566583340184592\n",
      "23 Train Loss 10979046000000.0 Test MSE 8012376.817508535 Test RE 1.2652242478460585\n",
      "24 Train Loss 11016897000000.0 Test MSE 8107357.920554651 Test RE 1.2727013269280016\n",
      "25 Train Loss 11069006000000.0 Test MSE 8187411.623489899 Test RE 1.2789693480324755\n",
      "26 Train Loss 11131178000000.0 Test MSE 8199928.505792859 Test RE 1.279946616288565\n",
      "27 Train Loss 11194817000000.0 Test MSE 8209075.525992386 Test RE 1.280660307534829\n",
      "28 Train Loss 11247159000000.0 Test MSE 8131112.51206954 Test RE 1.2745644731887444\n",
      "29 Train Loss 11306282000000.0 Test MSE 8148956.760437146 Test RE 1.2759622635011008\n",
      "30 Train Loss 11365943000000.0 Test MSE 8142168.9929065155 Test RE 1.275430739050476\n",
      "31 Train Loss 11425960000000.0 Test MSE 8142896.740010538 Test RE 1.275487736779707\n",
      "32 Train Loss 11483345000000.0 Test MSE 8161138.080538687 Test RE 1.2769155818862636\n",
      "33 Train Loss 11538397000000.0 Test MSE 8151991.08829712 Test RE 1.2761997986612443\n",
      "34 Train Loss 11590557000000.0 Test MSE 8154908.199259622 Test RE 1.276428116099207\n",
      "35 Train Loss 11640611000000.0 Test MSE 8154763.153786444 Test RE 1.2764167645953763\n",
      "36 Train Loss 11685357000000.0 Test MSE 8149648.171465135 Test RE 1.2760163928635333\n",
      "37 Train Loss 11559669000000.0 Test MSE 8012693.287897217 Test RE 1.2652492343179198\n",
      "38 Train Loss 11503536000000.0 Test MSE 8026830.320815894 Test RE 1.266364900848611\n",
      "39 Train Loss 11435062000000.0 Test MSE 8069158.67301879 Test RE 1.269699508406987\n",
      "40 Train Loss 11393137000000.0 Test MSE 8037836.376760031 Test RE 1.267232796890226\n",
      "41 Train Loss 11370763000000.0 Test MSE 8031756.560861554 Test RE 1.266753439061593\n",
      "42 Train Loss 11381850000000.0 Test MSE 8088334.327333926 Test RE 1.2712072784350175\n",
      "43 Train Loss 11395717000000.0 Test MSE 8134598.675591376 Test RE 1.274837674672832\n",
      "44 Train Loss 11406389000000.0 Test MSE 8189187.497736326 Test RE 1.279108046664143\n",
      "45 Train Loss 11421216000000.0 Test MSE 8226030.31374241 Test RE 1.28198214479107\n",
      "46 Train Loss 11428900000000.0 Test MSE 8280553.687618097 Test RE 1.2862237137963675\n",
      "47 Train Loss 11452920000000.0 Test MSE 8305269.89366033 Test RE 1.2881418755731422\n",
      "48 Train Loss 11480194000000.0 Test MSE 8323252.881043976 Test RE 1.2895356962219784\n",
      "49 Train Loss 11504348000000.0 Test MSE 8332065.258811981 Test RE 1.2902181739462881\n",
      "50 Train Loss 11530575000000.0 Test MSE 8326516.572209437 Test RE 1.2897884960485368\n",
      "51 Train Loss 11553685000000.0 Test MSE 8310642.7101455135 Test RE 1.2885584683531237\n",
      "52 Train Loss 11575235000000.0 Test MSE 8320749.002382515 Test RE 1.2893417165467145\n",
      "53 Train Loss 11596643000000.0 Test MSE 8368123.7938987445 Test RE 1.293006987395854\n",
      "54 Train Loss 11609945000000.0 Test MSE 8379348.427582382 Test RE 1.2938738881761143\n",
      "55 Train Loss 11618088000000.0 Test MSE 8415087.685072962 Test RE 1.296630241480149\n",
      "56 Train Loss 11634028000000.0 Test MSE 8428137.94230313 Test RE 1.2976352697946658\n",
      "57 Train Loss 11654286000000.0 Test MSE 8451424.890913233 Test RE 1.2994267165989606\n",
      "58 Train Loss 11661288000000.0 Test MSE 8449436.316613752 Test RE 1.2992738335866523\n",
      "59 Train Loss 11670838000000.0 Test MSE 8454722.84400894 Test RE 1.2996802259437026\n",
      "60 Train Loss 11672335000000.0 Test MSE 8606498.825166397 Test RE 1.3112940196736735\n",
      "61 Train Loss 11671048000000.0 Test MSE 8714257.26667171 Test RE 1.3194775709060267\n",
      "62 Train Loss 11677265000000.0 Test MSE 8801971.6449912 Test RE 1.326101621954548\n",
      "63 Train Loss 11683846000000.0 Test MSE 8865892.454183364 Test RE 1.3309080536576858\n",
      "64 Train Loss 11693203000000.0 Test MSE 8978819.86739465 Test RE 1.3393573122969737\n",
      "65 Train Loss 11702833000000.0 Test MSE 9087661.10824594 Test RE 1.3474507028189318\n",
      "66 Train Loss 11713800000000.0 Test MSE 9140469.718182242 Test RE 1.351360065373896\n",
      "67 Train Loss 11716611000000.0 Test MSE 9190419.708996283 Test RE 1.3550474281972418\n",
      "68 Train Loss 11723762000000.0 Test MSE 9184997.88493355 Test RE 1.354647668836008\n",
      "69 Train Loss 11731436000000.0 Test MSE 9181579.821469963 Test RE 1.3543955891614592\n",
      "70 Train Loss 11742237000000.0 Test MSE 9193265.14203898 Test RE 1.355257179136571\n",
      "71 Train Loss 11754578000000.0 Test MSE 9227359.041724995 Test RE 1.357767888947778\n",
      "72 Train Loss 11767849000000.0 Test MSE 9257246.936913494 Test RE 1.359965051561365\n",
      "73 Train Loss 11780605000000.0 Test MSE 9251627.85225816 Test RE 1.3595522442342614\n",
      "74 Train Loss 11793431000000.0 Test MSE 9276838.880585428 Test RE 1.361403399069155\n",
      "75 Train Loss 11807596000000.0 Test MSE 9301122.222805956 Test RE 1.3631840605094065\n",
      "76 Train Loss 11819745000000.0 Test MSE 9305110.278062064 Test RE 1.3634762763626824\n",
      "77 Train Loss 11830002000000.0 Test MSE 9314705.05100471 Test RE 1.3641790554947462\n",
      "78 Train Loss 11836961000000.0 Test MSE 9342832.172740526 Test RE 1.3662371726071882\n",
      "79 Train Loss 11833705000000.0 Test MSE 9386446.200687356 Test RE 1.3694223805894599\n",
      "80 Train Loss 11836294000000.0 Test MSE 9462607.528464323 Test RE 1.3749668812909823\n",
      "81 Train Loss 11844007000000.0 Test MSE 9503077.415225953 Test RE 1.3779039884800772\n",
      "82 Train Loss 11852350000000.0 Test MSE 9512789.975596992 Test RE 1.3786079476997513\n",
      "83 Train Loss 11861950000000.0 Test MSE 9530984.844936376 Test RE 1.3799257319047145\n",
      "84 Train Loss 11869960000000.0 Test MSE 9564034.918630663 Test RE 1.3823162077767084\n",
      "85 Train Loss 11875613000000.0 Test MSE 9562629.240355995 Test RE 1.3822146207749633\n",
      "86 Train Loss 11875675000000.0 Test MSE 9551110.794709919 Test RE 1.3813819124905626\n",
      "87 Train Loss 11879514000000.0 Test MSE 9583868.968920965 Test RE 1.3837488002817266\n",
      "88 Train Loss 11886847000000.0 Test MSE 9600203.892677741 Test RE 1.3849275417689584\n",
      "89 Train Loss 11890799000000.0 Test MSE 9595904.046741702 Test RE 1.3846173586690358\n",
      "90 Train Loss 11895980000000.0 Test MSE 9626353.215852555 Test RE 1.3868124127038424\n",
      "91 Train Loss 11901056000000.0 Test MSE 9630231.599169571 Test RE 1.3870917525725177\n",
      "92 Train Loss 11907385000000.0 Test MSE 9626325.360032855 Test RE 1.3868104061898696\n",
      "93 Train Loss 11914554000000.0 Test MSE 9655713.351809286 Test RE 1.3889256741263423\n",
      "94 Train Loss 11922510000000.0 Test MSE 9670382.34269278 Test RE 1.3899803039122414\n",
      "95 Train Loss 11930138000000.0 Test MSE 9694032.956523068 Test RE 1.391678986087571\n",
      "96 Train Loss 11937214000000.0 Test MSE 9734775.032590836 Test RE 1.3946003935998972\n",
      "97 Train Loss 11943343000000.0 Test MSE 9787880.583078388 Test RE 1.3983991609437658\n",
      "98 Train Loss 11950468000000.0 Test MSE 9806618.388190096 Test RE 1.3997370605057045\n",
      "99 Train Loss 11957994000000.0 Test MSE 9800484.976728706 Test RE 1.3992992691194546\n",
      "100 Train Loss 11965094000000.0 Test MSE 9795039.339009205 Test RE 1.3989104549068592\n",
      "101 Train Loss 11971347000000.0 Test MSE 9808351.638918048 Test RE 1.3998607518738038\n",
      "102 Train Loss 11977783000000.0 Test MSE 9813927.471397815 Test RE 1.400258590386935\n",
      "103 Train Loss 11983667000000.0 Test MSE 9807194.153994562 Test RE 1.399778150557027\n",
      "104 Train Loss 11989406000000.0 Test MSE 9788464.674694631 Test RE 1.3984408850450136\n",
      "105 Train Loss 11996425000000.0 Test MSE 9781313.973853614 Test RE 1.3979299949468194\n",
      "106 Train Loss 12002327000000.0 Test MSE 9834018.559059935 Test RE 1.4016911633580647\n",
      "107 Train Loss 12008589000000.0 Test MSE 9864726.918997569 Test RE 1.4038779645187576\n",
      "108 Train Loss 12013465000000.0 Test MSE 9884260.635267464 Test RE 1.4052672270924444\n",
      "109 Train Loss 12015499000000.0 Test MSE 9906599.615366122 Test RE 1.4068543220073255\n",
      "110 Train Loss 12021343000000.0 Test MSE 9902064.106624164 Test RE 1.406532237194334\n",
      "111 Train Loss 12025741000000.0 Test MSE 9911719.98440977 Test RE 1.4072178515238378\n",
      "112 Train Loss 12030558000000.0 Test MSE 9934064.081517013 Test RE 1.4088031117884714\n",
      "113 Train Loss 12036194000000.0 Test MSE 9942332.687016223 Test RE 1.4093892975678501\n",
      "114 Train Loss 12042808000000.0 Test MSE 9926761.20452028 Test RE 1.4082851864321497\n",
      "115 Train Loss 12048781000000.0 Test MSE 9914453.861963812 Test RE 1.4074119094702358\n",
      "116 Train Loss 12053849000000.0 Test MSE 9923086.905350935 Test RE 1.4080245304189496\n",
      "117 Train Loss 12060020000000.0 Test MSE 9927289.102113485 Test RE 1.4083226317006337\n",
      "118 Train Loss 12065119000000.0 Test MSE 9931105.494644653 Test RE 1.408593309599751\n",
      "119 Train Loss 12068248000000.0 Test MSE 9911497.972636556 Test RE 1.4072020913590795\n",
      "120 Train Loss 12072550000000.0 Test MSE 9880214.965083286 Test RE 1.4049796067129645\n",
      "121 Train Loss 12077390000000.0 Test MSE 9867326.226632683 Test RE 1.4040629098493702\n",
      "122 Train Loss 12081853000000.0 Test MSE 9877210.844908454 Test RE 1.4047659955477279\n",
      "123 Train Loss 12086017000000.0 Test MSE 9855068.727650529 Test RE 1.4031905535794236\n",
      "124 Train Loss 12086421000000.0 Test MSE 9819231.662818164 Test RE 1.4006369422920781\n",
      "125 Train Loss 12087050000000.0 Test MSE 9791876.7003078 Test RE 1.3986845953989901\n",
      "126 Train Loss 12089478000000.0 Test MSE 9793327.840320196 Test RE 1.3987882329370087\n",
      "127 Train Loss 12090120000000.0 Test MSE 9788628.766450431 Test RE 1.3984526065798066\n",
      "128 Train Loss 12093170000000.0 Test MSE 9805144.189786358 Test RE 1.3996318474947613\n",
      "129 Train Loss 12094816000000.0 Test MSE 9830499.350579416 Test RE 1.4014403358455794\n",
      "130 Train Loss 12098036000000.0 Test MSE 9814099.923044411 Test RE 1.4002708930985386\n",
      "131 Train Loss 12100382000000.0 Test MSE 9805683.4331838 Test RE 1.3996703340198617\n",
      "132 Train Loss 12104855000000.0 Test MSE 9824603.089089597 Test RE 1.4010199859823615\n",
      "133 Train Loss 12109272000000.0 Test MSE 9837553.270792611 Test RE 1.401943050673328\n",
      "134 Train Loss 12113752000000.0 Test MSE 9856304.518656077 Test RE 1.4032785284051483\n",
      "135 Train Loss 12117955000000.0 Test MSE 9854880.249802979 Test RE 1.4031771355299378\n",
      "136 Train Loss 12119765000000.0 Test MSE 9840824.353205826 Test RE 1.4021761111703233\n",
      "137 Train Loss 12120734000000.0 Test MSE 9853764.578422962 Test RE 1.4030977064125272\n",
      "138 Train Loss 12121482000000.0 Test MSE 9861399.18757728 Test RE 1.4036411549799004\n",
      "139 Train Loss 12123559000000.0 Test MSE 9844745.007564431 Test RE 1.402455401819052\n",
      "140 Train Loss 12122069000000.0 Test MSE 9811124.146880837 Test RE 1.4000585858725962\n",
      "141 Train Loss 12121387000000.0 Test MSE 9818901.891104065 Test RE 1.4006134224109263\n",
      "142 Train Loss 12116913000000.0 Test MSE 9753917.364915019 Test RE 1.3959708820484622\n",
      "143 Train Loss 12107774000000.0 Test MSE 9704559.159066696 Test RE 1.392434353871569\n",
      "144 Train Loss 12106251000000.0 Test MSE 9744229.619940246 Test RE 1.3952774596482342\n",
      "145 Train Loss 12107630000000.0 Test MSE 9755209.966470791 Test RE 1.3960633769040636\n",
      "146 Train Loss 12106895000000.0 Test MSE 9730881.83061961 Test RE 1.3943214963506436\n",
      "147 Train Loss 12104630000000.0 Test MSE 9727465.972226659 Test RE 1.3940767486023302\n",
      "148 Train Loss 12100705000000.0 Test MSE 9708508.124370871 Test RE 1.39271762875285\n",
      "149 Train Loss 12095719000000.0 Test MSE 9704600.719052538 Test RE 1.3924373354335924\n",
      "150 Train Loss 12093007000000.0 Test MSE 9727000.190726817 Test RE 1.3940433718249683\n",
      "151 Train Loss 12092811000000.0 Test MSE 9764013.251304545 Test RE 1.3966931517718741\n",
      "152 Train Loss 12093115000000.0 Test MSE 9793683.660272809 Test RE 1.3988136437192313\n",
      "153 Train Loss 12093528000000.0 Test MSE 9802986.995928766 Test RE 1.3994778750875652\n",
      "154 Train Loss 12096226000000.0 Test MSE 9789207.374079771 Test RE 1.3984939373631966\n",
      "155 Train Loss 12096269000000.0 Test MSE 9757802.316531025 Test RE 1.3962488595635671\n",
      "156 Train Loss 12094128000000.0 Test MSE 9762564.279577758 Test RE 1.3965895138556388\n",
      "157 Train Loss 12084654000000.0 Test MSE 9859441.112150112 Test RE 1.4035017948522224\n",
      "158 Train Loss 12083086000000.0 Test MSE 9900458.75894175 Test RE 1.4064182172906348\n",
      "159 Train Loss 12083523000000.0 Test MSE 9918124.081729935 Test RE 1.407672389427882\n",
      "160 Train Loss 12084056000000.0 Test MSE 9942241.258781265 Test RE 1.4093828172841913\n",
      "161 Train Loss 12086707000000.0 Test MSE 9965820.367012568 Test RE 1.411053080011287\n",
      "162 Train Loss 12088779000000.0 Test MSE 9961027.386370057 Test RE 1.4107137219245904\n",
      "163 Train Loss 12091285000000.0 Test MSE 9960960.61883005 Test RE 1.4107089939964834\n",
      "164 Train Loss 12093590000000.0 Test MSE 9981405.194661349 Test RE 1.4121559710638827\n",
      "165 Train Loss 12095668000000.0 Test MSE 9974235.416699557 Test RE 1.4116486946123763\n",
      "166 Train Loss 12096371000000.0 Test MSE 9971879.253321478 Test RE 1.4114819514352461\n",
      "167 Train Loss 12095135000000.0 Test MSE 9961446.763127893 Test RE 1.4107434183754206\n",
      "168 Train Loss 12094548000000.0 Test MSE 9947206.30833793 Test RE 1.4097346887550901\n",
      "169 Train Loss 12094308000000.0 Test MSE 9976215.710426362 Test RE 1.4117888226621533\n",
      "170 Train Loss 12095339000000.0 Test MSE 9990334.27345665 Test RE 1.4127874669796814\n",
      "171 Train Loss 12096066000000.0 Test MSE 9977149.59236307 Test RE 1.411854900484871\n",
      "172 Train Loss 12095737000000.0 Test MSE 9972111.614166642 Test RE 1.411498396240698\n",
      "173 Train Loss 12094284000000.0 Test MSE 9975712.681060702 Test RE 1.411753228995729\n",
      "174 Train Loss 12093969000000.0 Test MSE 9973226.698979462 Test RE 1.4115773111430114\n",
      "175 Train Loss 12091716000000.0 Test MSE 9957264.05827126 Test RE 1.4104472092488474\n",
      "176 Train Loss 12089484000000.0 Test MSE 9947851.0006998 Test RE 1.4097803714539565\n",
      "177 Train Loss 12073682000000.0 Test MSE 9898325.22673106 Test RE 1.4062666687454148\n",
      "178 Train Loss 12048528000000.0 Test MSE 9864068.401982954 Test RE 1.4038311060014108\n",
      "179 Train Loss 12025100000000.0 Test MSE 9909069.165073838 Test RE 1.4070296637149018\n",
      "180 Train Loss 12013021000000.0 Test MSE 9887863.521476457 Test RE 1.405523318913899\n",
      "181 Train Loss 11999543000000.0 Test MSE 9805262.277101982 Test RE 1.3996402756354858\n",
      "182 Train Loss 11983902000000.0 Test MSE 9720552.52013697 Test RE 1.3935812652003245\n",
      "183 Train Loss 11961641000000.0 Test MSE 9505273.87870228 Test RE 1.3780632180007648\n",
      "184 Train Loss 11924660000000.0 Test MSE 9231472.52890135 Test RE 1.3580704965767565\n",
      "185 Train Loss 11879180000000.0 Test MSE 8967921.434616279 Test RE 1.3385442139196673\n",
      "186 Train Loss 11850043000000.0 Test MSE 8949435.834434846 Test RE 1.3371639298920543\n",
      "187 Train Loss 11816504000000.0 Test MSE 9003878.013567116 Test RE 1.3412249534956107\n",
      "188 Train Loss 11806838000000.0 Test MSE 8955864.306447443 Test RE 1.3376440929983144\n",
      "189 Train Loss 11788024000000.0 Test MSE 8962386.240929678 Test RE 1.3381310610417543\n",
      "190 Train Loss 11774091000000.0 Test MSE 9003003.581381125 Test RE 1.3411598238513256\n",
      "191 Train Loss 11760505000000.0 Test MSE 9054542.504285192 Test RE 1.344993171788994\n",
      "192 Train Loss 11750899000000.0 Test MSE 9075476.935751902 Test RE 1.3465471106733637\n",
      "193 Train Loss 11732256000000.0 Test MSE 9026015.47533699 Test RE 1.3428727483839367\n",
      "194 Train Loss 11712811000000.0 Test MSE 8996379.355505612 Test RE 1.3406663340774594\n",
      "195 Train Loss 11675497000000.0 Test MSE 9045906.925246555 Test RE 1.3443516393483028\n",
      "196 Train Loss 11622973000000.0 Test MSE 9000255.430762354 Test RE 1.340955114918722\n",
      "197 Train Loss 11575011000000.0 Test MSE 9148228.895462487 Test RE 1.3519335161022998\n",
      "198 Train Loss 11551257000000.0 Test MSE 9276132.96907423 Test RE 1.361351600788189\n",
      "199 Train Loss 11531549000000.0 Test MSE 9307048.323820818 Test RE 1.363618259732983\n",
      "Training time: 47.67\n",
      "Training time: 47.67\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "8\n",
      "0 Train Loss nan Test MSE nan Test RE nan\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 55.21\n",
      "Training time: 55.21\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "9\n",
      "0 Train Loss 12515660000000.0 Test MSE 5460002.30419184 Test RE 1.0444393760490431\n",
      "1 Train Loss 12415809000000.0 Test MSE 5622220.608553253 Test RE 1.0598411191490715\n",
      "2 Train Loss 12376226000000.0 Test MSE 5687864.070711429 Test RE 1.0660103668054692\n",
      "3 Train Loss 11394037000000.0 Test MSE 5799047.170197774 Test RE 1.0763788220185362\n",
      "4 Train Loss 11050906000000.0 Test MSE 5883263.000252996 Test RE 1.0841664287917103\n",
      "5 Train Loss 10967231000000.0 Test MSE 6004788.872919057 Test RE 1.0953065752802016\n",
      "6 Train Loss 10992922000000.0 Test MSE 6034105.88176053 Test RE 1.0979771116492016\n",
      "7 Train Loss 11092291000000.0 Test MSE 6062330.493631559 Test RE 1.1005420171980342\n",
      "8 Train Loss 11103677000000.0 Test MSE 6072988.207478116 Test RE 1.1015089812523142\n",
      "9 Train Loss 11121849000000.0 Test MSE 6119572.625751954 Test RE 1.1057256145050154\n",
      "10 Train Loss 11162773000000.0 Test MSE 6186843.752149909 Test RE 1.1117865037110197\n",
      "11 Train Loss 11221438000000.0 Test MSE 6331255.423560618 Test RE 1.12468717030554\n",
      "12 Train Loss 11161482000000.0 Test MSE 6567827.749764431 Test RE 1.1455068770047583\n",
      "13 Train Loss 11089651000000.0 Test MSE 6641589.694286125 Test RE 1.1519213952272451\n",
      "14 Train Loss 11123101000000.0 Test MSE 6695925.391596729 Test RE 1.156623805417712\n",
      "15 Train Loss 11133284000000.0 Test MSE 6868618.964923283 Test RE 1.171444009913868\n",
      "16 Train Loss 11150000000000.0 Test MSE 7048241.548087802 Test RE 1.1866624861456345\n",
      "17 Train Loss 11187728000000.0 Test MSE 7050824.973629129 Test RE 1.1868799427473273\n",
      "18 Train Loss 11233600000000.0 Test MSE 7052999.854076627 Test RE 1.1870629796943704\n",
      "19 Train Loss 11290185000000.0 Test MSE 7187606.389378813 Test RE 1.1983369940391642\n",
      "20 Train Loss 11343649000000.0 Test MSE 7239008.567495502 Test RE 1.2026143147718278\n",
      "21 Train Loss 11312223000000.0 Test MSE 7317060.878019325 Test RE 1.209080335477438\n",
      "22 Train Loss 11237644000000.0 Test MSE 7456463.731077588 Test RE 1.2205435458290508\n",
      "23 Train Loss 11264948000000.0 Test MSE 7408836.918571078 Test RE 1.2166393009335077\n",
      "24 Train Loss 11285214000000.0 Test MSE 7405672.100863763 Test RE 1.2163794185167738\n",
      "25 Train Loss 11296584000000.0 Test MSE 7514342.232229359 Test RE 1.2252714353802412\n",
      "26 Train Loss 11347595000000.0 Test MSE 7573665.487142311 Test RE 1.2300984841630869\n",
      "27 Train Loss 11397269000000.0 Test MSE 7614425.880646362 Test RE 1.2334041502651671\n",
      "28 Train Loss 11431710000000.0 Test MSE 7772259.054081595 Test RE 1.2461216956549286\n",
      "29 Train Loss 11462844000000.0 Test MSE 7854064.405504544 Test RE 1.252662431701115\n",
      "30 Train Loss 11509692000000.0 Test MSE 7864578.884607033 Test RE 1.2535006402445863\n",
      "31 Train Loss 11534124000000.0 Test MSE 8153676.3593972195 Test RE 1.2763317070177682\n",
      "32 Train Loss 11495218000000.0 Test MSE 8214830.979490952 Test RE 1.281109169854792\n",
      "33 Train Loss 11519459000000.0 Test MSE 8205903.193060176 Test RE 1.2804128330519011\n",
      "34 Train Loss 11571496000000.0 Test MSE 8290126.9216902265 Test RE 1.2869670073194797\n",
      "35 Train Loss 11604082000000.0 Test MSE 8267193.59415024 Test RE 1.2851856789857943\n",
      "36 Train Loss 11638316000000.0 Test MSE 8255818.592239697 Test RE 1.2843012179234037\n",
      "37 Train Loss 11675754000000.0 Test MSE 8274154.172710051 Test RE 1.2857265973533372\n",
      "38 Train Loss 11720387000000.0 Test MSE 8274022.187367557 Test RE 1.285716342665154\n",
      "39 Train Loss 11769978000000.0 Test MSE 8272963.748018269 Test RE 1.2856341035641323\n",
      "40 Train Loss 11813260000000.0 Test MSE 8217410.885455698 Test RE 1.2813103232162575\n",
      "41 Train Loss 11856250000000.0 Test MSE 8204504.452245222 Test RE 1.2803037017366816\n",
      "42 Train Loss 11896510000000.0 Test MSE 8240716.737617511 Test RE 1.2831260341511117\n",
      "43 Train Loss 11942703000000.0 Test MSE 8242388.874839933 Test RE 1.2832562081421737\n",
      "44 Train Loss 11981127000000.0 Test MSE 8277331.1788620455 Test RE 1.2859734122661186\n",
      "45 Train Loss 12021827000000.0 Test MSE 8290990.620105837 Test RE 1.2870340462457657\n",
      "46 Train Loss 12059567000000.0 Test MSE 8297183.751655404 Test RE 1.2875146452232453\n",
      "47 Train Loss 12096373000000.0 Test MSE 8288866.523545279 Test RE 1.2868691709110196\n",
      "48 Train Loss 12135591000000.0 Test MSE 8286773.7977928575 Test RE 1.2867067099611327\n",
      "49 Train Loss 12173134000000.0 Test MSE 8308515.383853049 Test RE 1.288393537682499\n",
      "50 Train Loss 12208903000000.0 Test MSE 8336455.946964659 Test RE 1.2905580776416796\n",
      "51 Train Loss 12243924000000.0 Test MSE 8335672.5360597735 Test RE 1.290497436703145\n",
      "52 Train Loss 12278545000000.0 Test MSE 8349971.16661093 Test RE 1.2916037925309019\n",
      "53 Train Loss 12306055000000.0 Test MSE 8332495.5698886905 Test RE 1.2902514902962123\n",
      "54 Train Loss 12326509000000.0 Test MSE 8299692.8793909075 Test RE 1.2877093073282015\n",
      "55 Train Loss 12355239000000.0 Test MSE 8259735.59612097 Test RE 1.284605852542151\n",
      "56 Train Loss 12380106000000.0 Test MSE 8303174.852395916 Test RE 1.287979395567664\n",
      "57 Train Loss 12401098000000.0 Test MSE 8327561.752186182 Test RE 1.2898694433929438\n",
      "58 Train Loss 12424419000000.0 Test MSE 8334724.341424359 Test RE 1.290424036654081\n",
      "59 Train Loss 12445789000000.0 Test MSE 8357113.046831061 Test RE 1.2921560404232273\n",
      "60 Train Loss 12470419000000.0 Test MSE 8310404.462776338 Test RE 1.2885399981892585\n",
      "61 Train Loss 12496055000000.0 Test MSE 8353198.786652441 Test RE 1.2918533983933516\n",
      "62 Train Loss 12521270000000.0 Test MSE 8385723.513673197 Test RE 1.2943659902292042\n",
      "63 Train Loss 12546814000000.0 Test MSE 8355047.612464856 Test RE 1.2919963543930075\n",
      "64 Train Loss 12566772000000.0 Test MSE 8365292.791606867 Test RE 1.2927882516623872\n",
      "65 Train Loss 12585023000000.0 Test MSE 8405280.013524117 Test RE 1.2958744186345363\n",
      "66 Train Loss 12606978000000.0 Test MSE 8388604.601416905 Test RE 1.2945883241596021\n",
      "67 Train Loss 12629108000000.0 Test MSE 8379289.3929992085 Test RE 1.2938693303372286\n",
      "68 Train Loss 12653375000000.0 Test MSE 8380776.962725747 Test RE 1.2939841751253491\n",
      "69 Train Loss 12648464000000.0 Test MSE 8393762.842231933 Test RE 1.2949862909492562\n",
      "70 Train Loss 12660668000000.0 Test MSE 8456668.205462603 Test RE 1.2998297401524503\n",
      "71 Train Loss 12675322000000.0 Test MSE 8528180.590996725 Test RE 1.3053140655086797\n",
      "72 Train Loss 12688369000000.0 Test MSE 8542154.386503866 Test RE 1.3063830347869094\n",
      "73 Train Loss 12693686000000.0 Test MSE 8473698.224592721 Test RE 1.3011378789838834\n",
      "74 Train Loss 12700760000000.0 Test MSE 8549461.045581354 Test RE 1.306941632461619\n",
      "75 Train Loss 12715086000000.0 Test MSE 8614522.535953823 Test RE 1.3119051272262825\n",
      "76 Train Loss 12722701000000.0 Test MSE 8602139.23431387 Test RE 1.31096186198349\n",
      "77 Train Loss 12726570000000.0 Test MSE 8692898.105853548 Test RE 1.317859519965096\n",
      "78 Train Loss 12741568000000.0 Test MSE 8704568.956295805 Test RE 1.318743884549942\n",
      "79 Train Loss 12755840000000.0 Test MSE 8706824.297594624 Test RE 1.3189147158071473\n",
      "80 Train Loss 12772058000000.0 Test MSE 8706631.75371887 Test RE 1.3189001323984055\n",
      "81 Train Loss 12787307000000.0 Test MSE 8729643.247432357 Test RE 1.3206418986672481\n",
      "82 Train Loss 12793269000000.0 Test MSE 8697512.108748829 Test RE 1.3182092192780785\n",
      "83 Train Loss 12801816000000.0 Test MSE 8734156.51103409 Test RE 1.320983243362186\n",
      "84 Train Loss 12812307000000.0 Test MSE 8778290.458045423 Test RE 1.3243165212649803\n",
      "85 Train Loss 12822695000000.0 Test MSE 8755777.001390627 Test RE 1.3226172106931078\n",
      "86 Train Loss 12830293000000.0 Test MSE 8793781.594379941 Test RE 1.3254845234532284\n",
      "87 Train Loss 12827429000000.0 Test MSE 8803915.25420608 Test RE 1.3262480255768492\n",
      "88 Train Loss 12826348000000.0 Test MSE 8827128.29490756 Test RE 1.327995315385146\n",
      "89 Train Loss 12834912000000.0 Test MSE 8837726.851010554 Test RE 1.3287923249037858\n",
      "90 Train Loss 12840432000000.0 Test MSE 8864109.667330312 Test RE 1.330774234942646\n",
      "91 Train Loss 12851420000000.0 Test MSE 8899644.783075767 Test RE 1.3334390213032505\n",
      "92 Train Loss 12859718000000.0 Test MSE 8920433.1358075 Test RE 1.3349954784845401\n",
      "93 Train Loss 12867281000000.0 Test MSE 8927883.911894 Test RE 1.3355528884275536\n",
      "94 Train Loss 12878920000000.0 Test MSE 8961516.278079093 Test RE 1.3380661144676913\n",
      "95 Train Loss 12885339000000.0 Test MSE 8973113.01670269 Test RE 1.3389316033931855\n",
      "96 Train Loss 12887410000000.0 Test MSE 8924822.973438034 Test RE 1.3353239206399625\n",
      "97 Train Loss 12897667000000.0 Test MSE 8914331.794195686 Test RE 1.3345388494955974\n",
      "98 Train Loss 12905740000000.0 Test MSE 8920010.515616035 Test RE 1.3349638543067273\n",
      "99 Train Loss 12912688000000.0 Test MSE 8938027.067918783 Test RE 1.3363113478769586\n",
      "100 Train Loss 12917327000000.0 Test MSE 8977413.344960354 Test RE 1.3392524037501876\n",
      "101 Train Loss 12923316000000.0 Test MSE 9016320.5035866 Test RE 1.3421513552466158\n",
      "102 Train Loss 12930765000000.0 Test MSE 9069388.24338358 Test RE 1.3460953390059076\n",
      "103 Train Loss 12930476000000.0 Test MSE 9145866.947207108 Test RE 1.3517589794142226\n",
      "104 Train Loss 12898708000000.0 Test MSE 9259538.535713982 Test RE 1.3601333684193182\n",
      "105 Train Loss 12782911000000.0 Test MSE 9633660.533868052 Test RE 1.3873386741387392\n",
      "106 Train Loss 12363759000000.0 Test MSE 9366899.693506341 Test RE 1.3679957824076057\n",
      "107 Train Loss 12152324000000.0 Test MSE 9340055.288069392 Test RE 1.3660341204202922\n",
      "108 Train Loss 12065312000000.0 Test MSE 9406323.822223859 Test RE 1.370871622565557\n",
      "109 Train Loss 12034997000000.0 Test MSE 9270529.35997424 Test RE 1.3609403499651147\n",
      "110 Train Loss 12015812000000.0 Test MSE 9292341.874710929 Test RE 1.362540479209047\n",
      "111 Train Loss 11994417000000.0 Test MSE 9368971.851165973 Test RE 1.3681470889377696\n",
      "112 Train Loss 11971781000000.0 Test MSE 9333752.909387182 Test RE 1.36557316398801\n",
      "113 Train Loss 11967577000000.0 Test MSE 9405182.203117682 Test RE 1.370788430621806\n",
      "114 Train Loss 11956668000000.0 Test MSE 9435014.388436971 Test RE 1.3729607031757964\n",
      "115 Train Loss 11946433000000.0 Test MSE 9423776.536596008 Test RE 1.3721428069120418\n",
      "116 Train Loss 11924946000000.0 Test MSE 9510737.67934524 Test RE 1.3784592287357627\n",
      "117 Train Loss 11913319000000.0 Test MSE 9481904.40103506 Test RE 1.3763681360500915\n",
      "118 Train Loss 11911171000000.0 Test MSE 9465654.667839548 Test RE 1.375188246198768\n",
      "119 Train Loss 11888946000000.0 Test MSE 9463324.291457163 Test RE 1.3750189550302774\n",
      "120 Train Loss 11835895000000.0 Test MSE 9422096.17364391 Test RE 1.3720204673849552\n",
      "121 Train Loss 11810917000000.0 Test MSE 9368789.266501125 Test RE 1.3681337574912282\n",
      "122 Train Loss 11804052000000.0 Test MSE 9298761.352231996 Test RE 1.3630110434633644\n",
      "123 Train Loss 11801596000000.0 Test MSE 9351113.558004595 Test RE 1.3668425474048718\n",
      "124 Train Loss 11797027000000.0 Test MSE 9391080.21126501 Test RE 1.3697603751095797\n",
      "125 Train Loss 11795718000000.0 Test MSE 9380394.549132155 Test RE 1.3689808608130005\n",
      "126 Train Loss 11795999000000.0 Test MSE 9394171.68082117 Test RE 1.3699858137153635\n",
      "127 Train Loss 11794252000000.0 Test MSE 9353854.245915972 Test RE 1.3670428344819725\n",
      "128 Train Loss 11790981000000.0 Test MSE 9343133.50810788 Test RE 1.3662592051265106\n",
      "129 Train Loss 11794009000000.0 Test MSE 9331009.310992725 Test RE 1.365372448384204\n",
      "130 Train Loss 11797749000000.0 Test MSE 9302478.40405013 Test RE 1.3632834386933492\n",
      "131 Train Loss 11802798000000.0 Test MSE 9311521.728290977 Test RE 1.3639459298489012\n",
      "132 Train Loss 11807182000000.0 Test MSE 9294867.075217905 Test RE 1.3627256023047463\n",
      "133 Train Loss 11809282000000.0 Test MSE 9271205.809235316 Test RE 1.360990001407083\n",
      "134 Train Loss 11807528000000.0 Test MSE 9316282.948033836 Test RE 1.364294595542219\n",
      "135 Train Loss 11809156000000.0 Test MSE 9307262.249425773 Test RE 1.3636339312528212\n",
      "136 Train Loss 11813276000000.0 Test MSE 9288949.058361068 Test RE 1.3622917113673196\n",
      "137 Train Loss 11815692000000.0 Test MSE 9304082.971116299 Test RE 1.3634010087264168\n",
      "138 Train Loss 11820146000000.0 Test MSE 9323823.09685782 Test RE 1.3648465809071177\n",
      "139 Train Loss 11823592000000.0 Test MSE 9326769.87639117 Test RE 1.3650622426694803\n",
      "140 Train Loss 11825383000000.0 Test MSE 9322672.778436288 Test RE 1.3647623849407287\n",
      "141 Train Loss 11827657000000.0 Test MSE 9325655.549848322 Test RE 1.3649806940440148\n",
      "142 Train Loss 11829929000000.0 Test MSE 9334407.35032197 Test RE 1.3656210370848156\n",
      "143 Train Loss 11833897000000.0 Test MSE 9331715.92807657 Test RE 1.3654241457516008\n",
      "144 Train Loss 11836434000000.0 Test MSE 9357711.141242154 Test RE 1.3673246433269044\n",
      "145 Train Loss 11835415000000.0 Test MSE 9398886.783994257 Test RE 1.3703295808046718\n",
      "146 Train Loss 11837678000000.0 Test MSE 9398709.317766033 Test RE 1.3703166437209684\n",
      "147 Train Loss 11840650000000.0 Test MSE 9419457.807254016 Test RE 1.3718283580036728\n",
      "148 Train Loss 11843791000000.0 Test MSE 9448583.268658625 Test RE 1.3739476039583518\n",
      "149 Train Loss 11846800000000.0 Test MSE 9453161.69757776 Test RE 1.3742804453702018\n",
      "150 Train Loss 11850048000000.0 Test MSE 9449497.08478038 Test RE 1.3740140427608447\n",
      "151 Train Loss 11853384000000.0 Test MSE 9462117.777202414 Test RE 1.3749312991072928\n",
      "152 Train Loss 11856262000000.0 Test MSE 9459358.706523677 Test RE 1.3747308255292567\n",
      "153 Train Loss 11857867000000.0 Test MSE 9451532.42446375 Test RE 1.3741620101451308\n",
      "154 Train Loss 11861366000000.0 Test MSE 9459124.704780033 Test RE 1.3747138216598784\n",
      "155 Train Loss 11865238000000.0 Test MSE 9458378.257084165 Test RE 1.3746595792092469\n",
      "156 Train Loss 11869292000000.0 Test MSE 9458471.859571984 Test RE 1.37466638118072\n",
      "157 Train Loss 11873048000000.0 Test MSE 9457827.615180906 Test RE 1.3746195640944134\n",
      "158 Train Loss 11876438000000.0 Test MSE 9458094.282149192 Test RE 1.3746389429128265\n",
      "159 Train Loss 11879761000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "160 Train Loss 11883481000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "161 Train Loss 11887201000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "162 Train Loss 11890700000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "163 Train Loss 11894165000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "164 Train Loss 11897492000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "165 Train Loss 11900631000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "166 Train Loss 11904137000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "167 Train Loss 11907340000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "168 Train Loss 11910767000000.0 Test MSE 9460329.624498297 Test RE 1.37480137558777\n",
      "169 Train Loss 11913815000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "170 Train Loss 11917015000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "171 Train Loss 11920635000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "172 Train Loss 11923866000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "173 Train Loss 11926758000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "174 Train Loss 11929574000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "175 Train Loss 11932515000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "176 Train Loss 11935329000000.0 Test MSE 9460625.14332866 Test RE 1.3748228482286748\n",
      "177 Train Loss 11938144000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "178 Train Loss 11940834000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "179 Train Loss 11943534000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "180 Train Loss 11946115000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "181 Train Loss 11948644000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "182 Train Loss 11951256000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "183 Train Loss 11953883000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "184 Train Loss 11956509000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "185 Train Loss 11959139000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "186 Train Loss 11961970000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "187 Train Loss 11964582000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "188 Train Loss 11966907000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "189 Train Loss 11969389000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "190 Train Loss 11971993000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "191 Train Loss 11974419000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "192 Train Loss 11976735000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "193 Train Loss 11979028000000.0 Test MSE 9460566.99072585 Test RE 1.374818622839302\n",
      "194 Train Loss 11981429000000.0 Test MSE 9462071.663192261 Test RE 1.3749279487118196\n",
      "195 Train Loss 11983736000000.0 Test MSE 9462071.663192261 Test RE 1.3749279487118196\n",
      "196 Train Loss 11985796000000.0 Test MSE 9458534.972340956 Test RE 1.374670967484845\n",
      "197 Train Loss 11988010000000.0 Test MSE 9459882.688007647 Test RE 1.3747689001781467\n",
      "198 Train Loss 11990222000000.0 Test MSE 9460945.989217164 Test RE 1.3748461607747147\n",
      "199 Train Loss 11992507000000.0 Test MSE 9460945.989217164 Test RE 1.3748461607747147\n",
      "Training time: 45.07\n",
      "Training time: 45.07\n"
     ]
    }
   ],
   "source": [
    " \n",
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "  optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=5e-3)\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "e199619a-d416-48f4-91f7-2c23d1e79435"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30427/2488343543.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_tanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_tanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tanh_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
