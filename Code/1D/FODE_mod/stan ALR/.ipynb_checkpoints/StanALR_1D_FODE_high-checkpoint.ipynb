{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "3df10486-4078-44cd-95da-12a75fb13c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "515a82ba-2a23-4124-c9e1-230f67f43912"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "d35a8c58-7c75-4550-d489-9565724f04e6"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "7b44eee8-32ab-4621-ca04-81e30b53601d"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = extent*np.sin(x)/2 + np.square(x)/2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.005\n",
    "level = \"high\"\n",
    "label = \"1D_FODE_stanALR_\" +level\n",
    "extent = 100.0\n",
    "\n",
    "x = np.linspace(extent,-1.0*extent,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "             \n",
    "        self.lambdas = torch.ones((1,),device = device)\n",
    "        self.lambda_alpha = 0.1\n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - extent*torch.cos(g)/2.0 - g\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + 100*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def lambda_update(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc1.backward()\n",
    "        bc1_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc1_grads.append(param.grad.view(-1))\n",
    "        bc1_grads = torch.cat(bc1_grads)\n",
    "        bc1_grads = torch.mean(torch.abs(bc1_grads))        \n",
    "    \n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        loss_f.backward()\n",
    "        f_grads = []\n",
    "        for param in self.parameters():\n",
    "            f_grads.append(param.grad.view(-1))   \n",
    "        f_grads = torch.cat(f_grads)\n",
    "        f_grads = torch.max(torch.abs(f_grads))\n",
    "    \n",
    "        self.lambdas[0] = (1.0-self.lambda_alpha)*self.lambdas[0] + self.lambda_alpha*f_grads/bc1_grads\n",
    "        \n",
    "        return None\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    PINN.lambda_update(x_bc1_train,y_bc1_train,x_coll_train,f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fAcpqTqePPt9"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    x_coll_np_array = colloc_pts(N_f,123)\n",
    "    x_coll = torch.from_numpy(x_coll_np_array).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(i)        \n",
    "    \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "3645d237-1d2a-45c9-8d9f-de486f1ca919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 566297.6 Test MSE 3802972.7737371544 Test RE 0.8716626390167711\n",
      "1 Train Loss 321795.12 Test MSE 2602275.3989470187 Test RE 0.7210467965049999\n",
      "2 Train Loss 283114.34 Test MSE 2454933.19916332 Test RE 0.7003363416279466\n",
      "3 Train Loss 258973.48 Test MSE 2595222.762606879 Test RE 0.7200690500631136\n",
      "4 Train Loss 203746.9 Test MSE 1692790.4540646856 Test RE 0.5815518268167786\n",
      "5 Train Loss 198159.83 Test MSE 1750496.9047949046 Test RE 0.5913811769255893\n",
      "6 Train Loss 175068.7 Test MSE 1492291.8008609219 Test RE 0.5460264742069114\n",
      "7 Train Loss 162634.56 Test MSE 1390078.3159183278 Test RE 0.5269949554410971\n",
      "8 Train Loss 157082.17 Test MSE 1329945.3031343685 Test RE 0.5154703785775092\n",
      "9 Train Loss 153061.53 Test MSE 1323322.7079560477 Test RE 0.514185358799269\n",
      "10 Train Loss 149591.98 Test MSE 1270660.1244283353 Test RE 0.5038503032180376\n",
      "11 Train Loss 147165.97 Test MSE 1222038.368816621 Test RE 0.49411637265579267\n",
      "12 Train Loss 142519.4 Test MSE 1115920.1068026149 Test RE 0.47217541975395577\n",
      "13 Train Loss 139585.83 Test MSE 1024267.5484285547 Test RE 0.4523697233245257\n",
      "14 Train Loss 136727.78 Test MSE 863389.9950110515 Test RE 0.41532715725429387\n",
      "15 Train Loss 134304.16 Test MSE 767657.5571838246 Test RE 0.3916251628595386\n",
      "16 Train Loss 130930.234 Test MSE 679303.8366623993 Test RE 0.368399351803322\n",
      "17 Train Loss 124496.58 Test MSE 288389.54962410865 Test RE 0.24003611167655592\n",
      "18 Train Loss 116329.15 Test MSE 166801.89089385318 Test RE 0.1825522997411749\n",
      "19 Train Loss 114071.48 Test MSE 205247.82122699037 Test RE 0.20250050074237128\n",
      "20 Train Loss 110798.36 Test MSE 130578.16219626894 Test RE 0.16151843561539503\n",
      "21 Train Loss 108854.68 Test MSE 112204.7581320333 Test RE 0.14972435836330403\n",
      "22 Train Loss 107032.56 Test MSE 93082.6071071361 Test RE 0.1363707073017876\n",
      "23 Train Loss 105965.67 Test MSE 52088.677191016 Test RE 0.10201369726612963\n",
      "24 Train Loss 105080.37 Test MSE 42607.38209554647 Test RE 0.09226335630738006\n",
      "25 Train Loss 103587.87 Test MSE 35796.002681542835 Test RE 0.08456761635517276\n",
      "26 Train Loss 102162.766 Test MSE 29285.046875215434 Test RE 0.07649090255517038\n",
      "27 Train Loss 101045.56 Test MSE 20032.15405753813 Test RE 0.06326312623890008\n",
      "28 Train Loss 100610.5 Test MSE 18685.08707539768 Test RE 0.06109904001589432\n",
      "29 Train Loss 99893.34 Test MSE 17585.13056637087 Test RE 0.05927337033029586\n",
      "30 Train Loss 99372.516 Test MSE 5874.265795688491 Test RE 0.03425812741863125\n",
      "31 Train Loss 98897.68 Test MSE 4444.98914087212 Test RE 0.029800405648882946\n",
      "32 Train Loss 98436.875 Test MSE 8481.295256610334 Test RE 0.04116403273253816\n",
      "33 Train Loss 97929.65 Test MSE 8090.755007036528 Test RE 0.04020511873173688\n",
      "34 Train Loss 97448.2 Test MSE 11104.771803913922 Test RE 0.04710224888489474\n",
      "35 Train Loss 96999.57 Test MSE 9862.128986276139 Test RE 0.044388673086745024\n",
      "36 Train Loss 96088.484 Test MSE 3592.027028777597 Test RE 0.026789007363753743\n",
      "37 Train Loss 95847.79 Test MSE 4930.114525298123 Test RE 0.03138450820340721\n",
      "38 Train Loss 95716.25 Test MSE 4652.340845480369 Test RE 0.03048755425420761\n",
      "39 Train Loss 95487.555 Test MSE 3359.763539057202 Test RE 0.025908434988697847\n",
      "40 Train Loss 95049.945 Test MSE 4358.087003565205 Test RE 0.029507660069992926\n",
      "41 Train Loss 94184.03 Test MSE 12134.118141861645 Test RE 0.04923692626190895\n",
      "42 Train Loss 93893.58 Test MSE 6300.852579050199 Test RE 0.03548023461221536\n",
      "43 Train Loss 93558.72 Test MSE 3778.6233684517474 Test RE 0.027476007528225637\n",
      "44 Train Loss 93157.0 Test MSE 5223.5378034361775 Test RE 0.03230495903832336\n",
      "45 Train Loss 92081.82 Test MSE 4793.354994363594 Test RE 0.030946149594885466\n",
      "46 Train Loss 91656.8 Test MSE 5283.341171461867 Test RE 0.03248935965244804\n",
      "47 Train Loss 90976.32 Test MSE 7497.633768356956 Test RE 0.038703383665088936\n",
      "48 Train Loss 90784.266 Test MSE 5891.888850668958 Test RE 0.03430947687707351\n",
      "49 Train Loss 90181.086 Test MSE 4924.17957354596 Test RE 0.03136561192502131\n",
      "50 Train Loss 89479.305 Test MSE 4520.685469114764 Test RE 0.030053078757259818\n",
      "51 Train Loss 88870.47 Test MSE 5908.341034550514 Test RE 0.034357345424850465\n",
      "52 Train Loss 88526.98 Test MSE 10368.187442607006 Test RE 0.04551329140164056\n",
      "53 Train Loss 87741.164 Test MSE 10072.747065376363 Test RE 0.04486015687419433\n",
      "54 Train Loss 87217.984 Test MSE 6790.687488858485 Test RE 0.03683356000076556\n",
      "55 Train Loss 86574.28 Test MSE 7977.585646770472 Test RE 0.039922944166601575\n",
      "56 Train Loss 85609.8 Test MSE 6678.985821461326 Test RE 0.0365293617699183\n",
      "57 Train Loss 85098.39 Test MSE 4319.828455315891 Test RE 0.029377854400961256\n",
      "58 Train Loss 84441.16 Test MSE 3449.5613099745437 Test RE 0.02625238457934224\n",
      "59 Train Loss 84152.09 Test MSE 5402.455733395966 Test RE 0.03285355967773524\n",
      "60 Train Loss 83719.914 Test MSE 3305.332658948024 Test RE 0.02569770924654471\n",
      "61 Train Loss 82597.92 Test MSE 2148.742100986603 Test RE 0.02071948565785712\n",
      "62 Train Loss 82038.55 Test MSE 1812.6444552480825 Test RE 0.019030190558164926\n",
      "63 Train Loss 81520.03 Test MSE 3326.4269989418103 Test RE 0.025779579083860443\n",
      "64 Train Loss 81197.94 Test MSE 1754.9136808874334 Test RE 0.018724692892410737\n",
      "65 Train Loss 80865.56 Test MSE 1716.8364097810313 Test RE 0.018520439186612166\n",
      "66 Train Loss 80609.32 Test MSE 1934.9784879965455 Test RE 0.019661873346123085\n",
      "67 Train Loss 80386.04 Test MSE 2283.4692070957585 Test RE 0.021359171526163918\n",
      "68 Train Loss 79894.016 Test MSE 1683.0619706260197 Test RE 0.018337362787853802\n",
      "69 Train Loss 79719.52 Test MSE 2579.9300363793395 Test RE 0.022703394102783984\n",
      "70 Train Loss 79391.984 Test MSE 1828.5643546161375 Test RE 0.019113576029763546\n",
      "71 Train Loss 79148.27 Test MSE 1561.2260659798537 Test RE 0.0176611801475776\n",
      "72 Train Loss 78969.63 Test MSE 1751.7199623475765 Test RE 0.01870764685988907\n",
      "73 Train Loss 78759.02 Test MSE 1785.4306555037197 Test RE 0.018886797210637858\n",
      "74 Train Loss 78618.71 Test MSE 1777.8932988158629 Test RE 0.018846888887250425\n",
      "75 Train Loss 78515.57 Test MSE 1808.2658017374445 Test RE 0.01900719184139933\n",
      "76 Train Loss 78020.72 Test MSE 2299.18034912304 Test RE 0.021432525204231605\n",
      "77 Train Loss 77745.86 Test MSE 1948.5205994025268 Test RE 0.01973055603110178\n",
      "78 Train Loss 77634.03 Test MSE 2081.585456688247 Test RE 0.0203931327421292\n",
      "79 Train Loss 77408.59 Test MSE 1737.500702773621 Test RE 0.018631564234261048\n",
      "80 Train Loss 77172.695 Test MSE 1554.1479028190176 Test RE 0.017621099236753743\n",
      "81 Train Loss 76942.836 Test MSE 1498.9049073043109 Test RE 0.017305090082638152\n",
      "82 Train Loss 76782.95 Test MSE 1970.556861550462 Test RE 0.01984181103381296\n",
      "83 Train Loss 76729.35 Test MSE 2234.795948528737 Test RE 0.02113030478210429\n",
      "84 Train Loss 76587.016 Test MSE 1942.238670111926 Test RE 0.019698725210816097\n",
      "85 Train Loss 76456.93 Test MSE 1575.742001909008 Test RE 0.01774309506201007\n",
      "86 Train Loss 76249.72 Test MSE 1934.6697342985426 Test RE 0.01966030461594644\n",
      "87 Train Loss 76061.36 Test MSE 3062.794991986627 Test RE 0.024736929299085234\n",
      "88 Train Loss 75840.19 Test MSE 2804.711358119508 Test RE 0.023671779691437893\n",
      "89 Train Loss 75726.13 Test MSE 1993.4693358300713 Test RE 0.019956832095507696\n",
      "90 Train Loss 75620.04 Test MSE 1648.1625814554063 Test RE 0.01814624828028224\n",
      "91 Train Loss 75401.87 Test MSE 1488.9086719401148 Test RE 0.01724728950770868\n",
      "92 Train Loss 75238.58 Test MSE 2071.4862709966546 Test RE 0.02034360211937414\n",
      "93 Train Loss 75033.62 Test MSE 2093.7268045180135 Test RE 0.020452520195927887\n",
      "94 Train Loss 74821.74 Test MSE 3444.8934555222318 Test RE 0.02623461655283882\n",
      "95 Train Loss 74402.39 Test MSE 4441.274693238856 Test RE 0.029787951717355068\n",
      "96 Train Loss 74064.97 Test MSE 2676.2286242098185 Test RE 0.023123226293874193\n",
      "97 Train Loss 73723.37 Test MSE 2545.16263737127 Test RE 0.022549898596914704\n",
      "98 Train Loss 73437.82 Test MSE 2389.684958702976 Test RE 0.02185028714759023\n",
      "99 Train Loss 73217.48 Test MSE 2035.2055894720688 Test RE 0.020164662940192157\n",
      "100 Train Loss 73072.586 Test MSE 2121.7054378060593 Test RE 0.02058872099543355\n",
      "101 Train Loss 72924.766 Test MSE 2716.245233253245 Test RE 0.023295461161466906\n",
      "102 Train Loss 72622.63 Test MSE 4464.149489249219 Test RE 0.029864564653825688\n",
      "103 Train Loss 72407.82 Test MSE 5920.364226059925 Test RE 0.03439228543617591\n",
      "104 Train Loss 71960.31 Test MSE 8591.181199166507 Test RE 0.041429840673542224\n",
      "105 Train Loss 71525.83 Test MSE 6851.913736448861 Test RE 0.03699923690471757\n",
      "106 Train Loss 71205.53 Test MSE 5316.178151399625 Test RE 0.03259016706529257\n",
      "107 Train Loss 70903.836 Test MSE 6155.239636740142 Test RE 0.03506786342313974\n",
      "108 Train Loss 70582.86 Test MSE 8375.850984860734 Test RE 0.04090734511775152\n",
      "109 Train Loss 70231.0 Test MSE 6150.012657401162 Test RE 0.03505297058882865\n",
      "110 Train Loss 70033.24 Test MSE 5032.094306933054 Test RE 0.031707442203228395\n",
      "111 Train Loss 69831.945 Test MSE 3095.8253908098923 Test RE 0.024869958042099287\n",
      "112 Train Loss 69723.695 Test MSE 2663.8754173588313 Test RE 0.023069797303115477\n",
      "113 Train Loss 69622.93 Test MSE 3052.9293658921306 Test RE 0.02469705687247872\n",
      "114 Train Loss 69543.7 Test MSE 2471.4889880399955 Test RE 0.022221131197715748\n",
      "115 Train Loss 69365.055 Test MSE 2714.17136804491 Test RE 0.02328656636969993\n",
      "116 Train Loss 69116.43 Test MSE 2050.1905981983664 Test RE 0.020238761960617798\n",
      "117 Train Loss 68952.06 Test MSE 1615.3494562392204 Test RE 0.0179647041129492\n",
      "118 Train Loss 68875.46 Test MSE 2371.7223970911336 Test RE 0.021768011141823432\n",
      "119 Train Loss 68811.52 Test MSE 2991.220110160243 Test RE 0.02444618024940836\n",
      "120 Train Loss 68717.42 Test MSE 3539.8927115876736 Test RE 0.026593890326389927\n",
      "121 Train Loss 68628.76 Test MSE 3925.473406365551 Test RE 0.028004823728046892\n",
      "122 Train Loss 68520.18 Test MSE 3686.9314185320245 Test RE 0.02714059430478434\n",
      "123 Train Loss 68393.086 Test MSE 2301.3420913350774 Test RE 0.021442598513415775\n",
      "124 Train Loss 68265.63 Test MSE 2727.044199145275 Test RE 0.02334172306463901\n",
      "125 Train Loss 68095.1 Test MSE 2402.7434855456686 Test RE 0.021909906684234447\n",
      "126 Train Loss 67931.15 Test MSE 2250.7374461930362 Test RE 0.02120553539226772\n",
      "127 Train Loss 67785.195 Test MSE 2359.180034783559 Test RE 0.021710377036791916\n",
      "128 Train Loss 67692.28 Test MSE 2234.9484078041646 Test RE 0.021131025531579705\n",
      "129 Train Loss 67598.586 Test MSE 1922.0985657724746 Test RE 0.019596325788469735\n",
      "130 Train Loss 67476.164 Test MSE 1761.1803284848586 Test RE 0.01875809524316163\n",
      "131 Train Loss 67338.445 Test MSE 1501.4279423372268 Test RE 0.01731964837466026\n",
      "132 Train Loss 67284.84 Test MSE 1765.2563421954594 Test RE 0.018779789235879328\n",
      "133 Train Loss 67208.03 Test MSE 2075.2038580329763 Test RE 0.020361848730778224\n",
      "134 Train Loss 67109.44 Test MSE 2895.8700702276064 Test RE 0.02405339370392947\n",
      "135 Train Loss 67043.13 Test MSE 3830.5495075193035 Test RE 0.027664152085989865\n",
      "136 Train Loss 66977.33 Test MSE 3803.3222988358034 Test RE 0.027565659576021927\n",
      "137 Train Loss 66852.86 Test MSE 3220.080422711807 Test RE 0.025364142409069656\n",
      "138 Train Loss 66738.58 Test MSE 2567.06384883716 Test RE 0.022646712096166442\n",
      "139 Train Loss 66597.766 Test MSE 1690.8225707385384 Test RE 0.018379590962650568\n",
      "140 Train Loss 66519.59 Test MSE 1493.2408461628902 Test RE 0.017272362902979926\n",
      "141 Train Loss 66413.53 Test MSE 1667.3120004602235 Test RE 0.01825136126965379\n",
      "142 Train Loss 66334.9 Test MSE 1488.6806939506632 Test RE 0.017245969026141988\n",
      "143 Train Loss 66247.01 Test MSE 1594.6187138863968 Test RE 0.017849055991866373\n",
      "144 Train Loss 66198.945 Test MSE 1704.3032983073078 Test RE 0.018452714648467945\n",
      "145 Train Loss 66108.51 Test MSE 1477.221214395333 Test RE 0.017179463284841785\n",
      "146 Train Loss 66017.51 Test MSE 1432.0792584862581 Test RE 0.01691493569163134\n",
      "147 Train Loss 65945.77 Test MSE 1565.4051046354753 Test RE 0.017684801784208993\n",
      "148 Train Loss 65853.25 Test MSE 1611.2618264277976 Test RE 0.01794195994012462\n",
      "149 Train Loss 65795.18 Test MSE 1561.1131870745185 Test RE 0.01766054167158706\n",
      "150 Train Loss 65742.586 Test MSE 1731.791685891902 Test RE 0.018600929589515588\n",
      "151 Train Loss 65683.81 Test MSE 1790.8439034731255 Test RE 0.018915406985830215\n",
      "152 Train Loss 65596.65 Test MSE 1540.2745563934143 Test RE 0.017542274170262665\n",
      "153 Train Loss 65515.957 Test MSE 1799.7090847524037 Test RE 0.018962167481627296\n",
      "154 Train Loss 65443.918 Test MSE 1777.8565817556237 Test RE 0.018846694273229464\n",
      "155 Train Loss 65387.156 Test MSE 1683.1344793951123 Test RE 0.018337757783791916\n",
      "156 Train Loss 65337.04 Test MSE 1502.5504131736905 Test RE 0.017326121268733827\n",
      "157 Train Loss 65272.21 Test MSE 1490.249003307954 Test RE 0.01725505085780104\n",
      "158 Train Loss 65187.34 Test MSE 1454.8838262644672 Test RE 0.017049081295008144\n",
      "159 Train Loss 65081.195 Test MSE 1622.114256682311 Test RE 0.018002281328666537\n",
      "160 Train Loss 65007.266 Test MSE 1578.0242632923312 Test RE 0.01775593971821131\n",
      "161 Train Loss 64854.18 Test MSE 1467.4302306329105 Test RE 0.017122436116331106\n",
      "162 Train Loss 64770.992 Test MSE 1752.6885728039113 Test RE 0.018712818325228787\n",
      "163 Train Loss 64691.61 Test MSE 1707.4101975825013 Test RE 0.01846952639245665\n",
      "164 Train Loss 64452.1 Test MSE 1673.554920951904 Test RE 0.018285498653153393\n",
      "165 Train Loss 64369.05 Test MSE 1814.6931116650294 Test RE 0.019040941511694373\n",
      "166 Train Loss 64253.773 Test MSE 1602.7888780988299 Test RE 0.017894723147747284\n",
      "167 Train Loss 64175.21 Test MSE 1435.841663114213 Test RE 0.01693714084786137\n",
      "168 Train Loss 64090.066 Test MSE 1585.2223415387914 Test RE 0.017796390055286028\n",
      "169 Train Loss 64005.863 Test MSE 1773.2594171783376 Test RE 0.018822311699902117\n",
      "170 Train Loss 63948.07 Test MSE 1753.7143327396632 Test RE 0.018718293357130454\n",
      "171 Train Loss 63881.453 Test MSE 1961.7546394460555 Test RE 0.01979744603605516\n",
      "172 Train Loss 63835.348 Test MSE 2108.8706444469976 Test RE 0.020526353047985363\n",
      "173 Train Loss 63798.15 Test MSE 1710.5231435591415 Test RE 0.01848635552226905\n",
      "174 Train Loss 63762.836 Test MSE 1499.5112702832887 Test RE 0.017308590006121668\n",
      "175 Train Loss 63717.22 Test MSE 1534.1983282625106 Test RE 0.017507638724988943\n",
      "176 Train Loss 63687.12 Test MSE 1353.322279712903 Test RE 0.016443241659435927\n",
      "177 Train Loss 63642.746 Test MSE 1325.0264647012455 Test RE 0.016270432590610794\n",
      "178 Train Loss 63571.24 Test MSE 1467.3354029861384 Test RE 0.017121882868032803\n",
      "179 Train Loss 63504.09 Test MSE 1419.6673570153641 Test RE 0.016841474882732826\n",
      "180 Train Loss 63420.945 Test MSE 1303.6403869014832 Test RE 0.016138595142751823\n",
      "181 Train Loss 63369.63 Test MSE 1271.501687083686 Test RE 0.015938420995107085\n",
      "182 Train Loss 63332.945 Test MSE 1402.3790982710584 Test RE 0.01673861570749324\n",
      "183 Train Loss 63270.715 Test MSE 1612.7091222050474 Test RE 0.017950016201944648\n",
      "184 Train Loss 63210.36 Test MSE 1771.6388705802642 Test RE 0.018813709065404336\n",
      "185 Train Loss 63168.01 Test MSE 2197.1104380110783 Test RE 0.02095138644015238\n",
      "186 Train Loss 63128.336 Test MSE 2106.2825490899318 Test RE 0.020513753776374172\n",
      "187 Train Loss 63072.098 Test MSE 2193.0001594865153 Test RE 0.02093177969993147\n",
      "188 Train Loss 63014.83 Test MSE 1894.1676800881773 Test RE 0.019453423195839887\n",
      "189 Train Loss 62923.406 Test MSE 1895.643431942305 Test RE 0.01946099983124649\n",
      "190 Train Loss 62861.457 Test MSE 1892.3941735297046 Test RE 0.019444313956588063\n",
      "191 Train Loss 62816.035 Test MSE 1645.7597232357846 Test RE 0.018133015736937378\n",
      "192 Train Loss 62767.18 Test MSE 1126.0142501025564 Test RE 0.014998877806266886\n",
      "193 Train Loss 62685.2 Test MSE 1158.0056875915923 Test RE 0.015210453749898137\n",
      "194 Train Loss 62525.09 Test MSE 1619.8725599530005 Test RE 0.017989837813321023\n",
      "195 Train Loss 62448.17 Test MSE 1964.6620479810058 Test RE 0.019812110956900348\n",
      "196 Train Loss 62391.695 Test MSE 2267.6652662641245 Test RE 0.021285129543164378\n",
      "197 Train Loss 62319.758 Test MSE 2248.500401050674 Test RE 0.02119499450639705\n",
      "198 Train Loss 62234.918 Test MSE 2566.24014432353 Test RE 0.022643078432042\n",
      "199 Train Loss 62150.613 Test MSE 2026.2538776775689 Test RE 0.02012026762749933\n",
      "Training time: 48.13\n",
      "Training time: 48.13\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 467535.1 Test MSE 4662856.752864778 Test RE 0.9651901041027455\n",
      "1 Train Loss 431551.3 Test MSE 3030735.4877435304 Test RE 0.7781455546771444\n",
      "2 Train Loss 159228.56 Test MSE 266121.09605243127 Test RE 0.2305825700043088\n",
      "3 Train Loss 140302.73 Test MSE 259614.9070337925 Test RE 0.22774646074921737\n",
      "4 Train Loss 128611.43 Test MSE 33701.87259845046 Test RE 0.08205666080595132\n",
      "5 Train Loss 126012.516 Test MSE 60169.64708565723 Test RE 0.10964164826826249\n",
      "6 Train Loss 124835.836 Test MSE 15572.266256207344 Test RE 0.05577797380678575\n",
      "7 Train Loss 124034.78 Test MSE 7608.951512146917 Test RE 0.03898964059926182\n",
      "8 Train Loss 123965.65 Test MSE 5570.202558286002 Test RE 0.033359713937009636\n",
      "9 Train Loss 123756.56 Test MSE 5393.984938225347 Test RE 0.03282779315944653\n",
      "10 Train Loss 123437.74 Test MSE 9602.550155107154 Test RE 0.04380060565350102\n",
      "11 Train Loss 123161.51 Test MSE 6528.970035532728 Test RE 0.036116791445515245\n",
      "12 Train Loss 123026.52 Test MSE 6413.509105601943 Test RE 0.035796015007498065\n",
      "13 Train Loss 122935.55 Test MSE 5710.047159368682 Test RE 0.03377587986127669\n",
      "14 Train Loss 122762.38 Test MSE 8822.959185902297 Test RE 0.04198498078584424\n",
      "15 Train Loss 122588.45 Test MSE 9794.268342959622 Test RE 0.044235691738485204\n",
      "16 Train Loss 122423.984 Test MSE 12790.842075524131 Test RE 0.05055177298912235\n",
      "17 Train Loss 122291.734 Test MSE 10036.200460283873 Test RE 0.04477870063121561\n",
      "18 Train Loss 122143.17 Test MSE 10465.562531164409 Test RE 0.04572651592558694\n",
      "19 Train Loss 121895.586 Test MSE 10053.454817528713 Test RE 0.04481717614334469\n",
      "20 Train Loss 121679.08 Test MSE 11940.3406628251 Test RE 0.048842196040519543\n",
      "21 Train Loss 121582.766 Test MSE 12860.495340748312 Test RE 0.05068922740595096\n",
      "22 Train Loss 121121.45 Test MSE 17391.53320973567 Test RE 0.05894619261070719\n",
      "23 Train Loss 120707.44 Test MSE 19652.64270166413 Test RE 0.06266099731981342\n",
      "24 Train Loss 120001.05 Test MSE 9177.943918984673 Test RE 0.042821268023600534\n",
      "25 Train Loss 119645.805 Test MSE 9511.909009347008 Test RE 0.04359339244947784\n",
      "26 Train Loss 119259.16 Test MSE 16827.89501413178 Test RE 0.057983138704438296\n",
      "27 Train Loss 118850.09 Test MSE 10913.55124489829 Test RE 0.046694945157782344\n",
      "28 Train Loss 118412.734 Test MSE 8564.015423616356 Test RE 0.04136428712285931\n",
      "29 Train Loss 117860.45 Test MSE 5227.479523078066 Test RE 0.03231714551835357\n",
      "30 Train Loss 117399.56 Test MSE 4650.758601298498 Test RE 0.030482369460241042\n",
      "31 Train Loss 116569.234 Test MSE 3511.090103300537 Test RE 0.026485477735954926\n",
      "32 Train Loss 115693.66 Test MSE 3102.153647088358 Test RE 0.024895363725958453\n",
      "33 Train Loss 114922.47 Test MSE 2626.474678004389 Test RE 0.022907275167607124\n",
      "34 Train Loss 114361.46 Test MSE 3475.9010717432366 Test RE 0.026352421442798694\n",
      "35 Train Loss 113691.39 Test MSE 5168.752462144114 Test RE 0.03213510257709822\n",
      "36 Train Loss 113156.78 Test MSE 8422.660051432149 Test RE 0.04102149273062714\n",
      "37 Train Loss 112474.06 Test MSE 7336.854313562414 Test RE 0.03828615660024764\n",
      "38 Train Loss 111894.734 Test MSE 4709.232965399773 Test RE 0.030673399525253567\n",
      "39 Train Loss 110918.82 Test MSE 3108.645543636685 Test RE 0.024921399454319898\n",
      "40 Train Loss 110361.01 Test MSE 2738.0785410516874 Test RE 0.023388898783190278\n",
      "41 Train Loss 109684.09 Test MSE 2984.153411952277 Test RE 0.024417286365919253\n",
      "42 Train Loss 108567.21 Test MSE 5409.257648647155 Test RE 0.03287423516730028\n",
      "43 Train Loss 107632.16 Test MSE 6483.852526362177 Test RE 0.03599178516312313\n",
      "44 Train Loss 106920.84 Test MSE 3748.7054919301586 Test RE 0.027367018443947223\n",
      "45 Train Loss 105981.414 Test MSE 2561.0628459506875 Test RE 0.022620226096899603\n",
      "46 Train Loss 105026.54 Test MSE 3516.1124636983945 Test RE 0.026504413746687303\n",
      "47 Train Loss 103708.01 Test MSE 2702.577729391492 Test RE 0.02323677862258357\n",
      "48 Train Loss 102837.52 Test MSE 5444.6367787449035 Test RE 0.032981566557950794\n",
      "49 Train Loss 101723.984 Test MSE 5377.148549413048 Test RE 0.032776519985651925\n",
      "50 Train Loss 100189.836 Test MSE 4180.8386060407865 Test RE 0.028901376326816646\n",
      "51 Train Loss 99373.11 Test MSE 4417.609722986729 Test RE 0.02970848438060841\n",
      "52 Train Loss 98298.71 Test MSE 4897.12229901148 Test RE 0.03127931968235946\n",
      "53 Train Loss 97489.2 Test MSE 3539.473173724944 Test RE 0.02659231436414814\n",
      "54 Train Loss 96502.87 Test MSE 2318.517599023412 Test RE 0.02152246558602926\n",
      "55 Train Loss 95243.73 Test MSE 11900.56535188255 Test RE 0.04876077733700895\n",
      "56 Train Loss 93950.55 Test MSE 21328.71477325925 Test RE 0.06527834976100302\n",
      "57 Train Loss 92720.664 Test MSE 26889.412238030825 Test RE 0.07329552809879414\n",
      "58 Train Loss 91780.97 Test MSE 26443.667129773075 Test RE 0.07268548028678821\n",
      "59 Train Loss 91015.94 Test MSE 22281.301242338242 Test RE 0.06672016302843452\n",
      "60 Train Loss 89114.39 Test MSE 5699.001862313728 Test RE 0.033743196662597806\n",
      "61 Train Loss 87483.44 Test MSE 3683.2889968463883 Test RE 0.027127184519503154\n",
      "62 Train Loss 86373.08 Test MSE 4188.590680554998 Test RE 0.028928158257938985\n",
      "63 Train Loss 85270.83 Test MSE 5806.355318971075 Test RE 0.03405952822924382\n",
      "64 Train Loss 83951.805 Test MSE 4622.250527955144 Test RE 0.030388800918155195\n",
      "65 Train Loss 82645.23 Test MSE 2202.858249845689 Test RE 0.02097877376844846\n",
      "66 Train Loss 81104.086 Test MSE 2504.340910238761 Test RE 0.022368329301431983\n",
      "67 Train Loss 80193.89 Test MSE 2216.577484671039 Test RE 0.021043999480256714\n",
      "68 Train Loss 79381.445 Test MSE 1994.7996268357326 Test RE 0.019963489826865674\n",
      "69 Train Loss 78511.44 Test MSE 2997.1562832056366 Test RE 0.024470425344226797\n",
      "70 Train Loss 77265.91 Test MSE 12465.98825013731 Test RE 0.04990570329582271\n",
      "71 Train Loss 76404.87 Test MSE 19275.67279989983 Test RE 0.06205711712373112\n",
      "72 Train Loss 75974.41 Test MSE 9861.900040006298 Test RE 0.04438815784910608\n",
      "73 Train Loss 74931.84 Test MSE 15187.174321736098 Test RE 0.05508398008534631\n",
      "74 Train Loss 74258.05 Test MSE 7643.92897419247 Test RE 0.03907915326103463\n",
      "75 Train Loss 73565.72 Test MSE 4485.82564488604 Test RE 0.029936982147575755\n",
      "76 Train Loss 72819.64 Test MSE 3219.174206647974 Test RE 0.025360573086617832\n",
      "77 Train Loss 71698.08 Test MSE 1641.0864671685804 Test RE 0.018107252417206347\n",
      "78 Train Loss 70900.01 Test MSE 2152.216095861497 Test RE 0.020736228085239235\n",
      "79 Train Loss 70479.266 Test MSE 1576.6331159836477 Test RE 0.0177481113930378\n",
      "80 Train Loss 70268.664 Test MSE 1685.657008275478 Test RE 0.01835149412081961\n",
      "81 Train Loss 69941.54 Test MSE 1557.0657582813394 Test RE 0.017637632962523034\n",
      "82 Train Loss 69418.45 Test MSE 1904.2186805033923 Test RE 0.019504967647985965\n",
      "83 Train Loss 68568.31 Test MSE 3374.349003437908 Test RE 0.025964611162581863\n",
      "84 Train Loss 67641.95 Test MSE 2687.141223360334 Test RE 0.02317032200730301\n",
      "85 Train Loss 66369.54 Test MSE 2628.8345678764776 Test RE 0.022917563960724804\n",
      "86 Train Loss 65603.68 Test MSE 2287.9706727434927 Test RE 0.021380214121931713\n",
      "87 Train Loss 65284.367 Test MSE 2327.729774272964 Test RE 0.021565180847211297\n",
      "88 Train Loss 64780.316 Test MSE 1808.7279693220337 Test RE 0.01900962067313052\n",
      "89 Train Loss 64220.543 Test MSE 2352.0245164431676 Test RE 0.021677427670233763\n",
      "90 Train Loss 63625.016 Test MSE 1969.8873850675586 Test RE 0.01983844022159227\n",
      "91 Train Loss 63053.973 Test MSE 1837.3856147866115 Test RE 0.01915962389534107\n",
      "92 Train Loss 62675.24 Test MSE 1738.4784583120274 Test RE 0.018636805830120794\n",
      "93 Train Loss 61821.84 Test MSE 2271.1526696465044 Test RE 0.02130149027254438\n",
      "94 Train Loss 61017.758 Test MSE 2883.0441752647575 Test RE 0.024000067993844038\n",
      "95 Train Loss 60292.23 Test MSE 1738.0728899468481 Test RE 0.018634631819934723\n",
      "96 Train Loss 59242.062 Test MSE 2037.3657554545343 Test RE 0.020175361482616425\n",
      "97 Train Loss 58368.062 Test MSE 2205.73920797064 Test RE 0.020992487592459207\n",
      "98 Train Loss 57876.844 Test MSE 2240.291120325045 Test RE 0.02115626763647312\n",
      "99 Train Loss 57142.695 Test MSE 2203.761913446422 Test RE 0.020983076317288095\n",
      "100 Train Loss 56498.36 Test MSE 1502.8632641268541 Test RE 0.017327924939140062\n",
      "101 Train Loss 56012.055 Test MSE 1421.096570739726 Test RE 0.01684995011180274\n",
      "102 Train Loss 55560.59 Test MSE 1343.423452176034 Test RE 0.016382994536117794\n",
      "103 Train Loss 55155.766 Test MSE 2324.569590103189 Test RE 0.021550537160878656\n",
      "104 Train Loss 54748.074 Test MSE 1467.8491665169913 Test RE 0.017124880079548866\n",
      "105 Train Loss 54399.56 Test MSE 1272.4619360709005 Test RE 0.01594443827541249\n",
      "106 Train Loss 54032.805 Test MSE 1068.5177054056653 Test RE 0.014610924112325875\n",
      "107 Train Loss 53747.004 Test MSE 1066.8917754767522 Test RE 0.014599803387379194\n",
      "108 Train Loss 53532.082 Test MSE 1439.6115704708097 Test RE 0.01695936112353222\n",
      "109 Train Loss 53272.156 Test MSE 1232.419921345768 Test RE 0.015691562051967815\n",
      "110 Train Loss 53022.855 Test MSE 971.1643019123621 Test RE 0.013929424661191174\n",
      "111 Train Loss 52782.508 Test MSE 1125.3233517389094 Test RE 0.014994275604206466\n",
      "112 Train Loss 52426.74 Test MSE 1200.136214140958 Test RE 0.015484674980352841\n",
      "113 Train Loss 52132.824 Test MSE 1214.0965749724492 Test RE 0.015574475885184399\n",
      "114 Train Loss 51770.81 Test MSE 941.3134078378617 Test RE 0.01371367793963646\n",
      "115 Train Loss 51321.05 Test MSE 1024.6272815503887 Test RE 0.014307698544652123\n",
      "116 Train Loss 50765.17 Test MSE 1205.154748278304 Test RE 0.015517016850947083\n",
      "117 Train Loss 50416.01 Test MSE 1228.0843283057175 Test RE 0.01566393666023616\n",
      "118 Train Loss 50035.926 Test MSE 964.8268081720713 Test RE 0.013883900885836707\n",
      "119 Train Loss 49609.76 Test MSE 1332.3769108119357 Test RE 0.016315499440021877\n",
      "120 Train Loss 49270.305 Test MSE 2039.3683759603414 Test RE 0.020185274692541624\n",
      "121 Train Loss 49011.805 Test MSE 1737.7817875775895 Test RE 0.018633071237332405\n",
      "122 Train Loss 48715.26 Test MSE 2060.695177619966 Test RE 0.02029054447618387\n",
      "123 Train Loss 48160.64 Test MSE 1773.2108296674303 Test RE 0.01882205383138869\n",
      "124 Train Loss 47670.74 Test MSE 1290.3786963604502 Test RE 0.016056297849627532\n",
      "125 Train Loss 47190.824 Test MSE 1129.1201360462699 Test RE 0.01501954926946028\n",
      "126 Train Loss 46754.066 Test MSE 856.2853864689022 Test RE 0.013079649006672394\n",
      "127 Train Loss 46339.65 Test MSE 973.1889345219851 Test RE 0.013943936770085152\n",
      "128 Train Loss 45985.95 Test MSE 970.7089696551565 Test RE 0.01392615885953452\n",
      "129 Train Loss 45700.016 Test MSE 1300.2144982829634 Test RE 0.016117375563394078\n",
      "130 Train Loss 45348.926 Test MSE 1097.9472157322289 Test RE 0.014810767162310579\n",
      "131 Train Loss 44880.56 Test MSE 863.5200473767843 Test RE 0.013134787044681207\n",
      "132 Train Loss 44536.55 Test MSE 930.9656616189875 Test RE 0.013638093227757837\n",
      "133 Train Loss 44333.31 Test MSE 1132.6743005844646 Test RE 0.015043169434265539\n",
      "134 Train Loss 44052.125 Test MSE 1036.6274164156096 Test RE 0.014391238447043342\n",
      "135 Train Loss 43732.145 Test MSE 1576.4009115693555 Test RE 0.017746804385871378\n",
      "136 Train Loss 43368.043 Test MSE 2704.0132793195107 Test RE 0.023242949236474183\n",
      "137 Train Loss 42843.54 Test MSE 984.742576254579 Test RE 0.014026463353270357\n",
      "138 Train Loss 42228.844 Test MSE 1116.9957795445237 Test RE 0.014938692567546055\n",
      "139 Train Loss 41730.52 Test MSE 872.0628779582632 Test RE 0.013199598571156148\n",
      "140 Train Loss 41236.4 Test MSE 1110.1145616233673 Test RE 0.0148926068038609\n",
      "141 Train Loss 40941.36 Test MSE 1204.4493468934477 Test RE 0.015512474974748823\n",
      "142 Train Loss 40671.664 Test MSE 1281.0441784975278 Test RE 0.01599811731860089\n",
      "143 Train Loss 40329.133 Test MSE 2142.5227214209867 Test RE 0.020689478397669443\n",
      "144 Train Loss 39994.95 Test MSE 2479.8053313435003 Test RE 0.022258485877384986\n",
      "145 Train Loss 39507.684 Test MSE 1435.105478800884 Test RE 0.016932798288235098\n",
      "146 Train Loss 39136.51 Test MSE 625.6296576486106 Test RE 0.011180094840503791\n",
      "147 Train Loss 38973.902 Test MSE 598.8355153628066 Test RE 0.010938067479347967\n",
      "148 Train Loss 38735.03 Test MSE 580.4406617858473 Test RE 0.010768760988584697\n",
      "149 Train Loss 38522.46 Test MSE 871.3266645189211 Test RE 0.013194025708264551\n",
      "150 Train Loss 38282.184 Test MSE 924.4044328610568 Test RE 0.013589949201377653\n",
      "151 Train Loss 38134.71 Test MSE 1049.1075953915881 Test RE 0.014477608864228062\n",
      "152 Train Loss 37915.33 Test MSE 1542.1308522071752 Test RE 0.017552841716379223\n",
      "153 Train Loss 37670.348 Test MSE 1613.850235308573 Test RE 0.01795636557282003\n",
      "154 Train Loss 37495.375 Test MSE 2047.2130178771943 Test RE 0.02022405980664072\n",
      "155 Train Loss 37123.875 Test MSE 1173.0333115280405 Test RE 0.01530882985651304\n",
      "156 Train Loss 36916.89 Test MSE 799.1026379350246 Test RE 0.012635374130891436\n",
      "157 Train Loss 36632.336 Test MSE 823.6390117473221 Test RE 0.012827891503578938\n",
      "158 Train Loss 36266.332 Test MSE 720.222905518793 Test RE 0.011995553397050865\n",
      "159 Train Loss 35901.086 Test MSE 976.6257689692019 Test RE 0.013968536703782074\n",
      "160 Train Loss 35485.57 Test MSE 1381.3466923786104 Test RE 0.01661262147866325\n",
      "161 Train Loss 35272.527 Test MSE 1215.541691400142 Test RE 0.015583742132205631\n",
      "162 Train Loss 34905.125 Test MSE 1241.0663719974384 Test RE 0.015746510521711644\n",
      "163 Train Loss 33975.887 Test MSE 1008.4231463652839 Test RE 0.014194111955958719\n",
      "164 Train Loss 33535.785 Test MSE 525.8833868679721 Test RE 0.010250181353156192\n",
      "165 Train Loss 33186.52 Test MSE 496.3875795099305 Test RE 0.009958576794486099\n",
      "166 Train Loss 32956.83 Test MSE 671.236314156115 Test RE 0.011580426264051586\n",
      "167 Train Loss 32779.844 Test MSE 749.0332998523365 Test RE 0.012233124239769883\n",
      "168 Train Loss 32464.156 Test MSE 699.49757306946 Test RE 0.011821699881186129\n",
      "169 Train Loss 32302.162 Test MSE 607.5211076503066 Test RE 0.011017105530402168\n",
      "170 Train Loss 32079.64 Test MSE 813.186619552988 Test RE 0.012746235423445301\n",
      "171 Train Loss 31970.016 Test MSE 747.4404762171642 Test RE 0.012220110412943852\n",
      "172 Train Loss 31804.215 Test MSE 552.2348331092297 Test RE 0.010503855102295533\n",
      "173 Train Loss 31718.16 Test MSE 563.0500994621772 Test RE 0.010606212970671842\n",
      "174 Train Loss 31607.496 Test MSE 632.5796736693835 Test RE 0.011242022239129491\n",
      "175 Train Loss 31380.254 Test MSE 529.7716826343035 Test RE 0.010288005647628235\n",
      "176 Train Loss 31219.346 Test MSE 425.3203022061623 Test RE 0.009218173555245086\n",
      "177 Train Loss 30990.812 Test MSE 650.2689089893375 Test RE 0.011398122456762078\n",
      "178 Train Loss 30674.152 Test MSE 1367.1054218184102 Test RE 0.01652676403899106\n",
      "179 Train Loss 30444.463 Test MSE 1333.9433115746715 Test RE 0.01632508723143577\n",
      "180 Train Loss 30159.26 Test MSE 695.9948904480183 Test RE 0.01179206458939018\n",
      "181 Train Loss 29839.986 Test MSE 534.8657675881944 Test RE 0.010337350105574327\n",
      "182 Train Loss 29502.723 Test MSE 638.6474179572989 Test RE 0.011295810656097787\n",
      "183 Train Loss 29140.451 Test MSE 522.2367497700889 Test RE 0.01021458057454395\n",
      "184 Train Loss 28934.184 Test MSE 468.3536230736404 Test RE 0.00967328015201954\n",
      "185 Train Loss 28741.146 Test MSE 656.4444682629439 Test RE 0.011452118155333828\n",
      "186 Train Loss 28584.93 Test MSE 753.839459531839 Test RE 0.012272308303161712\n",
      "187 Train Loss 28481.543 Test MSE 563.059822790386 Test RE 0.010606304549773451\n",
      "188 Train Loss 28354.273 Test MSE 444.4395571194604 Test RE 0.009423086477260487\n",
      "189 Train Loss 28208.566 Test MSE 435.1538380995157 Test RE 0.009324128129766155\n",
      "190 Train Loss 27992.322 Test MSE 396.48567677062294 Test RE 0.008900216581618405\n",
      "191 Train Loss 27796.672 Test MSE 369.305058508149 Test RE 0.008589728792118539\n",
      "192 Train Loss 27574.43 Test MSE 485.08324252993305 Test RE 0.009844529384601097\n",
      "193 Train Loss 27440.756 Test MSE 488.97488391880023 Test RE 0.00988393998661906\n",
      "194 Train Loss 27292.387 Test MSE 522.2207399722587 Test RE 0.010214424003196432\n",
      "195 Train Loss 27191.717 Test MSE 1084.4300812633335 Test RE 0.014719315074412672\n",
      "196 Train Loss 27096.357 Test MSE 1800.6034720555826 Test RE 0.0189668786361809\n",
      "197 Train Loss 26795.244 Test MSE 1906.1427534589432 Test RE 0.01951481932779882\n",
      "198 Train Loss 26618.031 Test MSE 910.5445808262406 Test RE 0.013487685497997193\n",
      "199 Train Loss 26417.646 Test MSE 884.4060561265657 Test RE 0.013292683906589147\n",
      "Training time: 46.95\n",
      "Training time: 46.95\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 477390.25 Test MSE 4599298.575730052 Test RE 0.9585894072853122\n",
      "1 Train Loss 445741.2 Test MSE 1505425.2818237392 Test RE 0.5484239674693032\n",
      "2 Train Loss 230730.42 Test MSE 1539101.5855978176 Test RE 0.5545241521188633\n",
      "3 Train Loss 158217.89 Test MSE 122517.2637895149 Test RE 0.15645356665865504\n",
      "4 Train Loss 133469.48 Test MSE 78864.5578595711 Test RE 0.12552428610480837\n",
      "5 Train Loss 125049.125 Test MSE 8849.772797965777 Test RE 0.042048730086812265\n",
      "6 Train Loss 124426.375 Test MSE 5175.09101943556 Test RE 0.032154800539134526\n",
      "7 Train Loss 124187.66 Test MSE 3244.6167199388437 Test RE 0.025460593587868136\n",
      "8 Train Loss 124080.95 Test MSE 2068.6294880980977 Test RE 0.020329569367474692\n",
      "9 Train Loss 123999.65 Test MSE 1349.4408823726008 Test RE 0.016419644699986354\n",
      "10 Train Loss 123895.766 Test MSE 1458.7656278126651 Test RE 0.01707181062328459\n",
      "11 Train Loss 123722.3 Test MSE 1800.6782303425089 Test RE 0.018967272369909593\n",
      "12 Train Loss 123465.37 Test MSE 5114.335803367639 Test RE 0.031965495708285226\n",
      "13 Train Loss 122680.42 Test MSE 1717.7399253879123 Test RE 0.018525311900389422\n",
      "14 Train Loss 122173.555 Test MSE 2670.684350291562 Test RE 0.023099261979672853\n",
      "15 Train Loss 121437.5 Test MSE 2972.7490144646554 Test RE 0.024370584511325722\n",
      "16 Train Loss 121103.31 Test MSE 1916.8741387637895 Test RE 0.019569675428455637\n",
      "17 Train Loss 120649.16 Test MSE 1667.050432570187 Test RE 0.018249929576813442\n",
      "18 Train Loss 120255.6 Test MSE 2465.9751116999364 Test RE 0.022196329755270274\n",
      "19 Train Loss 119929.805 Test MSE 3068.41840366693 Test RE 0.02475962787099093\n",
      "20 Train Loss 119648.914 Test MSE 3472.864067723298 Test RE 0.02634090645753962\n",
      "21 Train Loss 119250.13 Test MSE 3025.9811455929535 Test RE 0.02458781475575503\n",
      "22 Train Loss 118941.195 Test MSE 2146.08019970368 Test RE 0.020706647840506884\n",
      "23 Train Loss 118446.15 Test MSE 2309.2932558832144 Test RE 0.02147960879085986\n",
      "24 Train Loss 117844.47 Test MSE 2214.770323660371 Test RE 0.021035419214018677\n",
      "25 Train Loss 117177.805 Test MSE 2454.4290357954455 Test RE 0.02214430546728985\n",
      "26 Train Loss 116699.77 Test MSE 2645.7156924755636 Test RE 0.022991029054922053\n",
      "27 Train Loss 115947.984 Test MSE 4687.2016820985555 Test RE 0.0306015654722808\n",
      "28 Train Loss 115062.05 Test MSE 4821.497878603252 Test RE 0.03103686260404065\n",
      "29 Train Loss 114492.664 Test MSE 4639.199706587699 Test RE 0.03044446578388029\n",
      "30 Train Loss 113472.15 Test MSE 4262.633042481651 Test RE 0.02918272194799422\n",
      "31 Train Loss 112516.63 Test MSE 8012.338175919001 Test RE 0.040009807261922936\n",
      "32 Train Loss 111288.766 Test MSE 9179.108743711822 Test RE 0.04282398528194113\n",
      "33 Train Loss 109971.81 Test MSE 10796.271321703145 Test RE 0.04644336930935915\n",
      "34 Train Loss 109009.29 Test MSE 6854.942469417374 Test RE 0.03700741333776861\n",
      "35 Train Loss 107840.75 Test MSE 13768.461641923406 Test RE 0.05244807261217229\n",
      "36 Train Loss 107065.54 Test MSE 11554.879533429083 Test RE 0.048047360517262404\n",
      "37 Train Loss 105836.56 Test MSE 11694.05817238834 Test RE 0.04833585984109453\n",
      "38 Train Loss 104756.24 Test MSE 11123.280374550255 Test RE 0.047141485726418894\n",
      "39 Train Loss 103205.42 Test MSE 5478.617689478715 Test RE 0.03308432827209044\n",
      "40 Train Loss 101343.375 Test MSE 5087.039668014058 Test RE 0.03188007876785275\n",
      "41 Train Loss 100304.51 Test MSE 6966.412743080188 Test RE 0.037307094269338315\n",
      "42 Train Loss 98258.21 Test MSE 5313.961609464159 Test RE 0.03258337224002344\n",
      "43 Train Loss 96911.234 Test MSE 6694.375858625226 Test RE 0.03657142390087586\n",
      "44 Train Loss 95885.1 Test MSE 6767.981097053059 Test RE 0.03677192724554057\n",
      "45 Train Loss 93739.305 Test MSE 6806.539809703561 Test RE 0.03687652745020154\n",
      "46 Train Loss 92457.125 Test MSE 12587.732998653722 Test RE 0.05014880456381666\n",
      "47 Train Loss 91532.74 Test MSE 13789.727592639545 Test RE 0.052488561078116065\n",
      "48 Train Loss 89973.055 Test MSE 5657.565123496819 Test RE 0.03362030155814583\n",
      "49 Train Loss 88901.734 Test MSE 3404.703127961745 Test RE 0.026081132681925846\n",
      "50 Train Loss 87603.39 Test MSE 1952.0053850820564 Test RE 0.019748191473340757\n",
      "51 Train Loss 86787.21 Test MSE 2854.957108081295 Test RE 0.023882875661091783\n",
      "52 Train Loss 85731.234 Test MSE 3386.2444572663894 Test RE 0.02601033688746752\n",
      "53 Train Loss 85173.91 Test MSE 2197.0538738401688 Test RE 0.020951116743779703\n",
      "54 Train Loss 84529.375 Test MSE 2067.8689797872194 Test RE 0.020325832055450334\n",
      "55 Train Loss 83933.1 Test MSE 2032.0803484329208 Test RE 0.02014917466616212\n",
      "56 Train Loss 83229.39 Test MSE 1855.854274811498 Test RE 0.019255675541286026\n",
      "57 Train Loss 82503.2 Test MSE 2481.835173412379 Test RE 0.022267593844142765\n",
      "58 Train Loss 81513.21 Test MSE 2771.767029353354 Test RE 0.02353234386594122\n",
      "59 Train Loss 80740.914 Test MSE 4939.768860699107 Test RE 0.031415222335632924\n",
      "60 Train Loss 79840.71 Test MSE 11775.567474291756 Test RE 0.04850402134123959\n",
      "61 Train Loss 78765.79 Test MSE 12741.063293620875 Test RE 0.05045330962174417\n",
      "62 Train Loss 77510.445 Test MSE 6956.52287951697 Test RE 0.03728060336734154\n",
      "63 Train Loss 76277.266 Test MSE 2486.867878003245 Test RE 0.022290159699458745\n",
      "64 Train Loss 75562.62 Test MSE 2406.868310443821 Test RE 0.021928705148487725\n",
      "65 Train Loss 74994.09 Test MSE 2443.3980042525486 Test RE 0.022094487442391578\n",
      "66 Train Loss 74297.97 Test MSE 2335.2269162891566 Test RE 0.02159988144951064\n",
      "67 Train Loss 73359.1 Test MSE 3415.976287675906 Test RE 0.026124275033675284\n",
      "68 Train Loss 72905.945 Test MSE 2287.3378319018625 Test RE 0.021377257089179803\n",
      "69 Train Loss 71974.12 Test MSE 1670.3741377874449 Test RE 0.018268113544418128\n",
      "70 Train Loss 71388.92 Test MSE 2701.7781491750466 Test RE 0.02323334097068566\n",
      "71 Train Loss 70173.92 Test MSE 7392.82646338485 Test RE 0.0384319198124128\n",
      "72 Train Loss 69161.375 Test MSE 6525.806861052981 Test RE 0.03610804140125069\n",
      "73 Train Loss 68015.0 Test MSE 3490.1338972325398 Test RE 0.026406319157569167\n",
      "74 Train Loss 67059.81 Test MSE 2842.333916051512 Test RE 0.02383001810502429\n",
      "75 Train Loss 66399.31 Test MSE 2187.5405176305826 Test RE 0.02090570782848939\n",
      "76 Train Loss 65332.65 Test MSE 2821.685368817954 Test RE 0.023743302015091866\n",
      "77 Train Loss 64865.23 Test MSE 3113.7005694363033 Test RE 0.02494165379710657\n",
      "78 Train Loss 64118.895 Test MSE 2940.141312462489 Test RE 0.024236557060241786\n",
      "79 Train Loss 63171.688 Test MSE 3423.241524568665 Test RE 0.026152041361643393\n",
      "80 Train Loss 62314.75 Test MSE 2750.1202018051004 Test RE 0.023440272804313468\n",
      "81 Train Loss 61513.055 Test MSE 3266.218419425945 Test RE 0.025545207542660243\n",
      "82 Train Loss 61165.652 Test MSE 3061.8040198456533 Test RE 0.024732927139125505\n",
      "83 Train Loss 60558.465 Test MSE 2836.5509298024463 Test RE 0.023805763594726227\n",
      "84 Train Loss 59619.3 Test MSE 1148.4772611651226 Test RE 0.015147746347180689\n",
      "85 Train Loss 58851.547 Test MSE 2807.2459511705847 Test RE 0.02368247326588887\n",
      "86 Train Loss 57992.04 Test MSE 4268.141656418539 Test RE 0.029201572321025025\n",
      "87 Train Loss 57510.246 Test MSE 3025.8663940234796 Test RE 0.024587348540509934\n",
      "88 Train Loss 56581.41 Test MSE 5598.292466217543 Test RE 0.03344372280978912\n",
      "89 Train Loss 56077.58 Test MSE 6045.325537446951 Test RE 0.034753349654402535\n",
      "90 Train Loss 55395.117 Test MSE 4307.295941938346 Test RE 0.029335208520316857\n",
      "91 Train Loss 55022.004 Test MSE 3756.638664027117 Test RE 0.027395960771639215\n",
      "92 Train Loss 54619.133 Test MSE 2631.7804438116123 Test RE 0.022930401093571123\n",
      "93 Train Loss 54257.723 Test MSE 971.2244476572669 Test RE 0.013929855990189569\n",
      "94 Train Loss 53868.453 Test MSE 783.6659670472881 Test RE 0.012512737017058474\n",
      "95 Train Loss 53611.375 Test MSE 1189.1269195991717 Test RE 0.015413488015208554\n",
      "96 Train Loss 53412.4 Test MSE 1111.685537247455 Test RE 0.014903140694577665\n",
      "97 Train Loss 53122.74 Test MSE 1352.5287588586448 Test RE 0.016438420203154763\n",
      "98 Train Loss 52806.88 Test MSE 3429.0471154606744 Test RE 0.02617420803232598\n",
      "99 Train Loss 52349.547 Test MSE 5008.1888617742425 Test RE 0.03163203792482955\n",
      "100 Train Loss 51816.777 Test MSE 1959.0874429596045 Test RE 0.019783983180509113\n",
      "101 Train Loss 51326.105 Test MSE 959.3036254771769 Test RE 0.013844104427160728\n",
      "102 Train Loss 50925.977 Test MSE 800.2671645037711 Test RE 0.012644577499364124\n",
      "103 Train Loss 50522.58 Test MSE 1782.6270513132617 Test RE 0.018871962720474367\n",
      "104 Train Loss 50084.668 Test MSE 3951.5458548551446 Test RE 0.02809767188448862\n",
      "105 Train Loss 49788.934 Test MSE 3485.4703159565033 Test RE 0.02638867095482255\n",
      "106 Train Loss 49464.656 Test MSE 4243.443723950397 Test RE 0.029116961160526564\n",
      "107 Train Loss 49207.23 Test MSE 2996.3330417179973 Test RE 0.024467064416236636\n",
      "108 Train Loss 48964.95 Test MSE 1375.9131202080678 Test RE 0.016579916141473482\n",
      "109 Train Loss 48660.54 Test MSE 920.8824735545094 Test RE 0.013564035800878164\n",
      "110 Train Loss 48395.85 Test MSE 898.9112436246182 Test RE 0.013401247560815885\n",
      "111 Train Loss 48080.09 Test MSE 1001.9035636867043 Test RE 0.014148154193872324\n",
      "112 Train Loss 47768.266 Test MSE 951.085032147234 Test RE 0.013784673917261184\n",
      "113 Train Loss 47562.8 Test MSE 931.6577179672038 Test RE 0.013643161392988153\n",
      "114 Train Loss 47240.613 Test MSE 1039.0076381966244 Test RE 0.014407750984808626\n",
      "115 Train Loss 46979.598 Test MSE 750.47418625964 Test RE 0.01224488478079546\n",
      "116 Train Loss 46759.586 Test MSE 688.9966354215892 Test RE 0.01173262997921595\n",
      "117 Train Loss 46507.426 Test MSE 1862.9971139387258 Test RE 0.01929269571763649\n",
      "118 Train Loss 46247.03 Test MSE 1692.156881612403 Test RE 0.01838684165001465\n",
      "119 Train Loss 46014.965 Test MSE 1119.3457909687884 Test RE 0.014954398827851954\n",
      "120 Train Loss 45797.38 Test MSE 1081.6750365789078 Test RE 0.01470060563407285\n",
      "121 Train Loss 45463.777 Test MSE 1384.7972178895739 Test RE 0.01663335722958888\n",
      "122 Train Loss 45189.36 Test MSE 1474.4726864310626 Test RE 0.017163473729963602\n",
      "123 Train Loss 44977.594 Test MSE 1398.3505698010697 Test RE 0.01671455641902171\n",
      "124 Train Loss 44671.957 Test MSE 1191.4828765251723 Test RE 0.0154287494412639\n",
      "125 Train Loss 44458.35 Test MSE 768.6349928757479 Test RE 0.012392156794395192\n",
      "126 Train Loss 44185.758 Test MSE 994.8328371376767 Test RE 0.014098141968367818\n",
      "127 Train Loss 43984.082 Test MSE 882.7743538990275 Test RE 0.01328041594707031\n",
      "128 Train Loss 43520.85 Test MSE 1299.2620299696914 Test RE 0.016111471113800086\n",
      "129 Train Loss 43206.92 Test MSE 1623.2761349817206 Test RE 0.018008727457782\n",
      "130 Train Loss 42893.797 Test MSE 1245.374804012551 Test RE 0.01577381929163997\n",
      "131 Train Loss 42516.65 Test MSE 2917.809679150739 Test RE 0.024144338096911515\n",
      "132 Train Loss 42201.15 Test MSE 2995.6358900082064 Test RE 0.02446421789553284\n",
      "133 Train Loss 42002.5 Test MSE 2618.6052918031796 Test RE 0.022872932284598156\n",
      "134 Train Loss 41694.84 Test MSE 1909.6619995713695 Test RE 0.019532825791887403\n",
      "135 Train Loss 41397.285 Test MSE 758.9877465561701 Test RE 0.012314143377296258\n",
      "136 Train Loss 41217.332 Test MSE 600.4036699545442 Test RE 0.010952379728479948\n",
      "137 Train Loss 41035.203 Test MSE 593.8159319822595 Test RE 0.010892128251591054\n",
      "138 Train Loss 40874.293 Test MSE 780.8184488050105 Test RE 0.012489983271361112\n",
      "139 Train Loss 40719.406 Test MSE 936.412911743838 Test RE 0.013677934520311519\n",
      "140 Train Loss 40646.395 Test MSE 967.5170908316527 Test RE 0.013903244055149791\n",
      "141 Train Loss 40466.63 Test MSE 896.172772913594 Test RE 0.013380819005141745\n",
      "142 Train Loss 40297.29 Test MSE 964.5313160137201 Test RE 0.013881774650344824\n",
      "143 Train Loss 40169.42 Test MSE 667.647535954511 Test RE 0.011549427285568434\n",
      "144 Train Loss 40012.926 Test MSE 655.231970738674 Test RE 0.011441536846896711\n",
      "145 Train Loss 39733.26 Test MSE 1133.1544162622088 Test RE 0.015046357330506422\n",
      "146 Train Loss 39493.312 Test MSE 1227.9531504870515 Test RE 0.015663100066254702\n",
      "147 Train Loss 39302.84 Test MSE 1095.1086653026991 Test RE 0.014791609448464032\n",
      "148 Train Loss 39164.715 Test MSE 1077.6986185378812 Test RE 0.014673559814519113\n",
      "149 Train Loss 38988.617 Test MSE 855.2580164224444 Test RE 0.013071800179134712\n",
      "150 Train Loss 38838.586 Test MSE 931.0277608401599 Test RE 0.01363854807850042\n",
      "151 Train Loss 38682.117 Test MSE 1175.1763395493672 Test RE 0.01532280741397861\n",
      "152 Train Loss 38557.88 Test MSE 980.9773290186603 Test RE 0.013999621981605476\n",
      "153 Train Loss 38412.582 Test MSE 1151.8775406972013 Test RE 0.015170153627781072\n",
      "154 Train Loss 38286.895 Test MSE 1618.4502557414646 Test RE 0.017981938228576064\n",
      "155 Train Loss 38192.85 Test MSE 1603.1780649146165 Test RE 0.017896895600930923\n",
      "156 Train Loss 38069.727 Test MSE 1682.4513861068547 Test RE 0.018334036253925406\n",
      "157 Train Loss 37980.297 Test MSE 2040.231705979006 Test RE 0.020189546777435164\n",
      "158 Train Loss 37868.01 Test MSE 2453.051123353658 Test RE 0.022138088706232482\n",
      "159 Train Loss 37639.312 Test MSE 2230.6363529571286 Test RE 0.021110630847416937\n",
      "160 Train Loss 37491.707 Test MSE 2359.035527222249 Test RE 0.021709712110495775\n",
      "161 Train Loss 37394.63 Test MSE 2635.3698590554213 Test RE 0.02294603284707802\n",
      "162 Train Loss 37319.734 Test MSE 1936.7683535302817 Test RE 0.0196709649135974\n",
      "163 Train Loss 37245.57 Test MSE 1264.1223327770872 Test RE 0.015892103163470526\n",
      "164 Train Loss 37094.188 Test MSE 812.4186471356677 Test RE 0.012740215237421023\n",
      "165 Train Loss 36954.707 Test MSE 982.2606087864289 Test RE 0.014008775893484378\n",
      "166 Train Loss 36799.14 Test MSE 1469.2376868255824 Test RE 0.017132977854061957\n",
      "167 Train Loss 36682.797 Test MSE 1459.3069516974408 Test RE 0.017074977863278037\n",
      "168 Train Loss 36512.605 Test MSE 782.1231587357189 Test RE 0.01250041399539112\n",
      "169 Train Loss 36377.457 Test MSE 598.6739131028951 Test RE 0.010936591501682073\n",
      "170 Train Loss 36282.266 Test MSE 579.8409385120614 Test RE 0.010763196297748462\n",
      "171 Train Loss 36204.793 Test MSE 593.5488943246509 Test RE 0.010889678893692268\n",
      "172 Train Loss 36098.367 Test MSE 594.8086820517018 Test RE 0.010901229258207802\n",
      "173 Train Loss 35942.594 Test MSE 743.2660704208092 Test RE 0.012185938378381327\n",
      "174 Train Loss 35773.26 Test MSE 890.2753957957228 Test RE 0.013336719257153584\n",
      "175 Train Loss 35585.355 Test MSE 811.842775670547 Test RE 0.012735699076304289\n",
      "176 Train Loss 35459.473 Test MSE 654.3211883619339 Test RE 0.011433582126768824\n",
      "177 Train Loss 35340.383 Test MSE 607.7538541319432 Test RE 0.011019215701533502\n",
      "178 Train Loss 35176.47 Test MSE 546.5688856426392 Test RE 0.010449831224723993\n",
      "179 Train Loss 34992.297 Test MSE 775.3175900610715 Test RE 0.012445909601247035\n",
      "180 Train Loss 34804.297 Test MSE 904.6439989696544 Test RE 0.013443912492799538\n",
      "181 Train Loss 34518.215 Test MSE 719.3549205315977 Test RE 0.011988322927777163\n",
      "182 Train Loss 34284.207 Test MSE 1152.4472569324419 Test RE 0.015173904727228925\n",
      "183 Train Loss 34130.402 Test MSE 1137.4508210266945 Test RE 0.015074854806211919\n",
      "184 Train Loss 33834.734 Test MSE 1218.2141638554667 Test RE 0.015600863821997457\n",
      "185 Train Loss 33664.15 Test MSE 989.7622607325237 Test RE 0.01406216756878335\n",
      "186 Train Loss 33536.555 Test MSE 897.7915249870723 Test RE 0.01339289840366792\n",
      "187 Train Loss 33418.42 Test MSE 775.4881900528796 Test RE 0.012447278817756178\n",
      "188 Train Loss 33290.688 Test MSE 716.5240121123155 Test RE 0.01196471059229006\n",
      "189 Train Loss 33133.332 Test MSE 500.38304117837606 Test RE 0.009998575142258565\n",
      "190 Train Loss 32989.82 Test MSE 619.7431069954395 Test RE 0.011127373767305316\n",
      "191 Train Loss 32835.23 Test MSE 792.8702228287103 Test RE 0.01258600435003125\n",
      "192 Train Loss 32683.732 Test MSE 548.6211737107828 Test RE 0.010469431654313398\n",
      "193 Train Loss 32475.227 Test MSE 457.68823810078317 Test RE 0.009562505535908981\n",
      "194 Train Loss 32284.521 Test MSE 486.82889131032766 Test RE 0.009862227025076258\n",
      "195 Train Loss 32091.771 Test MSE 522.0207486582534 Test RE 0.010212467941766652\n",
      "196 Train Loss 31908.914 Test MSE 622.7856105495015 Test RE 0.011154654122966299\n",
      "197 Train Loss 31710.541 Test MSE 1180.4430061087212 Test RE 0.015357104353246899\n",
      "198 Train Loss 31519.027 Test MSE 1469.3205762289617 Test RE 0.017133461139474466\n",
      "199 Train Loss 31391.89 Test MSE 1347.31448918419 Test RE 0.016406702900829392\n",
      "Training time: 46.75\n",
      "Training time: 46.75\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1389069.0 Test MSE 812924.7997919924 Test RE 0.40300646212168356\n",
      "1 Train Loss 195153.11 Test MSE 735452.097158139 Test RE 0.3833222414646742\n",
      "2 Train Loss 154826.34 Test MSE 298129.3790682022 Test RE 0.24405584399327876\n",
      "3 Train Loss 130862.586 Test MSE 86345.66590381988 Test RE 0.13134304943767028\n",
      "4 Train Loss 125290.5 Test MSE 4021.398940532872 Test RE 0.02834493094216482\n",
      "5 Train Loss 124462.21 Test MSE 3431.9200453367216 Test RE 0.026185170395622976\n",
      "6 Train Loss 124011.65 Test MSE 2755.694132544035 Test RE 0.023464015097754216\n",
      "7 Train Loss 123681.766 Test MSE 2717.3507628131106 Test RE 0.02330020138165197\n",
      "8 Train Loss 123495.63 Test MSE 2045.5294173507295 Test RE 0.02021574209843707\n",
      "9 Train Loss 123233.65 Test MSE 2118.6812712331907 Test RE 0.020574042726862026\n",
      "10 Train Loss 122914.36 Test MSE 2329.3567292234993 Test RE 0.021572715967813804\n",
      "11 Train Loss 122534.6 Test MSE 4640.2217658986565 Test RE 0.03044781920021161\n",
      "12 Train Loss 122248.84 Test MSE 5908.538356773177 Test RE 0.034357919140138846\n",
      "13 Train Loss 121977.29 Test MSE 3096.1290216672996 Test RE 0.02487117760402333\n",
      "14 Train Loss 121654.2 Test MSE 2900.272601410115 Test RE 0.024071670697720902\n",
      "15 Train Loss 121439.336 Test MSE 2491.056132649604 Test RE 0.02230892177204704\n",
      "16 Train Loss 121141.31 Test MSE 1895.3075371892764 Test RE 0.019459275578357364\n",
      "17 Train Loss 120884.31 Test MSE 1907.1920296558183 Test RE 0.019520189759321974\n",
      "18 Train Loss 120557.625 Test MSE 2001.967901684675 Test RE 0.01999932687294934\n",
      "19 Train Loss 120241.96 Test MSE 6059.548120989775 Test RE 0.0347942070108757\n",
      "20 Train Loss 119614.31 Test MSE 9095.27680515729 Test RE 0.04262798301895471\n",
      "21 Train Loss 119007.73 Test MSE 14062.026548275406 Test RE 0.05300426058275023\n",
      "22 Train Loss 118462.35 Test MSE 10287.644644910606 Test RE 0.04533616715546655\n",
      "23 Train Loss 117586.94 Test MSE 7769.0629039993955 Test RE 0.03939772482902282\n",
      "24 Train Loss 115760.984 Test MSE 6922.236073265401 Test RE 0.03718861691408613\n",
      "25 Train Loss 115016.695 Test MSE 7187.680248128724 Test RE 0.03789493775584124\n",
      "26 Train Loss 114008.48 Test MSE 9278.962741438541 Test RE 0.043056283409737106\n",
      "27 Train Loss 112828.91 Test MSE 7117.923840380935 Test RE 0.03771060431775875\n",
      "28 Train Loss 111597.766 Test MSE 3964.354074366866 Test RE 0.02814317179917551\n",
      "29 Train Loss 110497.375 Test MSE 2665.9848589700723 Test RE 0.023078929629636515\n",
      "30 Train Loss 109869.49 Test MSE 3741.316353756862 Test RE 0.027340033338205488\n",
      "31 Train Loss 109191.87 Test MSE 3774.822577171561 Test RE 0.027462185451550488\n",
      "32 Train Loss 107973.23 Test MSE 1921.2966275490082 Test RE 0.019592237371135364\n",
      "33 Train Loss 106731.73 Test MSE 2679.47530057337 Test RE 0.02313724805502298\n",
      "34 Train Loss 105658.39 Test MSE 3577.743447420962 Test RE 0.02673569149040876\n",
      "35 Train Loss 104795.766 Test MSE 3707.8098868614184 Test RE 0.027217332114812743\n",
      "36 Train Loss 104061.79 Test MSE 6012.079600741364 Test RE 0.03465765583575565\n",
      "37 Train Loss 103026.875 Test MSE 4637.115668071511 Test RE 0.030437626827763267\n",
      "38 Train Loss 102297.57 Test MSE 3267.4099278244184 Test RE 0.025549866532057184\n",
      "39 Train Loss 100701.63 Test MSE 2439.427382500777 Test RE 0.022076527920035335\n",
      "40 Train Loss 99783.445 Test MSE 2535.123351982714 Test RE 0.02250538109793384\n",
      "41 Train Loss 99079.16 Test MSE 2534.351051430637 Test RE 0.02250195281461847\n",
      "42 Train Loss 98490.336 Test MSE 4317.24119302018 Test RE 0.029369055486830636\n",
      "43 Train Loss 98012.14 Test MSE 4513.854187964058 Test RE 0.030030363325172924\n",
      "44 Train Loss 97242.125 Test MSE 4278.839862489995 Test RE 0.029238146660159666\n",
      "45 Train Loss 96789.63 Test MSE 3850.0059611520314 Test RE 0.027734320157826033\n",
      "46 Train Loss 96281.14 Test MSE 2570.9296119198193 Test RE 0.02266375761862653\n",
      "47 Train Loss 95907.414 Test MSE 2340.5763741031415 Test RE 0.021624607431454584\n",
      "48 Train Loss 95616.59 Test MSE 2346.0636018643117 Test RE 0.021649940867068555\n",
      "49 Train Loss 95257.19 Test MSE 2747.299293710785 Test RE 0.023428247908063783\n",
      "50 Train Loss 94828.56 Test MSE 4788.931853280067 Test RE 0.03093186828490268\n",
      "51 Train Loss 94362.8 Test MSE 6622.984699276815 Test RE 0.036375895998334316\n",
      "52 Train Loss 93761.45 Test MSE 6996.905895723098 Test RE 0.037388654808524296\n",
      "53 Train Loss 93449.21 Test MSE 5708.419795141048 Test RE 0.03377106645308828\n",
      "54 Train Loss 92964.62 Test MSE 4160.816319107674 Test RE 0.028832088058187804\n",
      "55 Train Loss 92647.16 Test MSE 3627.338398194461 Test RE 0.02692035981211057\n",
      "56 Train Loss 92408.69 Test MSE 2785.508370391809 Test RE 0.023590603843935586\n",
      "57 Train Loss 92079.49 Test MSE 3118.3461526787423 Test RE 0.02496025310364303\n",
      "58 Train Loss 91811.76 Test MSE 3480.8878628908815 Test RE 0.026371318258172616\n",
      "59 Train Loss 91487.53 Test MSE 2886.4393143913067 Test RE 0.024014195351910194\n",
      "60 Train Loss 91238.21 Test MSE 2491.226454426317 Test RE 0.022309684426528354\n",
      "61 Train Loss 90964.69 Test MSE 1981.9141888268682 Test RE 0.019898908135820834\n",
      "62 Train Loss 90670.664 Test MSE 2076.763473070486 Test RE 0.020369498745722626\n",
      "63 Train Loss 90356.6 Test MSE 1849.6593635493514 Test RE 0.01922351059286504\n",
      "64 Train Loss 89977.52 Test MSE 2033.9872342659924 Test RE 0.020158626351037604\n",
      "65 Train Loss 89522.19 Test MSE 2309.831782042521 Test RE 0.021482113162468057\n",
      "66 Train Loss 89145.51 Test MSE 2497.7540701296307 Test RE 0.022338893689116044\n",
      "67 Train Loss 88767.11 Test MSE 3216.2518331744486 Test RE 0.025349059280755912\n",
      "68 Train Loss 88250.98 Test MSE 2209.5214099402183 Test RE 0.02101047789265632\n",
      "69 Train Loss 87741.45 Test MSE 1953.0222756795436 Test RE 0.019753334680342415\n",
      "70 Train Loss 87401.8 Test MSE 2106.2542275918295 Test RE 0.020513615859884312\n",
      "71 Train Loss 87129.61 Test MSE 2756.395447756973 Test RE 0.023467000665656115\n",
      "72 Train Loss 86744.17 Test MSE 2543.983603120395 Test RE 0.022544674926291634\n",
      "73 Train Loss 86282.39 Test MSE 1982.593344072096 Test RE 0.01990231728701886\n",
      "74 Train Loss 85728.44 Test MSE 4392.838820503514 Test RE 0.029625074965651012\n",
      "75 Train Loss 85137.31 Test MSE 4637.5711339683085 Test RE 0.030439121610467527\n",
      "76 Train Loss 84703.39 Test MSE 2497.7508008540576 Test RE 0.022338879069577596\n",
      "77 Train Loss 84157.3 Test MSE 1290.1500732715378 Test RE 0.01605487539781897\n",
      "78 Train Loss 83871.41 Test MSE 1063.0498590898596 Test RE 0.014573492465815562\n",
      "79 Train Loss 83549.18 Test MSE 1420.3122711225194 Test RE 0.016845299747574895\n",
      "80 Train Loss 83270.83 Test MSE 1114.5275083073702 Test RE 0.01492217811950561\n",
      "81 Train Loss 83179.805 Test MSE 956.4734782126731 Test RE 0.013823667834471427\n",
      "82 Train Loss 83060.85 Test MSE 1060.8047351744704 Test RE 0.014558094980009314\n",
      "83 Train Loss 82956.19 Test MSE 1191.3646364222898 Test RE 0.015427983864930715\n",
      "84 Train Loss 82823.27 Test MSE 1592.473342188033 Test RE 0.01783704504920701\n",
      "85 Train Loss 82632.15 Test MSE 1590.059867949823 Test RE 0.017823523450203254\n",
      "86 Train Loss 82420.664 Test MSE 1092.5040605887611 Test RE 0.014774008806664662\n",
      "87 Train Loss 82192.77 Test MSE 1141.5894300430891 Test RE 0.015102254796359835\n",
      "88 Train Loss 82047.63 Test MSE 1273.3138090706352 Test RE 0.01594977453088391\n",
      "89 Train Loss 81791.12 Test MSE 1140.7356695337467 Test RE 0.015096606478315629\n",
      "90 Train Loss 81643.805 Test MSE 1190.254532935398 Test RE 0.015420794357352944\n",
      "91 Train Loss 81473.56 Test MSE 1222.991802445441 Test RE 0.01563142591664449\n",
      "92 Train Loss 81312.72 Test MSE 1111.1642056340634 Test RE 0.01489964582609998\n",
      "93 Train Loss 81070.65 Test MSE 1043.2146143591121 Test RE 0.014436890247385111\n",
      "94 Train Loss 80939.05 Test MSE 1229.414758568159 Test RE 0.015672419031441155\n",
      "95 Train Loss 80787.48 Test MSE 1626.867691044066 Test RE 0.018028638924495785\n",
      "96 Train Loss 80654.766 Test MSE 2033.5394313704323 Test RE 0.020156407165979672\n",
      "97 Train Loss 80453.99 Test MSE 1262.8456354490033 Test RE 0.015884076040415293\n",
      "98 Train Loss 80137.625 Test MSE 1251.6433388967714 Test RE 0.015813467846828588\n",
      "99 Train Loss 79958.56 Test MSE 1238.663010737763 Test RE 0.01573125634443729\n",
      "100 Train Loss 79800.04 Test MSE 1346.2636254752754 Test RE 0.016400303291824143\n",
      "101 Train Loss 79602.72 Test MSE 1170.6947448971048 Test RE 0.015293562353343188\n",
      "102 Train Loss 79496.08 Test MSE 1153.1796388655887 Test RE 0.015178725480879299\n",
      "103 Train Loss 79406.766 Test MSE 1611.2003836093195 Test RE 0.017941617843303524\n",
      "104 Train Loss 79331.69 Test MSE 1715.6280985789497 Test RE 0.018513920687560654\n",
      "105 Train Loss 79175.11 Test MSE 1182.0924299449402 Test RE 0.015367829789179598\n",
      "106 Train Loss 79062.266 Test MSE 1124.004761298496 Test RE 0.01498548830606508\n",
      "107 Train Loss 78895.72 Test MSE 1256.4032409033337 Test RE 0.015843508006326442\n",
      "108 Train Loss 78753.16 Test MSE 1415.8953226289073 Test RE 0.016819086231078088\n",
      "109 Train Loss 78635.19 Test MSE 1216.1893779800575 Test RE 0.015587893382853657\n",
      "110 Train Loss 78540.96 Test MSE 1144.4022293771093 Test RE 0.015120848818246562\n",
      "111 Train Loss 78467.0 Test MSE 1168.5537311595765 Test RE 0.015279571211884556\n",
      "112 Train Loss 78415.375 Test MSE 1118.6029655163434 Test RE 0.014949435950541523\n",
      "113 Train Loss 78353.56 Test MSE 1222.7154694334859 Test RE 0.015629659869227635\n",
      "114 Train Loss 78221.07 Test MSE 1180.725853302495 Test RE 0.015358944109033087\n",
      "115 Train Loss 78117.08 Test MSE 1001.1619684747726 Test RE 0.01414291709019629\n",
      "116 Train Loss 78051.46 Test MSE 1254.432363450893 Test RE 0.015831076540481683\n",
      "117 Train Loss 77994.0 Test MSE 1451.8926152405515 Test RE 0.017031545998245443\n",
      "118 Train Loss 77874.43 Test MSE 1392.964785294917 Test RE 0.016682337085818903\n",
      "119 Train Loss 77804.45 Test MSE 1052.001908636062 Test RE 0.014497565765883085\n",
      "120 Train Loss 77708.69 Test MSE 998.0449208485364 Test RE 0.014120883436163155\n",
      "121 Train Loss 77583.21 Test MSE 1025.7767008853468 Test RE 0.014315721430683083\n",
      "122 Train Loss 77497.555 Test MSE 1025.9449210147477 Test RE 0.014316895221131781\n",
      "123 Train Loss 77418.97 Test MSE 1056.9477860921695 Test RE 0.014531605202678032\n",
      "124 Train Loss 77313.38 Test MSE 1129.9827597504395 Test RE 0.015025285481749947\n",
      "125 Train Loss 77239.914 Test MSE 1006.5041360819506 Test RE 0.014180599960568996\n",
      "126 Train Loss 77128.9 Test MSE 995.4427392290025 Test RE 0.014102462879622886\n",
      "127 Train Loss 77046.086 Test MSE 1126.765515151966 Test RE 0.01500388051995659\n",
      "128 Train Loss 76963.195 Test MSE 1303.1447357200097 Test RE 0.016135526860155854\n",
      "129 Train Loss 76912.555 Test MSE 1232.6306622676718 Test RE 0.015692903604810994\n",
      "130 Train Loss 76796.28 Test MSE 1328.705458236133 Test RE 0.01629300471476692\n",
      "131 Train Loss 76703.38 Test MSE 1433.546036666615 Test RE 0.01692359586487124\n",
      "132 Train Loss 76607.11 Test MSE 1226.1199569202415 Test RE 0.015651404091223585\n",
      "133 Train Loss 76502.97 Test MSE 1045.109044559031 Test RE 0.014449992669129792\n",
      "134 Train Loss 76406.805 Test MSE 1343.676208413876 Test RE 0.016384535639670895\n",
      "135 Train Loss 76249.18 Test MSE 1769.0819658715159 Test RE 0.018800127790072164\n",
      "136 Train Loss 76145.37 Test MSE 1216.4706250304432 Test RE 0.015589695649708041\n",
      "137 Train Loss 76023.49 Test MSE 1189.0214276463046 Test RE 0.015412804305594314\n",
      "138 Train Loss 75935.73 Test MSE 1115.6535616023582 Test RE 0.014929714463661892\n",
      "139 Train Loss 75874.69 Test MSE 1105.3479596460447 Test RE 0.014860599524369938\n",
      "140 Train Loss 75808.836 Test MSE 1132.8723219867597 Test RE 0.015044484348588178\n",
      "141 Train Loss 75708.72 Test MSE 1180.7359123230524 Test RE 0.015359009533028439\n",
      "142 Train Loss 75607.32 Test MSE 1303.5218950870042 Test RE 0.01613786168324248\n",
      "143 Train Loss 75505.836 Test MSE 1223.6953799196874 Test RE 0.015635921587770104\n",
      "144 Train Loss 75270.734 Test MSE 1293.8617025396895 Test RE 0.016077952925697782\n",
      "145 Train Loss 75120.664 Test MSE 1460.8571183026868 Test RE 0.017084044507858404\n",
      "146 Train Loss 75052.33 Test MSE 1306.8435675025523 Test RE 0.01615841008537686\n",
      "147 Train Loss 74923.38 Test MSE 1280.9026103293254 Test RE 0.015997233318354696\n",
      "148 Train Loss 74742.05 Test MSE 1307.563750391671 Test RE 0.01616286180704504\n",
      "149 Train Loss 74567.94 Test MSE 1297.3505076777878 Test RE 0.01609961485563175\n",
      "150 Train Loss 74454.38 Test MSE 1464.3639945749048 Test RE 0.017104537859768957\n",
      "151 Train Loss 74343.266 Test MSE 1440.0103346311653 Test RE 0.01696170978385454\n",
      "152 Train Loss 74053.77 Test MSE 1334.5214526008979 Test RE 0.01632862455576505\n",
      "153 Train Loss 73917.586 Test MSE 1286.8784640217982 Test RE 0.016034506209339713\n",
      "154 Train Loss 73781.94 Test MSE 1298.42457366263 Test RE 0.016106277847274003\n",
      "155 Train Loss 73631.58 Test MSE 1420.0011857017878 Test RE 0.01684345486661478\n",
      "156 Train Loss 73491.11 Test MSE 1510.3979073567837 Test RE 0.017371307628449604\n",
      "157 Train Loss 73315.23 Test MSE 1269.5190808151585 Test RE 0.015925990047771003\n",
      "158 Train Loss 73197.695 Test MSE 1194.8680033448363 Test RE 0.015450651237198236\n",
      "159 Train Loss 73099.54 Test MSE 1553.1555031872665 Test RE 0.017615472370463296\n",
      "160 Train Loss 72990.984 Test MSE 1622.1528729156742 Test RE 0.01800249560955232\n",
      "161 Train Loss 72860.836 Test MSE 1210.8116963493171 Test RE 0.015553392342140317\n",
      "162 Train Loss 72722.75 Test MSE 1307.303055803188 Test RE 0.01616125049721538\n",
      "163 Train Loss 72647.43 Test MSE 1143.1135074172505 Test RE 0.015112332555993542\n",
      "164 Train Loss 72533.99 Test MSE 1047.8195851860785 Test RE 0.01446871891100306\n",
      "165 Train Loss 72458.14 Test MSE 1083.6008388315338 Test RE 0.014713686212243653\n",
      "166 Train Loss 72363.68 Test MSE 1166.4404745745203 Test RE 0.015265748883783173\n",
      "167 Train Loss 72247.63 Test MSE 1453.271810605429 Test RE 0.017039633460527278\n",
      "168 Train Loss 72103.39 Test MSE 1149.4519477403271 Test RE 0.015154172758179095\n",
      "169 Train Loss 71956.33 Test MSE 1155.677612968473 Test RE 0.015195156378118725\n",
      "170 Train Loss 71798.11 Test MSE 1373.4599458038028 Test RE 0.01656512902625392\n",
      "171 Train Loss 71684.65 Test MSE 1600.038624072556 Test RE 0.017879363618669358\n",
      "172 Train Loss 71454.82 Test MSE 1119.5888670371025 Test RE 0.014956022481228539\n",
      "173 Train Loss 71323.47 Test MSE 1096.1439228206705 Test RE 0.014798599397509982\n",
      "174 Train Loss 71159.516 Test MSE 1412.8562127144057 Test RE 0.016801026100147526\n",
      "175 Train Loss 70917.59 Test MSE 1678.7006755562365 Test RE 0.01831358869995601\n",
      "176 Train Loss 70833.086 Test MSE 1288.9734702245808 Test RE 0.016047552790710466\n",
      "177 Train Loss 70768.945 Test MSE 1354.9260959063108 Test RE 0.016452982180578155\n",
      "178 Train Loss 70697.71 Test MSE 1153.408138530964 Test RE 0.015180229219665055\n",
      "179 Train Loss 70646.11 Test MSE 1151.6062762839388 Test RE 0.015168367255106258\n",
      "180 Train Loss 70588.92 Test MSE 1227.4073470383198 Test RE 0.015659618693936585\n",
      "181 Train Loss 70534.28 Test MSE 1391.4717844201475 Test RE 0.016673394497477418\n",
      "182 Train Loss 70491.49 Test MSE 1434.0388155423786 Test RE 0.016926504342790402\n",
      "183 Train Loss 70398.77 Test MSE 1265.3829640744793 Test RE 0.015900025296443673\n",
      "184 Train Loss 70337.1 Test MSE 1620.5636111010238 Test RE 0.017993674711598035\n",
      "185 Train Loss 70284.914 Test MSE 1752.6584273666815 Test RE 0.018712657398603096\n",
      "186 Train Loss 70228.2 Test MSE 1757.8084396805957 Test RE 0.018740129872596385\n",
      "187 Train Loss 70169.9 Test MSE 1593.7977902178757 Test RE 0.017844460975290122\n",
      "188 Train Loss 70120.67 Test MSE 1355.936205075488 Test RE 0.016459113958505207\n",
      "189 Train Loss 70038.71 Test MSE 1002.409444896062 Test RE 0.014151725586557673\n",
      "190 Train Loss 69962.41 Test MSE 1003.039171308834 Test RE 0.01415617003600876\n",
      "191 Train Loss 69879.24 Test MSE 1156.2224012282159 Test RE 0.015198737466544397\n",
      "192 Train Loss 69761.35 Test MSE 1044.1673743019435 Test RE 0.014443481293662462\n",
      "193 Train Loss 69675.07 Test MSE 962.1333265554193 Test RE 0.013864507681087343\n",
      "194 Train Loss 69611.484 Test MSE 1073.683221205208 Test RE 0.01464619819547426\n",
      "195 Train Loss 69518.21 Test MSE 1064.2288212204064 Test RE 0.014581571501084467\n",
      "196 Train Loss 69422.12 Test MSE 994.0315180117997 Test RE 0.014092462930545602\n",
      "197 Train Loss 69336.72 Test MSE 997.9431043760977 Test RE 0.014120163140323215\n",
      "198 Train Loss 69263.28 Test MSE 950.1559924401841 Test RE 0.01377793969375345\n",
      "199 Train Loss 69202.664 Test MSE 1032.8611943597264 Test RE 0.014365071900538384\n",
      "Training time: 47.30\n",
      "Training time: 47.30\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 631318.75 Test MSE 3668777.1426281394 Test RE 0.8561453248518438\n",
      "1 Train Loss 250400.89 Test MSE 720694.395648268 Test RE 0.3794568490525218\n",
      "2 Train Loss 181307.89 Test MSE 274217.9227124372 Test RE 0.2340640643441313\n",
      "3 Train Loss 156517.66 Test MSE 181992.66175981797 Test RE 0.1906837841984506\n",
      "4 Train Loss 146514.72 Test MSE 159433.25934091295 Test RE 0.17847454602932442\n",
      "5 Train Loss 138112.95 Test MSE 158135.63688806372 Test RE 0.17774676269896003\n",
      "6 Train Loss 130714.0 Test MSE 77485.52986457518 Test RE 0.1244219854453879\n",
      "7 Train Loss 129118.36 Test MSE 71201.81941925426 Test RE 0.1192703158382126\n",
      "8 Train Loss 126779.16 Test MSE 25079.87758629601 Test RE 0.07078635175019626\n",
      "9 Train Loss 125413.07 Test MSE 9532.812198012367 Test RE 0.04364126616357626\n",
      "10 Train Loss 124654.836 Test MSE 2746.0135645208784 Test RE 0.023422765085824953\n",
      "11 Train Loss 124526.6 Test MSE 2470.055077988706 Test RE 0.022214684127455735\n",
      "12 Train Loss 124374.445 Test MSE 1381.9580488955937 Test RE 0.016616297279594707\n",
      "13 Train Loss 124305.99 Test MSE 1482.7901629162443 Test RE 0.017211815090951814\n",
      "14 Train Loss 124150.17 Test MSE 1608.3832350658367 Test RE 0.0179259257182633\n",
      "15 Train Loss 124026.77 Test MSE 1533.2006210698123 Test RE 0.017501945087629735\n",
      "16 Train Loss 123848.055 Test MSE 1817.4931783117063 Test RE 0.01905562591044544\n",
      "17 Train Loss 123718.164 Test MSE 1597.8799042074154 Test RE 0.017867298421015632\n",
      "18 Train Loss 123455.19 Test MSE 1688.3599730394646 Test RE 0.01836620161255059\n",
      "19 Train Loss 123268.305 Test MSE 2705.6847745458076 Test RE 0.023250131981647108\n",
      "20 Train Loss 122892.2 Test MSE 2868.7206367209487 Test RE 0.02394037519755287\n",
      "21 Train Loss 122514.18 Test MSE 4739.741886533503 Test RE 0.030772598434184513\n",
      "22 Train Loss 122108.57 Test MSE 5714.388721145017 Test RE 0.03378871795188169\n",
      "23 Train Loss 121488.28 Test MSE 7738.061641052384 Test RE 0.03931904094455166\n",
      "24 Train Loss 120987.484 Test MSE 7934.16424275198 Test RE 0.039814147116443266\n",
      "25 Train Loss 120691.734 Test MSE 4728.448030633357 Test RE 0.030735914097983742\n",
      "26 Train Loss 120289.78 Test MSE 12175.683632640104 Test RE 0.04932118485214289\n",
      "27 Train Loss 119908.41 Test MSE 11171.60632339712 Test RE 0.0472437796498179\n",
      "28 Train Loss 119461.234 Test MSE 10358.975133897371 Test RE 0.045493067246290944\n",
      "29 Train Loss 118984.84 Test MSE 11895.658136768823 Test RE 0.04875072301234705\n",
      "30 Train Loss 118494.32 Test MSE 10901.869549363208 Test RE 0.04666994769519743\n",
      "31 Train Loss 118230.23 Test MSE 14755.47057660002 Test RE 0.054295439869087116\n",
      "32 Train Loss 117908.945 Test MSE 12309.641921857237 Test RE 0.0495917613690576\n",
      "33 Train Loss 117673.63 Test MSE 8434.490970323955 Test RE 0.04105029311213667\n",
      "34 Train Loss 117462.93 Test MSE 9363.96212513031 Test RE 0.043253041095785566\n",
      "35 Train Loss 117160.54 Test MSE 16043.436376711845 Test RE 0.05661552277231594\n",
      "36 Train Loss 116849.76 Test MSE 15463.070540582 Test RE 0.05558206684685041\n",
      "37 Train Loss 116678.42 Test MSE 10593.29411367224 Test RE 0.04600471438571318\n",
      "38 Train Loss 116287.625 Test MSE 8978.940506051274 Test RE 0.04235448160932977\n",
      "39 Train Loss 115991.48 Test MSE 9300.608393281656 Test RE 0.04310647427997088\n",
      "40 Train Loss 115917.766 Test MSE 9760.317442476657 Test RE 0.04415895577037602\n",
      "41 Train Loss 115694.06 Test MSE 9447.527730460552 Test RE 0.043445611199460124\n",
      "42 Train Loss 115541.2 Test MSE 8184.210401258588 Test RE 0.04043665442594953\n",
      "43 Train Loss 115270.99 Test MSE 9361.905779025219 Test RE 0.043248291604866616\n",
      "44 Train Loss 115023.98 Test MSE 8028.986381478791 Test RE 0.04005135230379104\n",
      "45 Train Loss 114829.13 Test MSE 6883.235756923118 Test RE 0.037083707427483056\n",
      "46 Train Loss 114677.89 Test MSE 6776.6698649614345 Test RE 0.036795523666523985\n",
      "47 Train Loss 114438.93 Test MSE 7472.86665909981 Test RE 0.03863940588962419\n",
      "48 Train Loss 114266.99 Test MSE 6565.094038695611 Test RE 0.03621656854612239\n",
      "49 Train Loss 113876.445 Test MSE 7618.241477637312 Test RE 0.039013435068439885\n",
      "50 Train Loss 113628.63 Test MSE 5508.9928812493545 Test RE 0.03317591649657725\n",
      "51 Train Loss 113353.46 Test MSE 5162.034060279728 Test RE 0.032114211004474286\n",
      "52 Train Loss 113059.77 Test MSE 5294.965312307051 Test RE 0.03252508074076215\n",
      "53 Train Loss 112477.875 Test MSE 4152.374495972307 Test RE 0.028802824693727268\n",
      "54 Train Loss 111984.26 Test MSE 4444.956437424944 Test RE 0.029800296022317648\n",
      "55 Train Loss 111644.56 Test MSE 3827.677633296835 Test RE 0.027653779832374197\n",
      "56 Train Loss 111053.664 Test MSE 2398.1400544672106 Test RE 0.021888907958771513\n",
      "57 Train Loss 110641.13 Test MSE 2477.9732735351754 Test RE 0.02225026217391932\n",
      "58 Train Loss 110231.2 Test MSE 2494.319739865768 Test RE 0.02232353078183494\n",
      "59 Train Loss 110011.41 Test MSE 3119.5131752063967 Test RE 0.0249649232799795\n",
      "60 Train Loss 109709.96 Test MSE 4702.754283800406 Test RE 0.030652292947443007\n",
      "61 Train Loss 109384.98 Test MSE 5448.9519047957565 Test RE 0.03299463367659309\n",
      "62 Train Loss 108996.484 Test MSE 4855.724891858492 Test RE 0.03114683055645444\n",
      "63 Train Loss 108695.664 Test MSE 4769.729608652314 Test RE 0.03086979203072171\n",
      "64 Train Loss 108459.984 Test MSE 5817.352751832626 Test RE 0.034091767918462274\n",
      "65 Train Loss 108134.836 Test MSE 7319.467374960524 Test RE 0.03824076426474605\n",
      "66 Train Loss 107755.36 Test MSE 5583.357428503076 Test RE 0.033399082695145736\n",
      "67 Train Loss 107387.625 Test MSE 4475.794484434297 Test RE 0.0299034910172228\n",
      "68 Train Loss 106916.8 Test MSE 3570.439121212942 Test RE 0.026708385733170265\n",
      "69 Train Loss 106432.36 Test MSE 4076.4144926342906 Test RE 0.028538161551688517\n",
      "70 Train Loss 105865.67 Test MSE 3834.6980977129774 Test RE 0.027679128549893545\n",
      "71 Train Loss 105257.96 Test MSE 4874.35590883608 Test RE 0.03120652726179226\n",
      "72 Train Loss 104119.9 Test MSE 6070.797699075759 Test RE 0.03482648983469548\n",
      "73 Train Loss 103059.914 Test MSE 3120.053496688832 Test RE 0.024967085235955904\n",
      "74 Train Loss 102221.95 Test MSE 4571.212826510971 Test RE 0.030220562556141854\n",
      "75 Train Loss 101611.055 Test MSE 4320.373308565713 Test RE 0.02937970703410607\n",
      "76 Train Loss 101272.8 Test MSE 5455.8290252896595 Test RE 0.03301544836952875\n",
      "77 Train Loss 101040.73 Test MSE 6065.133938648258 Test RE 0.03481024032910119\n",
      "78 Train Loss 100268.65 Test MSE 6855.8713665941705 Test RE 0.037009920646724634\n",
      "79 Train Loss 99592.17 Test MSE 6280.361455218243 Test RE 0.03542249465306078\n",
      "80 Train Loss 98957.734 Test MSE 6091.777176594355 Test RE 0.03488661466893221\n",
      "81 Train Loss 98461.19 Test MSE 4994.101644054563 Test RE 0.03158751871697032\n",
      "82 Train Loss 97992.76 Test MSE 4479.213995074869 Test RE 0.029914911982009292\n",
      "83 Train Loss 96862.59 Test MSE 3022.6136362181437 Test RE 0.024574129484471897\n",
      "84 Train Loss 95996.77 Test MSE 2397.573421933475 Test RE 0.02188632185040721\n",
      "85 Train Loss 95378.23 Test MSE 3136.877449824454 Test RE 0.025034308498653967\n",
      "86 Train Loss 94623.42 Test MSE 3118.5844815201363 Test RE 0.024961206916057285\n",
      "87 Train Loss 93940.08 Test MSE 5165.873160336267 Test RE 0.03212615075081873\n",
      "88 Train Loss 93267.836 Test MSE 5192.2565891594295 Test RE 0.032208084483563035\n",
      "89 Train Loss 92587.805 Test MSE 2113.2337929966084 Test RE 0.020547576078442844\n",
      "90 Train Loss 92100.17 Test MSE 1673.6794965176216 Test RE 0.01828617920564716\n",
      "91 Train Loss 91597.54 Test MSE 1492.6128684445284 Test RE 0.01726873060223745\n",
      "92 Train Loss 90310.32 Test MSE 2732.5001058214275 Test RE 0.023365060902364544\n",
      "93 Train Loss 89179.83 Test MSE 5072.690221348121 Test RE 0.031835083586389874\n",
      "94 Train Loss 88123.76 Test MSE 3505.0094167499483 Test RE 0.026462533340826752\n",
      "95 Train Loss 87091.89 Test MSE 2364.1840550139195 Test RE 0.02173338961258138\n",
      "96 Train Loss 85868.89 Test MSE 2932.0660875403346 Test RE 0.024203250800606722\n",
      "97 Train Loss 84697.445 Test MSE 1931.5011851912311 Test RE 0.01964419846445257\n",
      "98 Train Loss 83614.53 Test MSE 1993.4860999311913 Test RE 0.019956916008924852\n",
      "99 Train Loss 81736.125 Test MSE 3441.866218519894 Test RE 0.02622308704429095\n",
      "100 Train Loss 80217.74 Test MSE 3503.5322474310024 Test RE 0.02645695649963921\n",
      "101 Train Loss 78928.6 Test MSE 6830.954831370294 Test RE 0.03694260619996722\n",
      "102 Train Loss 77454.02 Test MSE 5109.547761125869 Test RE 0.03195052915267993\n",
      "103 Train Loss 76348.93 Test MSE 2126.027516592326 Test RE 0.020609680738685104\n",
      "104 Train Loss 75762.086 Test MSE 1622.7129711518833 Test RE 0.018005603299331728\n",
      "105 Train Loss 74976.414 Test MSE 1828.0645740292273 Test RE 0.019110963803664817\n",
      "106 Train Loss 74321.26 Test MSE 2101.5003329959213 Test RE 0.020490452783268307\n",
      "107 Train Loss 73760.53 Test MSE 2601.0374219200853 Test RE 0.022796077455926957\n",
      "108 Train Loss 73100.7 Test MSE 2141.774799529869 Test RE 0.020685866892331713\n",
      "109 Train Loss 72253.37 Test MSE 2262.2249434737064 Test RE 0.021259581788647737\n",
      "110 Train Loss 71724.78 Test MSE 2180.6134659030167 Test RE 0.02087258164345819\n",
      "111 Train Loss 71159.98 Test MSE 1497.0798794250761 Test RE 0.01729455175862706\n",
      "112 Train Loss 70644.19 Test MSE 1241.2970475414284 Test RE 0.015747973846395043\n",
      "113 Train Loss 70215.31 Test MSE 1214.9088279005682 Test RE 0.015579684819452903\n",
      "114 Train Loss 69779.0 Test MSE 1214.5450535398543 Test RE 0.01557735216943586\n",
      "115 Train Loss 69399.44 Test MSE 1900.4627434022254 Test RE 0.019485722066364123\n",
      "116 Train Loss 68972.234 Test MSE 1751.2759419390657 Test RE 0.01870527573218959\n",
      "117 Train Loss 68586.625 Test MSE 1397.2712201646245 Test RE 0.016708104412836235\n",
      "118 Train Loss 67859.445 Test MSE 1622.1210583232519 Test RE 0.018002319071052705\n",
      "119 Train Loss 67419.46 Test MSE 1971.40844433729 Test RE 0.019846097923458726\n",
      "120 Train Loss 66819.305 Test MSE 2621.0338736752046 Test RE 0.022883536386598454\n",
      "121 Train Loss 66266.0 Test MSE 1624.5587752643012 Test RE 0.01801584089883047\n",
      "122 Train Loss 65685.05 Test MSE 1473.1595568522778 Test RE 0.017155829341094452\n",
      "123 Train Loss 65296.12 Test MSE 1108.0757790385776 Test RE 0.014878924999313013\n",
      "124 Train Loss 64922.56 Test MSE 1283.795073968839 Test RE 0.016015285168235915\n",
      "125 Train Loss 64385.695 Test MSE 979.1118166995842 Test RE 0.013986304193999169\n",
      "126 Train Loss 63986.027 Test MSE 992.4888388643819 Test RE 0.01408152334275687\n",
      "127 Train Loss 63633.66 Test MSE 1103.378345011521 Test RE 0.014847353602783801\n",
      "128 Train Loss 63380.97 Test MSE 1287.8677257897414 Test RE 0.016040668126379738\n",
      "129 Train Loss 63193.65 Test MSE 1186.8020276355714 Test RE 0.015398412994074061\n",
      "130 Train Loss 62794.703 Test MSE 1313.2027760305398 Test RE 0.016197676452741956\n",
      "131 Train Loss 62514.344 Test MSE 1725.4185836084462 Test RE 0.018566671747538517\n",
      "132 Train Loss 62261.69 Test MSE 2342.3131440051757 Test RE 0.021632628960523734\n",
      "133 Train Loss 61928.5 Test MSE 1347.3597978779994 Test RE 0.016406978768132307\n",
      "134 Train Loss 61606.188 Test MSE 1031.6619416823971 Test RE 0.014356729852935499\n",
      "135 Train Loss 61186.258 Test MSE 1073.1717750412706 Test RE 0.01464270944106482\n",
      "136 Train Loss 60878.266 Test MSE 1703.152982793523 Test RE 0.018446486289100644\n",
      "137 Train Loss 60636.594 Test MSE 1476.8236082937408 Test RE 0.017177151133130418\n",
      "138 Train Loss 60276.906 Test MSE 1105.6684587354434 Test RE 0.014862753806818355\n",
      "139 Train Loss 59895.36 Test MSE 1371.7256165410317 Test RE 0.016554666958065865\n",
      "140 Train Loss 59573.33 Test MSE 961.4778049608511 Test RE 0.013859783786537915\n",
      "141 Train Loss 59212.426 Test MSE 1184.5696181665153 Test RE 0.015383923743652297\n",
      "142 Train Loss 58901.65 Test MSE 1405.0975204435222 Test RE 0.01675483122100757\n",
      "143 Train Loss 58549.438 Test MSE 1137.2362968357802 Test RE 0.015073433173968517\n",
      "144 Train Loss 58122.184 Test MSE 1570.0856269009735 Test RE 0.017711220609426644\n",
      "145 Train Loss 57532.66 Test MSE 2713.3143915142614 Test RE 0.023282889811804677\n",
      "146 Train Loss 56921.844 Test MSE 1135.4941099215555 Test RE 0.015061882890472495\n",
      "147 Train Loss 56163.324 Test MSE 1483.7217663436004 Test RE 0.01721722113847624\n",
      "148 Train Loss 55569.63 Test MSE 1461.3449802223113 Test RE 0.01708689692884757\n",
      "149 Train Loss 54823.117 Test MSE 979.9007113671128 Test RE 0.013991937615518677\n",
      "150 Train Loss 54305.8 Test MSE 1109.6018283057306 Test RE 0.01488916715093093\n",
      "151 Train Loss 53935.207 Test MSE 1145.8990230294755 Test RE 0.01513073406413282\n",
      "152 Train Loss 53328.82 Test MSE 1429.2628031959916 Test RE 0.01689829429198694\n",
      "153 Train Loss 53063.336 Test MSE 1476.2323146407525 Test RE 0.017173712077432738\n",
      "154 Train Loss 52851.465 Test MSE 1978.9224709629655 Test RE 0.01988388367052203\n",
      "155 Train Loss 52583.047 Test MSE 1635.0199359319752 Test RE 0.018073753293175177\n",
      "156 Train Loss 52166.39 Test MSE 927.8642463908607 Test RE 0.01361535733216932\n",
      "157 Train Loss 51731.492 Test MSE 1643.1817775356408 Test RE 0.018118808240683983\n",
      "158 Train Loss 51354.332 Test MSE 1784.629555675945 Test RE 0.018882559603450585\n",
      "159 Train Loss 51006.543 Test MSE 2592.7539266233503 Test RE 0.022759749300700533\n",
      "160 Train Loss 50707.77 Test MSE 3704.278455103396 Test RE 0.027204367716579084\n",
      "161 Train Loss 50395.574 Test MSE 1609.9122437425287 Test RE 0.017934444330119866\n",
      "162 Train Loss 50039.984 Test MSE 1186.6336578367732 Test RE 0.015397320680614527\n",
      "163 Train Loss 49683.348 Test MSE 1666.442211059669 Test RE 0.01824660003975878\n",
      "164 Train Loss 49149.09 Test MSE 1620.6471296125374 Test RE 0.017994138372992245\n",
      "165 Train Loss 48818.395 Test MSE 2979.7924916658676 Test RE 0.024399438629014614\n",
      "166 Train Loss 48535.273 Test MSE 2135.3283103106783 Test RE 0.02065471242359634\n",
      "167 Train Loss 48095.793 Test MSE 1318.2179775233735 Test RE 0.0162285769352777\n",
      "168 Train Loss 47690.723 Test MSE 1381.9676632665087 Test RE 0.01661635507981793\n",
      "169 Train Loss 47333.62 Test MSE 1082.875305535028 Test RE 0.01470875955641044\n",
      "170 Train Loss 47006.457 Test MSE 1240.1867259423566 Test RE 0.01574092910743599\n",
      "171 Train Loss 46661.625 Test MSE 1060.0051194731188 Test RE 0.01455260712939878\n",
      "172 Train Loss 46352.926 Test MSE 772.4146342985127 Test RE 0.012422587669386382\n",
      "173 Train Loss 46095.86 Test MSE 806.1728229554357 Test RE 0.012691147756170678\n",
      "174 Train Loss 45790.16 Test MSE 1418.1798349439107 Test RE 0.016832649353539255\n",
      "175 Train Loss 45481.82 Test MSE 1875.7951769998242 Test RE 0.019358848945276428\n",
      "176 Train Loss 45147.47 Test MSE 1747.0350927420423 Test RE 0.018682613873610757\n",
      "177 Train Loss 44900.39 Test MSE 867.0240959397776 Test RE 0.013161409666559718\n",
      "178 Train Loss 44637.324 Test MSE 823.9988631956927 Test RE 0.012830693478217711\n",
      "179 Train Loss 44445.316 Test MSE 979.8260413104762 Test RE 0.013991404500955991\n",
      "180 Train Loss 44183.63 Test MSE 872.0798530733084 Test RE 0.013199727038735483\n",
      "181 Train Loss 43980.6 Test MSE 1022.4785085981496 Test RE 0.01429268814446547\n",
      "182 Train Loss 43778.004 Test MSE 1569.8865335586895 Test RE 0.01771009764586419\n",
      "183 Train Loss 43534.805 Test MSE 1255.2341455039732 Test RE 0.01583613502184943\n",
      "184 Train Loss 43158.613 Test MSE 1441.5099909762316 Test RE 0.016970539622068324\n",
      "185 Train Loss 42765.844 Test MSE 2532.2979494534293 Test RE 0.02249283644455137\n",
      "186 Train Loss 42490.27 Test MSE 1694.0604645319063 Test RE 0.01839718083313314\n",
      "187 Train Loss 42142.816 Test MSE 1529.1210813362059 Test RE 0.01747864499307385\n",
      "188 Train Loss 41887.05 Test MSE 1271.9876874023691 Test RE 0.015941466739251987\n",
      "189 Train Loss 41635.51 Test MSE 826.5440135110372 Test RE 0.012850493788057039\n",
      "190 Train Loss 41453.77 Test MSE 982.6124730824156 Test RE 0.014011284772833862\n",
      "191 Train Loss 41311.535 Test MSE 897.235211238598 Test RE 0.013388748326585525\n",
      "192 Train Loss 41125.746 Test MSE 854.1648915305976 Test RE 0.013063443824319766\n",
      "193 Train Loss 40925.086 Test MSE 857.364091164604 Test RE 0.01308788495133904\n",
      "194 Train Loss 40733.246 Test MSE 759.5469451574064 Test RE 0.012318678881257292\n",
      "195 Train Loss 40571.3 Test MSE 824.1135689138653 Test RE 0.012831586502929317\n",
      "196 Train Loss 40441.84 Test MSE 651.9711513877131 Test RE 0.011413031432101586\n",
      "197 Train Loss 40319.75 Test MSE 725.1630994412225 Test RE 0.012036623381121445\n",
      "198 Train Loss 40139.26 Test MSE 775.067660834984 Test RE 0.012443903425122757\n",
      "199 Train Loss 39989.285 Test MSE 694.2645286966867 Test RE 0.011777396927390873\n",
      "Training time: 47.11\n",
      "Training time: 47.11\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 574850.0 Test MSE 1778135.3372870774 Test RE 0.5960315239846298\n",
      "1 Train Loss 248215.58 Test MSE 2592241.380121969 Test RE 0.7196553248526038\n",
      "2 Train Loss 175715.88 Test MSE 1056784.901988626 Test RE 0.4594942950232502\n",
      "3 Train Loss 151003.33 Test MSE 233354.97840240816 Test RE 0.21592123667080573\n",
      "4 Train Loss 138533.31 Test MSE 243457.18561557584 Test RE 0.22054546007978745\n",
      "5 Train Loss 130702.445 Test MSE 32745.15295679233 Test RE 0.08088357440940436\n",
      "6 Train Loss 127347.63 Test MSE 4694.778557537002 Test RE 0.03062628924553304\n",
      "7 Train Loss 126329.07 Test MSE 3751.2923439232463 Test RE 0.027376459331925077\n",
      "8 Train Loss 125206.19 Test MSE 1850.051509398073 Test RE 0.01922554827085945\n",
      "9 Train Loss 124936.01 Test MSE 1752.2440477900498 Test RE 0.01871044515881068\n",
      "10 Train Loss 124835.36 Test MSE 2542.259061181682 Test RE 0.022537032222039028\n",
      "11 Train Loss 124703.87 Test MSE 1874.3962265337266 Test RE 0.019351628774769633\n",
      "12 Train Loss 124667.445 Test MSE 1414.8069804719535 Test RE 0.016812620908673272\n",
      "13 Train Loss 124660.89 Test MSE 1412.4346940815192 Test RE 0.016798519661847675\n",
      "14 Train Loss 124563.38 Test MSE 1534.299016812318 Test RE 0.017508213223669807\n",
      "15 Train Loss 124529.1 Test MSE 1450.0573869286136 Test RE 0.017020778446063412\n",
      "16 Train Loss 124492.25 Test MSE 1378.1093225406112 Test RE 0.01659314311460332\n",
      "17 Train Loss 124400.47 Test MSE 1297.8742956887609 Test RE 0.016102864530337366\n",
      "18 Train Loss 124362.05 Test MSE 1375.3477972141957 Test RE 0.01657650968700117\n",
      "19 Train Loss 124344.164 Test MSE 1260.341176303498 Test RE 0.015868317676418615\n",
      "20 Train Loss 124330.43 Test MSE 1256.977533997798 Test RE 0.01584712857074006\n",
      "21 Train Loss 124326.305 Test MSE 1260.5165098716584 Test RE 0.01586942140612853\n",
      "22 Train Loss 124323.2 Test MSE 1262.7488623967652 Test RE 0.015883467422890426\n",
      "23 Train Loss 124318.125 Test MSE 1249.9056272594723 Test RE 0.015802486766780317\n",
      "24 Train Loss 124307.29 Test MSE 1275.859245401965 Test RE 0.015965708884796426\n",
      "25 Train Loss 124282.13 Test MSE 1253.459705027841 Test RE 0.015824937821325036\n",
      "26 Train Loss 124274.016 Test MSE 1247.934790159642 Test RE 0.01579002326028598\n",
      "27 Train Loss 124259.28 Test MSE 1258.7710735440992 Test RE 0.015858430411656844\n",
      "28 Train Loss 124245.56 Test MSE 1259.6033554650962 Test RE 0.015863672232123097\n",
      "29 Train Loss 124231.22 Test MSE 1269.0698624382042 Test RE 0.01592317209863497\n",
      "30 Train Loss 124198.45 Test MSE 1374.5294315072988 Test RE 0.01657157723803472\n",
      "31 Train Loss 124162.81 Test MSE 1301.2358626778146 Test RE 0.01612370470447635\n",
      "32 Train Loss 124140.57 Test MSE 1311.3707865683307 Test RE 0.016186374188711213\n",
      "33 Train Loss 124066.57 Test MSE 2361.161168119661 Test RE 0.021719490823178367\n",
      "34 Train Loss 123971.21 Test MSE 2051.8999522332283 Test RE 0.020247197274261754\n",
      "35 Train Loss 123862.664 Test MSE 1454.2150712737514 Test RE 0.01704516243615284\n",
      "36 Train Loss 123652.55 Test MSE 1601.359506263955 Test RE 0.017886742084603047\n",
      "37 Train Loss 123540.586 Test MSE 1641.6352566671524 Test RE 0.018110279752925017\n",
      "38 Train Loss 123242.03 Test MSE 1425.047737929991 Test RE 0.016873358358139273\n",
      "39 Train Loss 123049.02 Test MSE 1795.8881082070195 Test RE 0.01894202742243589\n",
      "40 Train Loss 122900.08 Test MSE 1706.8817035929624 Test RE 0.01846666773870403\n",
      "41 Train Loss 122418.62 Test MSE 2330.5757162981085 Test RE 0.02157835989133014\n",
      "42 Train Loss 122102.18 Test MSE 1989.5139960003341 Test RE 0.019937023602456055\n",
      "43 Train Loss 121905.46 Test MSE 2057.7712512903563 Test RE 0.020276144210192987\n",
      "44 Train Loss 121707.195 Test MSE 2384.0712406692905 Test RE 0.021824607303760388\n",
      "45 Train Loss 121572.125 Test MSE 1658.9146876752782 Test RE 0.0182053423329192\n",
      "46 Train Loss 121408.93 Test MSE 1731.897974710028 Test RE 0.01860150039733948\n",
      "47 Train Loss 121238.055 Test MSE 1735.590959852975 Test RE 0.018621322143305544\n",
      "48 Train Loss 121062.93 Test MSE 2688.929340944474 Test RE 0.023178029895837463\n",
      "49 Train Loss 120817.56 Test MSE 2717.9021319573553 Test RE 0.023302565147245224\n",
      "50 Train Loss 120492.98 Test MSE 1840.0300442928024 Test RE 0.019173406537723807\n",
      "51 Train Loss 120119.7 Test MSE 1807.2515166174353 Test RE 0.01900186037518677\n",
      "52 Train Loss 119333.48 Test MSE 2254.0779237845613 Test RE 0.02122126587029408\n",
      "53 Train Loss 118596.74 Test MSE 2802.469124945524 Test RE 0.02366231556920949\n",
      "54 Train Loss 118019.09 Test MSE 5576.242005153133 Test RE 0.033377794036934826\n",
      "55 Train Loss 117791.41 Test MSE 5784.802780145055 Test RE 0.03399625688886775\n",
      "56 Train Loss 117204.39 Test MSE 6775.332292143113 Test RE 0.036791892154136696\n",
      "57 Train Loss 116921.375 Test MSE 5578.436906670754 Test RE 0.033384362417840235\n",
      "58 Train Loss 116421.31 Test MSE 3155.0988632395397 Test RE 0.025106912541315952\n",
      "59 Train Loss 116074.61 Test MSE 4192.921291379004 Test RE 0.028943108898916065\n",
      "60 Train Loss 115725.33 Test MSE 6756.977839954648 Test RE 0.03674202353055254\n",
      "61 Train Loss 115288.51 Test MSE 5618.797176073195 Test RE 0.033504913526537956\n",
      "62 Train Loss 115137.67 Test MSE 5504.548747461522 Test RE 0.03316253220280342\n",
      "63 Train Loss 114890.55 Test MSE 3912.670148676701 Test RE 0.027959116398266537\n",
      "64 Train Loss 114667.83 Test MSE 3615.19265344733 Test RE 0.02687525208489854\n",
      "65 Train Loss 114468.01 Test MSE 3085.0255409736546 Test RE 0.02482654046603592\n",
      "66 Train Loss 114255.23 Test MSE 2465.437320092935 Test RE 0.022193909282597533\n",
      "67 Train Loss 114063.875 Test MSE 2072.495549847971 Test RE 0.020348557466485805\n",
      "68 Train Loss 113872.516 Test MSE 2930.120247814446 Test RE 0.02419521833105397\n",
      "69 Train Loss 113593.21 Test MSE 3034.470879310738 Test RE 0.024622282550037917\n",
      "70 Train Loss 113342.875 Test MSE 3426.927951972182 Test RE 0.02616611890485838\n",
      "71 Train Loss 112948.234 Test MSE 2122.2418275244236 Test RE 0.020591323354876137\n",
      "72 Train Loss 112674.32 Test MSE 2179.972345001042 Test RE 0.020869513050073566\n",
      "73 Train Loss 112374.67 Test MSE 2830.262660943079 Test RE 0.02377936179506578\n",
      "74 Train Loss 112180.84 Test MSE 4143.642193831517 Test RE 0.028772523074851952\n",
      "75 Train Loss 111921.0 Test MSE 4100.5090246886775 Test RE 0.028622377787418222\n",
      "76 Train Loss 111702.836 Test MSE 3477.881744054842 Test RE 0.026359928572158183\n",
      "77 Train Loss 111362.164 Test MSE 6789.901367817678 Test RE 0.03683142792837748\n",
      "78 Train Loss 110989.06 Test MSE 2641.4302608517582 Test RE 0.022972401503030936\n",
      "79 Train Loss 110680.85 Test MSE 2636.503430262454 Test RE 0.022950967290262737\n",
      "80 Train Loss 110231.164 Test MSE 3349.300755589906 Test RE 0.025868062249305275\n",
      "81 Train Loss 109367.26 Test MSE 8312.660667188544 Test RE 0.040752743391209094\n",
      "82 Train Loss 108589.46 Test MSE 5246.078391474882 Test RE 0.0323745851173333\n",
      "83 Train Loss 108198.46 Test MSE 5166.431273095496 Test RE 0.03212788613318548\n",
      "84 Train Loss 107860.35 Test MSE 6425.467801069213 Test RE 0.035829372275224515\n",
      "85 Train Loss 107549.695 Test MSE 6208.034154480045 Test RE 0.03521793377449644\n",
      "86 Train Loss 107199.484 Test MSE 12628.093531437256 Test RE 0.05022913724299729\n",
      "87 Train Loss 106664.945 Test MSE 11296.341969341098 Test RE 0.04750679577270796\n",
      "88 Train Loss 106253.06 Test MSE 8690.338762586578 Test RE 0.041668241903019654\n",
      "89 Train Loss 105959.64 Test MSE 7433.5233659581245 Test RE 0.0385375569122978\n",
      "90 Train Loss 105582.22 Test MSE 5271.410044897561 Test RE 0.03245265430433048\n",
      "91 Train Loss 105061.83 Test MSE 5502.573410025606 Test RE 0.033156581390821005\n",
      "92 Train Loss 104850.87 Test MSE 8435.260228490535 Test RE 0.041052165042128996\n",
      "93 Train Loss 104532.805 Test MSE 8014.625903494875 Test RE 0.04001551876614632\n",
      "94 Train Loss 104177.85 Test MSE 6690.338736833211 Test RE 0.036560394824996914\n",
      "95 Train Loss 103856.68 Test MSE 6999.040746249594 Test RE 0.037394358265383856\n",
      "96 Train Loss 103265.375 Test MSE 4212.265762739891 Test RE 0.029009798065163624\n",
      "97 Train Loss 102776.5 Test MSE 2835.7277963564916 Test RE 0.02380230926894556\n",
      "98 Train Loss 102290.91 Test MSE 4199.349617352458 Test RE 0.028965287288171004\n",
      "99 Train Loss 101602.55 Test MSE 3427.2123444806844 Test RE 0.026167204613971158\n",
      "100 Train Loss 101149.734 Test MSE 3195.331679901596 Test RE 0.025266483133891203\n",
      "101 Train Loss 100573.336 Test MSE 2435.492065035694 Test RE 0.022058713654880706\n",
      "102 Train Loss 100178.586 Test MSE 2028.111682423593 Test RE 0.02012948931610792\n",
      "103 Train Loss 99819.55 Test MSE 2097.90665519834 Test RE 0.020472925402608907\n",
      "104 Train Loss 99561.75 Test MSE 2185.960767096507 Test RE 0.02089815784999542\n",
      "105 Train Loss 99440.375 Test MSE 2117.4828307364073 Test RE 0.020568223009166116\n",
      "106 Train Loss 99278.234 Test MSE 1980.7917840560674 Test RE 0.019893272727233534\n",
      "107 Train Loss 99071.63 Test MSE 1850.7044243813116 Test RE 0.019228940484845736\n",
      "108 Train Loss 98933.92 Test MSE 2120.0267953969865 Test RE 0.020580574732931855\n",
      "109 Train Loss 98845.32 Test MSE 2110.4565911224367 Test RE 0.020534069875798477\n",
      "110 Train Loss 98732.2 Test MSE 2454.265874510597 Test RE 0.02214356941965358\n",
      "111 Train Loss 98595.49 Test MSE 2688.4572726233177 Test RE 0.023175995239559308\n",
      "112 Train Loss 98280.79 Test MSE 1746.4226766153727 Test RE 0.018679339029005808\n",
      "113 Train Loss 97962.43 Test MSE 1964.2601675487176 Test RE 0.019810084525195077\n",
      "114 Train Loss 97853.695 Test MSE 2001.2442540028646 Test RE 0.01999571198617028\n",
      "115 Train Loss 97767.54 Test MSE 1810.798301372143 Test RE 0.019020497093445388\n",
      "116 Train Loss 97634.95 Test MSE 1877.6871796378207 Test RE 0.01936860954241182\n",
      "117 Train Loss 97530.625 Test MSE 1964.8984834056175 Test RE 0.019813303056057645\n",
      "118 Train Loss 97381.02 Test MSE 1858.9587106607496 Test RE 0.019271774065907746\n",
      "119 Train Loss 97267.69 Test MSE 2255.2401318624916 Test RE 0.021226736033511127\n",
      "120 Train Loss 97197.55 Test MSE 2561.986766589405 Test RE 0.0226243059282919\n",
      "121 Train Loss 97020.36 Test MSE 3017.8006592548763 Test RE 0.02455455671504932\n",
      "122 Train Loss 96774.57 Test MSE 4623.5474436820205 Test RE 0.030393063879394156\n",
      "123 Train Loss 96362.766 Test MSE 3205.8923829699547 Test RE 0.025308202076352034\n",
      "124 Train Loss 96031.82 Test MSE 3328.8046008700944 Test RE 0.025788790563369836\n",
      "125 Train Loss 95694.836 Test MSE 5116.2972750856225 Test RE 0.03197162489178624\n",
      "126 Train Loss 94594.0 Test MSE 6263.784138843845 Test RE 0.03537571407289266\n",
      "127 Train Loss 93708.54 Test MSE 3420.958489708346 Test RE 0.02614331922214442\n",
      "128 Train Loss 92789.14 Test MSE 2233.418362210358 Test RE 0.02112379114357021\n",
      "129 Train Loss 92184.9 Test MSE 2472.1371225709395 Test RE 0.022224044692052428\n",
      "130 Train Loss 91544.945 Test MSE 4075.8747888455136 Test RE 0.028536272310079953\n",
      "131 Train Loss 90379.39 Test MSE 6861.437086599135 Test RE 0.037024940259528995\n",
      "132 Train Loss 89700.17 Test MSE 3542.502678297619 Test RE 0.026603692371952398\n",
      "133 Train Loss 89157.33 Test MSE 2598.1471345358796 Test RE 0.022783408370841484\n",
      "134 Train Loss 88809.82 Test MSE 2776.8218950252594 Test RE 0.023553792034531024\n",
      "135 Train Loss 88400.66 Test MSE 2975.707569206491 Test RE 0.024382708605485456\n",
      "136 Train Loss 88104.734 Test MSE 4153.068808509238 Test RE 0.028805232632414794\n",
      "137 Train Loss 87751.31 Test MSE 6488.680119978046 Test RE 0.036005181626201824\n",
      "138 Train Loss 87435.586 Test MSE 5633.568251209195 Test RE 0.03354892461533711\n",
      "139 Train Loss 87053.195 Test MSE 6734.3432379413 Test RE 0.03668043248136913\n",
      "140 Train Loss 86791.73 Test MSE 9045.144598180452 Test RE 0.04251034021186908\n",
      "141 Train Loss 86444.8 Test MSE 8640.818783413459 Test RE 0.04154935365222888\n",
      "142 Train Loss 85888.72 Test MSE 6550.32019764985 Test RE 0.03617579539181152\n",
      "143 Train Loss 85611.14 Test MSE 6419.947561537882 Test RE 0.035813978126334065\n",
      "144 Train Loss 85201.32 Test MSE 4709.541312001856 Test RE 0.0306744037104119\n",
      "145 Train Loss 84510.695 Test MSE 2732.8019512693577 Test RE 0.023366351376700192\n",
      "146 Train Loss 84152.93 Test MSE 2830.0588872940125 Test RE 0.023778505744739754\n",
      "147 Train Loss 83760.56 Test MSE 2386.388317206467 Test RE 0.021835210385489134\n",
      "148 Train Loss 83240.41 Test MSE 2806.881953841651 Test RE 0.023680937839956152\n",
      "149 Train Loss 82994.51 Test MSE 2635.6567311614676 Test RE 0.02294728170359421\n",
      "150 Train Loss 82767.17 Test MSE 2989.766309059007 Test RE 0.024440238827211636\n",
      "151 Train Loss 82641.05 Test MSE 3523.015435945616 Test RE 0.02653041825084268\n",
      "152 Train Loss 82329.52 Test MSE 2295.685994270096 Test RE 0.021416232153066126\n",
      "153 Train Loss 82108.375 Test MSE 1972.261174063456 Test RE 0.019850389659158873\n",
      "154 Train Loss 81820.51 Test MSE 2095.6493149978355 Test RE 0.02046190804019837\n",
      "155 Train Loss 81562.27 Test MSE 3874.152929109046 Test RE 0.02782115806789498\n",
      "156 Train Loss 81106.91 Test MSE 8286.804961680476 Test RE 0.04068931535047846\n",
      "157 Train Loss 80604.36 Test MSE 8177.732821287255 Test RE 0.04042064900218596\n",
      "158 Train Loss 80300.85 Test MSE 7158.669316519089 Test RE 0.03781838461412293\n",
      "159 Train Loss 79932.67 Test MSE 8989.144273101821 Test RE 0.042378540826067464\n",
      "160 Train Loss 79751.01 Test MSE 7313.980613552197 Test RE 0.03822642870765521\n",
      "161 Train Loss 79511.2 Test MSE 7849.502312485418 Test RE 0.03960115790529413\n",
      "162 Train Loss 79122.375 Test MSE 9471.653521268905 Test RE 0.04350104852914578\n",
      "163 Train Loss 78796.77 Test MSE 10824.906239256357 Test RE 0.046504919327655714\n",
      "164 Train Loss 78431.945 Test MSE 12187.252689677342 Test RE 0.04934461122143695\n",
      "165 Train Loss 78187.414 Test MSE 9676.386218269616 Test RE 0.04396867929891047\n",
      "166 Train Loss 77852.01 Test MSE 4408.873652890856 Test RE 0.02967909474947952\n",
      "167 Train Loss 77496.25 Test MSE 2965.1749636781387 Test RE 0.024339518692118468\n",
      "168 Train Loss 77299.47 Test MSE 3792.586926979796 Test RE 0.027526728254730633\n",
      "169 Train Loss 77070.93 Test MSE 3683.7692976465896 Test RE 0.027128953153965265\n",
      "170 Train Loss 76586.87 Test MSE 3755.666068996474 Test RE 0.027392414130276568\n",
      "171 Train Loss 76033.56 Test MSE 6070.290576941843 Test RE 0.03482503519450113\n",
      "172 Train Loss 75730.71 Test MSE 4702.061276292291 Test RE 0.030650034371882254\n",
      "173 Train Loss 75148.04 Test MSE 3894.4311154699817 Test RE 0.027893874133274193\n",
      "174 Train Loss 74727.05 Test MSE 3623.192798347209 Test RE 0.02690497209085931\n",
      "175 Train Loss 74169.92 Test MSE 4184.651274425279 Test RE 0.028914551465069695\n",
      "176 Train Loss 73789.65 Test MSE 4903.147028339716 Test RE 0.031298554602213605\n",
      "177 Train Loss 73536.87 Test MSE 6329.80161414486 Test RE 0.035561647522763966\n",
      "178 Train Loss 73059.805 Test MSE 4747.066774505397 Test RE 0.030796367536187447\n",
      "179 Train Loss 72798.375 Test MSE 3192.5265846720895 Test RE 0.025255390317866833\n",
      "180 Train Loss 72644.62 Test MSE 2645.735782686789 Test RE 0.02299111634581171\n",
      "181 Train Loss 72466.375 Test MSE 2319.3677193915078 Test RE 0.02152641099698725\n",
      "182 Train Loss 72329.836 Test MSE 2110.553925025233 Test RE 0.020534543384262423\n",
      "183 Train Loss 72202.69 Test MSE 1802.5416750564307 Test RE 0.018977084040715245\n",
      "184 Train Loss 72043.21 Test MSE 1604.5954624405388 Test RE 0.017904805330899193\n",
      "185 Train Loss 71826.47 Test MSE 1724.6354132118554 Test RE 0.018562457547512012\n",
      "186 Train Loss 71629.01 Test MSE 1837.7568017595001 Test RE 0.019161559102516542\n",
      "187 Train Loss 71419.055 Test MSE 1814.9124727354124 Test RE 0.019042092316496347\n",
      "188 Train Loss 71311.8 Test MSE 1572.0332559786013 Test RE 0.017722202238999\n",
      "189 Train Loss 71203.36 Test MSE 1669.7938431170232 Test RE 0.01826494006084147\n",
      "190 Train Loss 71094.625 Test MSE 1823.3980199648709 Test RE 0.019086555657903168\n",
      "191 Train Loss 70994.76 Test MSE 1825.782797348235 Test RE 0.019099032997678584\n",
      "192 Train Loss 70862.59 Test MSE 1686.757410661447 Test RE 0.018357483101143306\n",
      "193 Train Loss 70748.57 Test MSE 1831.5025294777747 Test RE 0.019128925909903192\n",
      "194 Train Loss 70644.47 Test MSE 1672.6442765435143 Test RE 0.01828052306116642\n",
      "195 Train Loss 70443.05 Test MSE 1790.3071381595198 Test RE 0.018912572038783555\n",
      "196 Train Loss 70189.055 Test MSE 2444.024276141099 Test RE 0.02209731880056773\n",
      "197 Train Loss 69866.36 Test MSE 2332.487365199355 Test RE 0.021587207875178677\n",
      "198 Train Loss 69552.984 Test MSE 2319.1645475866803 Test RE 0.02152546814184616\n",
      "199 Train Loss 69121.055 Test MSE 2727.0386028151797 Test RE 0.023341699114155495\n",
      "Training time: 47.70\n",
      "Training time: 47.70\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 2084179.8 Test MSE 1871043.312264296 Test RE 0.6114046594917261\n",
      "1 Train Loss 336227.16 Test MSE 1542104.4629208392 Test RE 0.5550648430642625\n",
      "2 Train Loss 178548.84 Test MSE 801988.1323142623 Test RE 0.40028636276726826\n",
      "3 Train Loss 141882.53 Test MSE 132332.61696198586 Test RE 0.1625999000027943\n",
      "4 Train Loss 128889.625 Test MSE 28139.481840550925 Test RE 0.07497990255998735\n",
      "5 Train Loss 125866.445 Test MSE 16956.772706804728 Test RE 0.05820474931080459\n",
      "6 Train Loss 124547.59 Test MSE 4890.668340307209 Test RE 0.031258701247450414\n",
      "7 Train Loss 123531.125 Test MSE 2053.389255902926 Test RE 0.02025454382045716\n",
      "8 Train Loss 123067.12 Test MSE 4062.701589926137 Test RE 0.028490120472654624\n",
      "9 Train Loss 122422.13 Test MSE 4135.410822608993 Test RE 0.028743930469116503\n",
      "10 Train Loss 121828.1 Test MSE 2759.8974357388615 Test RE 0.02348190329296981\n",
      "11 Train Loss 121325.06 Test MSE 2238.4254391220943 Test RE 0.021147456488760204\n",
      "12 Train Loss 120806.06 Test MSE 3134.745252482774 Test RE 0.02502579889627015\n",
      "13 Train Loss 120239.766 Test MSE 5005.053227108624 Test RE 0.031622133940827725\n",
      "14 Train Loss 119343.75 Test MSE 7739.04564231786 Test RE 0.0393215408442635\n",
      "15 Train Loss 118281.78 Test MSE 7012.9541110768605 Test RE 0.037431507859040036\n",
      "16 Train Loss 117043.21 Test MSE 7688.421994760735 Test RE 0.03919272225988676\n",
      "17 Train Loss 115185.49 Test MSE 18396.195189348968 Test RE 0.06062487108591485\n",
      "18 Train Loss 113878.28 Test MSE 27716.821879280724 Test RE 0.07441466638777809\n",
      "19 Train Loss 111444.23 Test MSE 47489.16578963224 Test RE 0.09740563771013631\n",
      "20 Train Loss 109207.37 Test MSE 32730.170203160633 Test RE 0.08086506789448648\n",
      "21 Train Loss 105779.23 Test MSE 20686.64432863145 Test RE 0.06428828604652431\n",
      "22 Train Loss 102506.35 Test MSE 25912.845565193547 Test RE 0.07195224964133995\n",
      "23 Train Loss 100852.016 Test MSE 13356.49021809638 Test RE 0.05165745414591157\n",
      "24 Train Loss 99146.93 Test MSE 3443.5129034434526 Test RE 0.026229359222785176\n",
      "25 Train Loss 96892.9 Test MSE 13641.030164170033 Test RE 0.05220479663430128\n",
      "26 Train Loss 95830.695 Test MSE 13032.257484765703 Test RE 0.05102660213944616\n",
      "27 Train Loss 94559.73 Test MSE 7296.806264352536 Test RE 0.03818152156870615\n",
      "28 Train Loss 93006.02 Test MSE 5151.248194885474 Test RE 0.0320806427754629\n",
      "29 Train Loss 91231.7 Test MSE 3431.6400444847277 Test RE 0.026184102186067336\n",
      "30 Train Loss 89841.85 Test MSE 2634.493954750183 Test RE 0.0229422193028086\n",
      "31 Train Loss 89046.8 Test MSE 2990.580717180738 Test RE 0.02444356734383256\n",
      "32 Train Loss 87190.234 Test MSE 2625.9388336146353 Test RE 0.022904938316406512\n",
      "33 Train Loss 86495.33 Test MSE 2785.1691785354724 Test RE 0.02358916748416673\n",
      "34 Train Loss 85752.6 Test MSE 3850.564307868518 Test RE 0.027736331168373825\n",
      "35 Train Loss 85151.36 Test MSE 3294.65288327843 Test RE 0.025656160052278724\n",
      "36 Train Loss 84403.16 Test MSE 3731.451556093815 Test RE 0.027303965562917334\n",
      "37 Train Loss 83552.28 Test MSE 2148.430027447952 Test RE 0.020717981001249976\n",
      "38 Train Loss 82620.305 Test MSE 1468.2454630816144 Test RE 0.017127191649871327\n",
      "39 Train Loss 81793.9 Test MSE 2069.8879516239253 Test RE 0.0203357522368154\n",
      "40 Train Loss 80771.89 Test MSE 1442.5514013925647 Test RE 0.016976668648576644\n",
      "41 Train Loss 79625.22 Test MSE 1273.0901044083175 Test RE 0.015948373385452075\n",
      "42 Train Loss 78923.49 Test MSE 1538.4561838893078 Test RE 0.017531916338648375\n",
      "43 Train Loss 77980.8 Test MSE 1514.6395615407114 Test RE 0.01739568247058263\n",
      "44 Train Loss 77255.7 Test MSE 1800.5586603088946 Test RE 0.01896664261968357\n",
      "45 Train Loss 76720.15 Test MSE 3160.515110781976 Test RE 0.02512845337870366\n",
      "46 Train Loss 75936.52 Test MSE 2250.021538364845 Test RE 0.021202162627442487\n",
      "47 Train Loss 75172.19 Test MSE 2492.4893860906845 Test RE 0.022315338677032295\n",
      "48 Train Loss 74373.13 Test MSE 3147.383705555233 Test RE 0.02507619680806797\n",
      "49 Train Loss 73671.24 Test MSE 2131.596605599198 Test RE 0.02063665641978131\n",
      "50 Train Loss 72922.21 Test MSE 1949.356916203437 Test RE 0.019734789813722124\n",
      "51 Train Loss 72216.8 Test MSE 4436.837781870661 Test RE 0.02977306865602658\n",
      "52 Train Loss 71342.28 Test MSE 1998.7776916736007 Test RE 0.019983385685591385\n",
      "53 Train Loss 70693.93 Test MSE 1370.1934212768133 Test RE 0.016545418727576117\n",
      "54 Train Loss 70192.42 Test MSE 1420.5407707441777 Test RE 0.01684665472770754\n",
      "55 Train Loss 69614.67 Test MSE 1647.9823859455305 Test RE 0.01814525627807396\n",
      "56 Train Loss 69082.266 Test MSE 1213.2621981477753 Test RE 0.01556912325696633\n",
      "57 Train Loss 68615.67 Test MSE 1416.5330965363673 Test RE 0.016822873787441805\n",
      "58 Train Loss 68211.24 Test MSE 1415.112072827675 Test RE 0.016814433567817012\n",
      "59 Train Loss 67813.72 Test MSE 1369.4676053227734 Test RE 0.016541035945025245\n",
      "60 Train Loss 67511.625 Test MSE 1173.4330246331108 Test RE 0.01531143788932369\n",
      "61 Train Loss 67193.55 Test MSE 1317.2230896947494 Test RE 0.016222451749058172\n",
      "62 Train Loss 66865.22 Test MSE 1256.9346346137918 Test RE 0.0158468581451214\n",
      "63 Train Loss 66524.89 Test MSE 1127.8906854916158 Test RE 0.015011369970384101\n",
      "64 Train Loss 66231.43 Test MSE 1595.683529906708 Test RE 0.01785501440337424\n",
      "65 Train Loss 65910.49 Test MSE 1937.4920950316462 Test RE 0.01967463994367837\n",
      "66 Train Loss 65581.04 Test MSE 1367.135165369011 Test RE 0.01652694382101845\n",
      "67 Train Loss 65234.816 Test MSE 1542.8688303578033 Test RE 0.017557041121475204\n",
      "68 Train Loss 64742.44 Test MSE 1762.8993582762837 Test RE 0.018767247588146914\n",
      "69 Train Loss 64319.098 Test MSE 1328.5288168998516 Test RE 0.01629192166292648\n",
      "70 Train Loss 63920.965 Test MSE 2013.8063008466825 Test RE 0.020058371534034253\n",
      "71 Train Loss 63594.773 Test MSE 1306.6679711516197 Test RE 0.01615732447221072\n",
      "72 Train Loss 62815.715 Test MSE 1256.54825921966 Test RE 0.01584442233561407\n",
      "73 Train Loss 62600.93 Test MSE 1130.6008885144408 Test RE 0.01502939452291842\n",
      "74 Train Loss 62373.88 Test MSE 1148.1699229270928 Test RE 0.015145719405364291\n",
      "75 Train Loss 62191.656 Test MSE 1117.7238940607422 Test RE 0.014943560673196243\n",
      "76 Train Loss 61871.71 Test MSE 1240.6676522227901 Test RE 0.015743980862693626\n",
      "77 Train Loss 61575.64 Test MSE 1111.9855245678284 Test RE 0.014905151358351666\n",
      "78 Train Loss 61345.715 Test MSE 1077.186335677544 Test RE 0.014670071869662743\n",
      "79 Train Loss 61093.68 Test MSE 937.5803350861106 Test RE 0.013686457985834193\n",
      "80 Train Loss 60923.22 Test MSE 1040.71925052841 Test RE 0.0144196134271906\n",
      "81 Train Loss 60678.336 Test MSE 1036.9901312157315 Test RE 0.014393755966145862\n",
      "82 Train Loss 60416.473 Test MSE 949.9874346637279 Test RE 0.013776717535547791\n",
      "83 Train Loss 60228.156 Test MSE 1370.0832839753307 Test RE 0.016544753746865155\n",
      "84 Train Loss 60005.945 Test MSE 1611.382460575624 Test RE 0.01794263157911639\n",
      "85 Train Loss 59740.125 Test MSE 1662.748231255869 Test RE 0.018226365324856695\n",
      "86 Train Loss 59449.355 Test MSE 2178.1477057830034 Test RE 0.020860777319203674\n",
      "87 Train Loss 59182.645 Test MSE 1927.9679154023834 Test RE 0.019626222802708097\n",
      "88 Train Loss 58913.566 Test MSE 1997.9488608444678 Test RE 0.0199792420122915\n",
      "89 Train Loss 58641.61 Test MSE 1959.015980977051 Test RE 0.019783622345274556\n",
      "90 Train Loss 58405.625 Test MSE 1572.2778394302625 Test RE 0.01772358083222983\n",
      "91 Train Loss 58010.01 Test MSE 1442.266532121306 Test RE 0.016974992323559666\n",
      "92 Train Loss 57656.797 Test MSE 1534.8947264633705 Test RE 0.01751161177874217\n",
      "93 Train Loss 57176.04 Test MSE 1107.4409970408692 Test RE 0.014874662553111508\n",
      "94 Train Loss 56765.008 Test MSE 1099.6561886874877 Test RE 0.014822289282179851\n",
      "95 Train Loss 56414.117 Test MSE 898.5275318998318 Test RE 0.013398387008828925\n",
      "96 Train Loss 56036.16 Test MSE 1114.1800018150288 Test RE 0.014919851591891442\n",
      "97 Train Loss 55609.59 Test MSE 980.5478438762011 Test RE 0.013996557034174986\n",
      "98 Train Loss 55160.51 Test MSE 1638.7674395438553 Test RE 0.018094454169095886\n",
      "99 Train Loss 54850.76 Test MSE 2642.0638800426623 Test RE 0.0229751566167803\n",
      "100 Train Loss 54485.934 Test MSE 1601.8190295361162 Test RE 0.017889308274346576\n",
      "101 Train Loss 54016.273 Test MSE 1061.2619517639507 Test RE 0.014561231978388378\n",
      "102 Train Loss 53604.336 Test MSE 1041.4983019763977 Test RE 0.014425009464216026\n",
      "103 Train Loss 53257.758 Test MSE 1261.080862559758 Test RE 0.015872973500985696\n",
      "104 Train Loss 52910.574 Test MSE 1335.6683295802902 Test RE 0.016335639393127835\n",
      "105 Train Loss 52629.684 Test MSE 966.6918346874298 Test RE 0.013897313314803234\n",
      "106 Train Loss 52425.457 Test MSE 1068.912473090886 Test RE 0.014613622892147323\n",
      "107 Train Loss 52205.516 Test MSE 1038.3689269933175 Test RE 0.014403321851510621\n",
      "108 Train Loss 52018.73 Test MSE 1245.5554856606598 Test RE 0.01577496349991062\n",
      "109 Train Loss 51774.184 Test MSE 1234.0330895950603 Test RE 0.015701828378930468\n",
      "110 Train Loss 51500.875 Test MSE 1249.7811760161821 Test RE 0.015801700032150687\n",
      "111 Train Loss 51257.35 Test MSE 1203.8178608443582 Test RE 0.015508407889702546\n",
      "112 Train Loss 51024.45 Test MSE 1241.1119429036714 Test RE 0.01574679961832385\n",
      "113 Train Loss 50821.023 Test MSE 1096.1611079475958 Test RE 0.014798715401807818\n",
      "114 Train Loss 50584.355 Test MSE 988.1113414021713 Test RE 0.01405043485573158\n",
      "115 Train Loss 50187.473 Test MSE 951.0701891308235 Test RE 0.013784566352252508\n",
      "116 Train Loss 49927.96 Test MSE 898.1529908423104 Test RE 0.01339559423458439\n",
      "117 Train Loss 49641.938 Test MSE 1088.6976597921148 Test RE 0.01474824923774369\n",
      "118 Train Loss 49484.97 Test MSE 852.1362263649221 Test RE 0.013047921582790013\n",
      "119 Train Loss 49335.3 Test MSE 714.957463755389 Test RE 0.011951624112763437\n",
      "120 Train Loss 49197.61 Test MSE 932.0066839585368 Test RE 0.01364571627634641\n",
      "121 Train Loss 49032.082 Test MSE 925.745780611702 Test RE 0.013599805406757915\n",
      "122 Train Loss 48919.477 Test MSE 848.0573910642361 Test RE 0.013016656533563348\n",
      "123 Train Loss 48815.086 Test MSE 743.9574438914627 Test RE 0.012191604637443681\n",
      "124 Train Loss 48664.24 Test MSE 863.0274694264285 Test RE 0.013131040270404617\n",
      "125 Train Loss 48514.273 Test MSE 757.1041471880698 Test RE 0.012298853722406035\n",
      "126 Train Loss 48336.01 Test MSE 831.8220058601937 Test RE 0.012891457654065162\n",
      "127 Train Loss 48167.254 Test MSE 767.8812586446383 Test RE 0.012386079342930463\n",
      "128 Train Loss 47963.223 Test MSE 788.9471288555471 Test RE 0.012554828184161542\n",
      "129 Train Loss 47748.527 Test MSE 707.822552421028 Test RE 0.011891839021956768\n",
      "130 Train Loss 47546.996 Test MSE 872.1065077327298 Test RE 0.013199928758531595\n",
      "131 Train Loss 47345.06 Test MSE 643.621529450296 Test RE 0.011339714100705679\n",
      "132 Train Loss 47132.65 Test MSE 669.9029968108097 Test RE 0.01156891909582715\n",
      "133 Train Loss 46987.746 Test MSE 865.3182521318 Test RE 0.013148455953257683\n",
      "134 Train Loss 46819.05 Test MSE 824.2105161350152 Test RE 0.012832341222981498\n",
      "135 Train Loss 46620.73 Test MSE 675.2764542955515 Test RE 0.011615225003008864\n",
      "136 Train Loss 46455.273 Test MSE 642.4765949918728 Test RE 0.011329623536673328\n",
      "137 Train Loss 46306.363 Test MSE 623.8300790069549 Test RE 0.01116400389234429\n",
      "138 Train Loss 46162.27 Test MSE 619.3076275429066 Test RE 0.011123463603457402\n",
      "139 Train Loss 46001.41 Test MSE 678.0145377508048 Test RE 0.011638749650998629\n",
      "140 Train Loss 45869.91 Test MSE 619.0148963666752 Test RE 0.011120834401660326\n",
      "141 Train Loss 45782.918 Test MSE 635.5026236582479 Test RE 0.011267965214246192\n",
      "142 Train Loss 45716.273 Test MSE 600.3362767461253 Test RE 0.01095176502810372\n",
      "143 Train Loss 45645.555 Test MSE 574.3858464365932 Test RE 0.010712447056951844\n",
      "144 Train Loss 45548.01 Test MSE 678.4772910381507 Test RE 0.011642720768647755\n",
      "145 Train Loss 45445.484 Test MSE 702.257582658479 Test RE 0.011844999378226882\n",
      "146 Train Loss 45329.7 Test MSE 681.1843071048035 Test RE 0.011665923945948507\n",
      "147 Train Loss 45251.387 Test MSE 639.2152982062805 Test RE 0.011300831613086836\n",
      "148 Train Loss 45157.48 Test MSE 619.3530968030358 Test RE 0.011123871935564515\n",
      "149 Train Loss 45072.484 Test MSE 614.4365069348141 Test RE 0.011079631833878232\n",
      "150 Train Loss 45005.367 Test MSE 604.8552118252557 Test RE 0.010992906579392501\n",
      "151 Train Loss 44892.66 Test MSE 835.5749911068383 Test RE 0.012920506537944439\n",
      "152 Train Loss 44832.066 Test MSE 818.2691283864178 Test RE 0.01278600608778606\n",
      "153 Train Loss 44776.305 Test MSE 627.3102820451035 Test RE 0.011195101273050121\n",
      "154 Train Loss 44720.137 Test MSE 621.0640534828583 Test RE 0.011139226130908996\n",
      "155 Train Loss 44642.18 Test MSE 719.8584573458712 Test RE 0.01199251800989789\n",
      "156 Train Loss 44554.36 Test MSE 699.912579685609 Test RE 0.011825206223726748\n",
      "157 Train Loss 44482.96 Test MSE 688.1693185867633 Test RE 0.011725583850566457\n",
      "158 Train Loss 44408.2 Test MSE 630.5162390111782 Test RE 0.011223671881752488\n",
      "159 Train Loss 44347.91 Test MSE 574.6192572435108 Test RE 0.010714623422333527\n",
      "160 Train Loss 44297.004 Test MSE 582.7665963661246 Test RE 0.010790315639151178\n",
      "161 Train Loss 44231.973 Test MSE 667.8126676625232 Test RE 0.011550855478319001\n",
      "162 Train Loss 44135.707 Test MSE 817.6319046419692 Test RE 0.0127810265931258\n",
      "163 Train Loss 44037.39 Test MSE 833.296714152855 Test RE 0.012902880002107762\n",
      "164 Train Loss 43925.492 Test MSE 644.6550347798355 Test RE 0.011348814910297414\n",
      "165 Train Loss 43854.688 Test MSE 611.7929833371929 Test RE 0.011055771891451733\n",
      "166 Train Loss 43799.645 Test MSE 587.4377932489996 Test RE 0.010833474502843535\n",
      "167 Train Loss 43740.086 Test MSE 627.892683927788 Test RE 0.011200296896034103\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13456/44954379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13456/2875520225.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13456/3521064569.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13456/3521064569.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_coll_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13456/2681864328.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x_bc1, y_bc1, x_coll, f_hat)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss_bc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_bc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_bc1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13456/2681864328.py\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, x_coll, f_hat)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0my_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_unused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdy_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.01,\n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "e199619a-d416-48f4-91f7-2c23d1e79435"
   },
   "outputs": [],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tanh_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
