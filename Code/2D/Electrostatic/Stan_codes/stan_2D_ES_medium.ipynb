{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "label = \"ES_stan_medium\"\n",
    "ES_val = 200.0\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA_medium.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = ES_val*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 =self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_stan_medium\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 9777.699 Test MSE 2886.7934124915796 Test RE 0.4702223814118742\n",
      "1 Train Loss 7900.4233 Test MSE 2159.4501095019423 Test RE 0.4066933126338244\n",
      "2 Train Loss 5471.5537 Test MSE 1143.8813337760919 Test RE 0.29599606010740404\n",
      "3 Train Loss 4180.537 Test MSE 924.9774478104387 Test RE 0.2661711714063462\n",
      "4 Train Loss 3607.354 Test MSE 838.6131265467911 Test RE 0.25344064887362017\n",
      "5 Train Loss 3231.8494 Test MSE 790.7536981558511 Test RE 0.24610251938572478\n",
      "6 Train Loss 2869.3354 Test MSE 648.8664125745408 Test RE 0.22293234838906087\n",
      "7 Train Loss 2603.8152 Test MSE 537.3456694109898 Test RE 0.20287209796068428\n",
      "8 Train Loss 2363.7817 Test MSE 424.4740437570236 Test RE 0.18031049811507968\n",
      "9 Train Loss 2220.156 Test MSE 363.8087935147578 Test RE 0.16692909537356326\n",
      "10 Train Loss 2121.3682 Test MSE 321.2544615237797 Test RE 0.15686282341034918\n",
      "11 Train Loss 1994.6354 Test MSE 295.33876111409626 Test RE 0.15040271229737445\n",
      "12 Train Loss 1924.0635 Test MSE 287.6514882513164 Test RE 0.14843241602083912\n",
      "13 Train Loss 1869.2189 Test MSE 271.1209107980489 Test RE 0.14410430367599186\n",
      "14 Train Loss 1828.2638 Test MSE 273.47536981978124 Test RE 0.14472866394828374\n",
      "15 Train Loss 1795.3325 Test MSE 277.14348044189865 Test RE 0.1456960498239747\n",
      "16 Train Loss 1774.145 Test MSE 276.76251998293185 Test RE 0.1455958787481603\n",
      "17 Train Loss 1753.8624 Test MSE 280.8874579223859 Test RE 0.14667686455208254\n",
      "18 Train Loss 1745.2731 Test MSE 279.2848728860576 Test RE 0.1462578383825423\n",
      "19 Train Loss 1735.3954 Test MSE 279.74185025220083 Test RE 0.1463774460133665\n",
      "20 Train Loss 1718.881 Test MSE 267.1361888579825 Test RE 0.1430414179889696\n",
      "21 Train Loss 1703.28 Test MSE 262.66757622943305 Test RE 0.1418399851554887\n",
      "22 Train Loss 1686.8381 Test MSE 261.03371136462084 Test RE 0.14139815506766681\n",
      "23 Train Loss 1666.7404 Test MSE 255.3149607650162 Test RE 0.13984069568286467\n",
      "24 Train Loss 1652.2621 Test MSE 250.56208985603575 Test RE 0.13853296363177992\n",
      "25 Train Loss 1637.3303 Test MSE 246.27063506560978 Test RE 0.13734149135633028\n",
      "26 Train Loss 1626.4698 Test MSE 235.88906014449725 Test RE 0.13441549751059523\n",
      "27 Train Loss 1614.4469 Test MSE 229.0239716698628 Test RE 0.13244510584125832\n",
      "28 Train Loss 1595.3064 Test MSE 223.02837640492072 Test RE 0.1306999752185744\n",
      "29 Train Loss 1582.0546 Test MSE 218.09542277448247 Test RE 0.12924647860730362\n",
      "30 Train Loss 1563.6272 Test MSE 215.8613041105746 Test RE 0.12858278916503418\n",
      "31 Train Loss 1534.3723 Test MSE 213.59121554333697 Test RE 0.12790488674112596\n",
      "32 Train Loss 1510.9839 Test MSE 215.4371232317265 Test RE 0.1284563904529566\n",
      "33 Train Loss 1489.6877 Test MSE 214.9398643417984 Test RE 0.12830805718874275\n",
      "34 Train Loss 1463.5983 Test MSE 220.11547745482767 Test RE 0.12984365569558232\n",
      "35 Train Loss 1430.5333 Test MSE 212.97224797562885 Test RE 0.1277194240202203\n",
      "36 Train Loss 1389.0719 Test MSE 201.7502730765684 Test RE 0.12430898186260042\n",
      "37 Train Loss 1351.7812 Test MSE 190.35119357493662 Test RE 0.12074613717258488\n",
      "38 Train Loss 1317.9625 Test MSE 181.3131224039043 Test RE 0.11784470194327525\n",
      "39 Train Loss 1278.93 Test MSE 184.39881093446192 Test RE 0.11884324508045874\n",
      "40 Train Loss 1235.5637 Test MSE 183.61763071415908 Test RE 0.11859124637023856\n",
      "41 Train Loss 1191.0397 Test MSE 175.76085872239892 Test RE 0.11602632233338732\n",
      "42 Train Loss 1136.9747 Test MSE 164.35014020869338 Test RE 0.1121968033146839\n",
      "43 Train Loss 1072.0388 Test MSE 155.19886210064948 Test RE 0.10902841791488316\n",
      "44 Train Loss 1004.5077 Test MSE 158.13348683131704 Test RE 0.11005438910103446\n",
      "45 Train Loss 944.13245 Test MSE 163.2573695709293 Test RE 0.11182318071073391\n",
      "46 Train Loss 898.05396 Test MSE 156.73981032660768 Test RE 0.10956834511689807\n",
      "47 Train Loss 851.8812 Test MSE 157.5326527118254 Test RE 0.10984511223162166\n",
      "48 Train Loss 809.1452 Test MSE 176.27256518803412 Test RE 0.11619509787695441\n",
      "49 Train Loss 784.41504 Test MSE 190.58664617756753 Test RE 0.1208207918296443\n",
      "50 Train Loss 755.52985 Test MSE 201.13729420666897 Test RE 0.12411999390006828\n",
      "51 Train Loss 744.2364 Test MSE 217.16519305785954 Test RE 0.1289705503252627\n",
      "52 Train Loss 732.45734 Test MSE 220.6807430112394 Test RE 0.13001027070707966\n",
      "53 Train Loss 724.35394 Test MSE 223.17257151799987 Test RE 0.13074221928815824\n",
      "54 Train Loss 712.6582 Test MSE 229.59232033170494 Test RE 0.13260934270070973\n",
      "55 Train Loss 706.07684 Test MSE 238.6056022495481 Test RE 0.1351872588165938\n",
      "56 Train Loss 693.95264 Test MSE 253.81418328301123 Test RE 0.1394290882170215\n",
      "57 Train Loss 679.2096 Test MSE 248.59589978783262 Test RE 0.1379883509213947\n",
      "58 Train Loss 664.6219 Test MSE 245.5372416014562 Test RE 0.13713683753324238\n",
      "59 Train Loss 650.03284 Test MSE 247.9220739696844 Test RE 0.1378012134719467\n",
      "60 Train Loss 637.90497 Test MSE 250.43945731611572 Test RE 0.13849905840613508\n",
      "61 Train Loss 624.9392 Test MSE 249.90244743826534 Test RE 0.13835048901483898\n",
      "62 Train Loss 611.4534 Test MSE 246.46367085538424 Test RE 0.13739530741533879\n",
      "63 Train Loss 600.3126 Test MSE 249.8096627332469 Test RE 0.1383248029897725\n",
      "64 Train Loss 591.8081 Test MSE 238.91535992188406 Test RE 0.13527498037136884\n",
      "65 Train Loss 577.9977 Test MSE 239.94020623347177 Test RE 0.13556480625397427\n",
      "66 Train Loss 560.00684 Test MSE 229.77421350803968 Test RE 0.1326618617898376\n",
      "67 Train Loss 543.3258 Test MSE 242.2197331016695 Test RE 0.13620724364978082\n",
      "68 Train Loss 534.8152 Test MSE 244.38780703016292 Test RE 0.13681547134782898\n",
      "69 Train Loss 525.26855 Test MSE 245.3662379298039 Test RE 0.137089074947241\n",
      "70 Train Loss 511.23862 Test MSE 256.16635742573004 Test RE 0.14007366442296046\n",
      "71 Train Loss 500.31964 Test MSE 262.6883052046592 Test RE 0.14184558184841678\n",
      "72 Train Loss 485.35928 Test MSE 251.84939559747883 Test RE 0.13888837615716715\n",
      "73 Train Loss 471.92426 Test MSE 241.25319601177318 Test RE 0.13593521596433322\n",
      "74 Train Loss 462.51303 Test MSE 247.79122105812814 Test RE 0.13776484303180583\n",
      "75 Train Loss 449.0919 Test MSE 256.42575531120815 Test RE 0.14014456682226092\n",
      "76 Train Loss 438.7859 Test MSE 256.959307272268 Test RE 0.14029029234133478\n",
      "77 Train Loss 430.51898 Test MSE 284.288544753946 Test RE 0.14756220085978464\n",
      "78 Train Loss 424.99088 Test MSE 281.4096606784583 Test RE 0.14681314599508088\n",
      "79 Train Loss 417.8888 Test MSE 280.51467129932684 Test RE 0.14657949935167555\n",
      "80 Train Loss 410.52383 Test MSE 276.12194752395345 Test RE 0.1454272888350812\n",
      "81 Train Loss 404.32666 Test MSE 275.7204782645045 Test RE 0.1453215279129995\n",
      "82 Train Loss 397.39124 Test MSE 285.2558886035273 Test RE 0.14781304136775608\n",
      "83 Train Loss 391.84836 Test MSE 287.811967141921 Test RE 0.14847381498635592\n",
      "84 Train Loss 383.53116 Test MSE 300.771423623999 Test RE 0.15177971378822924\n",
      "85 Train Loss 376.08395 Test MSE 294.14484334788796 Test RE 0.15009840019930182\n",
      "86 Train Loss 371.77087 Test MSE 293.5398615946795 Test RE 0.14994396347347283\n",
      "87 Train Loss 366.04233 Test MSE 296.94686343444664 Test RE 0.15081162342471008\n",
      "88 Train Loss 359.24414 Test MSE 296.0022793192435 Test RE 0.15057156746140252\n",
      "89 Train Loss 353.69794 Test MSE 292.8743724027003 Test RE 0.14977389677889336\n",
      "90 Train Loss 348.45322 Test MSE 293.8793274809935 Test RE 0.150030640207146\n",
      "91 Train Loss 344.20584 Test MSE 295.00070936708346 Test RE 0.15031661039684563\n",
      "92 Train Loss 341.0577 Test MSE 292.71386384269846 Test RE 0.14973284967914627\n",
      "93 Train Loss 336.6545 Test MSE 304.1415676689169 Test RE 0.15262769091704903\n",
      "94 Train Loss 330.68646 Test MSE 307.03519845630785 Test RE 0.15335202906999934\n",
      "95 Train Loss 325.92828 Test MSE 305.8505945305936 Test RE 0.1530559115938439\n",
      "96 Train Loss 322.31 Test MSE 310.2442317618998 Test RE 0.15415133904935657\n",
      "97 Train Loss 318.14774 Test MSE 311.9805425859639 Test RE 0.15458209837846568\n",
      "98 Train Loss 313.91116 Test MSE 297.37218678347887 Test RE 0.15091959013606127\n",
      "99 Train Loss 310.9655 Test MSE 304.09736392054543 Test RE 0.1526165991066559\n",
      "100 Train Loss 307.90894 Test MSE 311.3098534196692 Test RE 0.1544158503198421\n",
      "101 Train Loss 303.77908 Test MSE 309.2467742244536 Test RE 0.15390333572607856\n",
      "102 Train Loss 298.52856 Test MSE 307.8088363603011 Test RE 0.1535451084028929\n",
      "103 Train Loss 293.6856 Test MSE 306.0019560617437 Test RE 0.15309377961178888\n",
      "104 Train Loss 290.50876 Test MSE 310.9743993186626 Test RE 0.15433263195811414\n",
      "105 Train Loss 287.74 Test MSE 302.23025477632785 Test RE 0.15214735698353604\n",
      "106 Train Loss 284.57178 Test MSE 305.32156735906614 Test RE 0.15292348455044905\n",
      "107 Train Loss 280.47577 Test MSE 308.4416749627471 Test RE 0.15370286764808425\n",
      "108 Train Loss 276.8817 Test MSE 306.9868636171327 Test RE 0.15333995791750665\n",
      "109 Train Loss 273.9721 Test MSE 309.39730070759856 Test RE 0.1539407875467592\n",
      "110 Train Loss 269.9795 Test MSE 308.3778389550598 Test RE 0.15368696142340962\n",
      "111 Train Loss 266.03326 Test MSE 314.65760478901353 Test RE 0.15524390557344175\n",
      "112 Train Loss 262.72525 Test MSE 318.9783353419334 Test RE 0.15630613970707252\n",
      "113 Train Loss 259.95663 Test MSE 328.25675802843955 Test RE 0.15856315617558384\n",
      "114 Train Loss 257.56345 Test MSE 327.1779413003499 Test RE 0.15830238261897875\n",
      "115 Train Loss 254.89713 Test MSE 325.2438878306822 Test RE 0.15783380109780765\n",
      "116 Train Loss 252.41882 Test MSE 323.5118863694519 Test RE 0.1574129887557042\n",
      "117 Train Loss 250.54169 Test MSE 321.2592374163252 Test RE 0.1568639893975874\n",
      "118 Train Loss 247.70721 Test MSE 317.40525151491903 Test RE 0.15592024114642394\n",
      "119 Train Loss 245.55179 Test MSE 324.1247381798658 Test RE 0.15756201757739433\n",
      "120 Train Loss 243.69084 Test MSE 322.56852713360973 Test RE 0.15718331340942618\n",
      "121 Train Loss 241.76883 Test MSE 321.4492481162706 Test RE 0.1569103716133239\n",
      "122 Train Loss 239.9548 Test MSE 325.1201498403174 Test RE 0.15780377456083233\n",
      "123 Train Loss 237.97194 Test MSE 328.61014197224733 Test RE 0.158648483590006\n",
      "124 Train Loss 235.99901 Test MSE 324.1771979417227 Test RE 0.15757476780982435\n",
      "125 Train Loss 233.21596 Test MSE 322.46240637016456 Test RE 0.1571574556707583\n",
      "126 Train Loss 231.5988 Test MSE 325.0784974343178 Test RE 0.1577936658096166\n",
      "127 Train Loss 229.88148 Test MSE 329.79510949989145 Test RE 0.1589342692465985\n",
      "128 Train Loss 228.21078 Test MSE 331.9300637894556 Test RE 0.15944787604526767\n",
      "129 Train Loss 226.35895 Test MSE 330.20787683046905 Test RE 0.15903369819213417\n",
      "130 Train Loss 224.20383 Test MSE 322.77243047409655 Test RE 0.15723298524138005\n",
      "131 Train Loss 222.7493 Test MSE 323.6724343428571 Test RE 0.15745204327382867\n",
      "132 Train Loss 221.65683 Test MSE 320.2551269044236 Test RE 0.15661865474214026\n",
      "133 Train Loss 220.71608 Test MSE 319.1174143824513 Test RE 0.15634021183114521\n",
      "134 Train Loss 219.49759 Test MSE 318.6781170524984 Test RE 0.156232565731324\n",
      "135 Train Loss 217.81714 Test MSE 319.7456730519972 Test RE 0.1564940326400189\n",
      "136 Train Loss 215.50787 Test MSE 323.6577234378389 Test RE 0.15744846513722066\n",
      "137 Train Loss 214.4198 Test MSE 323.80066190836675 Test RE 0.15748322864959302\n",
      "138 Train Loss 212.67307 Test MSE 327.6310392860208 Test RE 0.1584119585804136\n",
      "139 Train Loss 211.33641 Test MSE 327.2480873788241 Test RE 0.15831935151393373\n",
      "140 Train Loss 209.95508 Test MSE 329.39463957552715 Test RE 0.1588377430599777\n",
      "141 Train Loss 208.38235 Test MSE 329.09126426343585 Test RE 0.15876458074271638\n",
      "142 Train Loss 207.20354 Test MSE 329.16729248739375 Test RE 0.15878291895667299\n",
      "143 Train Loss 206.26225 Test MSE 326.09173606311737 Test RE 0.15803938837800116\n",
      "144 Train Loss 204.91917 Test MSE 326.8018564621109 Test RE 0.15821137366497035\n",
      "145 Train Loss 203.59439 Test MSE 329.54141066730807 Test RE 0.1588731264418314\n",
      "146 Train Loss 202.02863 Test MSE 328.0275645392487 Test RE 0.15850779099587425\n",
      "147 Train Loss 201.069 Test MSE 330.344173749107 Test RE 0.15906651625656287\n",
      "148 Train Loss 199.70941 Test MSE 329.0688440346639 Test RE 0.1587591725182578\n",
      "149 Train Loss 198.72906 Test MSE 326.304442128982 Test RE 0.1580909236559566\n",
      "150 Train Loss 197.4502 Test MSE 324.9639161387825 Test RE 0.15776585437903332\n",
      "151 Train Loss 196.24963 Test MSE 330.57869337259604 Test RE 0.1591229688959836\n",
      "152 Train Loss 194.98477 Test MSE 328.3283875661346 Test RE 0.15858045541735116\n",
      "153 Train Loss 193.60458 Test MSE 327.9094427851096 Test RE 0.15847924933323862\n",
      "154 Train Loss 192.27563 Test MSE 327.7817406214043 Test RE 0.15844838698042435\n",
      "155 Train Loss 191.02293 Test MSE 329.4293209349827 Test RE 0.15884610470769064\n",
      "156 Train Loss 190.09093 Test MSE 331.6851403230583 Test RE 0.15938903875178576\n",
      "157 Train Loss 189.20613 Test MSE 331.1595578810352 Test RE 0.15926270615154078\n",
      "158 Train Loss 187.98396 Test MSE 328.33752187748723 Test RE 0.158582661308213\n",
      "159 Train Loss 186.7959 Test MSE 328.0173892405659 Test RE 0.15850533254908641\n",
      "160 Train Loss 185.21371 Test MSE 332.4139609531544 Test RE 0.1595640575601028\n",
      "161 Train Loss 184.11763 Test MSE 329.24690249128895 Test RE 0.15880211883600504\n",
      "162 Train Loss 182.98047 Test MSE 329.2600930973002 Test RE 0.1588052998457396\n",
      "163 Train Loss 181.8522 Test MSE 324.09357215915685 Test RE 0.1575544422544191\n",
      "164 Train Loss 180.88898 Test MSE 329.8366070462238 Test RE 0.15894426814372214\n",
      "165 Train Loss 179.85007 Test MSE 330.63025720601195 Test RE 0.15913537845306577\n",
      "166 Train Loss 178.627 Test MSE 331.7355882567469 Test RE 0.15940115949667075\n",
      "167 Train Loss 177.77913 Test MSE 330.40937108307986 Test RE 0.15908221231231792\n",
      "168 Train Loss 176.79605 Test MSE 335.4007604307723 Test RE 0.16027931042229576\n",
      "169 Train Loss 175.83878 Test MSE 334.8485568983319 Test RE 0.1601473142087524\n",
      "170 Train Loss 175.11583 Test MSE 331.74339951056896 Test RE 0.15940303616536552\n",
      "171 Train Loss 174.38335 Test MSE 331.96320384512035 Test RE 0.15945583552254727\n",
      "172 Train Loss 173.61589 Test MSE 332.65128585920064 Test RE 0.15962100728669026\n",
      "173 Train Loss 172.80359 Test MSE 333.3125358862591 Test RE 0.15977957723472427\n",
      "174 Train Loss 172.21573 Test MSE 329.3547630757157 Test RE 0.15882812832406978\n",
      "175 Train Loss 171.4601 Test MSE 327.6383097703638 Test RE 0.15841371623633904\n",
      "176 Train Loss 170.68918 Test MSE 331.8933578843358 Test RE 0.1594390596693813\n",
      "177 Train Loss 170.0743 Test MSE 330.49945910077304 Test RE 0.159103898174857\n",
      "178 Train Loss 169.45732 Test MSE 330.48308647701765 Test RE 0.1590999571993825\n",
      "179 Train Loss 168.94135 Test MSE 327.1691481082159 Test RE 0.15830025534748096\n",
      "180 Train Loss 168.37914 Test MSE 328.2231908153481 Test RE 0.15855504871214618\n",
      "181 Train Loss 167.63869 Test MSE 328.09276039673233 Test RE 0.1585235420165868\n",
      "182 Train Loss 167.11034 Test MSE 328.8241016795936 Test RE 0.1587001235962681\n",
      "183 Train Loss 166.7034 Test MSE 331.5802555940833 Test RE 0.15936383593758302\n",
      "184 Train Loss 166.1945 Test MSE 333.09730048278146 Test RE 0.15972798035236166\n",
      "185 Train Loss 165.84677 Test MSE 333.09385538372277 Test RE 0.15972715434726226\n",
      "186 Train Loss 165.06941 Test MSE 331.8345903288204 Test RE 0.15942494329980286\n",
      "187 Train Loss 164.37238 Test MSE 329.1893344639304 Test RE 0.15878823514522708\n",
      "188 Train Loss 163.62016 Test MSE 327.70084976643193 Test RE 0.15842883461560975\n",
      "189 Train Loss 163.05296 Test MSE 332.24629628431893 Test RE 0.15952381161325782\n",
      "190 Train Loss 162.73816 Test MSE 334.3215575121298 Test RE 0.16002124114102315\n",
      "191 Train Loss 162.39156 Test MSE 334.3909683705578 Test RE 0.1600378518486083\n",
      "192 Train Loss 161.66176 Test MSE 334.66192315255273 Test RE 0.16010267752382742\n",
      "193 Train Loss 161.06712 Test MSE 334.61253607795015 Test RE 0.16009086366920502\n",
      "194 Train Loss 160.52652 Test MSE 333.75959791670857 Test RE 0.15988669508718406\n",
      "195 Train Loss 160.00858 Test MSE 333.33444573503374 Test RE 0.15978482859563972\n",
      "196 Train Loss 159.49527 Test MSE 332.9862973895977 Test RE 0.1597013638393456\n",
      "197 Train Loss 158.96274 Test MSE 334.9259226573887 Test RE 0.1601658139195749\n",
      "198 Train Loss 158.35599 Test MSE 337.50423753023136 Test RE 0.1607811233988264\n",
      "199 Train Loss 157.97464 Test MSE 338.6295988696531 Test RE 0.16104895156801577\n",
      "Training time: 176.21\n",
      "Training time: 176.21\n",
      "ES_stan_medium\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 8950.064 Test MSE 2538.190678867177 Test RE 0.4409177334723402\n",
      "1 Train Loss 5690.1436 Test MSE 1337.389641052683 Test RE 0.32005485632506364\n",
      "2 Train Loss 3987.1333 Test MSE 939.052906919077 Test RE 0.2681886996384298\n",
      "3 Train Loss 3212.2324 Test MSE 909.3115241913341 Test RE 0.2639075357141317\n",
      "4 Train Loss 2692.7878 Test MSE 570.3332818709645 Test RE 0.20900650431649515\n",
      "5 Train Loss 2389.0735 Test MSE 499.13667374988717 Test RE 0.19552630091000056\n",
      "6 Train Loss 2152.6228 Test MSE 476.6766354127498 Test RE 0.1910765434904822\n",
      "7 Train Loss 2024.2825 Test MSE 404.3895403449398 Test RE 0.1759930025851135\n",
      "8 Train Loss 1938.2286 Test MSE 371.2703386979082 Test RE 0.16863222524400578\n",
      "9 Train Loss 1854.168 Test MSE 372.8994281472073 Test RE 0.16900178921594852\n",
      "10 Train Loss 1791.7279 Test MSE 365.7208232881719 Test RE 0.16736717595786937\n",
      "11 Train Loss 1743.5383 Test MSE 375.14129563725 Test RE 0.16950904645307419\n",
      "12 Train Loss 1704.083 Test MSE 373.2567270364993 Test RE 0.16908273556429415\n",
      "13 Train Loss 1673.1781 Test MSE 374.3049555015144 Test RE 0.16931998925950445\n",
      "14 Train Loss 1636.7843 Test MSE 346.93282524346495 Test RE 0.16301146114730755\n",
      "15 Train Loss 1606.4213 Test MSE 330.4520148818388 Test RE 0.15909247783679795\n",
      "16 Train Loss 1576.4142 Test MSE 324.5225534291466 Test RE 0.15765867998066924\n",
      "17 Train Loss 1545.2064 Test MSE 329.72727224626504 Test RE 0.158917922401977\n",
      "18 Train Loss 1518.0175 Test MSE 308.66396384620384 Test RE 0.15375824324746803\n",
      "19 Train Loss 1491.167 Test MSE 297.1397012444454 Test RE 0.1508605841425884\n",
      "20 Train Loss 1457.8273 Test MSE 290.8315306875483 Test RE 0.14925063527405763\n",
      "21 Train Loss 1422.5624 Test MSE 274.05443206141524 Test RE 0.14488180861343233\n",
      "22 Train Loss 1389.4302 Test MSE 266.5775422280675 Test RE 0.1428917725444918\n",
      "23 Train Loss 1349.7433 Test MSE 250.8144583457243 Test RE 0.13860271192431556\n",
      "24 Train Loss 1306.7308 Test MSE 231.34221231221133 Test RE 0.13311374010874483\n",
      "25 Train Loss 1275.469 Test MSE 214.62507846805946 Test RE 0.1282140672655985\n",
      "26 Train Loss 1240.9708 Test MSE 207.8991858921625 Test RE 0.1261890986215055\n",
      "27 Train Loss 1202.2186 Test MSE 194.35826456562768 Test RE 0.12201042786461365\n",
      "28 Train Loss 1160.918 Test MSE 198.69746686169202 Test RE 0.12336489943354076\n",
      "29 Train Loss 1120.5991 Test MSE 185.07599104041427 Test RE 0.11906126310834199\n",
      "30 Train Loss 1082.2509 Test MSE 182.6151155687877 Test RE 0.11826706115218688\n",
      "31 Train Loss 1037.2306 Test MSE 173.53281349804337 Test RE 0.11528856880096136\n",
      "32 Train Loss 994.4125 Test MSE 176.19660957818695 Test RE 0.11617006102434425\n",
      "33 Train Loss 935.26416 Test MSE 200.3663439755539 Test RE 0.1238818923368876\n",
      "34 Train Loss 895.21313 Test MSE 217.84974262783243 Test RE 0.12917366130371796\n",
      "35 Train Loss 839.26636 Test MSE 225.4862919256459 Test RE 0.1314182005243256\n",
      "36 Train Loss 806.1524 Test MSE 223.4699918880892 Test RE 0.130829309856771\n",
      "37 Train Loss 777.5097 Test MSE 222.96502709965105 Test RE 0.13068141179450685\n",
      "38 Train Loss 758.5643 Test MSE 224.62045163048978 Test RE 0.13116564278555065\n",
      "39 Train Loss 732.19366 Test MSE 230.56052110744722 Test RE 0.13288865821482054\n",
      "40 Train Loss 710.6764 Test MSE 238.14403506441747 Test RE 0.13505643999385553\n",
      "41 Train Loss 689.6494 Test MSE 260.33900084179925 Test RE 0.14120987243119076\n",
      "42 Train Loss 669.7865 Test MSE 272.71975790766055 Test RE 0.14452858313920516\n",
      "43 Train Loss 655.4096 Test MSE 282.4046275637169 Test RE 0.14707245715208433\n",
      "44 Train Loss 637.9765 Test MSE 295.31646983951276 Test RE 0.15039703621982542\n",
      "45 Train Loss 623.69763 Test MSE 293.6853883637835 Test RE 0.14998112734722788\n",
      "46 Train Loss 610.5933 Test MSE 294.22299501059246 Test RE 0.15011833877844968\n",
      "47 Train Loss 601.2653 Test MSE 298.6168198225577 Test RE 0.15123509269317661\n",
      "48 Train Loss 587.80334 Test MSE 302.0119451041909 Test RE 0.1520923968342055\n",
      "49 Train Loss 576.4325 Test MSE 300.905033183617 Test RE 0.15181342205886597\n",
      "50 Train Loss 566.033 Test MSE 301.55042846917496 Test RE 0.15197614314109853\n",
      "51 Train Loss 556.37616 Test MSE 304.7050892673045 Test RE 0.152769021807671\n",
      "52 Train Loss 543.9294 Test MSE 312.0161319887493 Test RE 0.15459091515855952\n",
      "53 Train Loss 530.0436 Test MSE 310.4378598575534 Test RE 0.15419943563177257\n",
      "54 Train Loss 523.5141 Test MSE 302.595098022157 Test RE 0.15223916313515923\n",
      "55 Train Loss 517.05145 Test MSE 300.18227792520753 Test RE 0.15163098922497578\n",
      "56 Train Loss 508.19763 Test MSE 302.83898376420717 Test RE 0.15230050167504608\n",
      "57 Train Loss 499.18024 Test MSE 308.5343129616527 Test RE 0.15372594763091282\n",
      "58 Train Loss 492.0955 Test MSE 312.76736240972207 Test RE 0.15477690486840087\n",
      "59 Train Loss 481.59225 Test MSE 321.75569982365647 Test RE 0.15698514857506027\n",
      "60 Train Loss 470.75873 Test MSE 329.87742742325554 Test RE 0.15895410326220755\n",
      "61 Train Loss 460.0901 Test MSE 329.01343736747623 Test RE 0.15874580649000367\n",
      "62 Train Loss 452.42026 Test MSE 330.6308459577443 Test RE 0.15913552013880988\n",
      "63 Train Loss 445.06903 Test MSE 330.5143684491011 Test RE 0.15910748684742637\n",
      "64 Train Loss 435.05487 Test MSE 320.2128054184611 Test RE 0.15660830587862773\n",
      "65 Train Loss 426.9629 Test MSE 307.8498767393066 Test RE 0.15355534420295028\n",
      "66 Train Loss 418.7715 Test MSE 306.8859473155505 Test RE 0.1533147519963083\n",
      "67 Train Loss 411.89456 Test MSE 290.90784775127395 Test RE 0.1492702164093596\n",
      "68 Train Loss 406.59912 Test MSE 289.4052245541639 Test RE 0.14888420537254474\n",
      "69 Train Loss 396.06638 Test MSE 285.51148560480635 Test RE 0.1478792487841821\n",
      "70 Train Loss 385.05722 Test MSE 284.1381309465744 Test RE 0.1475231589612033\n",
      "71 Train Loss 377.19873 Test MSE 278.9383662760174 Test RE 0.14616707973105783\n",
      "72 Train Loss 367.6129 Test MSE 286.6288243379467 Test RE 0.14816832624654447\n",
      "73 Train Loss 361.10327 Test MSE 288.3849801700423 Test RE 0.14862154186425316\n",
      "74 Train Loss 355.40192 Test MSE 294.3768959400897 Test RE 0.15015759528092085\n",
      "75 Train Loss 350.55322 Test MSE 291.481451575152 Test RE 0.14941730732217753\n",
      "76 Train Loss 343.90567 Test MSE 296.0560192870531 Test RE 0.1505852351668889\n",
      "77 Train Loss 336.90536 Test MSE 293.01845871372313 Test RE 0.1498107346123131\n",
      "78 Train Loss 331.30807 Test MSE 290.95814160132096 Test RE 0.1492831192057038\n",
      "79 Train Loss 324.98584 Test MSE 289.51947852390487 Test RE 0.14891359138821347\n",
      "80 Train Loss 320.29465 Test MSE 293.06490802626035 Test RE 0.1498226081473732\n",
      "81 Train Loss 314.8943 Test MSE 292.2667981808682 Test RE 0.149618461528995\n",
      "82 Train Loss 309.4543 Test MSE 297.74536356329503 Test RE 0.15101425606504737\n",
      "83 Train Loss 304.46408 Test MSE 302.9125700673114 Test RE 0.1523190041644157\n",
      "84 Train Loss 300.7183 Test MSE 307.62263922809876 Test RE 0.15349866076992597\n",
      "85 Train Loss 298.0034 Test MSE 307.3899522625947 Test RE 0.15344059629460916\n",
      "86 Train Loss 295.1005 Test MSE 305.9107975714075 Test RE 0.1530709744677833\n",
      "87 Train Loss 292.04944 Test MSE 307.92581908765027 Test RE 0.15357428303521103\n",
      "88 Train Loss 289.33725 Test MSE 304.21090984042775 Test RE 0.15264508895380463\n",
      "89 Train Loss 287.17804 Test MSE 307.75361643249596 Test RE 0.15353133503270572\n",
      "90 Train Loss 284.54233 Test MSE 307.352181371166 Test RE 0.15343116891074943\n",
      "91 Train Loss 281.95227 Test MSE 304.5195837647739 Test RE 0.15272251157569808\n",
      "92 Train Loss 279.8182 Test MSE 308.0064526466708 Test RE 0.15359438922586444\n",
      "93 Train Loss 275.92667 Test MSE 313.8573798693921 Test RE 0.15504637478949038\n",
      "94 Train Loss 272.87256 Test MSE 319.20896411565906 Test RE 0.1563626359886004\n",
      "95 Train Loss 270.20752 Test MSE 322.65178563873036 Test RE 0.157203597476605\n",
      "96 Train Loss 267.50748 Test MSE 329.54239011471566 Test RE 0.15887336253925521\n",
      "97 Train Loss 264.05664 Test MSE 329.4681535469589 Test RE 0.15885546669672163\n",
      "98 Train Loss 260.6469 Test MSE 326.0834105425014 Test RE 0.1580373708957425\n",
      "99 Train Loss 257.3984 Test MSE 333.58083756666616 Test RE 0.15984387200463213\n",
      "100 Train Loss 254.72374 Test MSE 331.15267195712875 Test RE 0.15926105033857915\n",
      "101 Train Loss 251.33665 Test MSE 334.1717035355622 Test RE 0.15998537371560476\n",
      "102 Train Loss 248.76678 Test MSE 334.5981513349342 Test RE 0.16008742253889022\n",
      "103 Train Loss 246.74315 Test MSE 336.7890612682931 Test RE 0.1606106843208211\n",
      "104 Train Loss 243.77646 Test MSE 340.0321195127159 Test RE 0.16138211955421577\n",
      "105 Train Loss 240.97293 Test MSE 340.1200431399054 Test RE 0.1614029828541385\n",
      "106 Train Loss 237.64096 Test MSE 339.19215693352265 Test RE 0.16118266963372033\n",
      "107 Train Loss 234.88766 Test MSE 339.8057943766996 Test RE 0.16132840270246693\n",
      "108 Train Loss 232.2199 Test MSE 339.4965505872262 Test RE 0.16125497669961603\n",
      "109 Train Loss 230.08678 Test MSE 343.4072160620001 Test RE 0.16218106658173803\n",
      "110 Train Loss 227.29236 Test MSE 342.3506964239518 Test RE 0.16193139291748718\n",
      "111 Train Loss 225.0109 Test MSE 338.2782857466624 Test RE 0.1609653893330805\n",
      "112 Train Loss 223.04167 Test MSE 341.7904233418539 Test RE 0.16179883447490845\n",
      "113 Train Loss 220.8402 Test MSE 340.9809492566006 Test RE 0.1616071240100972\n",
      "114 Train Loss 219.08557 Test MSE 342.8280083279836 Test RE 0.1620442375248244\n",
      "115 Train Loss 217.02039 Test MSE 343.9741760306905 Test RE 0.16231489057749782\n",
      "116 Train Loss 214.32707 Test MSE 343.4950388415314 Test RE 0.16220180330605283\n",
      "117 Train Loss 211.49533 Test MSE 347.2075555330225 Test RE 0.16307599139669787\n",
      "118 Train Loss 208.73909 Test MSE 349.3481915304125 Test RE 0.16357792450471778\n",
      "119 Train Loss 206.9911 Test MSE 349.89318169565587 Test RE 0.1637054671985881\n",
      "120 Train Loss 204.62299 Test MSE 345.7379456631611 Test RE 0.1627305032033932\n",
      "121 Train Loss 201.8635 Test MSE 342.6861796335951 Test RE 0.16201071503022846\n",
      "122 Train Loss 199.29404 Test MSE 339.9192474141669 Test RE 0.16135533230479288\n",
      "123 Train Loss 197.42027 Test MSE 337.41266636878555 Test RE 0.16075931046826678\n",
      "124 Train Loss 195.0064 Test MSE 335.91983266115363 Test RE 0.16040328804060572\n",
      "125 Train Loss 192.70883 Test MSE 331.16111429417236 Test RE 0.15926308040961515\n",
      "126 Train Loss 191.13577 Test MSE 329.43408390758526 Test RE 0.1588472530221575\n",
      "127 Train Loss 189.40747 Test MSE 332.79100799336595 Test RE 0.15965452624080168\n",
      "128 Train Loss 188.0995 Test MSE 335.160195072985 Test RE 0.16022182014419803\n",
      "129 Train Loss 186.68013 Test MSE 332.29394874895627 Test RE 0.15953525106394167\n",
      "130 Train Loss 185.50092 Test MSE 332.04782312484906 Test RE 0.15947615732032636\n",
      "131 Train Loss 184.12636 Test MSE 327.3102163197917 Test RE 0.15833437948080337\n",
      "132 Train Loss 181.98701 Test MSE 331.9141566679531 Test RE 0.15944405538031056\n",
      "133 Train Loss 180.71655 Test MSE 335.33918772123775 Test RE 0.1602645977478481\n",
      "134 Train Loss 179.02336 Test MSE 340.8944922765026 Test RE 0.1615866346693963\n",
      "135 Train Loss 177.90236 Test MSE 340.69752140533365 Test RE 0.1615399450633277\n",
      "136 Train Loss 176.7328 Test MSE 341.6718391741916 Test RE 0.16177076399179022\n",
      "137 Train Loss 175.1043 Test MSE 342.2599557527184 Test RE 0.1619099313910148\n",
      "138 Train Loss 173.52266 Test MSE 341.4986507101597 Test RE 0.16172975917641053\n",
      "139 Train Loss 172.76459 Test MSE 342.59616506200524 Test RE 0.16198943567354268\n",
      "140 Train Loss 171.7735 Test MSE 342.766437248545 Test RE 0.16202968549524832\n",
      "141 Train Loss 170.91185 Test MSE 342.5978586403337 Test RE 0.1619898360596045\n",
      "142 Train Loss 170.1381 Test MSE 342.2687961970526 Test RE 0.16191202241345087\n",
      "143 Train Loss 169.50423 Test MSE 345.22617732322107 Test RE 0.16261002010099895\n",
      "144 Train Loss 168.46147 Test MSE 347.0188357834484 Test RE 0.16303166655004223\n",
      "145 Train Loss 167.61295 Test MSE 349.7974191492261 Test RE 0.16368306332503044\n",
      "146 Train Loss 166.77655 Test MSE 352.16405779432466 Test RE 0.16423584848489478\n",
      "147 Train Loss 165.89168 Test MSE 350.83738845291873 Test RE 0.16392620266395455\n",
      "148 Train Loss 164.69644 Test MSE 349.89094469222294 Test RE 0.16370494388133505\n",
      "149 Train Loss 163.70592 Test MSE 350.6649984689987 Test RE 0.16388592373610933\n",
      "150 Train Loss 162.40085 Test MSE 344.61654750690576 Test RE 0.16246638134214902\n",
      "151 Train Loss 160.69846 Test MSE 337.7271832581249 Test RE 0.16083421835724826\n",
      "152 Train Loss 159.69221 Test MSE 336.7168016131878 Test RE 0.16059345351292148\n",
      "153 Train Loss 158.45647 Test MSE 330.97539170578597 Test RE 0.1592184149906975\n",
      "154 Train Loss 157.05838 Test MSE 334.6245918773617 Test RE 0.16009374761070147\n",
      "155 Train Loss 156.16725 Test MSE 334.07581350704095 Test RE 0.15996241829693364\n",
      "156 Train Loss 154.61632 Test MSE 333.1817065076222 Test RE 0.15974821640634573\n",
      "157 Train Loss 153.61949 Test MSE 337.62004100728245 Test RE 0.1608087044099842\n",
      "158 Train Loss 152.4724 Test MSE 335.8688242777934 Test RE 0.16039110920768765\n",
      "159 Train Loss 151.35812 Test MSE 341.23297669146734 Test RE 0.1616668368851973\n",
      "160 Train Loss 149.88756 Test MSE 349.26284320539446 Test RE 0.16355794164090315\n",
      "161 Train Loss 148.5132 Test MSE 349.4312647383576 Test RE 0.16359737234432\n",
      "162 Train Loss 147.52867 Test MSE 351.1712184846755 Test RE 0.1640041739403341\n",
      "163 Train Loss 146.52779 Test MSE 355.33000170588485 Test RE 0.16497243429944802\n",
      "164 Train Loss 145.38933 Test MSE 360.3011119746023 Test RE 0.16612241845094605\n",
      "165 Train Loss 144.19594 Test MSE 359.62627174369567 Test RE 0.16596677275613497\n",
      "166 Train Loss 143.4794 Test MSE 358.499197319595 Test RE 0.1657064974238405\n",
      "167 Train Loss 142.63747 Test MSE 359.6874996284671 Test RE 0.1659809004252617\n",
      "168 Train Loss 142.10057 Test MSE 358.1769229519621 Test RE 0.16563199939961867\n",
      "169 Train Loss 141.39137 Test MSE 353.901464318253 Test RE 0.16464048001394493\n",
      "170 Train Loss 140.8176 Test MSE 353.7113320260473 Test RE 0.16459624781860144\n",
      "171 Train Loss 140.05626 Test MSE 354.3697190935603 Test RE 0.16474936372101417\n",
      "172 Train Loss 139.58304 Test MSE 353.4719210066535 Test RE 0.16454053455438897\n",
      "173 Train Loss 138.90881 Test MSE 350.9589897793678 Test RE 0.1639546088677591\n",
      "174 Train Loss 137.45865 Test MSE 347.91034160937164 Test RE 0.16324094978885664\n",
      "175 Train Loss 136.44229 Test MSE 346.52437951559085 Test RE 0.16291547578900217\n",
      "176 Train Loss 135.5751 Test MSE 348.12019084813846 Test RE 0.16329017342632812\n",
      "177 Train Loss 134.13434 Test MSE 352.04170103254216 Test RE 0.1642073147490637\n",
      "178 Train Loss 133.14583 Test MSE 353.8676756906573 Test RE 0.16463262032810802\n",
      "179 Train Loss 132.40948 Test MSE 354.8065803307394 Test RE 0.16485088261354144\n",
      "180 Train Loss 131.90523 Test MSE 353.7507069711596 Test RE 0.16460540894418937\n",
      "181 Train Loss 131.33107 Test MSE 351.7559829257168 Test RE 0.164140665647097\n",
      "182 Train Loss 130.58559 Test MSE 351.8292497380903 Test RE 0.1641577590837098\n",
      "183 Train Loss 130.03802 Test MSE 355.2357387942722 Test RE 0.16495055067944187\n",
      "184 Train Loss 129.32327 Test MSE 356.46302948292407 Test RE 0.16523524574482942\n",
      "185 Train Loss 128.51717 Test MSE 354.9912012201885 Test RE 0.16489376648344026\n",
      "186 Train Loss 128.0835 Test MSE 351.9521795061315 Test RE 0.16418643508612873\n",
      "187 Train Loss 127.444855 Test MSE 350.7032586159084 Test RE 0.16389486407592266\n",
      "188 Train Loss 126.7598 Test MSE 349.45719841398426 Test RE 0.16360344306966604\n",
      "189 Train Loss 126.022026 Test MSE 348.18245193407853 Test RE 0.16330477494795076\n",
      "190 Train Loss 125.06667 Test MSE 347.02315752068495 Test RE 0.16303268173676658\n",
      "191 Train Loss 123.96736 Test MSE 346.52775251370446 Test RE 0.16291626868019218\n",
      "192 Train Loss 123.2308 Test MSE 346.6509499430921 Test RE 0.1629452260750638\n",
      "193 Train Loss 122.69363 Test MSE 344.7543821163726 Test RE 0.16249886853941148\n",
      "194 Train Loss 122.22053 Test MSE 346.6660439803059 Test RE 0.16294877355493367\n",
      "195 Train Loss 121.56937 Test MSE 348.1997911189899 Test RE 0.16330884111559132\n",
      "196 Train Loss 120.96175 Test MSE 347.38149474070184 Test RE 0.16311683405213306\n",
      "197 Train Loss 120.32021 Test MSE 350.4909731996708 Test RE 0.1638452526804342\n",
      "198 Train Loss 119.58104 Test MSE 352.2882897632747 Test RE 0.16426481445017022\n",
      "199 Train Loss 119.26812 Test MSE 350.15359409005123 Test RE 0.16376637579257805\n",
      "Training time: 170.72\n",
      "Training time: 170.72\n",
      "ES_stan_medium\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 9545.483 Test MSE 2803.3746324481294 Test RE 0.4633786429820119\n",
      "1 Train Loss 7359.261 Test MSE 2036.8768405555502 Test RE 0.39498247693234434\n",
      "2 Train Loss 6416.7334 Test MSE 1857.3240115693982 Test RE 0.37717185741029646\n",
      "3 Train Loss 5621.8447 Test MSE 1761.7514293428123 Test RE 0.3673396095835693\n",
      "4 Train Loss 5053.6743 Test MSE 1457.6673709068436 Test RE 0.33413707001371246\n",
      "5 Train Loss 4500.885 Test MSE 1211.2394889376378 Test RE 0.3045863625429733\n",
      "6 Train Loss 3708.303 Test MSE 984.1840185966752 Test RE 0.2745576810719066\n",
      "7 Train Loss 3311.6228 Test MSE 898.623802409038 Test RE 0.26235201414440407\n",
      "8 Train Loss 2932.1316 Test MSE 869.8823746328326 Test RE 0.2581224089204741\n",
      "9 Train Loss 2625.8057 Test MSE 824.388322891675 Test RE 0.25128198817485065\n",
      "10 Train Loss 2417.3464 Test MSE 797.0584173336863 Test RE 0.2470816654743078\n",
      "11 Train Loss 2232.799 Test MSE 746.010367094394 Test RE 0.23903851118701283\n",
      "12 Train Loss 2110.0793 Test MSE 649.8568298605702 Test RE 0.22310242335110678\n",
      "13 Train Loss 1987.3256 Test MSE 581.1337446631239 Test RE 0.21097621225101376\n",
      "14 Train Loss 1899.3358 Test MSE 553.5765518887599 Test RE 0.2059132469901519\n",
      "15 Train Loss 1821.5945 Test MSE 506.8661442812554 Test RE 0.1970344136125761\n",
      "16 Train Loss 1739.0905 Test MSE 476.1967061922963 Test RE 0.19098032909311652\n",
      "17 Train Loss 1668.9938 Test MSE 430.77928888616174 Test RE 0.18164475048957673\n",
      "18 Train Loss 1610.8744 Test MSE 377.56440509534076 Test RE 0.17005561097622418\n",
      "19 Train Loss 1553.4667 Test MSE 388.48004804710604 Test RE 0.17249630762528761\n",
      "20 Train Loss 1509.5787 Test MSE 388.9124905732274 Test RE 0.17259228937759058\n",
      "21 Train Loss 1470.0729 Test MSE 354.76645523855785 Test RE 0.16484156085310728\n",
      "22 Train Loss 1429.5656 Test MSE 345.7116428558962 Test RE 0.16272431303750812\n",
      "23 Train Loss 1390.5548 Test MSE 332.8430343369057 Test RE 0.1596670054190548\n",
      "24 Train Loss 1364.096 Test MSE 314.8570576172789 Test RE 0.15529310020649564\n",
      "25 Train Loss 1332.5891 Test MSE 313.8667030877949 Test RE 0.15504867761964924\n",
      "26 Train Loss 1301.3618 Test MSE 312.33901951336463 Test RE 0.15467088309262064\n",
      "27 Train Loss 1274.2849 Test MSE 312.1160281963665 Test RE 0.15461566036993776\n",
      "28 Train Loss 1250.0295 Test MSE 308.08908664134555 Test RE 0.15361499150004618\n",
      "29 Train Loss 1228.7139 Test MSE 315.60486433503183 Test RE 0.15547740662242132\n",
      "30 Train Loss 1207.139 Test MSE 299.4157160415464 Test RE 0.15143725887196727\n",
      "31 Train Loss 1190.16 Test MSE 290.61028563360804 Test RE 0.14919385455431333\n",
      "32 Train Loss 1169.312 Test MSE 283.71835388927315 Test RE 0.1474141455625703\n",
      "33 Train Loss 1151.2158 Test MSE 279.76607418656647 Test RE 0.14638378357223197\n",
      "34 Train Loss 1133.4393 Test MSE 281.8381444067625 Test RE 0.14692487477534227\n",
      "35 Train Loss 1121.1764 Test MSE 287.7598436001601 Test RE 0.1484603698687117\n",
      "36 Train Loss 1105.9244 Test MSE 274.3331139246281 Test RE 0.14495545397078188\n",
      "37 Train Loss 1094.5032 Test MSE 269.018711549787 Test RE 0.14354454333364447\n",
      "38 Train Loss 1076.9479 Test MSE 270.71862770147607 Test RE 0.1439973546302251\n",
      "39 Train Loss 1065.7405 Test MSE 268.05454297150476 Test RE 0.14328707914403405\n",
      "40 Train Loss 1048.4489 Test MSE 272.44785921266515 Test RE 0.14445651844349308\n",
      "41 Train Loss 1031.078 Test MSE 267.26393018701117 Test RE 0.14307561424864032\n",
      "42 Train Loss 1006.90625 Test MSE 269.2868089874431 Test RE 0.14361605200450403\n",
      "43 Train Loss 986.6474 Test MSE 265.06660144494464 Test RE 0.14248624737814938\n",
      "44 Train Loss 974.30927 Test MSE 258.5516718369093 Test RE 0.140724307110988\n",
      "45 Train Loss 961.4257 Test MSE 256.3895308532489 Test RE 0.14013466758209578\n",
      "46 Train Loss 946.2307 Test MSE 253.41673651502555 Test RE 0.1393198796799914\n",
      "47 Train Loss 932.32684 Test MSE 249.18037866426062 Test RE 0.13815046929665223\n",
      "48 Train Loss 920.3341 Test MSE 247.9098198393847 Test RE 0.137797807855688\n",
      "49 Train Loss 899.99133 Test MSE 246.97694661228334 Test RE 0.13753830009639256\n",
      "50 Train Loss 888.6589 Test MSE 243.3314459548601 Test RE 0.13651946014976393\n",
      "51 Train Loss 876.0785 Test MSE 244.1535337516909 Test RE 0.13674987909405345\n",
      "52 Train Loss 862.5812 Test MSE 239.6556463128513 Test RE 0.13548439506443818\n",
      "53 Train Loss 852.3856 Test MSE 232.77353797114017 Test RE 0.13352489576162985\n",
      "54 Train Loss 838.74225 Test MSE 235.79304896434593 Test RE 0.13438813993837725\n",
      "55 Train Loss 828.5462 Test MSE 238.5079099490262 Test RE 0.13515958112110424\n",
      "56 Train Loss 814.5189 Test MSE 238.84272686408846 Test RE 0.13525441622190157\n",
      "57 Train Loss 802.5892 Test MSE 242.550243000136 Test RE 0.1363001396648986\n",
      "58 Train Loss 790.6684 Test MSE 242.1156551420895 Test RE 0.1361779774645622\n",
      "59 Train Loss 782.50446 Test MSE 242.3761268908813 Test RE 0.13625120895616627\n",
      "60 Train Loss 774.3785 Test MSE 245.45196941007396 Test RE 0.13711302246134272\n",
      "61 Train Loss 764.5412 Test MSE 249.65717572977914 Test RE 0.1382825789334976\n",
      "62 Train Loss 755.37976 Test MSE 249.8101757784319 Test RE 0.1383249450315913\n",
      "63 Train Loss 745.38855 Test MSE 251.98870390087157 Test RE 0.13892678329558958\n",
      "64 Train Loss 739.66077 Test MSE 253.27854406377008 Test RE 0.13928188775135392\n",
      "65 Train Loss 730.8967 Test MSE 252.917499304272 Test RE 0.13918258022912555\n",
      "66 Train Loss 723.89087 Test MSE 253.27489383211076 Test RE 0.13928088408760117\n",
      "67 Train Loss 714.38226 Test MSE 256.435076153607 Test RE 0.14014711386272619\n",
      "68 Train Loss 706.21826 Test MSE 263.43564168665955 Test RE 0.1420472107128052\n",
      "69 Train Loss 700.46 Test MSE 268.47517222327673 Test RE 0.14339945760002437\n",
      "70 Train Loss 692.4254 Test MSE 276.580416741318 Test RE 0.14554797153110863\n",
      "71 Train Loss 687.62384 Test MSE 280.8411007325704 Test RE 0.14666476040213103\n",
      "72 Train Loss 679.8962 Test MSE 278.8794983192498 Test RE 0.14615165515631054\n",
      "73 Train Loss 673.62024 Test MSE 273.38132666740125 Test RE 0.14470377704507112\n",
      "74 Train Loss 666.79926 Test MSE 273.17164789375266 Test RE 0.1446482737469289\n",
      "75 Train Loss 658.90765 Test MSE 274.1146699902339 Test RE 0.14489773045145155\n",
      "76 Train Loss 652.3882 Test MSE 275.50862127721683 Test RE 0.14526568639983078\n",
      "77 Train Loss 647.0836 Test MSE 274.9773867071661 Test RE 0.14512556847938332\n",
      "78 Train Loss 642.0934 Test MSE 274.6009668960778 Test RE 0.1450262024045768\n",
      "79 Train Loss 637.95154 Test MSE 273.5219078503225 Test RE 0.14474097785513967\n",
      "80 Train Loss 633.93085 Test MSE 275.53358731406405 Test RE 0.1452722680928191\n",
      "81 Train Loss 629.0877 Test MSE 279.58748598283734 Test RE 0.1463370541924039\n",
      "82 Train Loss 624.1788 Test MSE 280.86490598872 Test RE 0.146670976227018\n",
      "83 Train Loss 621.1099 Test MSE 282.98775608027006 Test RE 0.14722422151632067\n",
      "84 Train Loss 617.00433 Test MSE 285.83586665049756 Test RE 0.1479632307088487\n",
      "85 Train Loss 613.22345 Test MSE 288.65484413416584 Test RE 0.14869106389668685\n",
      "86 Train Loss 606.6702 Test MSE 291.4309981356795 Test RE 0.1494043752071504\n",
      "87 Train Loss 601.5258 Test MSE 293.61980182677684 Test RE 0.149964379336599\n",
      "88 Train Loss 596.9087 Test MSE 296.11487123173123 Test RE 0.15060020158029155\n",
      "89 Train Loss 593.604 Test MSE 294.6084796240852 Test RE 0.1502166474911225\n",
      "90 Train Loss 588.1047 Test MSE 291.91807928550145 Test RE 0.14952917605872015\n",
      "91 Train Loss 583.8311 Test MSE 290.0588306194283 Test RE 0.14905223402251153\n",
      "92 Train Loss 580.9949 Test MSE 291.72893711516616 Test RE 0.14948072606948362\n",
      "93 Train Loss 578.6679 Test MSE 293.1170439292332 Test RE 0.14983593418774935\n",
      "94 Train Loss 575.4326 Test MSE 295.7513155049465 Test RE 0.1505077233113402\n",
      "95 Train Loss 572.2711 Test MSE 292.9747656046673 Test RE 0.14979956476856743\n",
      "96 Train Loss 568.9834 Test MSE 295.4713538667086 Test RE 0.15043647026359766\n",
      "97 Train Loss 566.3595 Test MSE 295.91999768934653 Test RE 0.15055063834032228\n",
      "98 Train Loss 562.07416 Test MSE 295.8638272840378 Test RE 0.1505363491882307\n",
      "99 Train Loss 559.6362 Test MSE 296.0076505882364 Test RE 0.15057293359399532\n",
      "100 Train Loss 555.9304 Test MSE 295.30450694943147 Test RE 0.15039398999379744\n",
      "101 Train Loss 552.8661 Test MSE 295.24015520763174 Test RE 0.15037760243062553\n",
      "102 Train Loss 550.37225 Test MSE 296.24086931126965 Test RE 0.1506322386711796\n",
      "103 Train Loss 547.0353 Test MSE 296.658710330285 Test RE 0.15073843291424882\n",
      "104 Train Loss 544.3319 Test MSE 295.60391218848997 Test RE 0.15047021189312584\n",
      "105 Train Loss 541.0744 Test MSE 296.3277295366992 Test RE 0.15065432034967802\n",
      "106 Train Loss 538.13403 Test MSE 297.32166314812815 Test RE 0.1509067689466923\n",
      "107 Train Loss 534.92224 Test MSE 298.1336382891165 Test RE 0.15111268902614827\n",
      "108 Train Loss 531.00836 Test MSE 299.7595803805028 Test RE 0.15152419307019052\n",
      "109 Train Loss 527.0163 Test MSE 301.8911375051584 Test RE 0.1520619746019184\n",
      "110 Train Loss 524.7594 Test MSE 303.1576300111108 Test RE 0.15238060566891454\n",
      "111 Train Loss 522.62933 Test MSE 303.96513860823626 Test RE 0.15258341570233136\n",
      "112 Train Loss 519.6754 Test MSE 305.86583309708146 Test RE 0.15305972444180965\n",
      "113 Train Loss 516.10645 Test MSE 310.12639325776524 Test RE 0.15412206100556342\n",
      "114 Train Loss 513.6394 Test MSE 311.6257404426602 Test RE 0.154494173564694\n",
      "115 Train Loss 510.9709 Test MSE 313.326808164481 Test RE 0.15491526744531742\n",
      "116 Train Loss 508.84888 Test MSE 313.0007667983731 Test RE 0.15483464566232377\n",
      "117 Train Loss 505.86777 Test MSE 310.4739722934665 Test RE 0.15420840418236473\n",
      "118 Train Loss 503.11316 Test MSE 310.3931526943161 Test RE 0.15418833185151648\n",
      "119 Train Loss 499.92374 Test MSE 309.8753757253124 Test RE 0.15405967487545216\n",
      "120 Train Loss 497.5758 Test MSE 310.9944781351268 Test RE 0.15433761430807488\n",
      "121 Train Loss 495.28534 Test MSE 310.46572817734665 Test RE 0.15420635679587874\n",
      "122 Train Loss 492.11465 Test MSE 308.4936695394378 Test RE 0.15371582208878096\n",
      "123 Train Loss 488.59717 Test MSE 307.6446500453709 Test RE 0.1535041521904338\n",
      "124 Train Loss 485.38718 Test MSE 305.53611925382717 Test RE 0.15297720538927756\n",
      "125 Train Loss 482.1813 Test MSE 305.68872553073584 Test RE 0.15301540442034672\n",
      "126 Train Loss 479.7382 Test MSE 305.08738998438423 Test RE 0.1528648282135254\n",
      "127 Train Loss 478.10822 Test MSE 305.9768130441685 Test RE 0.1530874899156157\n",
      "128 Train Loss 475.9147 Test MSE 307.5909869912425 Test RE 0.15349076359277922\n",
      "129 Train Loss 473.37088 Test MSE 308.45021265145215 Test RE 0.15370499488681816\n",
      "130 Train Loss 472.05945 Test MSE 309.94169903121605 Test RE 0.15407616085832038\n",
      "131 Train Loss 470.34137 Test MSE 311.11783883998226 Test RE 0.15436822145796436\n",
      "132 Train Loss 467.96384 Test MSE 311.01131458542386 Test RE 0.1543417919745399\n",
      "133 Train Loss 465.88303 Test MSE 311.7235878598182 Test RE 0.15451842648644887\n",
      "134 Train Loss 464.0706 Test MSE 311.7148823840855 Test RE 0.15451626886050063\n",
      "135 Train Loss 462.19562 Test MSE 312.1100629237884 Test RE 0.15461418282812217\n",
      "136 Train Loss 459.33188 Test MSE 311.49679154095554 Test RE 0.15446220586638115\n",
      "137 Train Loss 457.2006 Test MSE 311.45338077532426 Test RE 0.15445144242285191\n",
      "138 Train Loss 454.93057 Test MSE 310.1471360424017 Test RE 0.15412721514164143\n",
      "139 Train Loss 452.60068 Test MSE 312.0788074394642 Test RE 0.15460644090831\n",
      "140 Train Loss 450.0818 Test MSE 311.024065292625 Test RE 0.15434495576109047\n",
      "141 Train Loss 447.1803 Test MSE 310.41621105686454 Test RE 0.15419405888576146\n",
      "142 Train Loss 443.68445 Test MSE 308.36839835292903 Test RE 0.15368460893828145\n",
      "143 Train Loss 442.07806 Test MSE 307.9880013201429 Test RE 0.15358978857115602\n",
      "144 Train Loss 439.6391 Test MSE 309.0646657636583 Test RE 0.15385801394867618\n",
      "145 Train Loss 437.3529 Test MSE 310.0413573508897 Test RE 0.15410092960875646\n",
      "146 Train Loss 435.60483 Test MSE 311.4329454943927 Test RE 0.1544463753556994\n",
      "147 Train Loss 433.4605 Test MSE 312.49981092212187 Test RE 0.15471069007755336\n",
      "148 Train Loss 431.0996 Test MSE 314.33615582007326 Test RE 0.15516458801659247\n",
      "149 Train Loss 427.79294 Test MSE 315.8283460943921 Test RE 0.15553244413838901\n",
      "150 Train Loss 425.70526 Test MSE 315.8474040592163 Test RE 0.15553713669871713\n",
      "151 Train Loss 423.43982 Test MSE 315.87156386104135 Test RE 0.15554308525883537\n",
      "152 Train Loss 420.96964 Test MSE 315.9856082299899 Test RE 0.1555711618795037\n",
      "153 Train Loss 418.37457 Test MSE 315.63637358923495 Test RE 0.1554851676796993\n",
      "154 Train Loss 416.1045 Test MSE 316.19371904621426 Test RE 0.15562238369405326\n",
      "155 Train Loss 412.6531 Test MSE 316.22714787178415 Test RE 0.1556306098790053\n",
      "156 Train Loss 409.66315 Test MSE 316.2093714992342 Test RE 0.15562623551298269\n",
      "157 Train Loss 406.21027 Test MSE 316.13168524115406 Test RE 0.15560711722673704\n",
      "158 Train Loss 404.07092 Test MSE 315.4968196498824 Test RE 0.15545079114956573\n",
      "159 Train Loss 401.56955 Test MSE 314.7463726112398 Test RE 0.1552658019004744\n",
      "160 Train Loss 399.3968 Test MSE 315.70932926279573 Test RE 0.1555031359347559\n",
      "161 Train Loss 395.52173 Test MSE 315.73407953841894 Test RE 0.15550923120925295\n",
      "162 Train Loss 392.55853 Test MSE 315.70883735468163 Test RE 0.15550301478964387\n",
      "163 Train Loss 389.04535 Test MSE 314.6904060374963 Test RE 0.1552519970054357\n",
      "164 Train Loss 386.1332 Test MSE 314.1704419121433 Test RE 0.1551236822544807\n",
      "165 Train Loss 383.44116 Test MSE 314.64760417981404 Test RE 0.15524143853316896\n",
      "166 Train Loss 379.7184 Test MSE 313.14056792959303 Test RE 0.15486922008579102\n",
      "167 Train Loss 377.00476 Test MSE 312.14128614471224 Test RE 0.15462191636931782\n",
      "168 Train Loss 374.20172 Test MSE 311.45669745905025 Test RE 0.15445226480150215\n",
      "169 Train Loss 371.76367 Test MSE 309.79426202624427 Test RE 0.15403951004636143\n",
      "170 Train Loss 368.77014 Test MSE 308.47812651248097 Test RE 0.15371194966043417\n",
      "171 Train Loss 366.52252 Test MSE 307.73114654053774 Test RE 0.15352573006935386\n",
      "172 Train Loss 364.68317 Test MSE 306.5304535682348 Test RE 0.15322592709391958\n",
      "173 Train Loss 363.3159 Test MSE 306.0395553670212 Test RE 0.15310318485059843\n",
      "174 Train Loss 361.56625 Test MSE 305.93546599114745 Test RE 0.15307714610827025\n",
      "175 Train Loss 358.9591 Test MSE 305.1071480260469 Test RE 0.15286977804240243\n",
      "176 Train Loss 356.88058 Test MSE 306.20315807779053 Test RE 0.15314410235451204\n",
      "177 Train Loss 354.98425 Test MSE 306.23166826872114 Test RE 0.15315123171594333\n",
      "178 Train Loss 353.03067 Test MSE 305.95650556532706 Test RE 0.1530824096735612\n",
      "179 Train Loss 350.82605 Test MSE 305.14240772659525 Test RE 0.15287861098340788\n",
      "180 Train Loss 349.39404 Test MSE 305.1283837458638 Test RE 0.15287509788382803\n",
      "181 Train Loss 347.8433 Test MSE 305.1811633870615 Test RE 0.15288831911192974\n",
      "182 Train Loss 345.88223 Test MSE 303.3133774617148 Test RE 0.15241974346570836\n",
      "183 Train Loss 344.4874 Test MSE 302.6091635791233 Test RE 0.15224270136785406\n",
      "184 Train Loss 342.60605 Test MSE 301.20217999341713 Test RE 0.15188836221889776\n",
      "185 Train Loss 341.2308 Test MSE 300.41004652508366 Test RE 0.1516885046610787\n",
      "186 Train Loss 339.66556 Test MSE 299.8466547457939 Test RE 0.15154619889719187\n",
      "187 Train Loss 337.9261 Test MSE 299.17121743533653 Test RE 0.15137541549068734\n",
      "188 Train Loss 335.98816 Test MSE 296.95592010517976 Test RE 0.15081392323146015\n",
      "189 Train Loss 333.717 Test MSE 296.2781721344322 Test RE 0.15064172222234992\n",
      "190 Train Loss 332.56805 Test MSE 295.5682009497574 Test RE 0.15046112263595315\n",
      "191 Train Loss 330.6913 Test MSE 295.6092951741218 Test RE 0.15047158192793902\n",
      "192 Train Loss 328.50742 Test MSE 296.61139716921383 Test RE 0.15072641203656692\n",
      "193 Train Loss 327.31042 Test MSE 297.08017071477934 Test RE 0.1508454712843893\n",
      "194 Train Loss 325.42395 Test MSE 297.8748496964792 Test RE 0.15104708970235453\n",
      "195 Train Loss 323.56058 Test MSE 299.2227510008802 Test RE 0.1513884524717448\n",
      "196 Train Loss 321.2465 Test MSE 300.37776895884076 Test RE 0.15168035535426028\n",
      "197 Train Loss 319.4481 Test MSE 300.24566341864585 Test RE 0.15164699732816067\n",
      "198 Train Loss 317.84 Test MSE 302.12705503717217 Test RE 0.15212137859801453\n",
      "199 Train Loss 315.8201 Test MSE 303.70484070040743 Test RE 0.15251806996801615\n",
      "Training time: 172.55\n",
      "Training time: 172.55\n",
      "ES_stan_medium\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 9233.678 Test MSE 2672.7830056645475 Test RE 0.4524569821284546\n",
      "1 Train Loss 5887.871 Test MSE 1531.581550290674 Test RE 0.3425038888221992\n",
      "2 Train Loss 4229.923 Test MSE 1006.5670387723043 Test RE 0.2776622229153032\n",
      "3 Train Loss 3578.089 Test MSE 850.714170763456 Test RE 0.255262652258924\n",
      "4 Train Loss 3094.2148 Test MSE 661.535341111705 Test RE 0.22509817244884056\n",
      "5 Train Loss 2801.7427 Test MSE 593.3887327976315 Test RE 0.2131891468816623\n",
      "6 Train Loss 2505.561 Test MSE 522.2472236724358 Test RE 0.20000162015351725\n",
      "7 Train Loss 2378.116 Test MSE 496.7033134215402 Test RE 0.19504910972447112\n",
      "8 Train Loss 2298.0366 Test MSE 475.30079750773905 Test RE 0.1908005908824093\n",
      "9 Train Loss 2189.1304 Test MSE 433.27495696069036 Test RE 0.18217015913898515\n",
      "10 Train Loss 2116.0452 Test MSE 382.226576213855 Test RE 0.1711023143706699\n",
      "11 Train Loss 2073.2363 Test MSE 401.48207832159835 Test RE 0.17535918793563215\n",
      "12 Train Loss 2020.627 Test MSE 371.77006048233875 Test RE 0.16874567473171428\n",
      "13 Train Loss 1979.4214 Test MSE 355.2161334011343 Test RE 0.16494599882181285\n",
      "14 Train Loss 1939.6996 Test MSE 357.4699883237844 Test RE 0.16546846453572547\n",
      "15 Train Loss 1894.03 Test MSE 338.02182922739684 Test RE 0.16090436198943464\n",
      "16 Train Loss 1859.9269 Test MSE 331.6216992337011 Test RE 0.15937379493078493\n",
      "17 Train Loss 1814.4534 Test MSE 323.28001982068344 Test RE 0.15735656834250167\n",
      "18 Train Loss 1769.6304 Test MSE 313.76202277291503 Test RE 0.1550228196714228\n",
      "19 Train Loss 1726.8333 Test MSE 311.91929578495433 Test RE 0.1545669241569427\n",
      "20 Train Loss 1692.032 Test MSE 306.3218137180484 Test RE 0.15317377162962292\n",
      "21 Train Loss 1659.2626 Test MSE 288.4961983991412 Test RE 0.14865019771052798\n",
      "22 Train Loss 1621.5139 Test MSE 271.1753630879997 Test RE 0.1441187740009706\n",
      "23 Train Loss 1584.5209 Test MSE 254.29051075594452 Test RE 0.13955985863775133\n",
      "24 Train Loss 1537.46 Test MSE 248.82406154155942 Test RE 0.1380516593740674\n",
      "25 Train Loss 1505.5973 Test MSE 236.90834188053174 Test RE 0.13470559061121104\n",
      "26 Train Loss 1469.8722 Test MSE 226.00045456544632 Test RE 0.13156794766205976\n",
      "27 Train Loss 1406.6401 Test MSE 220.0530835386519 Test RE 0.12982525165499895\n",
      "28 Train Loss 1353.6345 Test MSE 210.64725859522616 Test RE 0.127020363010722\n",
      "29 Train Loss 1303.3351 Test MSE 202.0552746035479 Test RE 0.12440291013834272\n",
      "30 Train Loss 1242.403 Test MSE 211.0637222853859 Test RE 0.12714586487786303\n",
      "31 Train Loss 1186.9209 Test MSE 203.48483015365863 Test RE 0.12484221423378913\n",
      "32 Train Loss 1150.9144 Test MSE 203.9078891194697 Test RE 0.12497192462713239\n",
      "33 Train Loss 1109.206 Test MSE 209.87968296429887 Test RE 0.12678872763151547\n",
      "34 Train Loss 1053.5109 Test MSE 209.03869366451912 Test RE 0.12653445101205962\n",
      "35 Train Loss 999.57446 Test MSE 225.4334130772451 Test RE 0.13140279016277268\n",
      "36 Train Loss 941.4089 Test MSE 216.84384890818288 Test RE 0.12887509470925548\n",
      "37 Train Loss 892.1754 Test MSE 217.11975276305606 Test RE 0.1289570565274253\n",
      "38 Train Loss 853.7336 Test MSE 236.50933384041232 Test RE 0.13459210523622236\n",
      "39 Train Loss 822.5025 Test MSE 239.04303806928667 Test RE 0.1353111215219001\n",
      "40 Train Loss 776.89374 Test MSE 236.63888302891627 Test RE 0.13462896194432306\n",
      "41 Train Loss 748.83295 Test MSE 244.96375140267918 Test RE 0.13697659176308005\n",
      "42 Train Loss 721.3521 Test MSE 256.79502660152616 Test RE 0.14024543958161406\n",
      "43 Train Loss 702.8048 Test MSE 271.56325268072266 Test RE 0.14422181100913392\n",
      "44 Train Loss 690.28503 Test MSE 290.0401204940321 Test RE 0.14904742666850504\n",
      "45 Train Loss 670.4632 Test MSE 286.93920517956525 Test RE 0.14824852782215225\n",
      "46 Train Loss 650.7046 Test MSE 288.6441973189254 Test RE 0.14868832169277899\n",
      "47 Train Loss 636.7856 Test MSE 282.7378029180777 Test RE 0.1471591881564474\n",
      "48 Train Loss 619.81177 Test MSE 267.24651084562055 Test RE 0.14307095158507316\n",
      "49 Train Loss 605.0977 Test MSE 264.51594628771613 Test RE 0.1423381684086652\n",
      "50 Train Loss 590.66675 Test MSE 267.1188880383796 Test RE 0.1430367859437033\n",
      "51 Train Loss 581.82764 Test MSE 268.28833016782824 Test RE 0.14334955035462826\n",
      "52 Train Loss 571.3279 Test MSE 270.56036264618183 Test RE 0.1439552572665451\n",
      "53 Train Loss 561.4676 Test MSE 285.14287648038487 Test RE 0.14778375832552298\n",
      "54 Train Loss 552.1585 Test MSE 300.5802829125896 Test RE 0.15173147800045828\n",
      "55 Train Loss 539.81067 Test MSE 303.17764756322 Test RE 0.15238563644508735\n",
      "56 Train Loss 522.40436 Test MSE 300.3028655372143 Test RE 0.1516614423601045\n",
      "57 Train Loss 505.08783 Test MSE 284.4589197304184 Test RE 0.14760641147139159\n",
      "58 Train Loss 496.3897 Test MSE 292.00863055784936 Test RE 0.14955236579628234\n",
      "59 Train Loss 484.15027 Test MSE 298.97527853671266 Test RE 0.15132583653977358\n",
      "60 Train Loss 474.43567 Test MSE 290.98964156263514 Test RE 0.14929119989591358\n",
      "61 Train Loss 465.0174 Test MSE 278.60321201337985 Test RE 0.14607924089287982\n",
      "62 Train Loss 458.37723 Test MSE 281.5378188815951 Test RE 0.1468465726500792\n",
      "63 Train Loss 445.33664 Test MSE 286.6514444252693 Test RE 0.14817417268289595\n",
      "64 Train Loss 429.2099 Test MSE 300.3135061994233 Test RE 0.151664129254018\n",
      "65 Train Loss 422.29077 Test MSE 292.86522328957744 Test RE 0.14977155736453096\n",
      "66 Train Loss 415.0636 Test MSE 297.6179097882957 Test RE 0.15098193079725306\n",
      "67 Train Loss 408.3009 Test MSE 301.70574879339574 Test RE 0.15201527746625856\n",
      "68 Train Loss 402.59174 Test MSE 296.80232643657985 Test RE 0.15077491565790768\n",
      "69 Train Loss 397.15546 Test MSE 297.3874321482408 Test RE 0.15092345867995224\n",
      "70 Train Loss 388.79922 Test MSE 314.91228323449013 Test RE 0.15530671873762059\n",
      "71 Train Loss 383.62305 Test MSE 309.5815758277365 Test RE 0.15398662381365824\n",
      "72 Train Loss 378.6569 Test MSE 308.4965704993429 Test RE 0.1537165448303351\n",
      "73 Train Loss 373.9139 Test MSE 310.8243184645856 Test RE 0.15429538585591457\n",
      "74 Train Loss 368.15607 Test MSE 320.660299047017 Test RE 0.15671769680639172\n",
      "75 Train Loss 362.06906 Test MSE 324.875581220833 Test RE 0.1577444101812167\n",
      "76 Train Loss 357.26865 Test MSE 324.13974754314904 Test RE 0.15756566567626706\n",
      "77 Train Loss 352.6892 Test MSE 323.95188611105965 Test RE 0.1575199989507939\n",
      "78 Train Loss 348.67706 Test MSE 322.726152772493 Test RE 0.15722171314791905\n",
      "79 Train Loss 345.08432 Test MSE 324.45906192781405 Test RE 0.1576432565904682\n",
      "80 Train Loss 342.08493 Test MSE 313.8117058435567 Test RE 0.1550350928343133\n",
      "81 Train Loss 338.09192 Test MSE 312.11874315846205 Test RE 0.1546163328357942\n",
      "82 Train Loss 335.97992 Test MSE 309.6647117704007 Test RE 0.15400729843512082\n",
      "83 Train Loss 330.37076 Test MSE 309.0530542623267 Test RE 0.15385512371321436\n",
      "84 Train Loss 327.52505 Test MSE 304.81121108256787 Test RE 0.1527956224703261\n",
      "85 Train Loss 324.5048 Test MSE 311.6379492366158 Test RE 0.1544971999017609\n",
      "86 Train Loss 319.97754 Test MSE 315.3204971509485 Test RE 0.15540734649193377\n",
      "87 Train Loss 317.11188 Test MSE 307.410103121846 Test RE 0.1534456255891016\n",
      "88 Train Loss 314.65683 Test MSE 301.0868428551488 Test RE 0.1518592786870302\n",
      "89 Train Loss 311.78363 Test MSE 309.3984972141312 Test RE 0.1539410852076946\n",
      "90 Train Loss 307.6941 Test MSE 318.3059250923609 Test RE 0.15614130516156527\n",
      "91 Train Loss 303.73077 Test MSE 325.86861383245315 Test RE 0.15798531136394092\n",
      "92 Train Loss 300.04712 Test MSE 318.12629073479326 Test RE 0.15609724016122492\n",
      "93 Train Loss 296.3444 Test MSE 322.12465621626694 Test RE 0.15707513001017787\n",
      "94 Train Loss 292.827 Test MSE 319.58059336775125 Test RE 0.1564536297152044\n",
      "95 Train Loss 289.77017 Test MSE 311.20205029276 Test RE 0.15438911176225908\n",
      "96 Train Loss 286.47775 Test MSE 321.9982649779323 Test RE 0.1570443114006697\n",
      "97 Train Loss 283.91266 Test MSE 317.95831036558684 Test RE 0.1560560226713163\n",
      "98 Train Loss 280.9324 Test MSE 316.7077762371516 Test RE 0.1557488351574271\n",
      "99 Train Loss 277.16858 Test MSE 317.74847739893823 Test RE 0.15600452047518887\n",
      "100 Train Loss 274.9522 Test MSE 315.6542190807483 Test RE 0.15548956303817876\n",
      "101 Train Loss 272.53882 Test MSE 317.19153795120366 Test RE 0.1558677406266421\n",
      "102 Train Loss 270.61832 Test MSE 322.9571407191213 Test RE 0.15727796800120428\n",
      "103 Train Loss 268.46692 Test MSE 321.46962668133386 Test RE 0.15691534527177628\n",
      "104 Train Loss 266.3236 Test MSE 318.9894282650187 Test RE 0.15630885756706336\n",
      "105 Train Loss 263.8609 Test MSE 320.0144174272158 Test RE 0.15655978498811923\n",
      "106 Train Loss 261.29492 Test MSE 319.92390184449135 Test RE 0.15653764207578805\n",
      "107 Train Loss 258.77863 Test MSE 322.525070397734 Test RE 0.15717272511076322\n",
      "108 Train Loss 256.4963 Test MSE 323.40522917942684 Test RE 0.1573870382237747\n",
      "109 Train Loss 254.32289 Test MSE 321.4257944621811 Test RE 0.1569046472435667\n",
      "110 Train Loss 252.43614 Test MSE 321.14772081313464 Test RE 0.15683676145277092\n",
      "111 Train Loss 251.06328 Test MSE 321.84135622779337 Test RE 0.15700604313655558\n",
      "112 Train Loss 249.28615 Test MSE 320.3757967476675 Test RE 0.15664815835816157\n",
      "113 Train Loss 247.10779 Test MSE 319.49887435325013 Test RE 0.1564336252873322\n",
      "114 Train Loss 244.99182 Test MSE 323.1757737445432 Test RE 0.157331195405079\n",
      "115 Train Loss 243.23366 Test MSE 320.6785764329868 Test RE 0.15672216313575257\n",
      "116 Train Loss 241.75613 Test MSE 321.49106481263266 Test RE 0.1569205773612388\n",
      "117 Train Loss 239.93073 Test MSE 318.51162453926855 Test RE 0.15619174876048597\n",
      "118 Train Loss 237.71387 Test MSE 322.73909433677517 Test RE 0.15722486547147882\n",
      "119 Train Loss 235.90433 Test MSE 324.6170743971139 Test RE 0.15768163827115145\n",
      "120 Train Loss 234.41013 Test MSE 326.81000154493614 Test RE 0.1582133452521685\n",
      "121 Train Loss 232.9888 Test MSE 325.5042925645781 Test RE 0.15789697283914944\n",
      "122 Train Loss 231.1804 Test MSE 330.3437180127803 Test RE 0.15906640653400594\n",
      "123 Train Loss 229.55885 Test MSE 328.87033763965076 Test RE 0.15871128061702702\n",
      "124 Train Loss 228.39252 Test MSE 332.3489157757447 Test RE 0.15954844542971242\n",
      "125 Train Loss 226.45587 Test MSE 335.06015997180424 Test RE 0.16019790769523107\n",
      "126 Train Loss 224.45386 Test MSE 326.15942314855556 Test RE 0.15805578969908785\n",
      "127 Train Loss 222.249 Test MSE 331.459152466842 Test RE 0.15933473103600365\n",
      "128 Train Loss 220.46098 Test MSE 328.7448266474606 Test RE 0.15868099221918944\n",
      "129 Train Loss 218.37723 Test MSE 325.03748821224053 Test RE 0.1577837125219809\n",
      "130 Train Loss 216.00946 Test MSE 327.14149062506584 Test RE 0.15829356419215285\n",
      "131 Train Loss 215.03067 Test MSE 325.4446994084567 Test RE 0.15788251832981873\n",
      "132 Train Loss 214.12146 Test MSE 322.78595015978556 Test RE 0.15723627814696986\n",
      "133 Train Loss 213.2369 Test MSE 328.86517363465816 Test RE 0.1587100345498555\n",
      "134 Train Loss 211.99667 Test MSE 332.7251247984792 Test RE 0.15963872192118445\n",
      "135 Train Loss 210.47993 Test MSE 329.64908532704624 Test RE 0.1588990794968169\n",
      "136 Train Loss 209.36594 Test MSE 329.14146754718405 Test RE 0.15877669014853676\n",
      "137 Train Loss 207.82619 Test MSE 330.50811061537206 Test RE 0.15910598059988354\n",
      "138 Train Loss 206.40929 Test MSE 330.3672903271787 Test RE 0.1590720816781871\n",
      "139 Train Loss 204.60191 Test MSE 324.35531481908987 Test RE 0.15761805103820214\n",
      "140 Train Loss 202.66978 Test MSE 319.903030492222 Test RE 0.156532535852808\n",
      "141 Train Loss 201.65587 Test MSE 321.28672241836097 Test RE 0.1568706994223201\n",
      "142 Train Loss 199.89116 Test MSE 327.27688347973884 Test RE 0.15832631699350663\n",
      "143 Train Loss 198.92622 Test MSE 325.5171269554068 Test RE 0.15790008568824368\n",
      "144 Train Loss 197.65393 Test MSE 328.8106228423925 Test RE 0.15869687092251308\n",
      "145 Train Loss 196.54727 Test MSE 328.7651054104289 Test RE 0.15868588629560645\n",
      "146 Train Loss 195.40485 Test MSE 330.35807268141747 Test RE 0.15906986251126265\n",
      "147 Train Loss 194.10835 Test MSE 328.06508622630776 Test RE 0.15851685625487388\n",
      "148 Train Loss 192.42305 Test MSE 325.90619957219474 Test RE 0.1579944221271925\n",
      "149 Train Loss 190.50644 Test MSE 325.57662263922975 Test RE 0.15791451495106273\n",
      "150 Train Loss 189.06818 Test MSE 322.1276584435358 Test RE 0.15707586198478876\n",
      "151 Train Loss 187.46667 Test MSE 329.4669007818495 Test RE 0.15885516468152963\n",
      "152 Train Loss 186.30559 Test MSE 330.131714250948 Test RE 0.15901535654123933\n",
      "153 Train Loss 185.22998 Test MSE 329.7021044024411 Test RE 0.158911857241292\n",
      "154 Train Loss 183.89838 Test MSE 331.57709902989217 Test RE 0.1593630773831055\n",
      "155 Train Loss 182.4129 Test MSE 329.3765444367383 Test RE 0.1588333801618998\n",
      "156 Train Loss 181.09041 Test MSE 328.256311371023 Test RE 0.15856304829747916\n",
      "157 Train Loss 179.97054 Test MSE 322.5283124223466 Test RE 0.15717351505952923\n",
      "158 Train Loss 178.81592 Test MSE 325.2911288731898 Test RE 0.15784526320801875\n",
      "159 Train Loss 177.45943 Test MSE 326.1006681985435 Test RE 0.15804155282959106\n",
      "160 Train Loss 176.35963 Test MSE 325.9494452705529 Test RE 0.15800490421143262\n",
      "161 Train Loss 175.67763 Test MSE 327.2991639369058 Test RE 0.15833170619584006\n",
      "162 Train Loss 174.0469 Test MSE 329.30334591593737 Test RE 0.15881573012777905\n",
      "163 Train Loss 173.07089 Test MSE 328.72085778293985 Test RE 0.15867520737905583\n",
      "164 Train Loss 172.1139 Test MSE 329.7814360756557 Test RE 0.15893097447637292\n",
      "165 Train Loss 171.29675 Test MSE 330.4610105279444 Test RE 0.1590946432493611\n",
      "166 Train Loss 170.8472 Test MSE 332.38712080064107 Test RE 0.15955761557721804\n",
      "167 Train Loss 170.07985 Test MSE 328.69436031309084 Test RE 0.15866881201935187\n",
      "168 Train Loss 169.44643 Test MSE 325.8807946175322 Test RE 0.15798826403728675\n",
      "169 Train Loss 168.89555 Test MSE 325.83218677081663 Test RE 0.15797648096225797\n",
      "170 Train Loss 168.44362 Test MSE 328.16665502554 Test RE 0.15854139272865633\n",
      "171 Train Loss 167.9433 Test MSE 330.83744871218414 Test RE 0.15918523223111183\n",
      "172 Train Loss 167.05307 Test MSE 330.41683660217217 Test RE 0.15908400951404514\n",
      "173 Train Loss 166.33263 Test MSE 327.3519234789234 Test RE 0.15834446695512974\n",
      "174 Train Loss 165.6677 Test MSE 327.8237672410993 Test RE 0.15845854440507834\n",
      "175 Train Loss 165.09886 Test MSE 326.6609866472578 Test RE 0.15817727102945897\n",
      "176 Train Loss 164.64818 Test MSE 326.9775496794742 Test RE 0.15825389626658456\n",
      "177 Train Loss 164.03848 Test MSE 328.6015259991986 Test RE 0.15864640373938016\n",
      "178 Train Loss 163.21355 Test MSE 329.71077355038904 Test RE 0.15891394643230264\n",
      "179 Train Loss 162.6128 Test MSE 326.4360342204386 Test RE 0.15812279790529304\n",
      "180 Train Loss 161.92169 Test MSE 328.6129650744491 Test RE 0.15864916506611432\n",
      "181 Train Loss 161.56778 Test MSE 331.04362371534023 Test RE 0.15923482592764074\n",
      "182 Train Loss 161.04837 Test MSE 331.9835605487596 Test RE 0.15946072453643242\n",
      "183 Train Loss 160.6231 Test MSE 331.36148940943343 Test RE 0.15931125564953147\n",
      "184 Train Loss 160.23672 Test MSE 330.6065275835909 Test RE 0.159129667708026\n",
      "185 Train Loss 159.9116 Test MSE 331.84562503203125 Test RE 0.15942759400645043\n",
      "186 Train Loss 159.67175 Test MSE 331.9102895788652 Test RE 0.15944312654652898\n",
      "187 Train Loss 159.38422 Test MSE 330.97498784305355 Test RE 0.15921831784993515\n",
      "188 Train Loss 159.066 Test MSE 331.04874297055466 Test RE 0.1592360571257415\n",
      "189 Train Loss 158.68332 Test MSE 329.6906047581167 Test RE 0.15890908588231706\n",
      "190 Train Loss 158.43147 Test MSE 329.13344234110474 Test RE 0.15877475447109737\n",
      "191 Train Loss 158.07744 Test MSE 331.560079169009 Test RE 0.15935898727522127\n",
      "192 Train Loss 157.82086 Test MSE 332.0398773936013 Test RE 0.1594742492182716\n",
      "193 Train Loss 157.39795 Test MSE 331.75091975998726 Test RE 0.15940484289882012\n",
      "194 Train Loss 157.01811 Test MSE 333.69206216541585 Test RE 0.1598705178529707\n",
      "195 Train Loss 156.55649 Test MSE 336.4113059079381 Test RE 0.16052058553743467\n",
      "196 Train Loss 156.0903 Test MSE 333.8217226122478 Test RE 0.15990157473430097\n",
      "197 Train Loss 155.72159 Test MSE 332.83036022168807 Test RE 0.1596639654616591\n",
      "198 Train Loss 155.32637 Test MSE 332.5990364398126 Test RE 0.15960847098692613\n",
      "199 Train Loss 154.96881 Test MSE 332.82131057968445 Test RE 0.15966179481900925\n",
      "Training time: 180.67\n",
      "Training time: 180.67\n",
      "ES_stan_medium\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 9523.763 Test MSE 2781.838393400939 Test RE 0.46159531491344236\n",
      "1 Train Loss 6744.5522 Test MSE 1740.6990192829753 Test RE 0.36513821330428065\n",
      "2 Train Loss 5083.496 Test MSE 1173.2456204840485 Test RE 0.29977120570614013\n",
      "3 Train Loss 3939.5298 Test MSE 946.973823650696 Test RE 0.2693174110853084\n",
      "4 Train Loss 3240.1855 Test MSE 876.595798976696 Test RE 0.2591165402623259\n",
      "5 Train Loss 2877.0054 Test MSE 673.3212742762471 Test RE 0.22709449789753341\n",
      "6 Train Loss 2580.9802 Test MSE 580.2478406369091 Test RE 0.21081534053842368\n",
      "7 Train Loss 2354.1387 Test MSE 454.8858493343721 Test RE 0.18665802112035018\n",
      "8 Train Loss 2244.2693 Test MSE 485.22688211433774 Test RE 0.1927826165482612\n",
      "9 Train Loss 2143.5815 Test MSE 452.75747907331714 Test RE 0.18622083102263967\n",
      "10 Train Loss 2071.2244 Test MSE 412.5555371894381 Test RE 0.17776106929827892\n",
      "11 Train Loss 1996.2683 Test MSE 399.87933314878717 Test RE 0.17500881469675636\n",
      "12 Train Loss 1954.2113 Test MSE 390.8992397147381 Test RE 0.1730325693701938\n",
      "13 Train Loss 1917.3931 Test MSE 383.90299755788834 Test RE 0.17147712575543803\n",
      "14 Train Loss 1873.8018 Test MSE 383.89659019477375 Test RE 0.17147569476792038\n",
      "15 Train Loss 1853.429 Test MSE 389.2794745503397 Test RE 0.17267370058387455\n",
      "16 Train Loss 1822.3219 Test MSE 381.6118416805851 Test RE 0.1709646671828775\n",
      "17 Train Loss 1793.6224 Test MSE 379.22931419800915 Test RE 0.1704301374114079\n",
      "18 Train Loss 1770.4109 Test MSE 379.23426530969044 Test RE 0.17043124995183978\n",
      "19 Train Loss 1748.4388 Test MSE 360.3319432632431 Test RE 0.1661295259209189\n",
      "20 Train Loss 1724.9271 Test MSE 365.13818377431585 Test RE 0.16723380428046505\n",
      "21 Train Loss 1705.7512 Test MSE 358.8801528867177 Test RE 0.16579451721831998\n",
      "22 Train Loss 1682.634 Test MSE 350.7361414551963 Test RE 0.16390254749776106\n",
      "23 Train Loss 1668.2096 Test MSE 339.0033008283427 Test RE 0.16113779157724892\n",
      "24 Train Loss 1655.1167 Test MSE 333.04557743340234 Test RE 0.15971557866231184\n",
      "25 Train Loss 1637.6414 Test MSE 321.3358914819998 Test RE 0.15688270254878758\n",
      "26 Train Loss 1612.2927 Test MSE 305.2430764884188 Test RE 0.1529038268022672\n",
      "27 Train Loss 1580.587 Test MSE 281.9306494064992 Test RE 0.1469489846602552\n",
      "28 Train Loss 1556.1586 Test MSE 278.25936886581223 Test RE 0.14598906990607774\n",
      "29 Train Loss 1529.6152 Test MSE 262.9009608085428 Test RE 0.14190298477610008\n",
      "30 Train Loss 1502.0198 Test MSE 249.76231935521363 Test RE 0.1383116948624209\n",
      "31 Train Loss 1475.4878 Test MSE 240.72816296129548 Test RE 0.13578721926413184\n",
      "32 Train Loss 1448.9365 Test MSE 227.79348456594244 Test RE 0.13208882984698436\n",
      "33 Train Loss 1429.2998 Test MSE 222.7986877051187 Test RE 0.13063265633960355\n",
      "34 Train Loss 1402.3317 Test MSE 220.6526824614521 Test RE 0.13000200474875087\n",
      "35 Train Loss 1373.6451 Test MSE 212.6509384930773 Test RE 0.12762304303081573\n",
      "36 Train Loss 1344.654 Test MSE 209.88978998457503 Test RE 0.1267917804301373\n",
      "37 Train Loss 1315.2545 Test MSE 199.8675391023389 Test RE 0.12372759647032984\n",
      "38 Train Loss 1286.769 Test MSE 189.14281546097106 Test RE 0.12036226963653847\n",
      "39 Train Loss 1268.0405 Test MSE 188.45649573618903 Test RE 0.12014369915489008\n",
      "40 Train Loss 1245.6871 Test MSE 181.37036712290913 Test RE 0.11786330361528274\n",
      "41 Train Loss 1230.4774 Test MSE 182.58747479191 Test RE 0.11825811031274554\n",
      "42 Train Loss 1215.8668 Test MSE 183.04255413921507 Test RE 0.1184053913140859\n",
      "43 Train Loss 1196.2832 Test MSE 180.11863498585208 Test RE 0.11745588116632182\n",
      "44 Train Loss 1178.157 Test MSE 180.95394816588507 Test RE 0.11772792119630512\n",
      "45 Train Loss 1153.9408 Test MSE 183.6623055894893 Test RE 0.11860567234585571\n",
      "46 Train Loss 1123.255 Test MSE 192.54291782464145 Test RE 0.12143928967245064\n",
      "47 Train Loss 1101.8269 Test MSE 194.2217227423455 Test RE 0.12196756255789505\n",
      "48 Train Loss 1071.2783 Test MSE 200.33207867479393 Test RE 0.12387129916112677\n",
      "49 Train Loss 1048.4594 Test MSE 205.3768173291786 Test RE 0.12542125829213585\n",
      "50 Train Loss 1027.75 Test MSE 206.5187903096554 Test RE 0.12576946979322623\n",
      "51 Train Loss 1005.92816 Test MSE 202.191140892998 Test RE 0.12444472869873796\n",
      "52 Train Loss 976.3768 Test MSE 218.84790086433563 Test RE 0.12946925127832282\n",
      "53 Train Loss 947.2805 Test MSE 234.43883712993886 Test RE 0.13400167378589756\n",
      "54 Train Loss 928.3649 Test MSE 248.29666621338035 Test RE 0.13790527798985838\n",
      "55 Train Loss 915.23676 Test MSE 243.13629690726174 Test RE 0.13646470564354138\n",
      "56 Train Loss 903.3532 Test MSE 243.89538488984687 Test RE 0.13667756565794018\n",
      "57 Train Loss 889.11615 Test MSE 251.70496162911496 Test RE 0.13884854466143648\n",
      "58 Train Loss 870.5268 Test MSE 245.43585960080293 Test RE 0.13710852280120422\n",
      "59 Train Loss 858.22174 Test MSE 253.5337866976559 Test RE 0.1393520510648986\n",
      "60 Train Loss 843.60315 Test MSE 258.3414982153325 Test RE 0.14066709891328322\n",
      "61 Train Loss 827.0256 Test MSE 257.9846093946371 Test RE 0.1405699022461103\n",
      "62 Train Loss 812.6705 Test MSE 262.6520529851417 Test RE 0.14183579383255607\n",
      "63 Train Loss 797.1786 Test MSE 262.7393931479361 Test RE 0.14185937432908377\n",
      "64 Train Loss 780.5773 Test MSE 269.96935399621054 Test RE 0.14379794428562462\n",
      "65 Train Loss 766.06024 Test MSE 280.4402817275379 Test RE 0.14656006238400981\n",
      "66 Train Loss 755.789 Test MSE 283.60901700369146 Test RE 0.14738573824157897\n",
      "67 Train Loss 743.347 Test MSE 296.1334934283693 Test RE 0.15060493701026942\n",
      "68 Train Loss 734.3372 Test MSE 297.51882302227887 Test RE 0.1509567952859825\n",
      "69 Train Loss 726.3195 Test MSE 294.1037462452203 Test RE 0.15008791416668382\n",
      "70 Train Loss 714.9762 Test MSE 296.0386686732888 Test RE 0.1505808225146112\n",
      "71 Train Loss 705.63324 Test MSE 297.35040770702943 Test RE 0.15091406347684222\n",
      "72 Train Loss 695.76416 Test MSE 302.87143905600067 Test RE 0.15230866248831632\n",
      "73 Train Loss 687.37805 Test MSE 289.83683908220553 Test RE 0.14899518582334584\n",
      "74 Train Loss 678.84937 Test MSE 290.2601716597591 Test RE 0.1491039565054499\n",
      "75 Train Loss 670.0934 Test MSE 289.9737470224133 Test RE 0.14903037150874912\n",
      "76 Train Loss 663.96625 Test MSE 275.8484914284609 Test RE 0.14535525937522373\n",
      "77 Train Loss 655.73206 Test MSE 277.78485014619594 Test RE 0.1458645384200759\n",
      "78 Train Loss 648.9849 Test MSE 273.3502745510687 Test RE 0.14469555869640857\n",
      "79 Train Loss 642.06805 Test MSE 273.1339244686012 Test RE 0.14463828585696067\n",
      "80 Train Loss 635.5307 Test MSE 280.05297084465434 Test RE 0.1464588217216721\n",
      "81 Train Loss 629.4623 Test MSE 296.4470141543795 Test RE 0.15068463971077548\n",
      "82 Train Loss 622.62683 Test MSE 307.7442753624852 Test RE 0.15352900499049754\n",
      "83 Train Loss 618.4892 Test MSE 308.0218149488549 Test RE 0.1535982195579562\n",
      "84 Train Loss 612.3562 Test MSE 311.8260785387624 Test RE 0.15454382622767043\n",
      "85 Train Loss 605.68146 Test MSE 313.44975753074414 Test RE 0.1549456588215796\n",
      "86 Train Loss 599.67004 Test MSE 302.128270294893 Test RE 0.15212168453965538\n",
      "87 Train Loss 595.26447 Test MSE 300.0176241299627 Test RE 0.15158939775829874\n",
      "88 Train Loss 590.8412 Test MSE 311.8711537522787 Test RE 0.15455499566589412\n",
      "89 Train Loss 583.74384 Test MSE 309.5099754165328 Test RE 0.1539688156751614\n",
      "90 Train Loss 576.8931 Test MSE 296.5422424652479 Test RE 0.15070884014261962\n",
      "91 Train Loss 567.438 Test MSE 296.6440582848218 Test RE 0.15073471036445163\n",
      "92 Train Loss 561.96375 Test MSE 299.4190060467685 Test RE 0.15143809087238871\n",
      "93 Train Loss 553.8768 Test MSE 301.564986240618 Test RE 0.1519798115279525\n",
      "94 Train Loss 548.0023 Test MSE 295.0500211705054 Test RE 0.15032917320292244\n",
      "95 Train Loss 543.8745 Test MSE 291.795934891119 Test RE 0.14949788977864686\n",
      "96 Train Loss 538.87933 Test MSE 287.2263014728889 Test RE 0.1483226741241513\n",
      "97 Train Loss 532.6 Test MSE 285.7831374787912 Test RE 0.14794958242436707\n",
      "98 Train Loss 525.9147 Test MSE 274.46803089629543 Test RE 0.1449910941212938\n",
      "99 Train Loss 520.2296 Test MSE 277.99350500720664 Test RE 0.14591931036287806\n",
      "100 Train Loss 510.19043 Test MSE 282.23680476412756 Test RE 0.14702875075158206\n",
      "101 Train Loss 505.03418 Test MSE 274.14865017233325 Test RE 0.14490671117964105\n",
      "102 Train Loss 497.3015 Test MSE 266.41824062029434 Test RE 0.14284907147786\n",
      "103 Train Loss 490.74933 Test MSE 271.6009036672517 Test RE 0.14423180850549733\n",
      "104 Train Loss 486.37415 Test MSE 275.34051080318 Test RE 0.14522136036441913\n",
      "105 Train Loss 477.45557 Test MSE 288.9899709835567 Test RE 0.14877735365411793\n",
      "106 Train Loss 471.2708 Test MSE 290.81273782086157 Test RE 0.14924581307923382\n",
      "107 Train Loss 464.04608 Test MSE 291.1505395724499 Test RE 0.14933246826766125\n",
      "108 Train Loss 457.48776 Test MSE 288.2759461644488 Test RE 0.14859344342791161\n",
      "109 Train Loss 449.6924 Test MSE 280.67305675878794 Test RE 0.14662087470127907\n",
      "110 Train Loss 445.28885 Test MSE 282.1923793005505 Test RE 0.14701717877132214\n",
      "111 Train Loss 439.1749 Test MSE 284.8846578942067 Test RE 0.1477168284466626\n",
      "112 Train Loss 432.4038 Test MSE 286.5788597795347 Test RE 0.14815541148184316\n",
      "113 Train Loss 429.0864 Test MSE 283.7191050305356 Test RE 0.14741434070111634\n",
      "114 Train Loss 424.01404 Test MSE 290.01970454463446 Test RE 0.14904218084550355\n",
      "115 Train Loss 420.18423 Test MSE 288.425378363596 Test RE 0.1486319512683035\n",
      "116 Train Loss 416.57538 Test MSE 294.1425313579771 Test RE 0.1500978103085105\n",
      "117 Train Loss 412.7893 Test MSE 298.73536791690333 Test RE 0.15126510917554653\n",
      "118 Train Loss 408.5102 Test MSE 294.9065933352115 Test RE 0.15029263023172273\n",
      "119 Train Loss 403.99713 Test MSE 294.1076990802935 Test RE 0.15008892277467561\n",
      "120 Train Loss 399.30258 Test MSE 295.83730950956715 Test RE 0.1505296028778679\n",
      "121 Train Loss 395.6649 Test MSE 297.9142686676018 Test RE 0.15105708370470664\n",
      "122 Train Loss 391.44745 Test MSE 290.0742697711833 Test RE 0.1490562008202711\n",
      "123 Train Loss 387.60004 Test MSE 296.32021357786795 Test RE 0.15065240976430816\n",
      "124 Train Loss 382.2899 Test MSE 291.3050302816029 Test RE 0.14937208251557807\n",
      "125 Train Loss 376.84464 Test MSE 285.07306463612275 Test RE 0.14776566618797873\n",
      "126 Train Loss 372.11926 Test MSE 284.37976566702343 Test RE 0.1475858734264718\n",
      "127 Train Loss 368.64685 Test MSE 290.9778741625291 Test RE 0.1492881812540248\n",
      "128 Train Loss 364.75476 Test MSE 294.05406445263145 Test RE 0.15007523674987322\n",
      "129 Train Loss 360.8158 Test MSE 298.0783719111639 Test RE 0.1510986821564346\n",
      "130 Train Loss 355.24042 Test MSE 299.9276677178777 Test RE 0.1515666699923837\n",
      "131 Train Loss 350.5648 Test MSE 298.4290482953006 Test RE 0.15118753658114858\n",
      "132 Train Loss 345.79312 Test MSE 298.55426547580544 Test RE 0.15121925147577714\n",
      "133 Train Loss 342.74634 Test MSE 300.1936043409482 Test RE 0.15163384985258316\n",
      "134 Train Loss 338.10873 Test MSE 297.61834154772174 Test RE 0.1509820403132566\n",
      "135 Train Loss 334.94202 Test MSE 303.0686549326707 Test RE 0.1523582425977745\n",
      "136 Train Loss 331.14792 Test MSE 301.97761657814567 Test RE 0.15208375271223476\n",
      "137 Train Loss 327.9959 Test MSE 306.2095704881907 Test RE 0.15314570589397186\n",
      "138 Train Loss 324.83728 Test MSE 313.5337396564027 Test RE 0.15496641461094854\n",
      "139 Train Loss 321.5891 Test MSE 325.4852669057493 Test RE 0.15789235824895445\n",
      "140 Train Loss 318.1353 Test MSE 325.37062593386923 Test RE 0.15786454972829167\n",
      "141 Train Loss 315.1788 Test MSE 319.55138898092196 Test RE 0.1564464809132927\n",
      "142 Train Loss 313.28928 Test MSE 325.21698396770745 Test RE 0.15782727303235689\n",
      "143 Train Loss 311.4756 Test MSE 325.2602686924732 Test RE 0.15783777568614776\n",
      "144 Train Loss 308.61426 Test MSE 325.81785166953097 Test RE 0.15797300580870366\n",
      "145 Train Loss 306.24533 Test MSE 330.4233020866675 Test RE 0.15908556595737894\n",
      "146 Train Loss 302.98425 Test MSE 336.01132453695146 Test RE 0.16042513045231618\n",
      "147 Train Loss 300.7217 Test MSE 336.9889313875134 Test RE 0.16065833508956376\n",
      "148 Train Loss 297.96732 Test MSE 334.59568287260896 Test RE 0.1600868320238223\n",
      "149 Train Loss 295.03458 Test MSE 335.2137632138514 Test RE 0.16023462363673913\n",
      "150 Train Loss 293.3308 Test MSE 333.18040356615506 Test RE 0.1597479040500926\n",
      "151 Train Loss 290.54184 Test MSE 336.95567808912165 Test RE 0.16065040819514798\n",
      "152 Train Loss 287.6857 Test MSE 333.23396717944564 Test RE 0.15976074443774738\n",
      "153 Train Loss 284.88052 Test MSE 329.7567293036446 Test RE 0.15892502091975894\n",
      "154 Train Loss 281.785 Test MSE 330.54934717320407 Test RE 0.15911590589085783\n",
      "155 Train Loss 278.50964 Test MSE 332.7983955745079 Test RE 0.15965629830524056\n",
      "156 Train Loss 276.69867 Test MSE 330.35818731190307 Test RE 0.1590698901089748\n",
      "157 Train Loss 274.70822 Test MSE 330.5594658642952 Test RE 0.15911834127969127\n",
      "158 Train Loss 272.46594 Test MSE 332.7280493432613 Test RE 0.15963942350566143\n",
      "159 Train Loss 269.01495 Test MSE 331.2540809548281 Test RE 0.15928543375535437\n",
      "160 Train Loss 266.62973 Test MSE 332.92693316147387 Test RE 0.15968712756168096\n",
      "161 Train Loss 264.792 Test MSE 329.9483200125338 Test RE 0.15897118242807237\n",
      "162 Train Loss 262.59464 Test MSE 332.6376205043108 Test RE 0.1596177286279154\n",
      "163 Train Loss 260.68625 Test MSE 327.81265417930786 Test RE 0.15845585854960428\n",
      "164 Train Loss 259.0052 Test MSE 328.05601033110145 Test RE 0.15851466356143198\n",
      "165 Train Loss 257.5659 Test MSE 332.549374106523 Test RE 0.1595965544987153\n",
      "166 Train Loss 254.61862 Test MSE 337.6426235433631 Test RE 0.1608140823603507\n",
      "167 Train Loss 251.28645 Test MSE 343.10843003181543 Test RE 0.16211049732804175\n",
      "168 Train Loss 249.12038 Test MSE 345.8799583599159 Test RE 0.162763920745899\n",
      "169 Train Loss 246.48201 Test MSE 348.7122951358383 Test RE 0.16342898142629309\n",
      "170 Train Loss 244.49425 Test MSE 347.8289512586449 Test RE 0.16322185432990402\n",
      "171 Train Loss 242.98418 Test MSE 345.2316869200175 Test RE 0.1626113176736196\n",
      "172 Train Loss 240.76828 Test MSE 350.6144501755445 Test RE 0.16387411124788911\n",
      "173 Train Loss 239.04645 Test MSE 347.18715430335635 Test RE 0.16307120031486055\n",
      "174 Train Loss 237.45209 Test MSE 347.67017493778616 Test RE 0.16318459647142078\n",
      "175 Train Loss 235.68459 Test MSE 345.1238463553712 Test RE 0.16258591811350004\n",
      "176 Train Loss 233.20473 Test MSE 347.4185601401199 Test RE 0.1631255360547299\n",
      "177 Train Loss 231.64223 Test MSE 345.9183162556891 Test RE 0.1627729457100118\n",
      "178 Train Loss 229.94783 Test MSE 345.1702803416961 Test RE 0.16259685514155778\n",
      "179 Train Loss 228.49936 Test MSE 350.1206256041959 Test RE 0.1637586659523409\n",
      "180 Train Loss 227.02061 Test MSE 354.5112834217241 Test RE 0.16478226763880988\n",
      "181 Train Loss 225.36891 Test MSE 359.12523998858455 Test RE 0.16585111990628296\n",
      "182 Train Loss 224.17917 Test MSE 355.6600630859813 Test RE 0.16504903688048378\n",
      "183 Train Loss 222.80008 Test MSE 351.63847229100696 Test RE 0.16411324623433773\n",
      "184 Train Loss 221.9002 Test MSE 347.3277344957979 Test RE 0.1631042117041054\n",
      "185 Train Loss 220.67519 Test MSE 342.67316551899665 Test RE 0.16200763867910153\n",
      "186 Train Loss 218.37364 Test MSE 343.0621436437212 Test RE 0.1620995623555733\n",
      "187 Train Loss 217.09097 Test MSE 339.5796576294322 Test RE 0.16127471269410765\n",
      "188 Train Loss 215.0828 Test MSE 340.55498912404386 Test RE 0.16150615106122526\n",
      "189 Train Loss 212.83665 Test MSE 347.0341448723314 Test RE 0.16303526266427332\n",
      "190 Train Loss 211.47766 Test MSE 349.84754054168457 Test RE 0.1636947897254345\n",
      "191 Train Loss 210.33348 Test MSE 349.9340691800035 Test RE 0.1637150319879917\n",
      "192 Train Loss 209.3025 Test MSE 344.75844600556223 Test RE 0.16249982628713477\n",
      "193 Train Loss 208.30269 Test MSE 341.7170281473638 Test RE 0.16178146140986993\n",
      "194 Train Loss 207.15094 Test MSE 344.0303352170767 Test RE 0.16232814026424433\n",
      "195 Train Loss 206.15056 Test MSE 344.37966352148806 Test RE 0.16241053332777713\n",
      "196 Train Loss 204.98671 Test MSE 343.87852621592464 Test RE 0.16229232134156954\n",
      "197 Train Loss 203.65944 Test MSE 347.73790760609097 Test RE 0.16320049140671436\n",
      "198 Train Loss 202.86633 Test MSE 349.11101383421794 Test RE 0.1635223873308741\n",
      "199 Train Loss 201.48134 Test MSE 353.03967371237627 Test RE 0.164439898615448\n",
      "Training time: 173.07\n",
      "Training time: 173.07\n",
      "ES_stan_medium\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 9385.6875 Test MSE 2721.537111835615 Test RE 0.4565649564636478\n",
      "1 Train Loss 7332.243 Test MSE 1907.0637216641496 Test RE 0.38218887934066764\n",
      "2 Train Loss 5527.4976 Test MSE 1267.9441889235688 Test RE 0.3116344868708544\n",
      "3 Train Loss 4318.9575 Test MSE 881.2095667196157 Test RE 0.2597975466028215\n",
      "4 Train Loss 3767.9473 Test MSE 808.9338618925987 Test RE 0.24891550599318318\n",
      "5 Train Loss 3440.5444 Test MSE 698.0473400688556 Test RE 0.2312266472830979\n",
      "6 Train Loss 3150.0508 Test MSE 630.9596182517989 Test RE 0.21983469051330606\n",
      "7 Train Loss 2926.7192 Test MSE 590.684149318695 Test RE 0.21270274874497688\n",
      "8 Train Loss 2687.0686 Test MSE 522.3890626125886 Test RE 0.20002877787754622\n",
      "9 Train Loss 2552.9807 Test MSE 484.7930060293949 Test RE 0.19269640690455553\n",
      "10 Train Loss 2462.8281 Test MSE 475.74223345057464 Test RE 0.1908891734094671\n",
      "11 Train Loss 2399.5654 Test MSE 441.43866012538547 Test RE 0.1838783619660527\n",
      "12 Train Loss 2334.4534 Test MSE 425.68179050183085 Test RE 0.18056683267391846\n",
      "13 Train Loss 2265.7112 Test MSE 416.1625991886479 Test RE 0.17853647976671153\n",
      "14 Train Loss 2194.2446 Test MSE 383.97366285390063 Test RE 0.1714929069871044\n",
      "15 Train Loss 2129.5747 Test MSE 389.2034238199111 Test RE 0.17265683275067165\n",
      "16 Train Loss 2072.369 Test MSE 362.60409225735845 Test RE 0.16665248517451284\n",
      "17 Train Loss 2024.7942 Test MSE 366.7268478339499 Test RE 0.16759721461530275\n",
      "18 Train Loss 1990.8107 Test MSE 373.4522380537911 Test RE 0.16912701234152314\n",
      "19 Train Loss 1954.5194 Test MSE 375.25208481154306 Test RE 0.16953407486393432\n",
      "20 Train Loss 1924.3923 Test MSE 377.5756405880718 Test RE 0.17005814119933466\n",
      "21 Train Loss 1898.8529 Test MSE 389.6214569515389 Test RE 0.1727495309411525\n",
      "22 Train Loss 1884.8934 Test MSE 397.7321738996626 Test RE 0.17453832578780232\n",
      "23 Train Loss 1863.9419 Test MSE 395.75922882061764 Test RE 0.1741048901022381\n",
      "24 Train Loss 1847.9701 Test MSE 393.38597816768515 Test RE 0.17358207745559878\n",
      "25 Train Loss 1825.3103 Test MSE 376.49732659204824 Test RE 0.1698151340151201\n",
      "26 Train Loss 1799.5862 Test MSE 364.3113824131412 Test RE 0.1670443588808836\n",
      "27 Train Loss 1782.8435 Test MSE 360.8508443174648 Test RE 0.16624910146088007\n",
      "28 Train Loss 1769.8334 Test MSE 347.6831876188668 Test RE 0.16318765029857535\n",
      "29 Train Loss 1754.837 Test MSE 344.67649874060675 Test RE 0.16248051246338538\n",
      "30 Train Loss 1739.6616 Test MSE 336.84800376845243 Test RE 0.1606247381934193\n",
      "31 Train Loss 1726.9528 Test MSE 322.1263987717972 Test RE 0.15707555486397437\n",
      "32 Train Loss 1709.5393 Test MSE 324.4837023488004 Test RE 0.15764924243416686\n",
      "33 Train Loss 1695.4789 Test MSE 322.9492605222409 Test RE 0.1572760491879054\n",
      "34 Train Loss 1682.8164 Test MSE 321.61354206545263 Test RE 0.15695046523871226\n",
      "35 Train Loss 1664.2356 Test MSE 314.6106227777524 Test RE 0.155232315287546\n",
      "36 Train Loss 1645.9396 Test MSE 300.92370488819193 Test RE 0.15181813213527334\n",
      "37 Train Loss 1625.1918 Test MSE 280.2868395451835 Test RE 0.14651996191653735\n",
      "38 Train Loss 1609.5818 Test MSE 266.26847183178745 Test RE 0.1428089140548701\n",
      "39 Train Loss 1596.5524 Test MSE 264.80618524536425 Test RE 0.14241623696379413\n",
      "40 Train Loss 1578.327 Test MSE 264.3945239320185 Test RE 0.14230549548327845\n",
      "41 Train Loss 1561.6171 Test MSE 255.75190036190298 Test RE 0.13996030445409235\n",
      "42 Train Loss 1544.631 Test MSE 250.0805463529136 Test RE 0.13839977961536667\n",
      "43 Train Loss 1527.533 Test MSE 231.34302559755838 Test RE 0.1331139740897336\n",
      "44 Train Loss 1503.7466 Test MSE 219.193913412628 Test RE 0.12957156044429283\n",
      "45 Train Loss 1475.965 Test MSE 224.9909741637959 Test RE 0.13127378031366616\n",
      "46 Train Loss 1461.7393 Test MSE 225.40583110411126 Test RE 0.13139475129434178\n",
      "47 Train Loss 1441.0664 Test MSE 223.52767697589928 Test RE 0.13084619448006962\n",
      "48 Train Loss 1413.3604 Test MSE 219.6152755056505 Test RE 0.129696040020754\n",
      "49 Train Loss 1386.2424 Test MSE 220.69900512293336 Test RE 0.13001565000056714\n",
      "50 Train Loss 1346.611 Test MSE 216.87480726891013 Test RE 0.12888429400012413\n",
      "51 Train Loss 1318.2963 Test MSE 202.65134872728657 Test RE 0.12458627270386169\n",
      "52 Train Loss 1288.1292 Test MSE 201.72337724612893 Test RE 0.12430069561672888\n",
      "53 Train Loss 1259.191 Test MSE 203.80442031432128 Test RE 0.1249402134056015\n",
      "54 Train Loss 1236.1646 Test MSE 216.97922450613004 Test RE 0.12891531678835844\n",
      "55 Train Loss 1213.1373 Test MSE 225.69006464994868 Test RE 0.13147756865060664\n",
      "56 Train Loss 1189.0295 Test MSE 233.8692354688944 Test RE 0.13383878696343096\n",
      "57 Train Loss 1156.9446 Test MSE 244.04710120574273 Test RE 0.13672006952281032\n",
      "58 Train Loss 1124.6868 Test MSE 241.58976231108272 Test RE 0.13603000282698738\n",
      "59 Train Loss 1070.0094 Test MSE 253.9031543690278 Test RE 0.13945352355455168\n",
      "60 Train Loss 1039.7369 Test MSE 254.30373911439318 Test RE 0.13956348858804082\n",
      "61 Train Loss 997.4409 Test MSE 234.51444017530648 Test RE 0.13402327881966508\n",
      "62 Train Loss 958.6732 Test MSE 222.49290385783 Test RE 0.13054298105474654\n",
      "63 Train Loss 922.5104 Test MSE 225.85150438830067 Test RE 0.1315245842556134\n",
      "64 Train Loss 893.7576 Test MSE 220.61927051900602 Test RE 0.12999216171422312\n",
      "65 Train Loss 862.7554 Test MSE 227.02502770960388 Test RE 0.1318658420873647\n",
      "66 Train Loss 848.2071 Test MSE 231.6419066941997 Test RE 0.13319993390787702\n",
      "67 Train Loss 835.16644 Test MSE 237.45705384371828 Test RE 0.13486149862806737\n",
      "68 Train Loss 824.3923 Test MSE 245.3910652541339 Test RE 0.13709601043449857\n",
      "69 Train Loss 815.9631 Test MSE 245.98224551030586 Test RE 0.13726105250525422\n",
      "70 Train Loss 806.9622 Test MSE 257.6659646584029 Test RE 0.14048306431607216\n",
      "71 Train Loss 793.2965 Test MSE 259.5245239632314 Test RE 0.1409888101406148\n",
      "72 Train Loss 777.6545 Test MSE 264.747121472263 Test RE 0.14240035344242796\n",
      "73 Train Loss 765.4663 Test MSE 263.89891908355924 Test RE 0.1421720578194395\n",
      "74 Train Loss 754.7918 Test MSE 268.6380672642725 Test RE 0.14344295420750863\n",
      "75 Train Loss 744.71466 Test MSE 266.4201642842732 Test RE 0.142849587195396\n",
      "76 Train Loss 733.4551 Test MSE 270.0015514654364 Test RE 0.1438065189474895\n",
      "77 Train Loss 724.9121 Test MSE 277.52325601755166 Test RE 0.1457958408392569\n",
      "78 Train Loss 718.95105 Test MSE 272.08521528940514 Test RE 0.1443603464224568\n",
      "79 Train Loss 711.78143 Test MSE 257.03409671024144 Test RE 0.14031070699158454\n",
      "80 Train Loss 705.8352 Test MSE 254.94903262729147 Test RE 0.13974044696714957\n",
      "81 Train Loss 699.2389 Test MSE 261.25648302972417 Test RE 0.14145847828140784\n",
      "82 Train Loss 692.12286 Test MSE 263.66630243934804 Test RE 0.14210938443269558\n",
      "83 Train Loss 683.4098 Test MSE 267.95409362150406 Test RE 0.14326022930508944\n",
      "84 Train Loss 676.1166 Test MSE 267.63636818144323 Test RE 0.14317526900496574\n",
      "85 Train Loss 668.38043 Test MSE 279.8582301173736 Test RE 0.14640789125420858\n",
      "86 Train Loss 659.58826 Test MSE 293.5064170166458 Test RE 0.14993542126832032\n",
      "87 Train Loss 652.7797 Test MSE 298.9750222054596 Test RE 0.15132577166894245\n",
      "88 Train Loss 642.1115 Test MSE 294.3514201377896 Test RE 0.15015109771261445\n",
      "89 Train Loss 634.2519 Test MSE 297.7714004030466 Test RE 0.15102085876740928\n",
      "90 Train Loss 626.29395 Test MSE 296.9809426927695 Test RE 0.15082027716289254\n",
      "91 Train Loss 615.8559 Test MSE 303.8005614179723 Test RE 0.1525421031525368\n",
      "92 Train Loss 603.1436 Test MSE 304.9297285703621 Test RE 0.1528253247790214\n",
      "93 Train Loss 591.93506 Test MSE 288.75529523362394 Test RE 0.1487169336888279\n",
      "94 Train Loss 582.621 Test MSE 274.87736928790713 Test RE 0.14509917284530358\n",
      "95 Train Loss 572.5993 Test MSE 282.130141855925 Test RE 0.14700096557997538\n",
      "96 Train Loss 565.3066 Test MSE 284.4575286861309 Test RE 0.14760605056285214\n",
      "97 Train Loss 556.6145 Test MSE 277.1242821527651 Test RE 0.14569100340663196\n",
      "98 Train Loss 548.4619 Test MSE 270.382502405451 Test RE 0.14390793302861263\n",
      "99 Train Loss 535.21875 Test MSE 275.99418265422486 Test RE 0.1453936394844396\n",
      "100 Train Loss 525.23895 Test MSE 287.47740805485716 Test RE 0.14838749524744219\n",
      "101 Train Loss 517.8345 Test MSE 294.34842958622454 Test RE 0.15015033495811403\n",
      "102 Train Loss 507.1213 Test MSE 308.39385523741265 Test RE 0.1536909524070298\n",
      "103 Train Loss 496.42462 Test MSE 310.3666377354651 Test RE 0.15418174603518636\n",
      "104 Train Loss 488.23743 Test MSE 301.2214348353881 Test RE 0.1518932169972954\n",
      "105 Train Loss 476.1096 Test MSE 307.5812426868997 Test RE 0.15348833232427148\n",
      "106 Train Loss 468.58014 Test MSE 301.06036986846425 Test RE 0.151852602445391\n",
      "107 Train Loss 461.11438 Test MSE 303.1841991773926 Test RE 0.15238728294922418\n",
      "108 Train Loss 454.0638 Test MSE 305.5047659406417 Test RE 0.1529693561286143\n",
      "109 Train Loss 445.83234 Test MSE 306.23209468805635 Test RE 0.15315133834538505\n",
      "110 Train Loss 440.64972 Test MSE 311.05901559888264 Test RE 0.15435362752083356\n",
      "111 Train Loss 434.36502 Test MSE 307.3491818763656 Test RE 0.15343042023033635\n",
      "112 Train Loss 427.7161 Test MSE 300.7952259503934 Test RE 0.1517857194099929\n",
      "113 Train Loss 424.82693 Test MSE 298.3749560833009 Test RE 0.15117383409651647\n",
      "114 Train Loss 416.88522 Test MSE 298.51572014354366 Test RE 0.1512094894574228\n",
      "115 Train Loss 412.71283 Test MSE 300.22443634644844 Test RE 0.15164163658685703\n",
      "116 Train Loss 408.214 Test MSE 299.84594143693295 Test RE 0.15154601863953499\n",
      "117 Train Loss 401.6625 Test MSE 300.02601195908056 Test RE 0.15159151679560726\n",
      "118 Train Loss 397.33813 Test MSE 295.62093779244077 Test RE 0.1504745450721561\n",
      "119 Train Loss 393.2385 Test MSE 294.0036800661201 Test RE 0.1500623789567355\n",
      "120 Train Loss 388.96628 Test MSE 307.33758885846146 Test RE 0.1534275265535559\n",
      "121 Train Loss 385.71005 Test MSE 312.9428438715685 Test RE 0.1548203183958173\n",
      "122 Train Loss 381.92636 Test MSE 305.73293200340305 Test RE 0.15302646800594222\n",
      "123 Train Loss 378.5377 Test MSE 305.1806292822019 Test RE 0.15288818532511764\n",
      "124 Train Loss 373.8534 Test MSE 301.5088536073255 Test RE 0.15196566627826974\n",
      "125 Train Loss 370.45343 Test MSE 304.3190547553121 Test RE 0.1526722186897499\n",
      "126 Train Loss 367.2699 Test MSE 305.71650023815704 Test RE 0.15302235570968284\n",
      "127 Train Loss 364.45474 Test MSE 309.41779938068834 Test RE 0.1539458870250133\n",
      "128 Train Loss 360.38034 Test MSE 319.194427935967 Test RE 0.1563590757169507\n",
      "129 Train Loss 356.31512 Test MSE 319.1805558195853 Test RE 0.1563556780153106\n",
      "130 Train Loss 352.0858 Test MSE 317.1048523066254 Test RE 0.1558464405331376\n",
      "131 Train Loss 346.37405 Test MSE 323.32613472456336 Test RE 0.157367791158356\n",
      "132 Train Loss 341.19885 Test MSE 325.26544911126797 Test RE 0.15783903262190463\n",
      "133 Train Loss 336.6045 Test MSE 329.48377969330966 Test RE 0.15885923378211714\n",
      "134 Train Loss 332.98035 Test MSE 320.21610587918326 Test RE 0.15660911296413732\n",
      "135 Train Loss 329.80444 Test MSE 320.07975178943207 Test RE 0.15657576584885688\n",
      "136 Train Loss 326.60388 Test MSE 331.1419287689765 Test RE 0.15925846696019966\n",
      "137 Train Loss 324.2227 Test MSE 336.04964786451393 Test RE 0.16043427872959648\n",
      "138 Train Loss 322.25476 Test MSE 335.0123214801552 Test RE 0.16018647109257025\n",
      "139 Train Loss 319.26782 Test MSE 336.1876604351744 Test RE 0.16046721980610443\n",
      "140 Train Loss 316.24942 Test MSE 335.4381731818063 Test RE 0.16028824946393155\n",
      "141 Train Loss 313.193 Test MSE 336.0019128867522 Test RE 0.1604228836890769\n",
      "142 Train Loss 310.884 Test MSE 332.20354141085585 Test RE 0.1595135471800791\n",
      "143 Train Loss 308.10266 Test MSE 336.5104079790428 Test RE 0.1605442273574507\n",
      "144 Train Loss 305.15994 Test MSE 336.55328981748073 Test RE 0.1605544561826936\n",
      "145 Train Loss 303.55167 Test MSE 336.7869682313702 Test RE 0.16061018524787982\n",
      "146 Train Loss 301.6359 Test MSE 337.4445197668498 Test RE 0.16076689851988654\n",
      "147 Train Loss 299.6717 Test MSE 341.18702761048274 Test RE 0.16165595181081124\n",
      "148 Train Loss 296.53998 Test MSE 334.8188911135994 Test RE 0.16014021995542269\n",
      "149 Train Loss 294.05023 Test MSE 334.63954738525933 Test RE 0.1600973251370001\n",
      "150 Train Loss 291.27777 Test MSE 332.9595025114002 Test RE 0.15969493825259665\n",
      "151 Train Loss 289.22772 Test MSE 330.7592831248187 Test RE 0.15916642610401668\n",
      "152 Train Loss 287.22546 Test MSE 332.65893597209157 Test RE 0.15962284270978477\n",
      "153 Train Loss 285.31195 Test MSE 338.22327459389055 Test RE 0.16095230062472435\n",
      "154 Train Loss 283.548 Test MSE 337.9790758784257 Test RE 0.1608941859929751\n",
      "155 Train Loss 281.72958 Test MSE 337.0807591712869 Test RE 0.1606802228915007\n",
      "156 Train Loss 280.36145 Test MSE 340.7537753355957 Test RE 0.16155328076604913\n",
      "157 Train Loss 278.48016 Test MSE 344.3699934763897 Test RE 0.16240825309991652\n",
      "158 Train Loss 276.71698 Test MSE 341.63309167545333 Test RE 0.16176159087671663\n",
      "159 Train Loss 275.57892 Test MSE 347.2378369066204 Test RE 0.16308310249966354\n",
      "160 Train Loss 273.20462 Test MSE 339.7557133421324 Test RE 0.16131651386602017\n",
      "161 Train Loss 271.30005 Test MSE 338.89856143957934 Test RE 0.1611128968673976\n",
      "162 Train Loss 269.37778 Test MSE 335.20197984573883 Test RE 0.1602318073440407\n",
      "163 Train Loss 267.445 Test MSE 330.5308937570515 Test RE 0.15911146438608093\n",
      "164 Train Loss 265.71353 Test MSE 334.58353543116675 Test RE 0.1600839260343437\n",
      "165 Train Loss 264.76285 Test MSE 337.30161571632783 Test RE 0.16073285340655333\n",
      "166 Train Loss 263.84552 Test MSE 337.0309902908213 Test RE 0.16066836049682431\n",
      "167 Train Loss 262.56985 Test MSE 333.098189886084 Test RE 0.15972819359710758\n",
      "168 Train Loss 261.2391 Test MSE 334.69472566548376 Test RE 0.16011052371292495\n",
      "169 Train Loss 259.7562 Test MSE 339.29936746327337 Test RE 0.1612081406149533\n",
      "170 Train Loss 257.72055 Test MSE 337.8802142320812 Test RE 0.16087065283870888\n",
      "171 Train Loss 255.55385 Test MSE 337.0285269805593 Test RE 0.1606677733449662\n",
      "172 Train Loss 253.5604 Test MSE 336.446775352625 Test RE 0.16052904753895264\n",
      "173 Train Loss 251.48114 Test MSE 335.04683154608057 Test RE 0.16019472139109334\n",
      "174 Train Loss 249.45384 Test MSE 337.15698715667475 Test RE 0.1606983901065723\n",
      "175 Train Loss 247.21596 Test MSE 336.97555642673683 Test RE 0.16065514682423548\n",
      "176 Train Loss 245.6903 Test MSE 337.9588720757682 Test RE 0.16088937693348604\n",
      "177 Train Loss 244.08342 Test MSE 339.7569476030274 Test RE 0.1613168068802009\n",
      "178 Train Loss 242.6823 Test MSE 339.730039984505 Test RE 0.16131041887684122\n",
      "179 Train Loss 240.60928 Test MSE 336.96467497599804 Test RE 0.16065255290437144\n",
      "180 Train Loss 238.73608 Test MSE 337.4644134306931 Test RE 0.16077163736629407\n",
      "181 Train Loss 237.56767 Test MSE 341.2202360561158 Test RE 0.161663818775014\n",
      "182 Train Loss 235.41823 Test MSE 344.77602218209785 Test RE 0.16250396844705242\n",
      "183 Train Loss 233.46678 Test MSE 343.8860295656345 Test RE 0.1622940919223224\n",
      "184 Train Loss 231.74237 Test MSE 339.7152192838152 Test RE 0.16130690026024957\n",
      "185 Train Loss 229.8528 Test MSE 342.8849493274286 Test RE 0.16205769409417625\n",
      "186 Train Loss 228.49332 Test MSE 341.5546593612407 Test RE 0.16174302115307546\n",
      "187 Train Loss 225.90005 Test MSE 340.33850427339394 Test RE 0.16145480958287683\n",
      "188 Train Loss 224.15628 Test MSE 341.40638722626005 Test RE 0.1617079102481673\n",
      "189 Train Loss 223.20792 Test MSE 341.17873452653714 Test RE 0.16165398714859258\n",
      "190 Train Loss 222.0457 Test MSE 340.22532890704605 Test RE 0.16142796244930668\n",
      "191 Train Loss 221.0979 Test MSE 336.3075659988927 Test RE 0.16049583358256148\n",
      "192 Train Loss 220.0846 Test MSE 332.22437270051563 Test RE 0.15951854836218893\n",
      "193 Train Loss 218.21147 Test MSE 331.1072839580394 Test RE 0.15925013575336822\n",
      "194 Train Loss 216.64047 Test MSE 331.26250805809286 Test RE 0.15928745985305243\n",
      "195 Train Loss 215.16714 Test MSE 329.1959750774395 Test RE 0.1587898367250343\n",
      "196 Train Loss 213.80254 Test MSE 332.63543775217033 Test RE 0.15961720492510958\n",
      "197 Train Loss 212.16975 Test MSE 331.0832352520375 Test RE 0.1592443523873362\n",
      "198 Train Loss 210.618 Test MSE 332.6287117867144 Test RE 0.15961559116855856\n",
      "199 Train Loss 209.6055 Test MSE 338.8093171561738 Test RE 0.1610916820359141\n",
      "Training time: 172.47\n",
      "Training time: 172.47\n",
      "ES_stan_medium\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 9316.352 Test MSE 2672.2360697841846 Test RE 0.45241068626036396\n",
      "1 Train Loss 7276.7886 Test MSE 1923.30165859318 Test RE 0.3838125284179338\n",
      "2 Train Loss 5626.732 Test MSE 1414.6996649667085 Test RE 0.3291755494576291\n",
      "3 Train Loss 4415.6885 Test MSE 1057.522911311162 Test RE 0.28460356537002746\n",
      "4 Train Loss 3736.6853 Test MSE 1036.6240658404884 Test RE 0.2817773544642823\n",
      "5 Train Loss 3234.2458 Test MSE 774.6429341497002 Test RE 0.24358257976572664\n",
      "6 Train Loss 2826.0303 Test MSE 664.2126571615885 Test RE 0.22555321270464207\n",
      "7 Train Loss 2545.1099 Test MSE 609.3404812050784 Test RE 0.21603566765321233\n",
      "8 Train Loss 2379.4797 Test MSE 581.45188795339 Test RE 0.21103395410744047\n",
      "9 Train Loss 2261.9695 Test MSE 532.1973187676175 Test RE 0.20189789218601148\n",
      "10 Train Loss 2192.0342 Test MSE 508.824117282093 Test RE 0.1974146088763189\n",
      "11 Train Loss 2113.4717 Test MSE 492.9705896779912 Test RE 0.19431483089267595\n",
      "12 Train Loss 2035.7708 Test MSE 467.5966722195784 Test RE 0.1892479351466336\n",
      "13 Train Loss 1973.3707 Test MSE 456.856447054245 Test RE 0.18706189207808316\n",
      "14 Train Loss 1915.3894 Test MSE 427.8106144745984 Test RE 0.18101777469347935\n",
      "15 Train Loss 1858.2028 Test MSE 404.1868404858807 Test RE 0.17594888889677657\n",
      "16 Train Loss 1806.2314 Test MSE 387.5881678549397 Test RE 0.1722981836251358\n",
      "17 Train Loss 1757.6005 Test MSE 375.031049401853 Test RE 0.1694841370287579\n",
      "18 Train Loss 1723.3676 Test MSE 359.8359851516941 Test RE 0.1660151568531907\n",
      "19 Train Loss 1680.6256 Test MSE 342.756425324174 Test RE 0.1620273191013618\n",
      "20 Train Loss 1639.2382 Test MSE 328.0693859774031 Test RE 0.15851789504385463\n",
      "21 Train Loss 1613.3348 Test MSE 315.0786571474617 Test RE 0.15534773900894955\n",
      "22 Train Loss 1585.99 Test MSE 309.2443655355152 Test RE 0.15390273635687718\n",
      "23 Train Loss 1556.1531 Test MSE 296.62198259960786 Test RE 0.15072910156522898\n",
      "24 Train Loss 1528.9772 Test MSE 288.71768227847474 Test RE 0.14870724751922726\n",
      "25 Train Loss 1504.5673 Test MSE 280.46588779699573 Test RE 0.14656675318722362\n",
      "26 Train Loss 1481.2755 Test MSE 274.1252808872555 Test RE 0.14490053489825083\n",
      "27 Train Loss 1457.2045 Test MSE 269.25117100837645 Test RE 0.14360654846583598\n",
      "28 Train Loss 1438.4886 Test MSE 259.99776652496917 Test RE 0.1411172980298271\n",
      "29 Train Loss 1418.7913 Test MSE 257.7845685489389 Test RE 0.14051539284087472\n",
      "30 Train Loss 1402.2128 Test MSE 257.75383562062393 Test RE 0.14050701650896688\n",
      "31 Train Loss 1384.2811 Test MSE 253.73807770335648 Test RE 0.13940818290943158\n",
      "32 Train Loss 1363.1425 Test MSE 252.79071136321974 Test RE 0.13914768963247476\n",
      "33 Train Loss 1345.0748 Test MSE 249.46778253290304 Test RE 0.13823011749616435\n",
      "34 Train Loss 1329.8756 Test MSE 244.0393375767973 Test RE 0.13671789483537142\n",
      "35 Train Loss 1314.582 Test MSE 242.0797032564578 Test RE 0.13616786651814478\n",
      "36 Train Loss 1299.2106 Test MSE 239.32638948687026 Test RE 0.13539129378566753\n",
      "37 Train Loss 1279.2799 Test MSE 242.21342180157106 Test RE 0.13620546912385956\n",
      "38 Train Loss 1262.7644 Test MSE 240.39745526825254 Test RE 0.1356939162812478\n",
      "39 Train Loss 1244.237 Test MSE 240.17004134907435 Test RE 0.1356297184578095\n",
      "40 Train Loss 1228.2065 Test MSE 229.74632877048037 Test RE 0.13265381181464073\n",
      "41 Train Loss 1213.2281 Test MSE 220.50178416552976 Test RE 0.1299575447498808\n",
      "42 Train Loss 1194.1619 Test MSE 208.1957471911662 Test RE 0.1262790688317296\n",
      "43 Train Loss 1182.1891 Test MSE 206.5987582620452 Test RE 0.12579381758630132\n",
      "44 Train Loss 1162.5392 Test MSE 203.42283620711282 Test RE 0.12482319549141319\n",
      "45 Train Loss 1149.657 Test MSE 202.46795530449143 Test RE 0.12452988651465939\n",
      "46 Train Loss 1135.9243 Test MSE 203.29847042238913 Test RE 0.12478503333532354\n",
      "47 Train Loss 1120.4253 Test MSE 200.6148496734733 Test RE 0.12395869120435946\n",
      "48 Train Loss 1100.6622 Test MSE 200.91423805159496 Test RE 0.12405115184701189\n",
      "49 Train Loss 1085.0355 Test MSE 200.77144284926928 Test RE 0.12400706075108811\n",
      "50 Train Loss 1064.272 Test MSE 198.72284714169308 Test RE 0.12337277808383605\n",
      "51 Train Loss 1046.7896 Test MSE 202.57214186330225 Test RE 0.12456192287236663\n",
      "52 Train Loss 1028.67 Test MSE 203.00930604994556 Test RE 0.12469625690272693\n",
      "53 Train Loss 999.51416 Test MSE 192.54062681488625 Test RE 0.12143856718567203\n",
      "54 Train Loss 980.55676 Test MSE 191.65530419464767 Test RE 0.12115905167369438\n",
      "55 Train Loss 954.7433 Test MSE 181.2705335665072 Test RE 0.11783086079504741\n",
      "56 Train Loss 929.17615 Test MSE 176.54458281665723 Test RE 0.11628471741190737\n",
      "57 Train Loss 898.89246 Test MSE 179.01329593205736 Test RE 0.11709492915619192\n",
      "58 Train Loss 875.6278 Test MSE 183.32296325161025 Test RE 0.11849605122857551\n",
      "59 Train Loss 854.4191 Test MSE 178.1477764688471 Test RE 0.11681151238628801\n",
      "60 Train Loss 828.60095 Test MSE 184.26156883408314 Test RE 0.1187990112472114\n",
      "61 Train Loss 802.1501 Test MSE 195.2401330474808 Test RE 0.12228691565913247\n",
      "62 Train Loss 774.8675 Test MSE 204.4216830402079 Test RE 0.12512927365947443\n",
      "63 Train Loss 747.25287 Test MSE 207.21103611470767 Test RE 0.1259800815036342\n",
      "64 Train Loss 725.4578 Test MSE 212.52786484132736 Test RE 0.1275861061911069\n",
      "65 Train Loss 698.6387 Test MSE 213.4377341841314 Test RE 0.1278589238402776\n",
      "66 Train Loss 683.83466 Test MSE 219.43212549432315 Test RE 0.1296419481875759\n",
      "67 Train Loss 662.76184 Test MSE 229.31687249388798 Test RE 0.1325297714005272\n",
      "68 Train Loss 648.41077 Test MSE 229.17693409440568 Test RE 0.13248932772506783\n",
      "69 Train Loss 636.5039 Test MSE 229.74806747733308 Test RE 0.13265431377185694\n",
      "70 Train Loss 624.03546 Test MSE 238.59359167142338 Test RE 0.13518385634500205\n",
      "71 Train Loss 612.2148 Test MSE 246.49599278393094 Test RE 0.13740431632059794\n",
      "72 Train Loss 597.76666 Test MSE 257.09209445662503 Test RE 0.14032653610900997\n",
      "73 Train Loss 590.0553 Test MSE 261.7253795188128 Test RE 0.14158536441291011\n",
      "74 Train Loss 582.1572 Test MSE 261.98171264262413 Test RE 0.14165468160492387\n",
      "75 Train Loss 568.8566 Test MSE 268.6002862667897 Test RE 0.14343286701378757\n",
      "76 Train Loss 560.0474 Test MSE 268.9535184516279 Test RE 0.14352714922660992\n",
      "77 Train Loss 548.2266 Test MSE 263.3418476125941 Test RE 0.14202192109627734\n",
      "78 Train Loss 538.1482 Test MSE 265.9003464361095 Test RE 0.14271016077267829\n",
      "79 Train Loss 530.78784 Test MSE 268.5176829105823 Test RE 0.14341081017344495\n",
      "80 Train Loss 520.51843 Test MSE 275.97015286267646 Test RE 0.14538730990374593\n",
      "81 Train Loss 512.52344 Test MSE 278.55075049326297 Test RE 0.14606548674450134\n",
      "82 Train Loss 506.72177 Test MSE 283.6764897300047 Test RE 0.14740326928887304\n",
      "83 Train Loss 499.44928 Test MSE 276.8282283995249 Test RE 0.1456131612669165\n",
      "84 Train Loss 491.63208 Test MSE 281.47055238974735 Test RE 0.1468290289255648\n",
      "85 Train Loss 485.54654 Test MSE 285.1557960568216 Test RE 0.14778710626456087\n",
      "86 Train Loss 478.98108 Test MSE 288.4181550511681 Test RE 0.14863009009064146\n",
      "87 Train Loss 472.14893 Test MSE 288.98616347730314 Test RE 0.14877637356375636\n",
      "88 Train Loss 466.23987 Test MSE 290.8767196839343 Test RE 0.14926223000114106\n",
      "89 Train Loss 459.78375 Test MSE 287.48353186225785 Test RE 0.14838907570485713\n",
      "90 Train Loss 453.88208 Test MSE 280.12507441896815 Test RE 0.1464776744495565\n",
      "91 Train Loss 446.26593 Test MSE 276.0588376381751 Test RE 0.14541066859871407\n",
      "92 Train Loss 438.75656 Test MSE 279.3344148001892 Test RE 0.14627081003383405\n",
      "93 Train Loss 430.03085 Test MSE 277.03947461767586 Test RE 0.14566870900459222\n",
      "94 Train Loss 424.29953 Test MSE 287.5200439221027 Test RE 0.14839849853815698\n",
      "95 Train Loss 419.8111 Test MSE 287.1700752539847 Test RE 0.14830815590058785\n",
      "96 Train Loss 414.70367 Test MSE 288.6773783862983 Test RE 0.14869686767341395\n",
      "97 Train Loss 409.66107 Test MSE 287.7403863395377 Test RE 0.14845535061219586\n",
      "98 Train Loss 404.11157 Test MSE 285.8332879854158 Test RE 0.14796256328302593\n",
      "99 Train Loss 398.8223 Test MSE 277.93239234324363 Test RE 0.14590327040552564\n",
      "100 Train Loss 389.78333 Test MSE 273.06561195911627 Test RE 0.144620197254142\n",
      "101 Train Loss 383.78064 Test MSE 272.39443783830416 Test RE 0.14444235528563387\n",
      "102 Train Loss 377.4698 Test MSE 270.34115657187823 Test RE 0.1438969296891293\n",
      "103 Train Loss 370.95032 Test MSE 272.5369759617433 Test RE 0.14448014212473112\n",
      "104 Train Loss 363.91605 Test MSE 274.19013257428384 Test RE 0.1449176739384628\n",
      "105 Train Loss 357.36493 Test MSE 275.02937439646195 Test RE 0.1451392866735984\n",
      "106 Train Loss 350.99274 Test MSE 271.35030563570854 Test RE 0.1441652539617669\n",
      "107 Train Loss 344.33755 Test MSE 264.89420973254886 Test RE 0.14243990535847545\n",
      "108 Train Loss 340.02155 Test MSE 263.5419614509064 Test RE 0.1420758721770852\n",
      "109 Train Loss 335.15976 Test MSE 266.91550322489326 Test RE 0.14298232133819372\n",
      "110 Train Loss 329.3554 Test MSE 266.454839783379 Test RE 0.14285888307504904\n",
      "111 Train Loss 324.10617 Test MSE 273.1697614388544 Test RE 0.14464777429371378\n",
      "112 Train Loss 316.725 Test MSE 276.765374639529 Test RE 0.14559662961783018\n",
      "113 Train Loss 310.5406 Test MSE 278.80869942708233 Test RE 0.14613310228286697\n",
      "114 Train Loss 305.81256 Test MSE 284.68868430034655 Test RE 0.14766601213008199\n",
      "115 Train Loss 301.2744 Test MSE 288.4812461100573 Test RE 0.14864634551153744\n",
      "116 Train Loss 296.36557 Test MSE 290.01219382547873 Test RE 0.14904025094006493\n",
      "117 Train Loss 292.1425 Test MSE 298.21930876769846 Test RE 0.1511343990327353\n",
      "118 Train Loss 289.85425 Test MSE 298.8296152962413 Test RE 0.15128896844600428\n",
      "119 Train Loss 286.94342 Test MSE 299.95459046755076 Test RE 0.15157347246575575\n",
      "120 Train Loss 284.41516 Test MSE 298.02691269288835 Test RE 0.15108563901676336\n",
      "121 Train Loss 280.28772 Test MSE 299.42012026381923 Test RE 0.1514383726426548\n",
      "122 Train Loss 275.51898 Test MSE 298.66710232280985 Test RE 0.15124782499422135\n",
      "123 Train Loss 272.0447 Test MSE 299.46874752029026 Test RE 0.15145066930063375\n",
      "124 Train Loss 268.61615 Test MSE 291.29167437216665 Test RE 0.14936865823060785\n",
      "125 Train Loss 265.80786 Test MSE 293.3293833327718 Test RE 0.14989019632228093\n",
      "126 Train Loss 263.48117 Test MSE 299.00238365515486 Test RE 0.15133269598946092\n",
      "127 Train Loss 261.58115 Test MSE 301.20461050427775 Test RE 0.1518889750391125\n",
      "128 Train Loss 259.33566 Test MSE 307.50031955575184 Test RE 0.15346813997865655\n",
      "129 Train Loss 257.10434 Test MSE 311.65972023469493 Test RE 0.1545025963875036\n",
      "130 Train Loss 254.639 Test MSE 311.2434530279776 Test RE 0.15439938148721505\n",
      "131 Train Loss 252.84573 Test MSE 313.7680363270013 Test RE 0.1550243052458013\n",
      "132 Train Loss 251.12677 Test MSE 316.02326860365184 Test RE 0.15558043238641073\n",
      "133 Train Loss 249.64867 Test MSE 313.82865888862256 Test RE 0.1550392805070009\n",
      "134 Train Loss 247.49344 Test MSE 314.045202656832 Test RE 0.15509276032745115\n",
      "135 Train Loss 245.68103 Test MSE 317.1624371038754 Test RE 0.1558605903927582\n",
      "136 Train Loss 244.44899 Test MSE 315.75335652467203 Test RE 0.15551397840594694\n",
      "137 Train Loss 242.85452 Test MSE 317.06303105375196 Test RE 0.15583616331999872\n",
      "138 Train Loss 240.77878 Test MSE 317.6029350910927 Test RE 0.15596878803247333\n",
      "139 Train Loss 238.8614 Test MSE 318.6120285206229 Test RE 0.15621636487578838\n",
      "140 Train Loss 236.54474 Test MSE 319.26001729824935 Test RE 0.15637513953947538\n",
      "141 Train Loss 234.3499 Test MSE 317.9644174919631 Test RE 0.15605752137294604\n",
      "142 Train Loss 232.85965 Test MSE 319.9853010053973 Test RE 0.15655266255209202\n",
      "143 Train Loss 230.90161 Test MSE 320.4770337391927 Test RE 0.15667290638193918\n",
      "144 Train Loss 229.51164 Test MSE 323.7655081750639 Test RE 0.1574746797577319\n",
      "145 Train Loss 228.16943 Test MSE 328.2601549844853 Test RE 0.15856397661669486\n",
      "146 Train Loss 226.32794 Test MSE 327.33802207475236 Test RE 0.15834110477265073\n",
      "147 Train Loss 224.45146 Test MSE 330.56068890949035 Test RE 0.1591186356423746\n",
      "148 Train Loss 222.88918 Test MSE 332.7916881102853 Test RE 0.15965468938176175\n",
      "149 Train Loss 221.22919 Test MSE 337.5543948064432 Test RE 0.16079306998012285\n",
      "150 Train Loss 218.97273 Test MSE 339.8003528104395 Test RE 0.16132711096065142\n",
      "151 Train Loss 217.26715 Test MSE 346.00210570048563 Test RE 0.16279265820904462\n",
      "152 Train Loss 215.4438 Test MSE 350.0393698066845 Test RE 0.16373966234022866\n",
      "153 Train Loss 213.98726 Test MSE 348.5833256615223 Test RE 0.1633987569413598\n",
      "154 Train Loss 212.0362 Test MSE 347.3131274257146 Test RE 0.16310078194675487\n",
      "155 Train Loss 210.35709 Test MSE 346.99707352958234 Test RE 0.16302655444692826\n",
      "156 Train Loss 208.4784 Test MSE 349.0346462029524 Test RE 0.16350450118594728\n",
      "157 Train Loss 206.2391 Test MSE 352.1071519506889 Test RE 0.16422257859421624\n",
      "158 Train Loss 204.4537 Test MSE 347.38312517592084 Test RE 0.163117216846155\n",
      "159 Train Loss 202.68387 Test MSE 346.297682167327 Test RE 0.1628621771600262\n",
      "160 Train Loss 200.90683 Test MSE 350.0834630980727 Test RE 0.1637499748849254\n",
      "161 Train Loss 198.32635 Test MSE 352.7222750957656 Test RE 0.1643659625447099\n",
      "162 Train Loss 196.86142 Test MSE 352.3248868402487 Test RE 0.16427334646232325\n",
      "163 Train Loss 195.51031 Test MSE 354.3682867228069 Test RE 0.16474903076027295\n",
      "164 Train Loss 194.24442 Test MSE 353.3112994307584 Test RE 0.1645031457811216\n",
      "165 Train Loss 192.91496 Test MSE 351.3387365726936 Test RE 0.16404328647086325\n",
      "166 Train Loss 191.32594 Test MSE 354.43071020788557 Test RE 0.16476354074351182\n",
      "167 Train Loss 189.83026 Test MSE 352.8103489292121 Test RE 0.16438648214025037\n",
      "168 Train Loss 188.34013 Test MSE 348.7961349570308 Test RE 0.16344862660843434\n",
      "169 Train Loss 186.38684 Test MSE 344.02902725346047 Test RE 0.16232783168752676\n",
      "170 Train Loss 184.17355 Test MSE 341.6207192760398 Test RE 0.16175866171502035\n",
      "171 Train Loss 183.34453 Test MSE 343.70303248215635 Test RE 0.16225090421611774\n",
      "172 Train Loss 182.30731 Test MSE 341.69730902212416 Test RE 0.16177679346144946\n",
      "173 Train Loss 181.19658 Test MSE 343.4406321534441 Test RE 0.1621889571053873\n",
      "174 Train Loss 179.85173 Test MSE 342.0701774476848 Test RE 0.16186503678322772\n",
      "175 Train Loss 178.10158 Test MSE 342.68882511998635 Test RE 0.16201134037834597\n",
      "176 Train Loss 177.05902 Test MSE 346.17807898765216 Test RE 0.16283405032375928\n",
      "177 Train Loss 175.98683 Test MSE 347.25867244931067 Test RE 0.16308799521815875\n",
      "178 Train Loss 175.0674 Test MSE 346.7027857214471 Test RE 0.16295740846904558\n",
      "179 Train Loss 173.62463 Test MSE 345.53964602602207 Test RE 0.16268382908425796\n",
      "180 Train Loss 172.3153 Test MSE 340.9649822578763 Test RE 0.16160334020493608\n",
      "181 Train Loss 171.36464 Test MSE 340.3629249923915 Test RE 0.1614606020097691\n",
      "182 Train Loss 170.29973 Test MSE 340.35396504391605 Test RE 0.16145847679563805\n",
      "183 Train Loss 168.946 Test MSE 344.98995402093476 Test RE 0.1625543770882881\n",
      "184 Train Loss 167.9583 Test MSE 346.059684745073 Test RE 0.1628062030049275\n",
      "185 Train Loss 166.81898 Test MSE 347.14123000628337 Test RE 0.1630604148093759\n",
      "186 Train Loss 165.97105 Test MSE 350.7529615261737 Test RE 0.1639064775453168\n",
      "187 Train Loss 165.07098 Test MSE 349.9417837952131 Test RE 0.16371683660152633\n",
      "188 Train Loss 164.1232 Test MSE 349.1862395338474 Test RE 0.16354000411024086\n",
      "189 Train Loss 163.16185 Test MSE 349.64587766535766 Test RE 0.1636476035702619\n",
      "190 Train Loss 161.72244 Test MSE 348.6310439502317 Test RE 0.16340994055466287\n",
      "191 Train Loss 160.7523 Test MSE 345.92046806835646 Test RE 0.16277345198031506\n",
      "192 Train Loss 159.66788 Test MSE 345.22168637514204 Test RE 0.1626089624240608\n",
      "193 Train Loss 158.88617 Test MSE 346.19534572507314 Test RE 0.1628381112073075\n",
      "194 Train Loss 157.62807 Test MSE 349.02402690720294 Test RE 0.16350201387432906\n",
      "195 Train Loss 156.65842 Test MSE 350.69356935223453 Test RE 0.16389260000867714\n",
      "196 Train Loss 155.63344 Test MSE 352.13960585768507 Test RE 0.16423014666186123\n",
      "197 Train Loss 154.77405 Test MSE 356.06570800740775 Test RE 0.1651431326635702\n",
      "198 Train Loss 153.89401 Test MSE 355.7106496484039 Test RE 0.1650607741647752\n",
      "199 Train Loss 152.84528 Test MSE 355.8680260225423 Test RE 0.1650972838863847\n",
      "Training time: 171.82\n",
      "Training time: 171.82\n",
      "ES_stan_medium\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 9117.887 Test MSE 2622.282164598341 Test RE 0.4481621287663053\n",
      "1 Train Loss 7287.7437 Test MSE 1943.9445080255846 Test RE 0.38586676627672073\n",
      "2 Train Loss 5611.052 Test MSE 1350.7032675139642 Test RE 0.321643973816387\n",
      "3 Train Loss 4495.2134 Test MSE 1053.4302670493967 Test RE 0.28405231950276166\n",
      "4 Train Loss 3816.9175 Test MSE 885.7401404937935 Test RE 0.260464540584717\n",
      "5 Train Loss 3369.4517 Test MSE 787.0411363575679 Test RE 0.24552411820194311\n",
      "6 Train Loss 2975.8997 Test MSE 657.1228270465369 Test RE 0.22434620153585857\n",
      "7 Train Loss 2707.0781 Test MSE 607.637857669773 Test RE 0.215733632313635\n",
      "8 Train Loss 2507.504 Test MSE 555.1000936679011 Test RE 0.20619640735747893\n",
      "9 Train Loss 2322.5894 Test MSE 529.6186102013648 Test RE 0.2014081602666604\n",
      "10 Train Loss 2207.757 Test MSE 498.269109256366 Test RE 0.19535630193000678\n",
      "11 Train Loss 2115.4568 Test MSE 470.69173737240783 Test RE 0.18987322683579544\n",
      "12 Train Loss 2050.2576 Test MSE 447.5023558851571 Test RE 0.18513695088067023\n",
      "13 Train Loss 1985.0336 Test MSE 435.5967519523429 Test RE 0.1826576055481093\n",
      "14 Train Loss 1931.79 Test MSE 410.95674055653046 Test RE 0.1774162918617176\n",
      "15 Train Loss 1871.307 Test MSE 388.51982597676533 Test RE 0.17250513867129602\n",
      "16 Train Loss 1816.1249 Test MSE 383.20496954092465 Test RE 0.1713211614919571\n",
      "17 Train Loss 1775.4387 Test MSE 378.56601594640114 Test RE 0.17028102514275045\n",
      "18 Train Loss 1743.3696 Test MSE 356.66400525682855 Test RE 0.16528181944141324\n",
      "19 Train Loss 1710.2274 Test MSE 361.3397264573946 Test RE 0.1663616808105765\n",
      "20 Train Loss 1673.1967 Test MSE 369.38097063825927 Test RE 0.1682025993281153\n",
      "21 Train Loss 1646.738 Test MSE 369.7791900883226 Test RE 0.16829324219944725\n",
      "22 Train Loss 1616.2059 Test MSE 348.9463529404509 Test RE 0.16348381948703067\n",
      "23 Train Loss 1576.273 Test MSE 328.9046431655961 Test RE 0.1587195582440299\n",
      "24 Train Loss 1542.5023 Test MSE 312.70996477034777 Test RE 0.15476270224105831\n",
      "25 Train Loss 1520.7446 Test MSE 290.8369905834963 Test RE 0.14925203623823835\n",
      "26 Train Loss 1499.0714 Test MSE 283.55795252523467 Test RE 0.1473724690678027\n",
      "27 Train Loss 1464.4061 Test MSE 285.700616499169 Test RE 0.14792822037680844\n",
      "28 Train Loss 1443.7988 Test MSE 275.31379642318416 Test RE 0.1452143152828622\n",
      "29 Train Loss 1422.6315 Test MSE 256.56390303582845 Test RE 0.14018231273061613\n",
      "30 Train Loss 1396.2874 Test MSE 254.28607550382415 Test RE 0.13955864155368453\n",
      "31 Train Loss 1370.662 Test MSE 250.00699091636264 Test RE 0.1383794245637162\n",
      "32 Train Loss 1339.8192 Test MSE 240.51675631207732 Test RE 0.1357275822322715\n",
      "33 Train Loss 1307.5876 Test MSE 222.85867385791684 Test RE 0.1306502408756381\n",
      "34 Train Loss 1286.9908 Test MSE 223.66018628419232 Test RE 0.13088497216106346\n",
      "35 Train Loss 1256.9061 Test MSE 221.4514874855055 Test RE 0.13023710825457158\n",
      "36 Train Loss 1231.6691 Test MSE 234.1011505914754 Test RE 0.13390513076334015\n",
      "37 Train Loss 1206.5774 Test MSE 221.23828381451256 Test RE 0.13017439990176258\n",
      "38 Train Loss 1180.3083 Test MSE 213.87410294401155 Test RE 0.12798955948477897\n",
      "39 Train Loss 1155.4008 Test MSE 213.03843135538455 Test RE 0.12773926755997062\n",
      "40 Train Loss 1122.0707 Test MSE 209.990198567262 Test RE 0.12682210458442236\n",
      "41 Train Loss 1092.8644 Test MSE 202.7145396987801 Test RE 0.12460569550571156\n",
      "42 Train Loss 1065.996 Test MSE 209.3765365047748 Test RE 0.12663666055795686\n",
      "43 Train Loss 1041.4225 Test MSE 204.0844335177695 Test RE 0.12502601355870094\n",
      "44 Train Loss 1017.9844 Test MSE 204.4181101584206 Test RE 0.12512818015009283\n",
      "45 Train Loss 993.9473 Test MSE 208.0759502916399 Test RE 0.12624273279242326\n",
      "46 Train Loss 979.1205 Test MSE 216.08348084103258 Test RE 0.12864894450632816\n",
      "47 Train Loss 958.37396 Test MSE 208.40209487928075 Test RE 0.12634163240696158\n",
      "48 Train Loss 941.7729 Test MSE 215.40214669930364 Test RE 0.12844596248814572\n",
      "49 Train Loss 924.2964 Test MSE 216.44274810277278 Test RE 0.12875584800110537\n",
      "50 Train Loss 906.40283 Test MSE 221.16773595844376 Test RE 0.1301536434194513\n",
      "51 Train Loss 892.199 Test MSE 222.7914000428698 Test RE 0.13063051984931953\n",
      "52 Train Loss 877.57074 Test MSE 219.97636108819216 Test RE 0.1298026176168308\n",
      "53 Train Loss 865.42615 Test MSE 215.01194311317008 Test RE 0.12832956904727674\n",
      "54 Train Loss 855.0424 Test MSE 211.40574346458135 Test RE 0.12724884083060986\n",
      "55 Train Loss 844.54443 Test MSE 209.10570794779778 Test RE 0.12655473179333376\n",
      "56 Train Loss 834.3528 Test MSE 203.68096487282318 Test RE 0.12490236612593189\n",
      "57 Train Loss 825.0689 Test MSE 208.60768895167115 Test RE 0.12640393669333855\n",
      "58 Train Loss 815.7118 Test MSE 211.93904909515408 Test RE 0.12740924276080756\n",
      "59 Train Loss 804.91833 Test MSE 209.3592637319496 Test RE 0.12663143692731538\n",
      "60 Train Loss 794.40875 Test MSE 204.23613351704657 Test RE 0.12507247208230118\n",
      "61 Train Loss 783.6068 Test MSE 208.22977112003687 Test RE 0.1262893868487669\n",
      "62 Train Loss 775.20056 Test MSE 211.49453669896312 Test RE 0.127275561131102\n",
      "63 Train Loss 766.135 Test MSE 211.47485233955285 Test RE 0.12726963805567443\n",
      "64 Train Loss 755.9128 Test MSE 211.79665650272588 Test RE 0.12736643521612853\n",
      "65 Train Loss 749.98584 Test MSE 214.64813300989707 Test RE 0.12822095331364056\n",
      "66 Train Loss 739.5953 Test MSE 225.09566255488053 Test RE 0.13130431763330352\n",
      "67 Train Loss 731.5765 Test MSE 228.9290421915127 Test RE 0.1324176540250921\n",
      "68 Train Loss 726.56445 Test MSE 234.23864519714468 Test RE 0.1339444482347228\n",
      "69 Train Loss 721.549 Test MSE 235.65166208718452 Test RE 0.13434784280348058\n",
      "70 Train Loss 717.0504 Test MSE 240.47083692636625 Test RE 0.13571462507950974\n",
      "71 Train Loss 706.997 Test MSE 248.2908290464723 Test RE 0.13790365698368764\n",
      "72 Train Loss 699.7313 Test MSE 252.43131019642047 Test RE 0.13904873894405223\n",
      "73 Train Loss 690.27655 Test MSE 260.24900030010605 Test RE 0.14118546182895902\n",
      "74 Train Loss 684.7447 Test MSE 265.5835218764096 Test RE 0.1426251147003143\n",
      "75 Train Loss 677.2821 Test MSE 268.73095051999377 Test RE 0.1434677502039692\n",
      "76 Train Loss 671.42755 Test MSE 271.39792539965833 Test RE 0.14417790331954233\n",
      "77 Train Loss 666.8613 Test MSE 277.64308420537196 Test RE 0.14582731309535174\n",
      "78 Train Loss 660.6345 Test MSE 287.2800956158687 Test RE 0.1483365630290752\n",
      "79 Train Loss 656.7174 Test MSE 288.3205128126813 Test RE 0.14860492904765052\n",
      "80 Train Loss 652.57556 Test MSE 284.8127967816754 Test RE 0.14769819675691237\n",
      "81 Train Loss 647.83954 Test MSE 281.3278113355695 Test RE 0.1467917937902673\n",
      "82 Train Loss 643.48303 Test MSE 281.3459499889517 Test RE 0.1467965259255967\n",
      "83 Train Loss 637.89087 Test MSE 292.77747426225704 Test RE 0.1497491182150275\n",
      "84 Train Loss 631.33984 Test MSE 286.6859903461403 Test RE 0.1481831010506566\n",
      "85 Train Loss 624.27997 Test MSE 299.89564092833376 Test RE 0.15155857750216709\n",
      "86 Train Loss 614.7474 Test MSE 295.03318956106233 Test RE 0.15032488525539364\n",
      "87 Train Loss 605.8642 Test MSE 284.1361542353921 Test RE 0.14752264581081379\n",
      "88 Train Loss 594.2317 Test MSE 273.9528944335503 Test RE 0.14485496665010847\n",
      "89 Train Loss 580.68146 Test MSE 268.18493134540597 Test RE 0.1433219241006446\n",
      "90 Train Loss 570.4829 Test MSE 268.262915880812 Test RE 0.14334276062043771\n",
      "91 Train Loss 560.6017 Test MSE 272.4702532462986 Test RE 0.1444624551714778\n",
      "92 Train Loss 549.83386 Test MSE 268.1296774516715 Test RE 0.14330715909826866\n",
      "93 Train Loss 536.92926 Test MSE 267.1820910138685 Test RE 0.14305370690321786\n",
      "94 Train Loss 526.49963 Test MSE 279.43720371780864 Test RE 0.1462977197781434\n",
      "95 Train Loss 516.57306 Test MSE 282.00168974752967 Test RE 0.14696749745492674\n",
      "96 Train Loss 509.05615 Test MSE 275.10380757650864 Test RE 0.14515892539008807\n",
      "97 Train Loss 501.823 Test MSE 274.5376897467806 Test RE 0.1450094920238954\n",
      "98 Train Loss 495.41806 Test MSE 279.85480672209496 Test RE 0.14640699577649433\n",
      "99 Train Loss 489.21146 Test MSE 273.88756159512184 Test RE 0.14483769296797733\n",
      "100 Train Loss 483.4793 Test MSE 273.71892486634124 Test RE 0.14479309671930923\n",
      "101 Train Loss 476.9632 Test MSE 284.3042866124559 Test RE 0.14756628627084795\n",
      "102 Train Loss 470.5791 Test MSE 288.5043610652465 Test RE 0.14865230063832124\n",
      "103 Train Loss 465.77325 Test MSE 286.36135937824264 Test RE 0.14809917918000742\n",
      "104 Train Loss 459.7435 Test MSE 286.5308071667979 Test RE 0.14814298985193528\n",
      "105 Train Loss 455.49915 Test MSE 286.69282619694127 Test RE 0.14818486770749698\n",
      "106 Train Loss 448.21503 Test MSE 284.9767316715186 Test RE 0.14774069731294096\n",
      "107 Train Loss 440.95218 Test MSE 285.58623872557075 Test RE 0.14789860652007364\n",
      "108 Train Loss 431.02118 Test MSE 287.0681231111588 Test RE 0.14828182711870438\n",
      "109 Train Loss 424.13138 Test MSE 281.68362349781097 Test RE 0.14688459265191373\n",
      "110 Train Loss 417.95892 Test MSE 285.56084428682294 Test RE 0.14789203077325813\n",
      "111 Train Loss 409.91696 Test MSE 284.2010275472639 Test RE 0.1475394858648923\n",
      "112 Train Loss 404.40442 Test MSE 282.4095982021043 Test RE 0.14707375146654156\n",
      "113 Train Loss 396.1416 Test MSE 282.2480908634257 Test RE 0.14703169041867034\n",
      "114 Train Loss 390.69766 Test MSE 285.1960657316504 Test RE 0.14779754113682786\n",
      "115 Train Loss 384.52277 Test MSE 290.02564099071543 Test RE 0.1490437062183125\n",
      "116 Train Loss 379.3563 Test MSE 297.959927175391 Test RE 0.15106865880787798\n",
      "117 Train Loss 371.74036 Test MSE 300.5308249614073 Test RE 0.15171899441923675\n",
      "118 Train Loss 366.28033 Test MSE 292.5446429949568 Test RE 0.14968956238398592\n",
      "119 Train Loss 362.71063 Test MSE 294.340104563263 Test RE 0.1501482116007268\n",
      "120 Train Loss 355.2407 Test MSE 298.26840030356806 Test RE 0.15114683805703766\n",
      "121 Train Loss 349.699 Test MSE 302.5450345367028 Test RE 0.15222656884918273\n",
      "122 Train Loss 345.3727 Test MSE 307.56010711464194 Test RE 0.1534830587265686\n",
      "123 Train Loss 340.92844 Test MSE 312.9115331021121 Test RE 0.15481257310837923\n",
      "124 Train Loss 336.25485 Test MSE 311.2433857971948 Test RE 0.15439936481153602\n",
      "125 Train Loss 332.07874 Test MSE 312.2974363031162 Test RE 0.1546605867070674\n",
      "126 Train Loss 327.11584 Test MSE 306.45446159181733 Test RE 0.15320693279391664\n",
      "127 Train Loss 323.52136 Test MSE 316.09237670815554 Test RE 0.15559744265449324\n",
      "128 Train Loss 318.91104 Test MSE 313.94792604476635 Test RE 0.15506873820159334\n",
      "129 Train Loss 314.22748 Test MSE 317.07115164185416 Test RE 0.1558381589376947\n",
      "130 Train Loss 308.09213 Test MSE 321.78944755981865 Test RE 0.15699338114788908\n",
      "131 Train Loss 303.63626 Test MSE 322.49364130320726 Test RE 0.15716506692038384\n",
      "132 Train Loss 298.0012 Test MSE 324.4038794305527 Test RE 0.15762985040228444\n",
      "133 Train Loss 294.58505 Test MSE 330.72653527360364 Test RE 0.15915854652552983\n",
      "134 Train Loss 290.75967 Test MSE 323.33008399182137 Test RE 0.15736875223990893\n",
      "135 Train Loss 286.54324 Test MSE 321.853291050781 Test RE 0.15700895423223213\n",
      "136 Train Loss 282.1491 Test MSE 313.90458113328583 Test RE 0.15505803312570904\n",
      "137 Train Loss 277.88678 Test MSE 320.86175268589085 Test RE 0.15676691773162413\n",
      "138 Train Loss 274.63187 Test MSE 322.256353937565 Test RE 0.1571072360945193\n",
      "139 Train Loss 270.44412 Test MSE 324.96838274938534 Test RE 0.15776693861666943\n",
      "140 Train Loss 267.04782 Test MSE 328.85135281398084 Test RE 0.15870669955705377\n",
      "141 Train Loss 264.09106 Test MSE 336.2869857184614 Test RE 0.16049092275073507\n",
      "142 Train Loss 261.52414 Test MSE 337.27422451996057 Test RE 0.16072632697096803\n",
      "143 Train Loss 259.45007 Test MSE 339.5683483280901 Test RE 0.16127202713937883\n",
      "144 Train Loss 257.922 Test MSE 337.3430928465271 Test RE 0.16074273555985777\n",
      "145 Train Loss 255.06145 Test MSE 343.26334390007963 Test RE 0.16214708974025502\n",
      "146 Train Loss 253.0936 Test MSE 346.1334951311671 Test RE 0.1628235643858411\n",
      "147 Train Loss 250.8126 Test MSE 347.9565591118733 Test RE 0.16325179214902513\n",
      "148 Train Loss 248.2301 Test MSE 350.0629327210296 Test RE 0.1637451733184859\n",
      "149 Train Loss 246.66751 Test MSE 354.1704338756488 Test RE 0.16470303254725852\n",
      "150 Train Loss 243.965 Test MSE 360.7055848267674 Test RE 0.1662156365390421\n",
      "151 Train Loss 241.06581 Test MSE 361.2363876288245 Test RE 0.16633789038666202\n",
      "152 Train Loss 238.47672 Test MSE 363.8565091600413 Test RE 0.16694004187690942\n",
      "153 Train Loss 236.54929 Test MSE 363.2533327734177 Test RE 0.16680161360318707\n",
      "154 Train Loss 234.85007 Test MSE 368.4628931753569 Test RE 0.1679934398153393\n",
      "155 Train Loss 232.96407 Test MSE 371.8858512757469 Test RE 0.16877195128873818\n",
      "156 Train Loss 230.31705 Test MSE 371.26054836199194 Test RE 0.1686300018277462\n",
      "157 Train Loss 227.5597 Test MSE 367.9282637647764 Test RE 0.16787151866870517\n",
      "158 Train Loss 225.46304 Test MSE 365.95056654198794 Test RE 0.16741973714702937\n",
      "159 Train Loss 223.84572 Test MSE 364.842576582106 Test RE 0.16716609631806922\n",
      "160 Train Loss 221.79282 Test MSE 365.9005649370619 Test RE 0.16740829907001853\n",
      "161 Train Loss 220.31694 Test MSE 366.1727395641695 Test RE 0.16747055071111416\n",
      "162 Train Loss 218.70227 Test MSE 365.3341147031544 Test RE 0.16727866658423757\n",
      "163 Train Loss 217.33249 Test MSE 368.12851515134173 Test RE 0.16791719595805793\n",
      "164 Train Loss 215.51543 Test MSE 369.99911564618264 Test RE 0.16834328082330505\n",
      "165 Train Loss 213.62328 Test MSE 370.20349132372 Test RE 0.16838976812670817\n",
      "166 Train Loss 211.71683 Test MSE 372.6680831770342 Test RE 0.1689493571433536\n",
      "167 Train Loss 210.24281 Test MSE 374.31464948297366 Test RE 0.16932218182226\n",
      "168 Train Loss 208.37537 Test MSE 368.80550075923355 Test RE 0.16807152427720992\n",
      "169 Train Loss 206.71796 Test MSE 363.529711602506 Test RE 0.16686505647491445\n",
      "170 Train Loss 205.20767 Test MSE 356.786787454938 Test RE 0.16531026626891435\n",
      "171 Train Loss 203.35135 Test MSE 354.8491998119649 Test RE 0.1648607832869105\n",
      "172 Train Loss 201.78264 Test MSE 354.3472864430633 Test RE 0.16474414907768145\n",
      "173 Train Loss 200.39484 Test MSE 350.7428058363889 Test RE 0.1639041046567435\n",
      "174 Train Loss 198.67297 Test MSE 347.9701803046407 Test RE 0.1632549874650933\n",
      "175 Train Loss 197.46353 Test MSE 348.043154340537 Test RE 0.16327210495310868\n",
      "176 Train Loss 196.30441 Test MSE 348.89137680152817 Test RE 0.16347094062596085\n",
      "177 Train Loss 195.17415 Test MSE 350.6265923591791 Test RE 0.16387694879825793\n",
      "178 Train Loss 194.33026 Test MSE 353.9923451566084 Test RE 0.16466161824858203\n",
      "179 Train Loss 193.36217 Test MSE 354.1391864547938 Test RE 0.16469576675372663\n",
      "180 Train Loss 191.69983 Test MSE 358.3452006742625 Test RE 0.16567090322182815\n",
      "181 Train Loss 190.32419 Test MSE 353.94769343931 Test RE 0.16465123291681244\n",
      "182 Train Loss 189.02754 Test MSE 353.03983792680435 Test RE 0.16443993685959274\n",
      "183 Train Loss 188.08301 Test MSE 355.0159090998756 Test RE 0.16489950480095328\n",
      "184 Train Loss 186.64537 Test MSE 350.6329882377178 Test RE 0.1638784434542579\n",
      "185 Train Loss 185.59676 Test MSE 350.2116095754396 Test RE 0.16377994211380134\n",
      "186 Train Loss 183.87216 Test MSE 350.52201290819033 Test RE 0.16385250764087675\n",
      "187 Train Loss 182.68185 Test MSE 354.9450550401625 Test RE 0.16488304866183165\n",
      "188 Train Loss 181.48006 Test MSE 355.63645656312985 Test RE 0.16504355932070233\n",
      "189 Train Loss 180.50934 Test MSE 355.16737104306486 Test RE 0.16493467693754196\n",
      "190 Train Loss 179.65936 Test MSE 357.5481181498289 Test RE 0.16548654621593864\n",
      "191 Train Loss 178.87732 Test MSE 354.86131415829465 Test RE 0.16486359738876089\n",
      "192 Train Loss 177.81154 Test MSE 356.2255090001336 Test RE 0.16518018632483863\n",
      "193 Train Loss 177.00432 Test MSE 355.7011574548343 Test RE 0.1650585718139694\n",
      "194 Train Loss 176.27739 Test MSE 357.04854316711675 Test RE 0.16537089487752796\n",
      "195 Train Loss 175.73473 Test MSE 359.162766941948 Test RE 0.1658597850235812\n",
      "196 Train Loss 175.06525 Test MSE 356.76162412263983 Test RE 0.1653044366941435\n",
      "197 Train Loss 174.28816 Test MSE 353.59574617957213 Test RE 0.16456935222690008\n",
      "198 Train Loss 173.62798 Test MSE 353.44082829644134 Test RE 0.16453329759529006\n",
      "199 Train Loss 173.01964 Test MSE 352.78694998967575 Test RE 0.16438103086430852\n",
      "Training time: 173.16\n",
      "Training time: 173.16\n",
      "ES_stan_medium\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 9674.36 Test MSE 2902.4772897088214 Test RE 0.471498004436328\n",
      "1 Train Loss 6986.415 Test MSE 1909.558036271325 Test RE 0.3824387366957245\n",
      "2 Train Loss 5095.5117 Test MSE 1317.5722294717796 Test RE 0.31767472316102097\n",
      "3 Train Loss 4123.1323 Test MSE 1083.5836137301058 Test RE 0.28808898817605705\n",
      "4 Train Loss 3454.8997 Test MSE 871.3288971147822 Test RE 0.25833693492110954\n",
      "5 Train Loss 2991.716 Test MSE 756.6239002531759 Test RE 0.24073291332656618\n",
      "6 Train Loss 2792.682 Test MSE 634.3954912304569 Test RE 0.22043242970871355\n",
      "7 Train Loss 2582.5208 Test MSE 615.3561053875425 Test RE 0.21709943878047944\n",
      "8 Train Loss 2433.5715 Test MSE 535.0639193706498 Test RE 0.20244090822470592\n",
      "9 Train Loss 2328.8147 Test MSE 515.1904665490737 Test RE 0.198645784290552\n",
      "10 Train Loss 2233.2537 Test MSE 449.0059413325581 Test RE 0.18544771549096506\n",
      "11 Train Loss 2156.2754 Test MSE 429.8168489186288 Test RE 0.181441723176393\n",
      "12 Train Loss 2101.1494 Test MSE 393.1985279208626 Test RE 0.17354071619449613\n",
      "13 Train Loss 2060.1714 Test MSE 371.0735975653285 Test RE 0.1685875390842042\n",
      "14 Train Loss 2014.8999 Test MSE 360.59573547012667 Test RE 0.16619032493888544\n",
      "15 Train Loss 1984.1377 Test MSE 339.5098273423127 Test RE 0.16125812978305276\n",
      "16 Train Loss 1950.8748 Test MSE 332.6440159471996 Test RE 0.15961926306223354\n",
      "17 Train Loss 1928.5063 Test MSE 327.77110634531385 Test RE 0.15844581667623836\n",
      "18 Train Loss 1901.8427 Test MSE 312.7122059368588 Test RE 0.15476325682582218\n",
      "19 Train Loss 1880.7915 Test MSE 313.53843580503417 Test RE 0.15496757516003595\n",
      "20 Train Loss 1856.4926 Test MSE 285.636535226498 Test RE 0.14791162965266327\n",
      "21 Train Loss 1832.6361 Test MSE 285.42866745289007 Test RE 0.14785779960444026\n",
      "22 Train Loss 1815.3182 Test MSE 277.22637506354187 Test RE 0.14571783730376836\n",
      "23 Train Loss 1800.8047 Test MSE 270.095397098223 Test RE 0.14383150850998913\n",
      "24 Train Loss 1782.1436 Test MSE 262.2797163435764 Test RE 0.14173522467278632\n",
      "25 Train Loss 1767.7838 Test MSE 263.9771130696689 Test RE 0.1421931192481399\n",
      "26 Train Loss 1753.2181 Test MSE 260.48476664939875 Test RE 0.14124939914561083\n",
      "27 Train Loss 1737.7977 Test MSE 249.91818782621735 Test RE 0.13835484602715073\n",
      "28 Train Loss 1723.2675 Test MSE 251.50594184514816 Test RE 0.13879364095249433\n",
      "29 Train Loss 1708.2054 Test MSE 248.27603662990253 Test RE 0.13789954898112367\n",
      "30 Train Loss 1694.6205 Test MSE 244.42156027288306 Test RE 0.13682491904988212\n",
      "31 Train Loss 1685.3013 Test MSE 244.49668773453664 Test RE 0.13684594526190166\n",
      "32 Train Loss 1678.7129 Test MSE 241.97520056169267 Test RE 0.13613847238710264\n",
      "33 Train Loss 1670.2059 Test MSE 242.38831872056699 Test RE 0.13625463571825372\n",
      "34 Train Loss 1664.408 Test MSE 242.2970031875442 Test RE 0.13622896753255986\n",
      "35 Train Loss 1658.468 Test MSE 242.9956158143342 Test RE 0.13642522000930415\n",
      "36 Train Loss 1652.5358 Test MSE 241.29213385676886 Test RE 0.13594618537510617\n",
      "37 Train Loss 1646.9016 Test MSE 241.50965705420177 Test RE 0.13600744884614646\n",
      "38 Train Loss 1640.0414 Test MSE 245.0922877641183 Test RE 0.13701252394409397\n",
      "39 Train Loss 1634.5339 Test MSE 244.58000900791518 Test RE 0.13686926092981258\n",
      "40 Train Loss 1624.495 Test MSE 246.75583148732784 Test RE 0.13747671821861804\n",
      "41 Train Loss 1620.9705 Test MSE 248.43121877965018 Test RE 0.1379426385315859\n",
      "42 Train Loss 1611.5729 Test MSE 249.39028416144743 Test RE 0.13820864490160029\n",
      "43 Train Loss 1602.3129 Test MSE 242.73308492661212 Test RE 0.1363515036316965\n",
      "44 Train Loss 1591.2831 Test MSE 241.0072610315948 Test RE 0.1358659117032072\n",
      "45 Train Loss 1576.8212 Test MSE 240.58802633159425 Test RE 0.13574769016797122\n",
      "46 Train Loss 1560.9219 Test MSE 238.8131235467697 Test RE 0.13524603392043788\n",
      "47 Train Loss 1544.5255 Test MSE 235.12880110914983 Test RE 0.13419871537537606\n",
      "48 Train Loss 1524.0759 Test MSE 237.52514528160842 Test RE 0.13488083318772176\n",
      "49 Train Loss 1490.0566 Test MSE 227.916182323573 Test RE 0.13212439895342767\n",
      "50 Train Loss 1452.0609 Test MSE 215.64048403240818 Test RE 0.12851700403770838\n",
      "51 Train Loss 1415.3485 Test MSE 204.13912034772918 Test RE 0.1250427635338346\n",
      "52 Train Loss 1386.9252 Test MSE 212.30002824546315 Test RE 0.12751769968126253\n",
      "53 Train Loss 1356.1508 Test MSE 220.1157577711158 Test RE 0.1298437383732759\n",
      "54 Train Loss 1314.4742 Test MSE 202.73805074000967 Test RE 0.12461292124469646\n",
      "55 Train Loss 1262.6729 Test MSE 193.96883519573913 Test RE 0.12188813239955454\n",
      "56 Train Loss 1207.1656 Test MSE 182.34318316107544 Test RE 0.1181789725289375\n",
      "57 Train Loss 1138.0017 Test MSE 178.884021697965 Test RE 0.1170526415399364\n",
      "58 Train Loss 1097.7446 Test MSE 172.10544845465418 Test RE 0.11481344638345628\n",
      "59 Train Loss 1043.0243 Test MSE 174.96446817711072 Test RE 0.11576316028570571\n",
      "60 Train Loss 972.26575 Test MSE 175.1759342807877 Test RE 0.11583309617728312\n",
      "61 Train Loss 924.621 Test MSE 183.00776418340732 Test RE 0.1183941384278278\n",
      "62 Train Loss 882.70654 Test MSE 202.64748621766643 Test RE 0.1245850853987367\n",
      "63 Train Loss 842.41614 Test MSE 204.28217068185884 Test RE 0.12508656767221807\n",
      "64 Train Loss 816.377 Test MSE 203.31729949313402 Test RE 0.12479081186335379\n",
      "65 Train Loss 788.1458 Test MSE 207.63529113882166 Test RE 0.12610898475505816\n",
      "66 Train Loss 765.3605 Test MSE 204.06680645482984 Test RE 0.12502061410475582\n",
      "67 Train Loss 742.0442 Test MSE 206.34237773789351 Test RE 0.12571574089083068\n",
      "68 Train Loss 720.43054 Test MSE 218.28694794433113 Test RE 0.12930321643628684\n",
      "69 Train Loss 708.3184 Test MSE 217.40540931808306 Test RE 0.12904186069561704\n",
      "70 Train Loss 695.0193 Test MSE 224.92743950759169 Test RE 0.13125524396254415\n",
      "71 Train Loss 686.98364 Test MSE 226.2700020638864 Test RE 0.13164638388299998\n",
      "72 Train Loss 679.1119 Test MSE 226.8374191486056 Test RE 0.1318113452897695\n",
      "73 Train Loss 669.9059 Test MSE 234.5251831150271 Test RE 0.1340263485400536\n",
      "74 Train Loss 658.6352 Test MSE 230.5978402915746 Test RE 0.13289941264844027\n",
      "75 Train Loss 642.0652 Test MSE 232.73394562130144 Test RE 0.13351353968299706\n",
      "76 Train Loss 631.4442 Test MSE 241.03320358079415 Test RE 0.1358732239585249\n",
      "77 Train Loss 620.6061 Test MSE 240.62648430057462 Test RE 0.13575853936093943\n",
      "78 Train Loss 612.75934 Test MSE 235.17854651294468 Test RE 0.13421291060836857\n",
      "79 Train Loss 605.55176 Test MSE 236.08583323803407 Test RE 0.13447154894360974\n",
      "80 Train Loss 596.77515 Test MSE 249.47194425923303 Test RE 0.1382312704977955\n",
      "81 Train Loss 588.9911 Test MSE 259.5536911531037 Test RE 0.14099673257468412\n",
      "82 Train Loss 581.92505 Test MSE 247.6550022672594 Test RE 0.13772697094727185\n",
      "83 Train Loss 576.082 Test MSE 246.79966237220526 Test RE 0.13748892757272274\n",
      "84 Train Loss 566.9935 Test MSE 250.14193034223007 Test RE 0.1384167641618649\n",
      "85 Train Loss 558.56244 Test MSE 259.26940264579105 Test RE 0.1409194947354061\n",
      "86 Train Loss 544.9148 Test MSE 256.6746054957725 Test RE 0.14021255247410658\n",
      "87 Train Loss 532.16547 Test MSE 254.3528119067049 Test RE 0.13957695366673978\n",
      "88 Train Loss 525.94403 Test MSE 257.3335979778501 Test RE 0.1403924296126091\n",
      "89 Train Loss 517.98883 Test MSE 256.9029020887211 Test RE 0.14027489392266587\n",
      "90 Train Loss 503.98517 Test MSE 251.02593749524857 Test RE 0.13866113241525643\n",
      "91 Train Loss 498.35907 Test MSE 253.18846596972375 Test RE 0.13925711786264985\n",
      "92 Train Loss 491.24817 Test MSE 252.65520301714352 Test RE 0.1391103896052071\n",
      "93 Train Loss 484.87256 Test MSE 264.1966046513223 Test RE 0.14225222230316875\n",
      "94 Train Loss 476.5772 Test MSE 262.5395673284769 Test RE 0.1418054186635923\n",
      "95 Train Loss 472.26974 Test MSE 263.7554942030405 Test RE 0.14213341843854022\n",
      "96 Train Loss 467.11652 Test MSE 269.9337498677371 Test RE 0.14378846178470286\n",
      "97 Train Loss 459.5417 Test MSE 270.0715184213184 Test RE 0.1438251504193471\n",
      "98 Train Loss 450.93503 Test MSE 274.01738625722896 Test RE 0.14487201595205063\n",
      "99 Train Loss 443.27567 Test MSE 279.71348898086706 Test RE 0.14637002567978488\n",
      "100 Train Loss 431.09476 Test MSE 278.78094654702966 Test RE 0.1461258289886734\n",
      "101 Train Loss 422.79144 Test MSE 276.0780389439233 Test RE 0.14541572553907794\n",
      "102 Train Loss 418.18652 Test MSE 284.25484993482553 Test RE 0.14755345582046744\n",
      "103 Train Loss 412.9034 Test MSE 275.846149740229 Test RE 0.145354642410779\n",
      "104 Train Loss 406.5688 Test MSE 267.6474388321116 Test RE 0.14317823016268783\n",
      "105 Train Loss 398.68372 Test MSE 262.43194685753315 Test RE 0.14177635117791232\n",
      "106 Train Loss 393.94754 Test MSE 258.71771458099215 Test RE 0.14076948666770694\n",
      "107 Train Loss 389.71585 Test MSE 258.89291302453455 Test RE 0.14081714173678855\n",
      "108 Train Loss 380.7353 Test MSE 265.87944853685786 Test RE 0.14270455265444515\n",
      "109 Train Loss 371.90842 Test MSE 264.25720596603725 Test RE 0.14226853624749286\n",
      "110 Train Loss 367.42694 Test MSE 269.8855197093387 Test RE 0.14377561557689764\n",
      "111 Train Loss 362.90912 Test MSE 270.7349568132198 Test RE 0.1440016973558414\n",
      "112 Train Loss 355.12784 Test MSE 277.7034811096932 Test RE 0.14584317345681058\n",
      "113 Train Loss 348.7021 Test MSE 280.0062789709295 Test RE 0.146446612028185\n",
      "114 Train Loss 340.53082 Test MSE 281.8937601489229 Test RE 0.1469393705649248\n",
      "115 Train Loss 336.62106 Test MSE 277.43721589911206 Test RE 0.14577323865445888\n",
      "116 Train Loss 331.92792 Test MSE 283.9715626019345 Test RE 0.1474799118781246\n",
      "117 Train Loss 327.3166 Test MSE 296.38357966961377 Test RE 0.15066851690733346\n",
      "118 Train Loss 322.5619 Test MSE 294.50127283557777 Test RE 0.15018931340061742\n",
      "119 Train Loss 319.23373 Test MSE 294.3155635796979 Test RE 0.15014195207043304\n",
      "120 Train Loss 312.9005 Test MSE 305.5492166668977 Test RE 0.15298048419013194\n",
      "121 Train Loss 307.85626 Test MSE 304.7972121453019 Test RE 0.1527921137396508\n",
      "122 Train Loss 303.2882 Test MSE 315.2799074761079 Test RE 0.15539734375387923\n",
      "123 Train Loss 301.13373 Test MSE 314.4907050563626 Test RE 0.15520272811157368\n",
      "124 Train Loss 297.8051 Test MSE 315.0258876993752 Test RE 0.15533472962617104\n",
      "125 Train Loss 295.80066 Test MSE 314.38108055372413 Test RE 0.1551756756350773\n",
      "126 Train Loss 294.4741 Test MSE 312.84207972725056 Test RE 0.15479539117156843\n",
      "127 Train Loss 292.1709 Test MSE 312.32432723029854 Test RE 0.15466724522642541\n",
      "128 Train Loss 288.69022 Test MSE 302.9548102371443 Test RE 0.15232962398834624\n",
      "129 Train Loss 286.46893 Test MSE 307.6080773680043 Test RE 0.15349502766203585\n",
      "130 Train Loss 283.64017 Test MSE 305.56511568139905 Test RE 0.15298446424827772\n",
      "131 Train Loss 280.6798 Test MSE 309.0369438501745 Test RE 0.15385111355747635\n",
      "132 Train Loss 278.48495 Test MSE 308.50469023306147 Test RE 0.1537185677526603\n",
      "133 Train Loss 276.94034 Test MSE 311.7220281791941 Test RE 0.15451803992651772\n",
      "134 Train Loss 273.14014 Test MSE 319.5326442912901 Test RE 0.156441892318412\n",
      "135 Train Loss 270.8289 Test MSE 319.4232478773995 Test RE 0.1564151100046521\n",
      "136 Train Loss 267.69415 Test MSE 316.02749253350174 Test RE 0.1555814721178195\n",
      "137 Train Loss 265.64276 Test MSE 322.0438051361911 Test RE 0.1570554163827772\n",
      "138 Train Loss 262.05276 Test MSE 327.9254824506633 Test RE 0.15848312528607916\n",
      "139 Train Loss 257.55646 Test MSE 327.2675261114862 Test RE 0.15832405357632556\n",
      "140 Train Loss 255.04317 Test MSE 325.7242867127739 Test RE 0.15795032166374623\n",
      "141 Train Loss 252.67421 Test MSE 324.25965660497 Test RE 0.1575948071257333\n",
      "142 Train Loss 250.31853 Test MSE 322.59711821761545 Test RE 0.15719027928080118\n",
      "143 Train Loss 249.105 Test MSE 324.7072986175822 Test RE 0.15770354980263024\n",
      "144 Train Loss 245.83385 Test MSE 324.4944084785977 Test RE 0.15765184318008704\n",
      "145 Train Loss 241.98524 Test MSE 322.9332876662107 Test RE 0.15727215975532136\n",
      "146 Train Loss 238.77945 Test MSE 326.0163829760448 Test RE 0.15802112749843444\n",
      "147 Train Loss 234.11957 Test MSE 318.3071177715063 Test RE 0.1561415976887997\n",
      "148 Train Loss 230.34062 Test MSE 321.3548448912404 Test RE 0.15688732920005533\n",
      "149 Train Loss 227.57338 Test MSE 320.17106249613823 Test RE 0.15659809782120343\n",
      "150 Train Loss 224.939 Test MSE 315.6646922207135 Test RE 0.1554921425228662\n",
      "151 Train Loss 223.03647 Test MSE 313.75600665862606 Test RE 0.15502133345033248\n",
      "152 Train Loss 220.20805 Test MSE 313.9431337335967 Test RE 0.15506755466059738\n",
      "153 Train Loss 218.44678 Test MSE 320.81242912665243 Test RE 0.15675486799452076\n",
      "154 Train Loss 216.65532 Test MSE 321.68143002464126 Test RE 0.15696702934930312\n",
      "155 Train Loss 214.6154 Test MSE 321.78406430012194 Test RE 0.15699206796052015\n",
      "156 Train Loss 213.30829 Test MSE 320.86263176006685 Test RE 0.1567671324808946\n",
      "157 Train Loss 210.61557 Test MSE 336.38075605219046 Test RE 0.16051329685244808\n",
      "158 Train Loss 208.10168 Test MSE 345.0371623034481 Test RE 0.16256549864467273\n",
      "159 Train Loss 205.76965 Test MSE 341.4058693697951 Test RE 0.1617077876061757\n",
      "160 Train Loss 204.3443 Test MSE 338.3375110653958 Test RE 0.16097947952104155\n",
      "161 Train Loss 202.6081 Test MSE 339.2407381063932 Test RE 0.16119421200375492\n",
      "162 Train Loss 199.57156 Test MSE 338.2114655550817 Test RE 0.160949490781792\n",
      "163 Train Loss 197.04362 Test MSE 333.60610133141256 Test RE 0.15984992478265048\n",
      "164 Train Loss 194.60439 Test MSE 334.71411719164837 Test RE 0.16011516188347527\n",
      "165 Train Loss 193.11087 Test MSE 344.71747721904546 Test RE 0.16249017080434397\n",
      "166 Train Loss 191.09941 Test MSE 340.8106961004907 Test RE 0.16156677342994272\n",
      "167 Train Loss 188.61243 Test MSE 336.2041040212626 Test RE 0.160471144138537\n",
      "168 Train Loss 185.88828 Test MSE 332.77307647385766 Test RE 0.15965022491241798\n",
      "169 Train Loss 184.54788 Test MSE 328.8115782004092 Test RE 0.1586971014689237\n",
      "170 Train Loss 183.38947 Test MSE 329.65031564187996 Test RE 0.15889937601775514\n",
      "171 Train Loss 181.87766 Test MSE 330.75866511967905 Test RE 0.15916627740718436\n",
      "172 Train Loss 179.82385 Test MSE 327.90433385810485 Test RE 0.15847801475142323\n",
      "173 Train Loss 178.59645 Test MSE 325.08197761936253 Test RE 0.15779451045128084\n",
      "174 Train Loss 177.62878 Test MSE 323.7713477683402 Test RE 0.15747609989655362\n",
      "175 Train Loss 176.25623 Test MSE 326.93067996119856 Test RE 0.15824255362033227\n",
      "176 Train Loss 175.02525 Test MSE 329.6573932966591 Test RE 0.15890108180846466\n",
      "177 Train Loss 173.64818 Test MSE 330.4612713581302 Test RE 0.1590947060354022\n",
      "178 Train Loss 172.42726 Test MSE 331.6595750325543 Test RE 0.15938289602016273\n",
      "179 Train Loss 171.1271 Test MSE 332.14149326439247 Test RE 0.1594986497137577\n",
      "180 Train Loss 170.04027 Test MSE 331.37748274770985 Test RE 0.15931510022432493\n",
      "181 Train Loss 168.3697 Test MSE 321.70502846732484 Test RE 0.1569727867682396\n",
      "182 Train Loss 166.33583 Test MSE 326.34832758710104 Test RE 0.15810155431011827\n",
      "183 Train Loss 164.91225 Test MSE 326.3138672892952 Test RE 0.15809320683287292\n",
      "184 Train Loss 163.18158 Test MSE 324.2812678577015 Test RE 0.15760005872640379\n",
      "185 Train Loss 161.81213 Test MSE 329.8926352907174 Test RE 0.15895776723687463\n",
      "186 Train Loss 160.48001 Test MSE 327.7084315577149 Test RE 0.1584306673350099\n",
      "187 Train Loss 158.79309 Test MSE 326.90600179513245 Test RE 0.15823658108555155\n",
      "188 Train Loss 157.28606 Test MSE 329.805615226467 Test RE 0.15893680067691837\n",
      "189 Train Loss 155.72165 Test MSE 336.7398681402184 Test RE 0.16059895408458916\n",
      "190 Train Loss 154.17914 Test MSE 331.39743215381463 Test RE 0.15931989565244878\n",
      "191 Train Loss 152.06474 Test MSE 333.8307335061481 Test RE 0.15990373284191303\n",
      "192 Train Loss 150.3639 Test MSE 335.77256264884863 Test RE 0.16036812312689872\n",
      "193 Train Loss 148.88641 Test MSE 337.7662453676399 Test RE 0.1608435192701023\n",
      "194 Train Loss 147.50252 Test MSE 340.21529233941277 Test RE 0.16142558138817706\n",
      "195 Train Loss 145.54848 Test MSE 341.2353015891733 Test RE 0.16166738762068256\n",
      "196 Train Loss 144.22998 Test MSE 334.2631791906863 Test RE 0.16000726929402814\n",
      "197 Train Loss 142.51694 Test MSE 336.90072020564014 Test RE 0.1606373065218067\n",
      "198 Train Loss 140.02316 Test MSE 335.6879728051644 Test RE 0.16034792140338036\n",
      "199 Train Loss 138.09668 Test MSE 340.50561826104195 Test RE 0.1614944437197915\n",
      "Training time: 171.43\n",
      "Training time: 171.43\n",
      "ES_stan_medium\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 9201.023 Test MSE 2620.5757641155533 Test RE 0.44801628852561776\n",
      "1 Train Loss 6867.352 Test MSE 1769.6339879645086 Test RE 0.36816048143983426\n",
      "2 Train Loss 5307.769 Test MSE 1199.8378606059464 Test RE 0.30314940820359637\n",
      "3 Train Loss 4536.0405 Test MSE 1184.2355089055284 Test RE 0.30117192398578724\n",
      "4 Train Loss 3907.9556 Test MSE 1010.8714426396007 Test RE 0.2782552759817193\n",
      "5 Train Loss 3425.768 Test MSE 828.1486722387616 Test RE 0.25185443259395385\n",
      "6 Train Loss 3046.0515 Test MSE 600.7967363620294 Test RE 0.21451577081037404\n",
      "7 Train Loss 2693.935 Test MSE 505.19367159749277 Test RE 0.1967090743059396\n",
      "8 Train Loss 2495.974 Test MSE 430.6623438020102 Test RE 0.18162009296417353\n",
      "9 Train Loss 2348.4983 Test MSE 401.01446861580007 Test RE 0.1752570369891094\n",
      "10 Train Loss 2218.1445 Test MSE 386.6795322982256 Test RE 0.17209610299976796\n",
      "11 Train Loss 2097.2847 Test MSE 333.06484804437196 Test RE 0.15972019930955958\n",
      "12 Train Loss 1984.9462 Test MSE 332.9926146321672 Test RE 0.15970287871770106\n",
      "13 Train Loss 1901.9629 Test MSE 330.1601971668749 Test RE 0.15902221611144687\n",
      "14 Train Loss 1862.7605 Test MSE 334.20418887894334 Test RE 0.15999314973829884\n",
      "15 Train Loss 1820.6793 Test MSE 331.79332435072956 Test RE 0.1594150301820136\n",
      "16 Train Loss 1791.3171 Test MSE 335.59188600542865 Test RE 0.16032497089195352\n",
      "17 Train Loss 1761.6278 Test MSE 322.2568031420267 Test RE 0.15710734559313116\n",
      "18 Train Loss 1724.587 Test MSE 313.5440901120871 Test RE 0.15496897248521635\n",
      "19 Train Loss 1688.16 Test MSE 300.8985546957892 Test RE 0.15181178777795953\n",
      "20 Train Loss 1644.5614 Test MSE 282.13877840452017 Test RE 0.1470032155545057\n",
      "21 Train Loss 1594.8318 Test MSE 268.44341058815235 Test RE 0.14339097499821385\n",
      "22 Train Loss 1552.3514 Test MSE 274.222806422698 Test RE 0.14492630823360086\n",
      "23 Train Loss 1515.9885 Test MSE 257.49041157127147 Test RE 0.14043519917056105\n",
      "24 Train Loss 1469.9818 Test MSE 248.12944065146692 Test RE 0.1378588311886291\n",
      "25 Train Loss 1422.1932 Test MSE 238.5066258175896 Test RE 0.13515921726964808\n",
      "26 Train Loss 1370.1505 Test MSE 240.45819137050358 Test RE 0.13571105664381608\n",
      "27 Train Loss 1324.0728 Test MSE 241.01004189713245 Test RE 0.13586669554627503\n",
      "28 Train Loss 1245.9233 Test MSE 215.94909611775077 Test RE 0.12860893417896027\n",
      "29 Train Loss 1186.4448 Test MSE 220.97990571752547 Test RE 0.1300983641474632\n",
      "30 Train Loss 1109.843 Test MSE 206.15979727938034 Test RE 0.12566010928085253\n",
      "31 Train Loss 1059.104 Test MSE 189.71316957244596 Test RE 0.12054360732247016\n",
      "32 Train Loss 995.92676 Test MSE 198.91429426365818 Test RE 0.1234321916785004\n",
      "33 Train Loss 938.4785 Test MSE 236.53273118479837 Test RE 0.13459876252096295\n",
      "34 Train Loss 897.5366 Test MSE 224.70931707298672 Test RE 0.13119158641661763\n",
      "35 Train Loss 858.35614 Test MSE 218.67454806375005 Test RE 0.12941796382365045\n",
      "36 Train Loss 831.836 Test MSE 229.51841221510927 Test RE 0.1325879968307175\n",
      "37 Train Loss 811.0928 Test MSE 259.9057235098639 Test RE 0.1410923170241571\n",
      "38 Train Loss 788.6425 Test MSE 273.7059053207637 Test RE 0.14478965310878733\n",
      "39 Train Loss 777.0969 Test MSE 271.51630156516467 Test RE 0.14420934307108949\n",
      "40 Train Loss 767.3087 Test MSE 267.0295894121179 Test RE 0.14301287513703254\n",
      "41 Train Loss 760.68634 Test MSE 260.72098910488893 Test RE 0.1413134311409325\n",
      "42 Train Loss 754.31354 Test MSE 264.22976543947357 Test RE 0.14226114945763194\n",
      "43 Train Loss 746.0282 Test MSE 253.56574697434792 Test RE 0.13936083409499894\n",
      "44 Train Loss 737.7269 Test MSE 256.661848927158 Test RE 0.14020906819238962\n",
      "45 Train Loss 730.49603 Test MSE 247.85890298079045 Test RE 0.13778365635536577\n",
      "46 Train Loss 722.90454 Test MSE 262.29087518894545 Test RE 0.14173823974512081\n",
      "47 Train Loss 715.86444 Test MSE 265.98762144770694 Test RE 0.14273357933767739\n",
      "48 Train Loss 707.55945 Test MSE 262.0004172028945 Test RE 0.14165973833428835\n",
      "49 Train Loss 700.2144 Test MSE 268.4673037477353 Test RE 0.14339735620758856\n",
      "50 Train Loss 696.21204 Test MSE 272.3855578868587 Test RE 0.14444000088448425\n",
      "51 Train Loss 692.5621 Test MSE 271.81584922322025 Test RE 0.14428886990425346\n",
      "52 Train Loss 688.45807 Test MSE 275.0415364933082 Test RE 0.14514249574635707\n",
      "53 Train Loss 684.37415 Test MSE 271.1323629243273 Test RE 0.14410734712116793\n",
      "54 Train Loss 678.9256 Test MSE 267.80112443101063 Test RE 0.14321933138575213\n",
      "55 Train Loss 675.9398 Test MSE 268.1503364309185 Test RE 0.14331267979002696\n",
      "56 Train Loss 671.0299 Test MSE 266.3506095671527 Test RE 0.14283093899879232\n",
      "57 Train Loss 667.0282 Test MSE 269.7375689333902 Test RE 0.1437362014023459\n",
      "58 Train Loss 661.0978 Test MSE 267.648636951606 Test RE 0.1431785506298619\n",
      "59 Train Loss 653.9357 Test MSE 266.37818708635575 Test RE 0.14283833305172358\n",
      "60 Train Loss 649.17596 Test MSE 269.6174849907368 Test RE 0.14370420302092937\n",
      "61 Train Loss 640.2025 Test MSE 277.159808483716 Test RE 0.14570034163728462\n",
      "62 Train Loss 633.8787 Test MSE 272.3494693819926 Test RE 0.1444304321019422\n",
      "63 Train Loss 626.1845 Test MSE 281.4082935550353 Test RE 0.14681278937631023\n",
      "64 Train Loss 618.0707 Test MSE 274.1829693717054 Test RE 0.14491578094285087\n",
      "65 Train Loss 612.1874 Test MSE 277.2570137329346 Test RE 0.145725889346602\n",
      "66 Train Loss 599.87646 Test MSE 292.203320946547 Test RE 0.14960221287889505\n",
      "67 Train Loss 592.3157 Test MSE 291.675322149546 Test RE 0.14946698939267158\n",
      "68 Train Loss 578.26996 Test MSE 272.98548262647626 Test RE 0.14459897676785818\n",
      "69 Train Loss 566.473 Test MSE 255.90209761153776 Test RE 0.14000139616695767\n",
      "70 Train Loss 557.83344 Test MSE 262.7151272314901 Test RE 0.1418528232992429\n",
      "71 Train Loss 548.0331 Test MSE 258.39060751839975 Test RE 0.14068046830026676\n",
      "72 Train Loss 541.35974 Test MSE 263.0773616936289 Test RE 0.14195058371695263\n",
      "73 Train Loss 537.652 Test MSE 250.8015140990918 Test RE 0.13859913531464269\n",
      "74 Train Loss 531.44403 Test MSE 256.94594979502335 Test RE 0.1402866459493027\n",
      "75 Train Loss 524.89984 Test MSE 254.0183989850204 Test RE 0.13948516838497824\n",
      "76 Train Loss 517.222 Test MSE 270.27069255425494 Test RE 0.1438781752073492\n",
      "77 Train Loss 512.2999 Test MSE 271.28166079066204 Test RE 0.14414701770541147\n",
      "78 Train Loss 506.66394 Test MSE 265.36566211214165 Test RE 0.14256660457804457\n",
      "79 Train Loss 503.4153 Test MSE 264.81518616616444 Test RE 0.1424186573498127\n",
      "80 Train Loss 498.76987 Test MSE 264.9061780263209 Test RE 0.14244312313997862\n",
      "81 Train Loss 495.2026 Test MSE 259.5289777217632 Test RE 0.1409900199057123\n",
      "82 Train Loss 492.16653 Test MSE 263.4693486713769 Test RE 0.1420562979987594\n",
      "83 Train Loss 490.15054 Test MSE 266.24930162647024 Test RE 0.14280377314302692\n",
      "84 Train Loss 486.73053 Test MSE 272.4027398206074 Test RE 0.1444445564112334\n",
      "85 Train Loss 483.30853 Test MSE 271.6562704143157 Test RE 0.14424650882316684\n",
      "86 Train Loss 480.31763 Test MSE 275.11684506755716 Test RE 0.14516236497492252\n",
      "87 Train Loss 478.46225 Test MSE 273.5330983699009 Test RE 0.14474393869691046\n",
      "88 Train Loss 475.21368 Test MSE 269.6263221243526 Test RE 0.14370655806633278\n",
      "89 Train Loss 471.94855 Test MSE 274.32317840767615 Test RE 0.14495282902261247\n",
      "90 Train Loss 467.0851 Test MSE 287.1951138228171 Test RE 0.14831462130793593\n",
      "91 Train Loss 463.76776 Test MSE 281.89602655492655 Test RE 0.14693996125483946\n",
      "92 Train Loss 461.67047 Test MSE 279.60708916743056 Test RE 0.146342184289686\n",
      "93 Train Loss 459.15997 Test MSE 276.31879442170083 Test RE 0.14547911704125913\n",
      "94 Train Loss 456.45596 Test MSE 268.57945105126953 Test RE 0.1434273038911752\n",
      "95 Train Loss 453.6128 Test MSE 274.6810589083638 Test RE 0.145047350533951\n",
      "96 Train Loss 449.8696 Test MSE 282.6141688062862 Test RE 0.1471270101372724\n",
      "97 Train Loss 445.45596 Test MSE 280.92695803432065 Test RE 0.14668717748833285\n",
      "98 Train Loss 442.71747 Test MSE 283.2772968912659 Test RE 0.1472995189833002\n",
      "99 Train Loss 441.05786 Test MSE 284.2033131405053 Test RE 0.1475400791325815\n",
      "100 Train Loss 436.9202 Test MSE 282.75624937529744 Test RE 0.14716398857819174\n",
      "101 Train Loss 434.13675 Test MSE 284.03839622755106 Test RE 0.14749726579421335\n",
      "102 Train Loss 431.13867 Test MSE 285.69961326974027 Test RE 0.14792796065375266\n",
      "103 Train Loss 428.7431 Test MSE 289.4703013897612 Test RE 0.14890094378554353\n",
      "104 Train Loss 426.48123 Test MSE 289.8577152268288 Test RE 0.14900055158197426\n",
      "105 Train Loss 422.13 Test MSE 294.15864349172045 Test RE 0.15010192117759516\n",
      "106 Train Loss 418.4732 Test MSE 287.81646749125827 Test RE 0.14847497578142219\n",
      "107 Train Loss 415.943 Test MSE 292.684000353496 Test RE 0.14972521140172296\n",
      "108 Train Loss 413.1454 Test MSE 292.87694623191544 Test RE 0.14977455489653796\n",
      "109 Train Loss 409.51578 Test MSE 290.7307837727526 Test RE 0.1492247820878044\n",
      "110 Train Loss 407.90527 Test MSE 289.3067043612111 Test RE 0.1488588614119861\n",
      "111 Train Loss 406.31592 Test MSE 287.7737863399625 Test RE 0.14846396647762203\n",
      "112 Train Loss 403.26984 Test MSE 293.60439053613635 Test RE 0.14996044367737493\n",
      "113 Train Loss 401.4222 Test MSE 296.41943293703645 Test RE 0.150677629752379\n",
      "114 Train Loss 399.50027 Test MSE 294.5056280450888 Test RE 0.15019042392808918\n",
      "115 Train Loss 397.34128 Test MSE 293.9557272205417 Test RE 0.1500501406549457\n",
      "116 Train Loss 395.83063 Test MSE 287.7595349096334 Test RE 0.14846029023925142\n",
      "117 Train Loss 394.9462 Test MSE 289.4635361061619 Test RE 0.14889920377457536\n",
      "118 Train Loss 393.90125 Test MSE 290.02266784228885 Test RE 0.14904294226827902\n",
      "119 Train Loss 392.74445 Test MSE 292.99656213506967 Test RE 0.14980513700622142\n",
      "120 Train Loss 390.7886 Test MSE 293.20770506911765 Test RE 0.1498591045332251\n",
      "121 Train Loss 389.40875 Test MSE 289.1765803034278 Test RE 0.14882538085769942\n",
      "122 Train Loss 387.1043 Test MSE 291.31572427430996 Test RE 0.14937482426243276\n",
      "123 Train Loss 384.23926 Test MSE 299.7742094960078 Test RE 0.15152789042971307\n",
      "124 Train Loss 382.05237 Test MSE 300.80205807905685 Test RE 0.1517874431967858\n",
      "125 Train Loss 379.9166 Test MSE 298.85838518371895 Test RE 0.15129625096021973\n",
      "126 Train Loss 378.15585 Test MSE 295.59369668704454 Test RE 0.1504676118904128\n",
      "127 Train Loss 375.4119 Test MSE 301.10875155131225 Test RE 0.15186480363498822\n",
      "128 Train Loss 372.5653 Test MSE 302.53949546740404 Test RE 0.1522251753419737\n",
      "129 Train Loss 370.68015 Test MSE 305.9133698562074 Test RE 0.15307161802354588\n",
      "130 Train Loss 367.957 Test MSE 306.2877775790701 Test RE 0.15316526164376315\n",
      "131 Train Loss 365.2938 Test MSE 305.4856699594404 Test RE 0.15296457527750673\n",
      "132 Train Loss 362.7779 Test MSE 302.39438223154195 Test RE 0.1521886635185644\n",
      "133 Train Loss 360.75543 Test MSE 310.16157722398304 Test RE 0.15413080336305812\n",
      "134 Train Loss 359.1665 Test MSE 306.9931789475573 Test RE 0.15334153516336946\n",
      "135 Train Loss 357.84183 Test MSE 307.6396719734077 Test RE 0.15350291024163443\n",
      "136 Train Loss 356.75394 Test MSE 303.82092836199183 Test RE 0.15254721631689724\n",
      "137 Train Loss 355.23257 Test MSE 302.3447210877374 Test RE 0.15217616630652242\n",
      "138 Train Loss 354.18417 Test MSE 302.8520864774751 Test RE 0.15230379637672986\n",
      "139 Train Loss 350.50604 Test MSE 308.61588085465957 Test RE 0.15374626672101735\n",
      "140 Train Loss 348.71625 Test MSE 304.25706351467556 Test RE 0.1526566678687755\n",
      "141 Train Loss 347.3886 Test MSE 306.18006737576707 Test RE 0.15313832796680663\n",
      "142 Train Loss 345.70206 Test MSE 300.08530377141045 Test RE 0.1516064949831417\n",
      "143 Train Loss 344.54642 Test MSE 297.80850225622623 Test RE 0.15103026695636948\n",
      "144 Train Loss 342.4278 Test MSE 299.25463285345114 Test RE 0.15139651739285725\n",
      "145 Train Loss 340.60895 Test MSE 298.9757392178986 Test RE 0.15132595312623404\n",
      "146 Train Loss 339.02594 Test MSE 303.02181200202557 Test RE 0.15234646773707844\n",
      "147 Train Loss 336.96988 Test MSE 311.45249385025403 Test RE 0.15445122250718815\n",
      "148 Train Loss 335.3783 Test MSE 308.40865795072125 Test RE 0.15369464089784834\n",
      "149 Train Loss 334.60028 Test MSE 309.5173288404828 Test RE 0.15397064468123872\n",
      "150 Train Loss 333.59854 Test MSE 308.5989888940736 Test RE 0.15374205904478094\n",
      "151 Train Loss 332.6256 Test MSE 310.5760391354218 Test RE 0.15423374973925216\n",
      "152 Train Loss 331.54645 Test MSE 308.7866989206897 Test RE 0.15378880990900567\n",
      "153 Train Loss 330.56427 Test MSE 310.7336263914182 Test RE 0.154272874121014\n",
      "154 Train Loss 328.82007 Test MSE 307.89122740271347 Test RE 0.15356565670041258\n",
      "155 Train Loss 328.0398 Test MSE 303.74625149583966 Test RE 0.15252846769381287\n",
      "156 Train Loss 327.36792 Test MSE 309.066950031186 Test RE 0.15385858252259405\n",
      "157 Train Loss 326.5137 Test MSE 311.4645623559609 Test RE 0.15445421490159883\n",
      "158 Train Loss 325.55862 Test MSE 313.52960080716645 Test RE 0.15496539177885377\n",
      "159 Train Loss 324.61337 Test MSE 314.07458620882517 Test RE 0.1551000157632221\n",
      "160 Train Loss 323.8434 Test MSE 317.66681509992065 Test RE 0.15598447237528448\n",
      "161 Train Loss 322.279 Test MSE 314.3509000950532 Test RE 0.15516822705474215\n",
      "162 Train Loss 321.21976 Test MSE 312.59341208238277 Test RE 0.15473385811701038\n",
      "163 Train Loss 320.42108 Test MSE 312.54868506525924 Test RE 0.15472278777573867\n",
      "164 Train Loss 318.97983 Test MSE 314.0152233749647 Test RE 0.15508535744323687\n",
      "165 Train Loss 317.62848 Test MSE 317.0999930207162 Test RE 0.15584524644022446\n",
      "166 Train Loss 316.3024 Test MSE 321.5578948688019 Test RE 0.15693688647089357\n",
      "167 Train Loss 315.1373 Test MSE 323.57601562707544 Test RE 0.15742858984845542\n",
      "168 Train Loss 314.04828 Test MSE 316.54728277362847 Test RE 0.15570936685340422\n",
      "169 Train Loss 312.8507 Test MSE 314.1796244838155 Test RE 0.1551259492149138\n",
      "170 Train Loss 311.88092 Test MSE 314.78916510958203 Test RE 0.15527635640773707\n",
      "171 Train Loss 310.5222 Test MSE 313.58624083539166 Test RE 0.15497938861961275\n",
      "172 Train Loss 309.94904 Test MSE 311.833396205139 Test RE 0.15454563956786882\n",
      "173 Train Loss 308.35187 Test MSE 312.50284103492737 Test RE 0.154711440141538\n",
      "174 Train Loss 307.56137 Test MSE 309.92397786718584 Test RE 0.15407175608161933\n",
      "175 Train Loss 306.7765 Test MSE 310.73614295310114 Test RE 0.15427349883038677\n",
      "176 Train Loss 305.87112 Test MSE 307.8887696503616 Test RE 0.15356504377761138\n",
      "177 Train Loss 304.9612 Test MSE 313.3913559865606 Test RE 0.1549312235140637\n",
      "178 Train Loss 303.84665 Test MSE 316.3631863722904 Test RE 0.15566408182860098\n",
      "179 Train Loss 301.5935 Test MSE 317.5149492960041 Test RE 0.15594718245698047\n",
      "180 Train Loss 299.95187 Test MSE 313.06121323310066 Test RE 0.15484959570634718\n",
      "181 Train Loss 299.21503 Test MSE 311.8032738959901 Test RE 0.15453817503068867\n",
      "182 Train Loss 298.39102 Test MSE 310.28813135731986 Test RE 0.15416224487991304\n",
      "183 Train Loss 297.62946 Test MSE 312.13936551423535 Test RE 0.15462144066803282\n",
      "184 Train Loss 296.84033 Test MSE 312.8612684344768 Test RE 0.1548001384196012\n",
      "185 Train Loss 295.48636 Test MSE 311.18099128062136 Test RE 0.15438388792587857\n",
      "186 Train Loss 294.39713 Test MSE 305.604665124583 Test RE 0.15299436435516295\n",
      "187 Train Loss 293.16473 Test MSE 310.6367754476464 Test RE 0.15424882999346087\n",
      "188 Train Loss 291.86002 Test MSE 309.60967656034563 Test RE 0.15399361234122233\n",
      "189 Train Loss 290.81104 Test MSE 308.68252440957167 Test RE 0.15376286606880607\n",
      "190 Train Loss 289.99396 Test MSE 305.7045467292333 Test RE 0.15301936409494596\n",
      "191 Train Loss 289.1515 Test MSE 305.7401283726403 Test RE 0.15302826897072452\n",
      "192 Train Loss 287.89636 Test MSE 308.2576326614007 Test RE 0.15365700476254168\n",
      "193 Train Loss 286.17334 Test MSE 308.7579282061247 Test RE 0.15378164522698154\n",
      "194 Train Loss 283.7359 Test MSE 312.11209211559293 Test RE 0.15461468544139867\n",
      "195 Train Loss 282.26422 Test MSE 313.557731551863 Test RE 0.15497234358513323\n",
      "196 Train Loss 281.41327 Test MSE 313.3950025605311 Test RE 0.15493212488947283\n",
      "197 Train Loss 280.63483 Test MSE 310.4882228381199 Test RE 0.15421194317231213\n",
      "198 Train Loss 280.0566 Test MSE 310.8573270409547 Test RE 0.15430357848357423\n",
      "199 Train Loss 279.4722 Test MSE 309.0694396267037 Test RE 0.15385920220202914\n",
      "Training time: 173.13\n",
      "Training time: 173.13\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init =1.0\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1541 into shape (500,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25497/2850883497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1541 into shape (500,500)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(u_pred.reshape(500,500)),vmin = 0,vmax = 500,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_loss[-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
