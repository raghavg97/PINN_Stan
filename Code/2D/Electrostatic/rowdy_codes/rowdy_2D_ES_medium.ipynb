{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_medium\"\n",
    "label = \"ES_rowdy\" + level\n",
    "ES_val = 200.0\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA' + level + '.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = ES_val*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_rowdy_medium\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 18892.32 Test MSE 11918.513954017657 Test RE 0.9554467741281972\n",
      "1 Train Loss 16132.558 Test MSE 9092.459970646149 Test RE 0.8345188342188734\n",
      "2 Train Loss 11191.358 Test MSE 4083.355755541488 Test RE 0.5592475023767522\n",
      "3 Train Loss 10004.26 Test MSE 3002.8250430961884 Test RE 0.47957933196450553\n",
      "4 Train Loss 9996.266 Test MSE 3001.5737742094148 Test RE 0.4794794018629443\n",
      "5 Train Loss 9986.961 Test MSE 2994.103375712531 Test RE 0.47888235945549\n",
      "6 Train Loss 9963.353 Test MSE 2985.8259564909235 Test RE 0.4782199485475994\n",
      "7 Train Loss 9912.267 Test MSE 2961.1294464312896 Test RE 0.4762381038116287\n",
      "8 Train Loss 9794.129 Test MSE 2908.299419097481 Test RE 0.4719706605282295\n",
      "9 Train Loss 9612.099 Test MSE 2848.2304840702327 Test RE 0.4670711137637552\n",
      "10 Train Loss 9242.32 Test MSE 2707.650504606712 Test RE 0.4553986584707365\n",
      "11 Train Loss 8925.987 Test MSE 2581.5127119220406 Test RE 0.44466462190806116\n",
      "12 Train Loss 8780.842 Test MSE 2496.6421389257516 Test RE 0.4372940739679121\n",
      "13 Train Loss 8368.948 Test MSE 2390.260371022142 Test RE 0.4278761204720888\n",
      "14 Train Loss 7981.557 Test MSE 2189.9555665689477 Test RE 0.4095558139466811\n",
      "15 Train Loss 7604.895 Test MSE 2116.174678912628 Test RE 0.4025976175939424\n",
      "16 Train Loss 7249.4517 Test MSE 1954.4858277844498 Test RE 0.38691156092974094\n",
      "17 Train Loss 7051.7104 Test MSE 1894.3346698173293 Test RE 0.3809112484113706\n",
      "18 Train Loss 6617.158 Test MSE 1717.5953791437944 Test RE 0.3627069482954506\n",
      "19 Train Loss 6407.051 Test MSE 1609.9268728709428 Test RE 0.3511547267552288\n",
      "20 Train Loss 6258.2695 Test MSE 1587.8476779515604 Test RE 0.3487384740870266\n",
      "21 Train Loss 6031.2183 Test MSE 1488.2005048571466 Test RE 0.3376184467977831\n",
      "22 Train Loss 5867.463 Test MSE 1458.2724408450574 Test RE 0.33420641207491275\n",
      "23 Train Loss 5766.488 Test MSE 1448.6079510190868 Test RE 0.3330971187540992\n",
      "24 Train Loss 5646.7817 Test MSE 1420.111731926096 Test RE 0.3298045944652633\n",
      "25 Train Loss 5486.417 Test MSE 1351.1560205051223 Test RE 0.32169787650303056\n",
      "26 Train Loss 5310.317 Test MSE 1271.3755343244968 Test RE 0.3120558788730992\n",
      "27 Train Loss 4990.4146 Test MSE 1209.2103055717855 Test RE 0.3043311196030339\n",
      "28 Train Loss 4869.5 Test MSE 1236.1468281744937 Test RE 0.307702109223033\n",
      "29 Train Loss 4747.4263 Test MSE 1172.67894295895 Test RE 0.2996988022301893\n",
      "30 Train Loss 4688.3853 Test MSE 1128.7959047252273 Test RE 0.2940377943793576\n",
      "31 Train Loss 4621.652 Test MSE 1151.0002292155934 Test RE 0.29691569077194846\n",
      "32 Train Loss 4529.9116 Test MSE 1138.9221853763074 Test RE 0.29535373700876094\n",
      "33 Train Loss 4422.276 Test MSE 1064.6429626850559 Test RE 0.28556004234776994\n",
      "34 Train Loss 4352.0894 Test MSE 997.1315416281885 Test RE 0.27635776449022403\n",
      "35 Train Loss 4264.049 Test MSE 973.9158835617873 Test RE 0.2731216756245944\n",
      "36 Train Loss 4182.4736 Test MSE 933.5030104507683 Test RE 0.26739501421039724\n",
      "37 Train Loss 4117.0605 Test MSE 936.3791734805778 Test RE 0.26780662519114445\n",
      "38 Train Loss 3945.2527 Test MSE 852.078442573122 Test RE 0.25546724984589836\n",
      "39 Train Loss 3861.9604 Test MSE 844.5804385927631 Test RE 0.25434075319351057\n",
      "40 Train Loss 3682.4565 Test MSE 773.249106710428 Test RE 0.24336334008227553\n",
      "41 Train Loss 3620.204 Test MSE 734.0699283342295 Test RE 0.23711780194377255\n",
      "42 Train Loss 3564.142 Test MSE 733.887495445336 Test RE 0.23708833555921385\n",
      "43 Train Loss 3531.8276 Test MSE 724.6428203563158 Test RE 0.23559031922490709\n",
      "44 Train Loss 3496.8462 Test MSE 693.3904777257409 Test RE 0.2304540688987847\n",
      "45 Train Loss 3427.1406 Test MSE 668.6781747890727 Test RE 0.22631014245138964\n",
      "46 Train Loss 3302.3386 Test MSE 621.6630685626675 Test RE 0.21820916011834943\n",
      "47 Train Loss 3252.1538 Test MSE 586.788907270742 Test RE 0.21200025898078262\n",
      "48 Train Loss 3208.8838 Test MSE 606.4364176511922 Test RE 0.21552024924109572\n",
      "49 Train Loss 3156.5823 Test MSE 586.8045168013177 Test RE 0.21200307873629962\n",
      "50 Train Loss 3121.502 Test MSE 564.1682070198914 Test RE 0.20787379657733301\n",
      "51 Train Loss 3072.0894 Test MSE 566.624047655411 Test RE 0.20832574561980194\n",
      "52 Train Loss 3032.757 Test MSE 545.3313216774726 Test RE 0.20437400945415296\n",
      "53 Train Loss 2948.6724 Test MSE 515.8409116421182 Test RE 0.1987711431892044\n",
      "54 Train Loss 2926.0864 Test MSE 513.4457020248311 Test RE 0.1983091281587317\n",
      "55 Train Loss 2882.1719 Test MSE 506.5772566809126 Test RE 0.19697825587510545\n",
      "56 Train Loss 2850.7183 Test MSE 485.67509086168906 Test RE 0.19287163357695197\n",
      "57 Train Loss 2829.3862 Test MSE 488.5896735209544 Test RE 0.19344948849055763\n",
      "58 Train Loss 2808.9277 Test MSE 467.2113855241959 Test RE 0.18916995155309044\n",
      "59 Train Loss 2783.5972 Test MSE 451.78066370256914 Test RE 0.1860198386616876\n",
      "60 Train Loss 2735.8533 Test MSE 432.24215572889915 Test RE 0.18195290931112132\n",
      "61 Train Loss 2687.9844 Test MSE 417.1110629964171 Test RE 0.17873981257188218\n",
      "62 Train Loss 2673.7383 Test MSE 415.05265385980914 Test RE 0.17829823389796437\n",
      "63 Train Loss 2651.0254 Test MSE 409.7512610768284 Test RE 0.17715588882094302\n",
      "64 Train Loss 2632.4248 Test MSE 406.0332053801918 Test RE 0.17635030682953537\n",
      "65 Train Loss 2592.3975 Test MSE 401.80331554725956 Test RE 0.17542932884401263\n",
      "66 Train Loss 2567.1887 Test MSE 406.7558841249095 Test RE 0.1765071757247063\n",
      "67 Train Loss 2548.7593 Test MSE 399.4097638612211 Test RE 0.174906030060288\n",
      "68 Train Loss 2517.5237 Test MSE 383.55220993864145 Test RE 0.17139876507552543\n",
      "69 Train Loss 2496.1057 Test MSE 372.85344717255225 Test RE 0.16899136937353973\n",
      "70 Train Loss 2460.485 Test MSE 360.4667815534041 Test RE 0.1661606063266681\n",
      "71 Train Loss 2410.8137 Test MSE 358.7097626839728 Test RE 0.16575515433478\n",
      "72 Train Loss 2387.729 Test MSE 364.616467649679 Test RE 0.1671142882122608\n",
      "73 Train Loss 2364.9277 Test MSE 342.4661557572141 Test RE 0.1619586966658837\n",
      "74 Train Loss 2324.8115 Test MSE 333.09310849131765 Test RE 0.1597269752700091\n",
      "75 Train Loss 2286.8936 Test MSE 323.3985854154432 Test RE 0.15738542160225477\n",
      "76 Train Loss 2276.27 Test MSE 321.5255686143348 Test RE 0.15692899783043943\n",
      "77 Train Loss 2267.0544 Test MSE 315.4725630568526 Test RE 0.15544481521126527\n",
      "78 Train Loss 2248.595 Test MSE 308.73625261936314 Test RE 0.15377624720219796\n",
      "79 Train Loss 2226.4177 Test MSE 302.13716810977905 Test RE 0.15212392454953988\n",
      "80 Train Loss 2188.157 Test MSE 290.3040959562593 Test RE 0.14911523783041558\n",
      "81 Train Loss 2176.4248 Test MSE 288.6611962654186 Test RE 0.14869269993359577\n",
      "82 Train Loss 2161.5254 Test MSE 285.69906868688923 Test RE 0.14792781966814186\n",
      "83 Train Loss 2146.6323 Test MSE 283.16357097958075 Test RE 0.14726994821656206\n",
      "84 Train Loss 2140.1313 Test MSE 282.5411736730935 Test RE 0.14710800852581052\n",
      "85 Train Loss 2134.9458 Test MSE 284.4281774472906 Test RE 0.14759843513388393\n",
      "86 Train Loss 2131.226 Test MSE 282.9538952514752 Test RE 0.1472154132142214\n",
      "87 Train Loss 2120.1824 Test MSE 283.4112894434658 Test RE 0.14733435182255056\n",
      "88 Train Loss 2094.3547 Test MSE 278.9027164883224 Test RE 0.14615773897250006\n",
      "89 Train Loss 2078.7302 Test MSE 276.3853108154887 Test RE 0.14549662609990385\n",
      "90 Train Loss 2052.9814 Test MSE 269.8448689588102 Test RE 0.14376478726942696\n",
      "91 Train Loss 2023.8104 Test MSE 262.7108235230584 Test RE 0.14185166140251781\n",
      "92 Train Loss 2000.6082 Test MSE 255.1154790791538 Test RE 0.1397860551153778\n",
      "93 Train Loss 1986.3567 Test MSE 253.42674165951016 Test RE 0.1393226298964667\n",
      "94 Train Loss 1967.065 Test MSE 251.28152638523724 Test RE 0.1387317052587428\n",
      "95 Train Loss 1958.9768 Test MSE 250.36684163833098 Test RE 0.13847897784007468\n",
      "96 Train Loss 1950.4985 Test MSE 248.84408726308763 Test RE 0.13805721456123024\n",
      "97 Train Loss 1937.692 Test MSE 245.34418125766567 Test RE 0.1370829131444573\n",
      "98 Train Loss 1929.7988 Test MSE 244.04493688153585 Test RE 0.13671946327260157\n",
      "99 Train Loss 1924.2295 Test MSE 244.33714976334286 Test RE 0.13680129090013932\n",
      "100 Train Loss 1917.1227 Test MSE 243.76382302077687 Test RE 0.13664069742945836\n",
      "101 Train Loss 1906.8007 Test MSE 242.39874856255094 Test RE 0.13625756716940718\n",
      "102 Train Loss 1901.1545 Test MSE 242.44134994959356 Test RE 0.13626954022283208\n",
      "103 Train Loss 1889.1638 Test MSE 242.51352596648977 Test RE 0.1362898227792875\n",
      "104 Train Loss 1875.0271 Test MSE 241.33316602552625 Test RE 0.13595774383249737\n",
      "105 Train Loss 1861.1334 Test MSE 241.44492395021044 Test RE 0.13598922022855373\n",
      "106 Train Loss 1851.8822 Test MSE 239.20028092686172 Test RE 0.1353556181317359\n",
      "107 Train Loss 1846.579 Test MSE 240.25877116270746 Test RE 0.13565477005930185\n",
      "108 Train Loss 1839.9783 Test MSE 238.99176756332326 Test RE 0.13529660982193595\n",
      "109 Train Loss 1834.5353 Test MSE 239.77336525607322 Test RE 0.13551766597206702\n",
      "110 Train Loss 1831.8267 Test MSE 239.21093741468883 Test RE 0.1353586331771753\n",
      "111 Train Loss 1822.3973 Test MSE 240.91429246544214 Test RE 0.13583970403355392\n",
      "112 Train Loss 1815.7391 Test MSE 240.31256326597872 Test RE 0.13566995524269043\n",
      "113 Train Loss 1809.3912 Test MSE 240.84756417884734 Test RE 0.13582089033355427\n",
      "114 Train Loss 1797.6514 Test MSE 240.06853339927574 Test RE 0.13560105345531448\n",
      "115 Train Loss 1787.6975 Test MSE 239.35146100598573 Test RE 0.1353983853072549\n",
      "116 Train Loss 1783.4508 Test MSE 238.9015957751868 Test RE 0.13527108365352533\n",
      "117 Train Loss 1781.6962 Test MSE 239.38067739723112 Test RE 0.13540664874436173\n",
      "118 Train Loss 1779.3508 Test MSE 238.90905148463136 Test RE 0.1352731944264235\n",
      "119 Train Loss 1772.4791 Test MSE 240.07807479779538 Test RE 0.1356037481250762\n",
      "120 Train Loss 1767.8766 Test MSE 239.33953483848583 Test RE 0.13539501201692472\n",
      "121 Train Loss 1763.5834 Test MSE 238.96387169193966 Test RE 0.13528871346842186\n",
      "122 Train Loss 1762.0753 Test MSE 239.78330145098377 Test RE 0.13552047386524513\n",
      "123 Train Loss 1758.8282 Test MSE 240.4013876655918 Test RE 0.13569502611041098\n",
      "124 Train Loss 1752.8412 Test MSE 239.5818751766749 Test RE 0.1354635409615065\n",
      "125 Train Loss 1747.1163 Test MSE 239.81556715361805 Test RE 0.13552959150645602\n",
      "126 Train Loss 1743.9108 Test MSE 238.6059350246518 Test RE 0.1351873530870928\n",
      "127 Train Loss 1740.5692 Test MSE 238.6662338330437 Test RE 0.13520443380505362\n",
      "128 Train Loss 1738.5503 Test MSE 238.37167245592806 Test RE 0.13512097352779506\n",
      "129 Train Loss 1736.4744 Test MSE 238.44806831862152 Test RE 0.1351426242891591\n",
      "130 Train Loss 1733.903 Test MSE 238.52521498900293 Test RE 0.13516448431180106\n",
      "131 Train Loss 1732.0 Test MSE 238.95523372122904 Test RE 0.1352862682651009\n",
      "132 Train Loss 1729.9473 Test MSE 239.5881033768118 Test RE 0.13546530171356805\n",
      "133 Train Loss 1729.3892 Test MSE 239.5800226588214 Test RE 0.13546301723843057\n",
      "134 Train Loss 1727.7194 Test MSE 240.74948443707945 Test RE 0.13579323251943068\n",
      "135 Train Loss 1726.8472 Test MSE 240.65452914786582 Test RE 0.1357664504115186\n",
      "136 Train Loss 1726.2681 Test MSE 240.52053834911115 Test RE 0.13572864936109122\n",
      "137 Train Loss 1725.2158 Test MSE 240.34770548723327 Test RE 0.13567987476006682\n",
      "138 Train Loss 1724.4795 Test MSE 240.42588320028065 Test RE 0.13570193921017215\n",
      "139 Train Loss 1724.055 Test MSE 240.12863735004117 Test RE 0.13561802704378792\n",
      "140 Train Loss 1723.3438 Test MSE 239.9944200916836 Test RE 0.13558012064458325\n",
      "141 Train Loss 1722.5743 Test MSE 239.64741329479054 Test RE 0.13548206786065908\n",
      "142 Train Loss 1722.4495 Test MSE 239.73878263585456 Test RE 0.13550789273272404\n",
      "143 Train Loss 1722.0773 Test MSE 239.83243418054653 Test RE 0.1355343575462568\n",
      "144 Train Loss 1721.381 Test MSE 239.58441924251565 Test RE 0.13546426018797578\n",
      "145 Train Loss 1720.3358 Test MSE 239.72397020816481 Test RE 0.13550370644328552\n",
      "146 Train Loss 1718.1907 Test MSE 239.62129722249844 Test RE 0.13547468544030994\n",
      "147 Train Loss 1714.8485 Test MSE 240.8732818070205 Test RE 0.13582814159625126\n",
      "148 Train Loss 1713.1726 Test MSE 241.72147230623005 Test RE 0.13606707821787686\n",
      "149 Train Loss 1712.489 Test MSE 242.11920586190453 Test RE 0.13617897601228082\n",
      "150 Train Loss 1712.1517 Test MSE 242.47622574175583 Test RE 0.1362793412267885\n",
      "151 Train Loss 1711.6663 Test MSE 243.66342735531427 Test RE 0.1366125563658182\n",
      "152 Train Loss 1711.5236 Test MSE 243.34989733458062 Test RE 0.1365246360624745\n",
      "153 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "154 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "155 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "156 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "157 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "158 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "159 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "160 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "161 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "162 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "163 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "164 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "165 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "166 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "167 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "168 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "169 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "170 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "171 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "172 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "173 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "174 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "175 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "176 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "177 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "178 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "179 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "180 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "181 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "182 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "183 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "184 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "185 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "186 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "187 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "188 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "189 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "190 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "191 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "192 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "193 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "194 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "195 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "196 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "197 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "198 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "199 Train Loss 1711.4674 Test MSE 243.28884894111047 Test RE 0.13650751024369123\n",
      "Training time: 519.64\n",
      "Training time: 519.64\n",
      "ES_rowdy_medium\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 18269.254 Test MSE 10978.877182048103 Test RE 0.9170107084523678\n",
      "1 Train Loss 15680.03 Test MSE 8662.17293563763 Test RE 0.8145333465511148\n",
      "2 Train Loss 10471.946 Test MSE 3434.712130532453 Test RE 0.5129093496860728\n",
      "3 Train Loss 9996.1455 Test MSE 3000.081987380552 Test RE 0.4793602360508962\n",
      "4 Train Loss 9980.573 Test MSE 2993.070325003097 Test RE 0.47879973832009814\n",
      "5 Train Loss 9921.115 Test MSE 2973.660478867131 Test RE 0.47724472221660064\n",
      "6 Train Loss 9663.163 Test MSE 2881.949293069922 Test RE 0.4698276926680405\n",
      "7 Train Loss 8897.209 Test MSE 2523.148537241034 Test RE 0.43960928122854287\n",
      "8 Train Loss 8340.858 Test MSE 2301.5093643754867 Test RE 0.41985740475740907\n",
      "9 Train Loss 7882.63 Test MSE 2124.0955368743525 Test RE 0.4033503768324484\n",
      "10 Train Loss 7361.804 Test MSE 1993.6922528967343 Test RE 0.39077295990944094\n",
      "11 Train Loss 7113.9663 Test MSE 1871.4858840715676 Test RE 0.37860707182625725\n",
      "12 Train Loss 6832.912 Test MSE 1718.2117722510807 Test RE 0.362772024742171\n",
      "13 Train Loss 6390.6724 Test MSE 1603.029399147745 Test RE 0.350401686239029\n",
      "14 Train Loss 6092.399 Test MSE 1497.3075581589994 Test RE 0.3386499004200091\n",
      "15 Train Loss 5650.834 Test MSE 1236.6692051558543 Test RE 0.3077671174881087\n",
      "16 Train Loss 5147.782 Test MSE 1111.032936220471 Test RE 0.29171510042337007\n",
      "17 Train Loss 4840.3394 Test MSE 975.6772355575426 Test RE 0.2733685378583481\n",
      "18 Train Loss 4540.7812 Test MSE 906.8268847795883 Test RE 0.26354673329086603\n",
      "19 Train Loss 4367.388 Test MSE 849.6189508732396 Test RE 0.2550982852002698\n",
      "20 Train Loss 4241.788 Test MSE 819.6281168425301 Test RE 0.2505554581363447\n",
      "21 Train Loss 4068.8696 Test MSE 749.4896789958974 Test RE 0.23959528767480545\n",
      "22 Train Loss 3823.6387 Test MSE 690.7968985827015 Test RE 0.23002266635708865\n",
      "23 Train Loss 3670.9817 Test MSE 644.5782316905455 Test RE 0.22219447777340748\n",
      "24 Train Loss 3470.2617 Test MSE 602.7305692642964 Test RE 0.21486073304874231\n",
      "25 Train Loss 3208.1475 Test MSE 544.1755090084 Test RE 0.20415731241422563\n",
      "26 Train Loss 3120.5322 Test MSE 503.72175881637423 Test RE 0.19642230328936017\n",
      "27 Train Loss 3006.8308 Test MSE 517.267663867328 Test RE 0.19904584158603333\n",
      "28 Train Loss 2758.913 Test MSE 462.98304246267287 Test RE 0.18831199574839513\n",
      "29 Train Loss 2618.197 Test MSE 385.4971788857943 Test RE 0.17183279167820267\n",
      "30 Train Loss 2496.5544 Test MSE 416.32764381313234 Test RE 0.1785718788695995\n",
      "31 Train Loss 2366.6233 Test MSE 365.766189052565 Test RE 0.1673775561494565\n",
      "32 Train Loss 2180.5535 Test MSE 309.00092784359094 Test RE 0.15384214818249084\n",
      "33 Train Loss 2127.2017 Test MSE 307.64831784008555 Test RE 0.15350506723972837\n",
      "34 Train Loss 2091.1816 Test MSE 296.50025002466936 Test RE 0.15069816905572464\n",
      "35 Train Loss 2069.7937 Test MSE 295.31431117288855 Test RE 0.15039648654229487\n",
      "36 Train Loss 2026.6456 Test MSE 290.7386263009357 Test RE 0.14922679476023593\n",
      "37 Train Loss 1986.7119 Test MSE 288.77986213513253 Test RE 0.14872325986988363\n",
      "38 Train Loss 1953.754 Test MSE 286.08853470823834 Test RE 0.14802861320326574\n",
      "39 Train Loss 1925.6378 Test MSE 281.63785056351713 Test RE 0.146872657964553\n",
      "40 Train Loss 1898.9939 Test MSE 275.1770370655277 Test RE 0.14517824392793433\n",
      "41 Train Loss 1882.9059 Test MSE 277.1106949069064 Test RE 0.14568743178935645\n",
      "42 Train Loss 1877.7952 Test MSE 277.3237434053494 Test RE 0.14574342480169314\n",
      "43 Train Loss 1864.0554 Test MSE 276.051681662111 Test RE 0.14540878392438839\n",
      "44 Train Loss 1849.5714 Test MSE 272.6682368068155 Test RE 0.14451493062008247\n",
      "45 Train Loss 1836.5862 Test MSE 279.8792130395307 Test RE 0.1464133797613765\n",
      "46 Train Loss 1813.4957 Test MSE 279.8923406459617 Test RE 0.14641681344740687\n",
      "47 Train Loss 1807.0892 Test MSE 280.4114518313165 Test RE 0.14655252883718628\n",
      "48 Train Loss 1785.3693 Test MSE 268.594546683017 Test RE 0.14343133453337711\n",
      "49 Train Loss 1780.5055 Test MSE 267.85808163625995 Test RE 0.14323456085721112\n",
      "50 Train Loss 1775.2123 Test MSE 266.05479172081175 Test RE 0.14275160056784944\n",
      "51 Train Loss 1769.701 Test MSE 264.45568062330676 Test RE 0.14232195276446405\n",
      "52 Train Loss 1765.3054 Test MSE 261.24028629599394 Test RE 0.14145409331688635\n",
      "53 Train Loss 1759.5845 Test MSE 262.4427728551981 Test RE 0.1417792754686402\n",
      "54 Train Loss 1752.0397 Test MSE 258.71388352381825 Test RE 0.14076844441602138\n",
      "55 Train Loss 1740.8612 Test MSE 247.8103995412642 Test RE 0.13777017427313468\n",
      "56 Train Loss 1732.8047 Test MSE 245.70999333436095 Test RE 0.13718507148068348\n",
      "57 Train Loss 1721.7208 Test MSE 244.5168234415431 Test RE 0.136851580170812\n",
      "58 Train Loss 1718.6901 Test MSE 241.59856623394518 Test RE 0.1360324813813972\n",
      "59 Train Loss 1715.9772 Test MSE 240.80813112656614 Test RE 0.13580977116046217\n",
      "60 Train Loss 1712.8864 Test MSE 238.17703192448738 Test RE 0.13506579627290788\n",
      "61 Train Loss 1709.9663 Test MSE 239.14225217636368 Test RE 0.1353391988495621\n",
      "62 Train Loss 1706.5967 Test MSE 240.8658716900601 Test RE 0.13582605230235117\n",
      "63 Train Loss 1704.8418 Test MSE 242.63742968206338 Test RE 0.13632463456759988\n",
      "64 Train Loss 1700.4484 Test MSE 245.25395921715577 Test RE 0.13705770562308653\n",
      "65 Train Loss 1697.6272 Test MSE 242.54389626343007 Test RE 0.13629835639156984\n",
      "66 Train Loss 1695.1722 Test MSE 241.8381953443881 Test RE 0.1360999264497076\n",
      "67 Train Loss 1692.5671 Test MSE 241.62715025578754 Test RE 0.1360405282843169\n",
      "68 Train Loss 1692.282 Test MSE 241.76571256205105 Test RE 0.136079529256847\n",
      "69 Train Loss 1691.5342 Test MSE 241.36416341276393 Test RE 0.13596647491489877\n",
      "70 Train Loss 1690.2836 Test MSE 241.2748608117516 Test RE 0.13594131939259713\n",
      "71 Train Loss 1689.3999 Test MSE 240.71900892215237 Test RE 0.13578463748615888\n",
      "72 Train Loss 1685.6791 Test MSE 237.93211610890825 Test RE 0.13499633479360632\n",
      "73 Train Loss 1680.8534 Test MSE 237.51797471011065 Test RE 0.13487879723494625\n",
      "74 Train Loss 1677.872 Test MSE 236.97098710390452 Test RE 0.13472339940588104\n",
      "75 Train Loss 1676.1995 Test MSE 238.47974965900588 Test RE 0.13515160183572747\n",
      "76 Train Loss 1672.5955 Test MSE 238.66672146904543 Test RE 0.13520457192789315\n",
      "77 Train Loss 1663.7605 Test MSE 236.433427116115 Test RE 0.13457050510416355\n",
      "78 Train Loss 1653.5764 Test MSE 238.1996221420964 Test RE 0.13507220136864725\n",
      "79 Train Loss 1645.2106 Test MSE 233.5807297315035 Test RE 0.13375620839400407\n",
      "80 Train Loss 1642.4956 Test MSE 231.46557116667816 Test RE 0.1331492255696521\n",
      "81 Train Loss 1638.5927 Test MSE 232.84606637000107 Test RE 0.13354569622015455\n",
      "82 Train Loss 1636.8724 Test MSE 232.5963178747168 Test RE 0.13347405707785148\n",
      "83 Train Loss 1634.5243 Test MSE 230.8731193032602 Test RE 0.13297871414397483\n",
      "84 Train Loss 1629.7834 Test MSE 231.93249020174088 Test RE 0.13328345414295284\n",
      "85 Train Loss 1626.1573 Test MSE 229.71401367482892 Test RE 0.13264448223930292\n",
      "86 Train Loss 1622.6292 Test MSE 226.9880378836835 Test RE 0.13185509901358738\n",
      "87 Train Loss 1619.4501 Test MSE 226.3807759666985 Test RE 0.1316786046794102\n",
      "88 Train Loss 1618.8376 Test MSE 225.30772731713395 Test RE 0.13136615459499587\n",
      "89 Train Loss 1616.913 Test MSE 227.1707025453411 Test RE 0.13190814238907275\n",
      "90 Train Loss 1615.1067 Test MSE 228.02088488693178 Test RE 0.13215474382561074\n",
      "91 Train Loss 1613.5789 Test MSE 227.89311906980907 Test RE 0.13211771382996979\n",
      "92 Train Loss 1612.2809 Test MSE 227.91903393768328 Test RE 0.13212522549987565\n",
      "93 Train Loss 1611.4022 Test MSE 229.55111369560285 Test RE 0.13259744197570736\n",
      "94 Train Loss 1609.9285 Test MSE 230.0733698304042 Test RE 0.13274819376962554\n",
      "95 Train Loss 1608.3837 Test MSE 229.01422984081933 Test RE 0.13244228894988588\n",
      "96 Train Loss 1605.7928 Test MSE 230.54049538659822 Test RE 0.13288288695582814\n",
      "97 Train Loss 1604.2858 Test MSE 229.83195372164857 Test RE 0.13267852911504205\n",
      "98 Train Loss 1603.5441 Test MSE 228.99661828191293 Test RE 0.13243719634053056\n",
      "99 Train Loss 1603.0481 Test MSE 229.08792712967045 Test RE 0.13246359734742352\n",
      "100 Train Loss 1602.4082 Test MSE 228.9800457257663 Test RE 0.1324324039933768\n",
      "101 Train Loss 1600.9816 Test MSE 229.334846545713 Test RE 0.13253496519694904\n",
      "102 Train Loss 1599.477 Test MSE 228.274060696604 Test RE 0.13222809040102024\n",
      "103 Train Loss 1597.9385 Test MSE 227.64445374644228 Test RE 0.13204561410847945\n",
      "104 Train Loss 1596.9553 Test MSE 226.19289227551727 Test RE 0.13162395030871712\n",
      "105 Train Loss 1596.5077 Test MSE 225.8187839167837 Test RE 0.13151505653007076\n",
      "106 Train Loss 1595.7751 Test MSE 224.9127533660006 Test RE 0.1312509588816136\n",
      "107 Train Loss 1594.3656 Test MSE 224.5079161018709 Test RE 0.13113278147638577\n",
      "108 Train Loss 1591.5637 Test MSE 225.61242215496455 Test RE 0.13145495107415642\n",
      "109 Train Loss 1591.0403 Test MSE 225.89761732238188 Test RE 0.13153801049989342\n",
      "110 Train Loss 1590.323 Test MSE 225.58968165375666 Test RE 0.13144832593628386\n",
      "111 Train Loss 1589.5548 Test MSE 227.04303804228653 Test RE 0.13187107256923936\n",
      "112 Train Loss 1586.973 Test MSE 227.73854804323756 Test RE 0.1320729010802427\n",
      "113 Train Loss 1583.6902 Test MSE 226.9983266697141 Test RE 0.1318580873065844\n",
      "114 Train Loss 1581.7786 Test MSE 228.40872312208492 Test RE 0.13226708635627227\n",
      "115 Train Loss 1580.6637 Test MSE 226.60080132392181 Test RE 0.1317425800667088\n",
      "116 Train Loss 1579.5055 Test MSE 226.2946743429589 Test RE 0.13165356098945916\n",
      "117 Train Loss 1578.4165 Test MSE 223.117484667864 Test RE 0.13072608240026928\n",
      "118 Train Loss 1577.9607 Test MSE 222.2619884918957 Test RE 0.13047522112813167\n",
      "119 Train Loss 1577.5764 Test MSE 222.2142396542234 Test RE 0.1304612052929588\n",
      "120 Train Loss 1576.1748 Test MSE 221.52412215528693 Test RE 0.13025846497225113\n",
      "121 Train Loss 1573.2852 Test MSE 221.39909200217386 Test RE 0.13022170027517543\n",
      "122 Train Loss 1570.249 Test MSE 221.86262853021393 Test RE 0.13035794959877828\n",
      "123 Train Loss 1568.4764 Test MSE 220.87253537375395 Test RE 0.13006675402608533\n",
      "124 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "125 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "126 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "127 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "128 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "129 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "130 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "131 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "132 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "133 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "134 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "135 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "136 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "137 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "138 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "139 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "140 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "141 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "142 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "143 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "144 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "145 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "146 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "147 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "148 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "149 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "150 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "151 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "152 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "153 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "154 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "155 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "156 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "157 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "158 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "159 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "160 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "161 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "162 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "163 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "164 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "165 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "166 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "167 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "168 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "169 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "170 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "171 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "172 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "173 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "174 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "175 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "176 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "177 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "178 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "179 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "180 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "181 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "182 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "183 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "184 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "185 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "186 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "187 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "188 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "189 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "190 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "191 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "192 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "193 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "194 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "195 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "196 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "197 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "198 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "199 Train Loss 1568.0555 Test MSE 220.65497992744722 Test RE 0.13000268154634162\n",
      "Training time: 447.77\n",
      "Training time: 447.77\n",
      "ES_rowdy_medium\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 1053.38\n",
      "Training time: 1053.38\n",
      "ES_rowdy_medium\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 18480.76 Test MSE 11335.292247435616 Test RE 0.9317766106333748\n",
      "1 Train Loss 10473.515 Test MSE 3413.9231677275748 Test RE 0.5113547744384458\n",
      "2 Train Loss 10000.543 Test MSE 3002.0358058430334 Test RE 0.47951630352578073\n",
      "3 Train Loss 9997.398 Test MSE 3001.4924100567932 Test RE 0.4794729031555078\n",
      "4 Train Loss 9975.379 Test MSE 2983.2457794548554 Test RE 0.47801327896486145\n",
      "5 Train Loss 9960.012 Test MSE 2982.361860989069 Test RE 0.4779424574338923\n",
      "6 Train Loss 9891.698 Test MSE 2948.3716436365007 Test RE 0.4752110784337917\n",
      "7 Train Loss 9717.981 Test MSE 2870.5980874775173 Test RE 0.46890151875393593\n",
      "8 Train Loss 9546.45 Test MSE 2797.505088319269 Test RE 0.4628932910591039\n",
      "9 Train Loss 9328.96 Test MSE 2722.028660200796 Test RE 0.4566061856690765\n",
      "10 Train Loss 8960.178 Test MSE 2564.437379464641 Test RE 0.4431915721165409\n",
      "11 Train Loss 8722.801 Test MSE 2501.8362650883964 Test RE 0.4377487207156716\n",
      "12 Train Loss 8511.713 Test MSE 2416.9303096385293 Test RE 0.43025656715438665\n",
      "13 Train Loss 8278.1 Test MSE 2331.886786448565 Test RE 0.4226191523120808\n",
      "14 Train Loss 8060.851 Test MSE 2264.3130787885207 Test RE 0.4164507814516685\n",
      "15 Train Loss 7828.235 Test MSE 2163.9582552404113 Test RE 0.4071176051026883\n",
      "16 Train Loss 7550.6777 Test MSE 2065.735312986598 Test RE 0.3977706918073304\n",
      "17 Train Loss 7225.145 Test MSE 1957.953459118853 Test RE 0.387254636348177\n",
      "18 Train Loss 6688.4736 Test MSE 1825.0135234716274 Test RE 0.3738767744976982\n",
      "19 Train Loss 6381.872 Test MSE 1722.6780917142282 Test RE 0.3632432135164699\n",
      "20 Train Loss 6007.3066 Test MSE 1532.7812628750173 Test RE 0.34263800697922997\n",
      "21 Train Loss 5671.35 Test MSE 1485.8703810387453 Test RE 0.337354033178529\n",
      "22 Train Loss 5320.6416 Test MSE 1316.9201126917305 Test RE 0.31759609875916467\n",
      "23 Train Loss 4943.8877 Test MSE 1209.7169292583033 Test RE 0.30439486583767966\n",
      "24 Train Loss 4285.2666 Test MSE 868.2414810800864 Test RE 0.2578788407450365\n",
      "25 Train Loss 3912.966 Test MSE 800.7655197777958 Test RE 0.24765558482025532\n",
      "26 Train Loss 3663.375 Test MSE 706.4280635475345 Test RE 0.23261055401348363\n",
      "27 Train Loss 3287.585 Test MSE 595.2890476514303 Test RE 0.21353024087804315\n",
      "28 Train Loss 3090.0918 Test MSE 492.98013290093365 Test RE 0.19431671171560588\n",
      "29 Train Loss 2650.0623 Test MSE 397.07493490680514 Test RE 0.17439405681714173\n",
      "30 Train Loss 2553.4385 Test MSE 383.49008409097644 Test RE 0.17138488336070637\n",
      "31 Train Loss 2379.5564 Test MSE 384.75530140528195 Test RE 0.17166736858702467\n",
      "32 Train Loss 2315.629 Test MSE 386.80006046806415 Test RE 0.17212292212322092\n",
      "33 Train Loss 2247.118 Test MSE 380.33973462296774 Test RE 0.1706794730587603\n",
      "34 Train Loss 2167.699 Test MSE 364.88107340005695 Test RE 0.1671749154546513\n",
      "35 Train Loss 2131.9333 Test MSE 363.9564297234984 Test RE 0.1669629624480527\n",
      "36 Train Loss 2084.2534 Test MSE 348.453604354814 Test RE 0.16336835062088817\n",
      "37 Train Loss 2006.0847 Test MSE 323.01478506799924 Test RE 0.15729200358115802\n",
      "38 Train Loss 1971.4877 Test MSE 313.1590033225762 Test RE 0.15487377879393277\n",
      "39 Train Loss 1951.387 Test MSE 306.3154212668386 Test RE 0.1531721733742485\n",
      "40 Train Loss 1917.6155 Test MSE 297.24718700924456 Test RE 0.15088786743567661\n",
      "41 Train Loss 1902.4474 Test MSE 302.0673404478407 Test RE 0.15210634466709913\n",
      "42 Train Loss 1885.0532 Test MSE 293.58391303107015 Test RE 0.1499552140735446\n",
      "43 Train Loss 1869.7725 Test MSE 295.32259823257164 Test RE 0.15039859672773023\n",
      "44 Train Loss 1858.8684 Test MSE 288.9342798154622 Test RE 0.1487630175444609\n",
      "45 Train Loss 1851.5892 Test MSE 289.4870040527429 Test RE 0.1489052395740834\n",
      "46 Train Loss 1846.8114 Test MSE 289.6323621103577 Test RE 0.1489426192486675\n",
      "47 Train Loss 1834.3052 Test MSE 295.4409071783683 Test RE 0.15042871924092707\n",
      "48 Train Loss 1827.0355 Test MSE 292.2478163299877 Test RE 0.14961360281521394\n",
      "49 Train Loss 1820.0178 Test MSE 288.8161552047331 Test RE 0.14873260514417452\n",
      "50 Train Loss 1806.0016 Test MSE 282.91020297189584 Test RE 0.14720404665340436\n",
      "51 Train Loss 1802.6555 Test MSE 275.714372015385 Test RE 0.14531991872101901\n",
      "52 Train Loss 1787.9968 Test MSE 278.4755519379829 Test RE 0.14604576923346566\n",
      "53 Train Loss 1781.891 Test MSE 277.65690677884635 Test RE 0.14583094308611488\n",
      "54 Train Loss 1779.2216 Test MSE 274.3142691972577 Test RE 0.1449504751826134\n",
      "55 Train Loss 1775.3406 Test MSE 267.87457891842695 Test RE 0.14323897167164204\n",
      "56 Train Loss 1771.792 Test MSE 265.4953458772204 Test RE 0.14260143635625364\n",
      "57 Train Loss 1762.3008 Test MSE 264.7300946519784 Test RE 0.14239577423454397\n",
      "58 Train Loss 1760.6636 Test MSE 264.7580811231128 Test RE 0.14240330086258263\n",
      "59 Train Loss 1758.5615 Test MSE 262.4516220464356 Test RE 0.1417816657446851\n",
      "60 Train Loss 1756.7598 Test MSE 262.5104172620618 Test RE 0.14179754603669442\n",
      "61 Train Loss 1751.4602 Test MSE 256.4654365080847 Test RE 0.140155409900437\n",
      "62 Train Loss 1745.6 Test MSE 253.98726075830467 Test RE 0.13947661889815757\n",
      "63 Train Loss 1737.4045 Test MSE 251.5899719085559 Test RE 0.1388168250259322\n",
      "64 Train Loss 1734.4651 Test MSE 251.12391149696697 Test RE 0.13868818910283298\n",
      "65 Train Loss 1732.6353 Test MSE 251.06395830242707 Test RE 0.13867163294085877\n",
      "66 Train Loss 1730.9524 Test MSE 248.15122953203817 Test RE 0.1378648839239693\n",
      "67 Train Loss 1728.7001 Test MSE 245.60244253483896 Test RE 0.13715504425463784\n",
      "68 Train Loss 1725.5585 Test MSE 246.60288614187243 Test RE 0.13743410588557334\n",
      "69 Train Loss 1722.7462 Test MSE 245.33781612924153 Test RE 0.1370811349160074\n",
      "70 Train Loss 1719.7217 Test MSE 243.32572794402455 Test RE 0.13651785611470937\n",
      "71 Train Loss 1717.1416 Test MSE 243.63054541382238 Test RE 0.1366033382453678\n",
      "72 Train Loss 1714.2444 Test MSE 243.0935607922636 Test RE 0.1364527119023313\n",
      "73 Train Loss 1711.124 Test MSE 243.33536170130614 Test RE 0.1365205585968598\n",
      "74 Train Loss 1709.9803 Test MSE 242.75431908771748 Test RE 0.13635746747976177\n",
      "75 Train Loss 1709.2991 Test MSE 242.01407673604592 Test RE 0.13614940807476014\n",
      "76 Train Loss 1707.1744 Test MSE 242.27716089939986 Test RE 0.13622338935838763\n",
      "77 Train Loss 1706.4788 Test MSE 241.56581710303544 Test RE 0.1360232613421614\n",
      "78 Train Loss 1705.9061 Test MSE 241.57687747512367 Test RE 0.136026375298185\n",
      "79 Train Loss 1705.608 Test MSE 241.72015536627117 Test RE 0.13606670755900502\n",
      "80 Train Loss 1705.4303 Test MSE 241.54744626967363 Test RE 0.13601808902885942\n",
      "81 Train Loss 1704.7489 Test MSE 241.21448072540036 Test RE 0.13592430837418537\n",
      "82 Train Loss 1704.5764 Test MSE 241.16915395564953 Test RE 0.13591153696164882\n",
      "83 Train Loss 1704.3534 Test MSE 241.18839786681352 Test RE 0.13591695933289277\n",
      "84 Train Loss 1704.2714 Test MSE 241.13763840093193 Test RE 0.13590265633294246\n",
      "85 Train Loss 1704.1757 Test MSE 241.3853597672832 Test RE 0.13597244500236663\n",
      "86 Train Loss 1704.0658 Test MSE 241.52837185049387 Test RE 0.13601271841263485\n",
      "87 Train Loss 1703.3833 Test MSE 241.5135528936394 Test RE 0.13600854582308033\n",
      "88 Train Loss 1702.9526 Test MSE 241.86721297588346 Test RE 0.13610809136974195\n",
      "89 Train Loss 1702.7695 Test MSE 242.0141605779847 Test RE 0.13614943165815974\n",
      "90 Train Loss 1702.7084 Test MSE 242.17537136763096 Test RE 0.13619477012806208\n",
      "91 Train Loss 1702.6646 Test MSE 242.31884972881468 Test RE 0.1362351088893247\n",
      "92 Train Loss 1702.5996 Test MSE 242.16099988242922 Test RE 0.13619072894470036\n",
      "93 Train Loss 1702.4169 Test MSE 242.22218758072083 Test RE 0.13620793376072882\n",
      "94 Train Loss 1702.3433 Test MSE 242.50542487290667 Test RE 0.13628754639939278\n",
      "95 Train Loss 1702.2705 Test MSE 242.2742565343986 Test RE 0.13622257284801442\n",
      "96 Train Loss 1701.7446 Test MSE 242.095295701581 Test RE 0.1361722517593895\n",
      "97 Train Loss 1701.6102 Test MSE 241.8691687255013 Test RE 0.13610864165683195\n",
      "98 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "99 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "100 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "101 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "102 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "103 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "104 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "105 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "106 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "107 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "108 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "109 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "110 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "111 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "112 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "113 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "114 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "115 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "116 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "117 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "118 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "119 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "120 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "121 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "122 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "123 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "124 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "125 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "126 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "127 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "128 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "129 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "130 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "131 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "132 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "133 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "134 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "135 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "136 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "137 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "138 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "139 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "140 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "141 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "142 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "143 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "144 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "145 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "146 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "147 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "148 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "149 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "150 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "151 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "152 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "153 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "154 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "155 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "156 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "157 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "158 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "159 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "160 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "161 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "162 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "163 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "164 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "165 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "166 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "167 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "168 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "169 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "170 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "171 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "172 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "173 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "174 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "175 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "176 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "177 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "178 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "179 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "180 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "181 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "182 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "183 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "184 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "185 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "186 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "187 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "188 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "189 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "190 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "191 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "192 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "193 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "194 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "195 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "196 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "197 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "198 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "199 Train Loss 1701.6035 Test MSE 241.8518531380455 Test RE 0.13610376951193784\n",
      "Training time: 378.81\n",
      "Training time: 378.81\n",
      "ES_rowdy_medium\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 18959.076 Test MSE 11996.006781984393 Test RE 0.958547844953255\n",
      "1 Train Loss 17175.229 Test MSE 10184.065065427845 Test RE 0.8831938301580726\n",
      "2 Train Loss 13363.885 Test MSE 6364.357923569708 Test RE 0.6981885812715388\n",
      "3 Train Loss 10000.186 Test MSE 3003.283707694996 Test RE 0.47961595708580496\n",
      "4 Train Loss 9999.324 Test MSE 3002.8478607384804 Test RE 0.47958115405681895\n",
      "5 Train Loss 9959.794 Test MSE 2979.4042378189483 Test RE 0.4777054096865189\n",
      "6 Train Loss 9761.663 Test MSE 2894.655005076443 Test RE 0.4708622234234915\n",
      "7 Train Loss 9247.96 Test MSE 2697.8371360512424 Test RE 0.4545726560971968\n",
      "8 Train Loss 7969.983 Test MSE 2203.0210036105186 Test RE 0.4107757172975735\n",
      "9 Train Loss 7221.255 Test MSE 1820.8409356228997 Test RE 0.37344912659923657\n",
      "10 Train Loss 6350.009 Test MSE 1677.0079532826094 Test RE 0.35839587706328396\n",
      "11 Train Loss 5855.067 Test MSE 1541.1853084877607 Test RE 0.34357604336016295\n",
      "12 Train Loss 5170.7783 Test MSE 1353.8739708331918 Test RE 0.32202127349347204\n",
      "13 Train Loss 4372.41 Test MSE 984.7077284816799 Test RE 0.2746307209940878\n",
      "14 Train Loss 3766.7024 Test MSE 732.4518338537671 Test RE 0.2368563209465857\n",
      "15 Train Loss 3399.0593 Test MSE 679.4856748624801 Test RE 0.22813167880475885\n",
      "16 Train Loss 2933.2114 Test MSE 550.3988905086436 Test RE 0.20532140086968767\n",
      "17 Train Loss 2602.293 Test MSE 517.2381197029879 Test RE 0.19904015717210452\n",
      "18 Train Loss 2454.571 Test MSE 458.782400511148 Test RE 0.18745577247669956\n",
      "19 Train Loss 2370.9077 Test MSE 470.9627076611988 Test RE 0.18992787257992433\n",
      "20 Train Loss 2198.5376 Test MSE 390.8832495361449 Test RE 0.17302903028684294\n",
      "21 Train Loss 2146.0557 Test MSE 356.71669971883335 Test RE 0.16529402857208156\n",
      "22 Train Loss 1983.7141 Test MSE 325.9361576119199 Test RE 0.15800168356378672\n",
      "23 Train Loss 1890.6003 Test MSE 270.15418457587197 Test RE 0.14384716044563617\n",
      "24 Train Loss 1852.7727 Test MSE 263.6625297516636 Test RE 0.14210836773803764\n",
      "25 Train Loss 1774.6472 Test MSE 263.56037670551876 Test RE 0.1420808359362573\n",
      "26 Train Loss 1760.0143 Test MSE 260.08921548348815 Test RE 0.1411421134263879\n",
      "27 Train Loss 1747.7101 Test MSE 263.02918230388536 Test RE 0.14193758486823793\n",
      "28 Train Loss 1727.999 Test MSE 261.48699881029233 Test RE 0.14152087141601158\n",
      "29 Train Loss 1722.4192 Test MSE 259.7602412096325 Test RE 0.14105282326508936\n",
      "30 Train Loss 1713.3837 Test MSE 256.56357515736937 Test RE 0.14018222315687867\n",
      "31 Train Loss 1705.2078 Test MSE 253.93611707277879 Test RE 0.13946257546257648\n",
      "32 Train Loss 1691.9478 Test MSE 250.05576575234718 Test RE 0.13839292239541837\n",
      "33 Train Loss 1687.2162 Test MSE 248.6674063287668 Test RE 0.13800819509466736\n",
      "34 Train Loss 1679.2192 Test MSE 248.28787860759104 Test RE 0.13790283762696448\n",
      "35 Train Loss 1671.9137 Test MSE 246.56257230358344 Test RE 0.13742287178596313\n",
      "36 Train Loss 1666.3604 Test MSE 244.1613928551776 Test RE 0.13675208000998235\n",
      "37 Train Loss 1661.6208 Test MSE 244.2524742361984 Test RE 0.1367775844658197\n",
      "38 Train Loss 1660.1833 Test MSE 245.71170895921017 Test RE 0.13718555041461603\n",
      "39 Train Loss 1657.0636 Test MSE 245.48618167029593 Test RE 0.13712257786023677\n",
      "40 Train Loss 1654.7311 Test MSE 245.4204444752144 Test RE 0.13710421703616818\n",
      "41 Train Loss 1652.1261 Test MSE 243.03679300694606 Test RE 0.13643677859149858\n",
      "42 Train Loss 1650.6766 Test MSE 242.86297305517232 Test RE 0.13638798006223263\n",
      "43 Train Loss 1646.6534 Test MSE 241.24485806001385 Test RE 0.13593286691552253\n",
      "44 Train Loss 1640.6063 Test MSE 238.24359635153843 Test RE 0.1350846686830617\n",
      "45 Train Loss 1638.2573 Test MSE 238.5210433317414 Test RE 0.13516330233536322\n",
      "46 Train Loss 1632.7123 Test MSE 238.57423393999653 Test RE 0.13517837232121444\n",
      "47 Train Loss 1629.6846 Test MSE 237.9802398132765 Test RE 0.13500998615633994\n",
      "48 Train Loss 1626.0884 Test MSE 237.20669840150293 Test RE 0.1347903863709286\n",
      "49 Train Loss 1622.353 Test MSE 237.16274938505575 Test RE 0.13477789899311812\n",
      "50 Train Loss 1616.9376 Test MSE 233.42201233895327 Test RE 0.1337107572041345\n",
      "51 Train Loss 1613.5254 Test MSE 232.28817744009783 Test RE 0.13338561546994482\n",
      "52 Train Loss 1609.9333 Test MSE 232.45518726317857 Test RE 0.13343355743930374\n",
      "53 Train Loss 1606.3118 Test MSE 230.83272148347442 Test RE 0.13296707943057823\n",
      "54 Train Loss 1605.2224 Test MSE 231.06214720920576 Test RE 0.13303314132213787\n",
      "55 Train Loss 1603.4131 Test MSE 231.7701354029592 Test RE 0.1332367961766737\n",
      "56 Train Loss 1600.6134 Test MSE 232.20817889430845 Test RE 0.13336264492045574\n",
      "57 Train Loss 1599.6443 Test MSE 231.68501250188177 Test RE 0.13321232679422035\n",
      "58 Train Loss 1597.5901 Test MSE 229.4869692536723 Test RE 0.1325789145483356\n",
      "59 Train Loss 1595.2933 Test MSE 231.79507614184678 Test RE 0.13324396477659967\n",
      "60 Train Loss 1593.4164 Test MSE 231.30397103703135 Test RE 0.13310273768619324\n",
      "61 Train Loss 1591.8466 Test MSE 230.78272370623526 Test RE 0.13295267848602868\n",
      "62 Train Loss 1590.1901 Test MSE 229.01322771473926 Test RE 0.13244199917746183\n",
      "63 Train Loss 1588.7195 Test MSE 229.79246870861377 Test RE 0.13266713157350699\n",
      "64 Train Loss 1584.7354 Test MSE 227.36735302205346 Test RE 0.13196522322634538\n",
      "65 Train Loss 1577.3926 Test MSE 229.3030703692743 Test RE 0.13252578298937218\n",
      "66 Train Loss 1573.6925 Test MSE 227.40285742851006 Test RE 0.1319755262974904\n",
      "67 Train Loss 1572.2001 Test MSE 226.8223738639839 Test RE 0.13180697393847668\n",
      "68 Train Loss 1568.7101 Test MSE 227.33671675170334 Test RE 0.13195633219955463\n",
      "69 Train Loss 1560.988 Test MSE 225.4707822021342 Test RE 0.13141368074844925\n",
      "70 Train Loss 1555.692 Test MSE 224.4367569233615 Test RE 0.1311119981546032\n",
      "71 Train Loss 1552.7062 Test MSE 224.3018474294855 Test RE 0.1310725863566716\n",
      "72 Train Loss 1550.0939 Test MSE 225.42011193602755 Test RE 0.131398913557201\n",
      "73 Train Loss 1546.5021 Test MSE 225.45697454600932 Test RE 0.1314096568508141\n",
      "74 Train Loss 1540.2847 Test MSE 224.3316683258473 Test RE 0.13108129910726127\n",
      "75 Train Loss 1534.6243 Test MSE 220.77448371110893 Test RE 0.13003788063726734\n",
      "76 Train Loss 1526.5002 Test MSE 220.91045310372817 Test RE 0.13007791798633622\n",
      "77 Train Loss 1523.7501 Test MSE 221.22243552837145 Test RE 0.1301697373315016\n",
      "78 Train Loss 1520.9089 Test MSE 219.3130487686311 Test RE 0.12960676775641242\n",
      "79 Train Loss 1516.0481 Test MSE 222.80844061074038 Test RE 0.13063551549923128\n",
      "80 Train Loss 1507.2339 Test MSE 221.7278361371428 Test RE 0.1303183441686984\n",
      "81 Train Loss 1493.0402 Test MSE 217.90245864764836 Test RE 0.12918928929875007\n",
      "82 Train Loss 1483.7076 Test MSE 220.99405293237908 Test RE 0.13010252855337787\n",
      "83 Train Loss 1473.8387 Test MSE 223.21501412524148 Test RE 0.13075465087153684\n",
      "84 Train Loss 1468.7705 Test MSE 224.26254753166475 Test RE 0.13106110324816772\n",
      "85 Train Loss 1454.3475 Test MSE 219.17793173527818 Test RE 0.12956683675332112\n",
      "86 Train Loss 1445.8019 Test MSE 217.00971928838408 Test RE 0.12892437550447902\n",
      "87 Train Loss 1439.6582 Test MSE 219.14953311979005 Test RE 0.12955844257343443\n",
      "88 Train Loss 1429.456 Test MSE 213.70906574774716 Test RE 0.12794016801040348\n",
      "89 Train Loss 1426.4504 Test MSE 212.7343219028692 Test RE 0.12764806196934322\n",
      "90 Train Loss 1416.4813 Test MSE 204.04406751740518 Test RE 0.1250136484566717\n",
      "91 Train Loss 1406.4346 Test MSE 199.7910056905059 Test RE 0.12370390527512849\n",
      "92 Train Loss 1397.9585 Test MSE 200.0301786253225 Test RE 0.12377792706758524\n",
      "93 Train Loss 1383.637 Test MSE 205.20863281033277 Test RE 0.12536989359939452\n",
      "94 Train Loss 1369.5778 Test MSE 206.43408343886793 Test RE 0.12574367400277245\n",
      "95 Train Loss 1349.8002 Test MSE 204.38071996876525 Test RE 0.12511673600665918\n",
      "96 Train Loss 1343.2012 Test MSE 198.18589417932913 Test RE 0.12320598752791408\n",
      "97 Train Loss 1332.1855 Test MSE 196.6529052876453 Test RE 0.12272855680697253\n",
      "98 Train Loss 1325.3053 Test MSE 197.63696153274662 Test RE 0.12303524206324713\n",
      "99 Train Loss 1320.1445 Test MSE 189.43293086334526 Test RE 0.12045454268538125\n",
      "100 Train Loss 1301.1965 Test MSE 188.62827500358986 Test RE 0.12019844254732158\n",
      "101 Train Loss 1282.1091 Test MSE 182.45977539892715 Test RE 0.1182167489589855\n",
      "102 Train Loss 1270.1487 Test MSE 182.63647564203723 Test RE 0.11827397766404002\n",
      "103 Train Loss 1265.3502 Test MSE 180.946303781447 Test RE 0.11772543446693917\n",
      "104 Train Loss 1257.9022 Test MSE 184.62391790570072 Test RE 0.1189157625853392\n",
      "105 Train Loss 1251.4124 Test MSE 182.70395162578075 Test RE 0.1182958241105916\n",
      "106 Train Loss 1241.7742 Test MSE 179.86660922710834 Test RE 0.11737367903958221\n",
      "107 Train Loss 1235.6002 Test MSE 181.6523565719268 Test RE 0.11795489327220961\n",
      "108 Train Loss 1229.154 Test MSE 185.7322295366002 Test RE 0.11927215876834767\n",
      "109 Train Loss 1220.6792 Test MSE 188.53659838908996 Test RE 0.12016922973309904\n",
      "110 Train Loss 1205.8827 Test MSE 180.77552677038784 Test RE 0.11766986675605617\n",
      "111 Train Loss 1187.1332 Test MSE 184.9743719495551 Test RE 0.11902857232420105\n",
      "112 Train Loss 1170.0586 Test MSE 179.34740388661007 Test RE 0.11720415040070192\n",
      "113 Train Loss 1145.4045 Test MSE 175.2398476338517 Test RE 0.11585422523947377\n",
      "114 Train Loss 1137.8688 Test MSE 178.14445659066186 Test RE 0.116810423959011\n",
      "115 Train Loss 1127.7455 Test MSE 184.20799245020692 Test RE 0.1187817388333235\n",
      "116 Train Loss 1122.8232 Test MSE 189.44389765533683 Test RE 0.12045802935686316\n",
      "117 Train Loss 1110.682 Test MSE 185.59614875760946 Test RE 0.11922845708463331\n",
      "118 Train Loss 1102.6956 Test MSE 192.22181401493114 Test RE 0.12133798527132172\n",
      "119 Train Loss 1097.4347 Test MSE 192.25465116515656 Test RE 0.12134834888047397\n",
      "120 Train Loss 1090.24 Test MSE 188.40475710228736 Test RE 0.12012720596494\n",
      "121 Train Loss 1084.2937 Test MSE 198.5563042228612 Test RE 0.12332106996477696\n",
      "122 Train Loss 1077.7548 Test MSE 199.05196143243984 Test RE 0.12347489756179855\n",
      "123 Train Loss 1070.6362 Test MSE 197.1919936530064 Test RE 0.12289666074763639\n",
      "124 Train Loss 1061.505 Test MSE 201.59694582979986 Test RE 0.12426173638346194\n",
      "125 Train Loss 1054.8474 Test MSE 212.2512516858179 Test RE 0.12750305005536414\n",
      "126 Train Loss 1048.5942 Test MSE 210.74442869521664 Test RE 0.12704965643328678\n",
      "127 Train Loss 1043.9764 Test MSE 213.93568556735207 Test RE 0.1280079847298838\n",
      "128 Train Loss 1036.293 Test MSE 222.2869415317386 Test RE 0.1304825450570653\n",
      "129 Train Loss 1023.85645 Test MSE 225.49553452245746 Test RE 0.1314208938875985\n",
      "130 Train Loss 1015.11664 Test MSE 210.1535576056804 Test RE 0.1268714247658663\n",
      "131 Train Loss 1010.0501 Test MSE 212.88395700362867 Test RE 0.12769294723474947\n",
      "132 Train Loss 1004.8851 Test MSE 210.77091576507627 Test RE 0.12705764019688257\n",
      "133 Train Loss 989.8425 Test MSE 200.3121444458867 Test RE 0.12386513604366976\n",
      "134 Train Loss 983.21875 Test MSE 197.70956850118515 Test RE 0.12305784005189674\n",
      "135 Train Loss 976.1009 Test MSE 189.79539417088813 Test RE 0.12056972721703682\n",
      "136 Train Loss 970.4871 Test MSE 189.24139823724767 Test RE 0.12039363244816755\n",
      "137 Train Loss 959.62103 Test MSE 200.12223976515062 Test RE 0.12380640733570661\n",
      "138 Train Loss 948.4043 Test MSE 191.3033312638228 Test RE 0.1210477468918372\n",
      "139 Train Loss 941.35565 Test MSE 186.46519225073186 Test RE 0.1195072713438402\n",
      "140 Train Loss 935.3883 Test MSE 185.60959002162383 Test RE 0.11923277439446414\n",
      "141 Train Loss 931.08466 Test MSE 186.6294236811025 Test RE 0.11955988848415602\n",
      "142 Train Loss 925.1042 Test MSE 188.61356406793797 Test RE 0.12019375537613594\n",
      "143 Train Loss 915.0412 Test MSE 195.95486121339133 Test RE 0.12251054298976927\n",
      "144 Train Loss 904.47974 Test MSE 189.19334517652052 Test RE 0.12037834602065162\n",
      "145 Train Loss 897.9607 Test MSE 196.58519633577544 Test RE 0.12270742684365171\n",
      "146 Train Loss 888.44226 Test MSE 202.90551907588215 Test RE 0.12466437781806528\n",
      "147 Train Loss 879.59674 Test MSE 202.24494228869037 Test RE 0.12446128445565488\n",
      "148 Train Loss 873.42535 Test MSE 199.12181443810329 Test RE 0.12349656109149115\n",
      "149 Train Loss 866.99365 Test MSE 202.01170468406903 Test RE 0.12438949668745519\n",
      "150 Train Loss 853.2529 Test MSE 210.71427113401904 Test RE 0.12704056569505257\n",
      "151 Train Loss 844.9525 Test MSE 207.70869737486007 Test RE 0.12613127472275032\n",
      "152 Train Loss 840.9176 Test MSE 201.9662732788744 Test RE 0.12437550861829486\n",
      "153 Train Loss 838.758 Test MSE 202.11098614330032 Test RE 0.12442005940607143\n",
      "154 Train Loss 834.7828 Test MSE 208.79415450347457 Test RE 0.12646041762945798\n",
      "155 Train Loss 830.7174 Test MSE 210.55914298049308 Test RE 0.12699379336018884\n",
      "156 Train Loss 826.1872 Test MSE 212.12326017118718 Test RE 0.1274646008841063\n",
      "157 Train Loss 823.2085 Test MSE 214.6968366518813 Test RE 0.1282354991499355\n",
      "158 Train Loss 820.8614 Test MSE 218.95336959666236 Test RE 0.12950044489036222\n",
      "159 Train Loss 818.35986 Test MSE 218.823194874082 Test RE 0.12946194310600778\n",
      "160 Train Loss 815.29254 Test MSE 223.75518892243556 Test RE 0.13091276677602723\n",
      "161 Train Loss 811.7445 Test MSE 229.95940985698186 Test RE 0.13271531327066244\n",
      "162 Train Loss 809.6986 Test MSE 233.47582742913508 Test RE 0.13372616972178009\n",
      "163 Train Loss 807.3342 Test MSE 234.64301202344285 Test RE 0.13406001271862927\n",
      "164 Train Loss 805.302 Test MSE 233.87134596725596 Test RE 0.13383939086052493\n",
      "165 Train Loss 802.46246 Test MSE 234.55301894590224 Test RE 0.1340343021082534\n",
      "166 Train Loss 801.24457 Test MSE 232.96864804811307 Test RE 0.1335808441224608\n",
      "167 Train Loss 800.3481 Test MSE 233.3528882265366 Test RE 0.13369095761093205\n",
      "168 Train Loss 798.89374 Test MSE 229.04144690837566 Test RE 0.1324501587308056\n",
      "169 Train Loss 796.10345 Test MSE 225.8856042395812 Test RE 0.13153451290208992\n",
      "170 Train Loss 794.1519 Test MSE 227.06598860227285 Test RE 0.13187773746966502\n",
      "171 Train Loss 792.2108 Test MSE 225.9365382886542 Test RE 0.13154934166405186\n",
      "172 Train Loss 790.8616 Test MSE 224.8561971611784 Test RE 0.13123445576494128\n",
      "173 Train Loss 789.9448 Test MSE 224.32423741847137 Test RE 0.13107912807838676\n",
      "174 Train Loss 787.2934 Test MSE 219.77932213553407 Test RE 0.12974447068324718\n",
      "175 Train Loss 785.4217 Test MSE 218.3040019871877 Test RE 0.12930826735560863\n",
      "176 Train Loss 783.55444 Test MSE 223.11659242759177 Test RE 0.13072582101512473\n",
      "177 Train Loss 781.5594 Test MSE 226.27094943648876 Test RE 0.1316466594786282\n",
      "178 Train Loss 780.62335 Test MSE 228.64097744881366 Test RE 0.1323343162707752\n",
      "179 Train Loss 779.8209 Test MSE 229.46991666364602 Test RE 0.13257398865671818\n",
      "180 Train Loss 777.4108 Test MSE 224.70981910885106 Test RE 0.13119173296782777\n",
      "181 Train Loss 774.98944 Test MSE 223.006950532635 Test RE 0.1306936970303874\n",
      "182 Train Loss 773.6126 Test MSE 224.62610514928056 Test RE 0.13116729344281455\n",
      "183 Train Loss 772.5401 Test MSE 222.07056576170413 Test RE 0.13041902325292556\n",
      "184 Train Loss 771.7594 Test MSE 222.3936122892829 Test RE 0.1305138491957921\n",
      "185 Train Loss 770.14014 Test MSE 220.67211936680306 Test RE 0.13000773044623584\n",
      "186 Train Loss 769.3265 Test MSE 221.8186970015303 Test RE 0.13034504272130334\n",
      "187 Train Loss 767.5921 Test MSE 219.6266457637845 Test RE 0.1296993973882317\n",
      "188 Train Loss 766.1253 Test MSE 220.19087604154717 Test RE 0.12986589218258235\n",
      "189 Train Loss 764.9114 Test MSE 221.21281017289724 Test RE 0.13016690546768134\n",
      "190 Train Loss 764.0168 Test MSE 220.63196093324274 Test RE 0.12999590035104688\n",
      "191 Train Loss 763.34344 Test MSE 220.62111005924288 Test RE 0.12999270365535304\n",
      "192 Train Loss 762.3487 Test MSE 221.38687071007178 Test RE 0.1302181060883479\n",
      "193 Train Loss 761.4106 Test MSE 220.15725862421863 Test RE 0.129855978232877\n",
      "194 Train Loss 760.6669 Test MSE 220.46309830452918 Test RE 0.12994614407105323\n",
      "195 Train Loss 759.4978 Test MSE 217.94141030964846 Test RE 0.1292008355490752\n",
      "196 Train Loss 757.7345 Test MSE 218.58975489070392 Test RE 0.12939286985660983\n",
      "197 Train Loss 756.34845 Test MSE 217.86606688910263 Test RE 0.12917850093554392\n",
      "198 Train Loss 755.3557 Test MSE 219.02742543915917 Test RE 0.12952234328312118\n",
      "199 Train Loss 755.03656 Test MSE 217.8509336335126 Test RE 0.1291740144057052\n",
      "Training time: 631.59\n",
      "Training time: 631.59\n",
      "ES_rowdy_medium\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 16653.037 Test MSE 9651.599919720322 Test RE 0.8597953646106475\n",
      "1 Train Loss 10220.103 Test MSE 3143.8109192415395 Test RE 0.4907085813362313\n",
      "2 Train Loss 10001.19 Test MSE 3003.3515107562853 Test RE 0.47962137103429586\n",
      "3 Train Loss 9998.981 Test MSE 3002.382661314373 Test RE 0.4795440044028304\n",
      "4 Train Loss 9995.556 Test MSE 3000.459831318971 Test RE 0.4793904215021201\n",
      "5 Train Loss 9990.584 Test MSE 2997.813571329904 Test RE 0.47917897532463566\n",
      "6 Train Loss 9923.633 Test MSE 2960.702948531276 Test RE 0.4762038057727992\n",
      "7 Train Loss 9679.661 Test MSE 2825.3268298794933 Test RE 0.46518937909926916\n",
      "8 Train Loss 9391.674 Test MSE 2735.4345945430464 Test RE 0.45772919263834594\n",
      "9 Train Loss 8890.887 Test MSE 2498.3946028368027 Test RE 0.4374475216004476\n",
      "10 Train Loss 8421.565 Test MSE 2376.8108910048054 Test RE 0.42667063897154317\n",
      "11 Train Loss 7946.5005 Test MSE 2213.8669377634005 Test RE 0.41178564328172623\n",
      "12 Train Loss 7685.3496 Test MSE 2145.402876752655 Test RE 0.40536838352995286\n",
      "13 Train Loss 7361.4565 Test MSE 1980.2183646396174 Test RE 0.3894502489130487\n",
      "14 Train Loss 6877.7285 Test MSE 1822.102676584554 Test RE 0.3735784938742814\n",
      "15 Train Loss 6688.088 Test MSE 1656.5357745352505 Test RE 0.35620158981095756\n",
      "16 Train Loss 6240.0913 Test MSE 1544.948214841357 Test RE 0.34399521952602174\n",
      "17 Train Loss 6098.6523 Test MSE 1542.0926340659998 Test RE 0.34367716340810306\n",
      "18 Train Loss 5863.289 Test MSE 1383.001694806136 Test RE 0.32546687946517633\n",
      "19 Train Loss 5565.5317 Test MSE 1256.1142681322804 Test RE 0.31017730496188767\n",
      "20 Train Loss 5417.266 Test MSE 1175.3750295800212 Test RE 0.30004312070122446\n",
      "21 Train Loss 5042.859 Test MSE 1118.2407576515682 Test RE 0.29265982091724313\n",
      "22 Train Loss 4897.1523 Test MSE 1042.4225039245 Test RE 0.2825643273756526\n",
      "23 Train Loss 4712.5664 Test MSE 992.296644817986 Test RE 0.2756869478259233\n",
      "24 Train Loss 4653.663 Test MSE 949.1456274361822 Test RE 0.26962606248891513\n",
      "25 Train Loss 4577.6973 Test MSE 914.8752390776854 Test RE 0.2647136770066418\n",
      "26 Train Loss 4415.951 Test MSE 834.8817001923449 Test RE 0.25287617557484376\n",
      "27 Train Loss 4245.8184 Test MSE 807.5692116743076 Test RE 0.24870546040387545\n",
      "28 Train Loss 4167.281 Test MSE 791.7802812502451 Test RE 0.24626221686276498\n",
      "29 Train Loss 4091.7554 Test MSE 734.2445918019711 Test RE 0.23714600998815724\n",
      "30 Train Loss 4043.9265 Test MSE 721.3158722736744 Test RE 0.23504888112383587\n",
      "31 Train Loss 3976.3223 Test MSE 687.649294575521 Test RE 0.22949802090875213\n",
      "32 Train Loss 3836.7144 Test MSE 658.9597524091702 Test RE 0.22465955210800756\n",
      "33 Train Loss 3770.224 Test MSE 640.0899598610006 Test RE 0.22141954366804067\n",
      "34 Train Loss 3696.7705 Test MSE 613.2932251925591 Test RE 0.216735238202055\n",
      "35 Train Loss 3618.749 Test MSE 593.4297261164955 Test RE 0.21319651067142015\n",
      "36 Train Loss 3598.495 Test MSE 595.3116003324236 Test RE 0.21353428566419697\n",
      "37 Train Loss 3527.0708 Test MSE 566.6743717542114 Test RE 0.20833499652671453\n",
      "38 Train Loss 3485.2314 Test MSE 560.66041582363 Test RE 0.20722654736858506\n",
      "39 Train Loss 3322.5266 Test MSE 543.8296987076585 Test RE 0.20409243360206017\n",
      "40 Train Loss 3245.942 Test MSE 491.2872676667698 Test RE 0.19398278863213886\n",
      "41 Train Loss 3173.6902 Test MSE 470.2558067194073 Test RE 0.18978528102459222\n",
      "42 Train Loss 3113.4404 Test MSE 439.34575580555395 Test RE 0.18344195134062624\n",
      "43 Train Loss 2953.1445 Test MSE 448.7888907741055 Test RE 0.185402887135588\n",
      "44 Train Loss 2828.2954 Test MSE 415.82147196247763 Test RE 0.17846329185892626\n",
      "45 Train Loss 2783.7905 Test MSE 405.0381810455624 Test RE 0.1761340923922866\n",
      "46 Train Loss 2645.749 Test MSE 372.48282422118865 Test RE 0.16890735827931785\n",
      "47 Train Loss 2569.836 Test MSE 363.7481769031085 Test RE 0.16691518820880008\n",
      "48 Train Loss 2480.2903 Test MSE 343.49829207310955 Test RE 0.16220257140852812\n",
      "49 Train Loss 2431.451 Test MSE 337.78114592707254 Test RE 0.16084706703726304\n",
      "50 Train Loss 2405.1113 Test MSE 344.0313575085391 Test RE 0.16232838144435496\n",
      "51 Train Loss 2376.059 Test MSE 323.09513194973846 Test RE 0.15731156481612493\n",
      "52 Train Loss 2319.6372 Test MSE 303.07039287543233 Test RE 0.1523586794452033\n",
      "53 Train Loss 2273.9646 Test MSE 296.24164197435005 Test RE 0.15063243511249763\n",
      "54 Train Loss 2238.525 Test MSE 290.61396472249044 Test RE 0.14919479893884047\n",
      "55 Train Loss 2170.9229 Test MSE 283.8547998172058 Test RE 0.14744958853245965\n",
      "56 Train Loss 2113.6008 Test MSE 280.62109331647963 Test RE 0.14660730147461493\n",
      "57 Train Loss 2076.8323 Test MSE 282.69059220617845 Test RE 0.14714690154318652\n",
      "58 Train Loss 2037.7378 Test MSE 275.5898737432179 Test RE 0.14528710555737853\n",
      "59 Train Loss 2016.6678 Test MSE 275.4075518500756 Test RE 0.14523903883724257\n",
      "60 Train Loss 2006.0911 Test MSE 277.8124545450278 Test RE 0.14587178576071583\n",
      "61 Train Loss 1992.8682 Test MSE 276.73421943979866 Test RE 0.14558843455415085\n",
      "62 Train Loss 1983.23 Test MSE 278.4084600253688 Test RE 0.14602817508531343\n",
      "63 Train Loss 1967.568 Test MSE 273.7670470265853 Test RE 0.1448058241008465\n",
      "64 Train Loss 1954.7233 Test MSE 269.08591270705597 Test RE 0.1435624710031245\n",
      "65 Train Loss 1943.5774 Test MSE 273.7051545904821 Test RE 0.14478945454154873\n",
      "66 Train Loss 1929.1975 Test MSE 270.31725046118277 Test RE 0.14389056718771348\n",
      "67 Train Loss 1924.1914 Test MSE 269.5375803577844 Test RE 0.14368290714228998\n",
      "68 Train Loss 1919.4861 Test MSE 267.43735632362507 Test RE 0.14312202722147738\n",
      "69 Train Loss 1901.118 Test MSE 265.164391523887 Test RE 0.1425125284267782\n",
      "70 Train Loss 1888.3933 Test MSE 261.7842171460081 Test RE 0.14160127819010057\n",
      "71 Train Loss 1880.3762 Test MSE 258.907083084122 Test RE 0.14082099537643294\n",
      "72 Train Loss 1873.0762 Test MSE 260.5974601372375 Test RE 0.1412799501954544\n",
      "73 Train Loss 1867.9329 Test MSE 261.45479991249476 Test RE 0.14151215787311416\n",
      "74 Train Loss 1854.8373 Test MSE 263.1834168729681 Test RE 0.14197919331979167\n",
      "75 Train Loss 1849.4745 Test MSE 264.18275474036056 Test RE 0.14224849362838723\n",
      "76 Train Loss 1841.6421 Test MSE 265.0496054097412 Test RE 0.14248167920531232\n",
      "77 Train Loss 1836.5209 Test MSE 260.8025772520097 Test RE 0.14133554021469955\n",
      "78 Train Loss 1832.1713 Test MSE 261.74558745036336 Test RE 0.14159083024150573\n",
      "79 Train Loss 1827.4625 Test MSE 258.73836819867927 Test RE 0.14077510541929064\n",
      "80 Train Loss 1825.3375 Test MSE 260.73007577890763 Test RE 0.14131589365440284\n",
      "81 Train Loss 1824.8138 Test MSE 260.32430741485194 Test RE 0.1412058874611806\n",
      "82 Train Loss 1823.5502 Test MSE 259.1794730339608 Test RE 0.14089505310357436\n",
      "83 Train Loss 1822.7595 Test MSE 259.7213542721312 Test RE 0.14104226484091095\n",
      "84 Train Loss 1820.7902 Test MSE 259.0113627418762 Test RE 0.14084935166295176\n",
      "85 Train Loss 1817.4443 Test MSE 259.4237609867711 Test RE 0.14096143732983524\n",
      "86 Train Loss 1814.1365 Test MSE 259.3793782927611 Test RE 0.1409493788431301\n",
      "87 Train Loss 1807.1211 Test MSE 260.2646803231458 Test RE 0.14118971498256871\n",
      "88 Train Loss 1805.5728 Test MSE 260.8862441782998 Test RE 0.14135820901289856\n",
      "89 Train Loss 1804.1417 Test MSE 261.0204750967641 Test RE 0.1413945700755763\n",
      "90 Train Loss 1801.2628 Test MSE 262.1530958680136 Test RE 0.14170100786727444\n",
      "91 Train Loss 1798.874 Test MSE 263.22511589241583 Test RE 0.14199044053043325\n",
      "92 Train Loss 1798.4048 Test MSE 263.96665186393875 Test RE 0.14219030171938882\n",
      "93 Train Loss 1797.8955 Test MSE 263.0624188453494 Test RE 0.14194655224852212\n",
      "94 Train Loss 1796.4086 Test MSE 264.16010093846864 Test RE 0.14224239455829032\n",
      "95 Train Loss 1794.8668 Test MSE 262.2845065173427 Test RE 0.14173651896512532\n",
      "96 Train Loss 1794.219 Test MSE 262.0855191414035 Test RE 0.141682743144113\n",
      "97 Train Loss 1793.4346 Test MSE 262.1148771591372 Test RE 0.14169067835553106\n",
      "98 Train Loss 1792.1666 Test MSE 261.77667222962776 Test RE 0.14159923762112941\n",
      "99 Train Loss 1791.0696 Test MSE 261.59979234182634 Test RE 0.14155139094074606\n",
      "100 Train Loss 1790.371 Test MSE 260.9764594778674 Test RE 0.14138264796123093\n",
      "101 Train Loss 1789.7561 Test MSE 260.56042148571123 Test RE 0.1412699097967717\n",
      "102 Train Loss 1788.689 Test MSE 262.45450689399775 Test RE 0.14178244496899625\n",
      "103 Train Loss 1784.5201 Test MSE 259.9249102600181 Test RE 0.14109752478380125\n",
      "104 Train Loss 1779.7124 Test MSE 252.68815066079821 Test RE 0.13911945969376358\n",
      "105 Train Loss 1764.3308 Test MSE 244.25140435687314 Test RE 0.13677728490760546\n",
      "106 Train Loss 1752.838 Test MSE 243.5590259139621 Test RE 0.1365832863271689\n",
      "107 Train Loss 1749.2227 Test MSE 244.68336854081028 Test RE 0.1368981783557437\n",
      "108 Train Loss 1745.4958 Test MSE 243.98034275399075 Test RE 0.13670136853356085\n",
      "109 Train Loss 1741.5784 Test MSE 244.9538547544141 Test RE 0.1369738247764676\n",
      "110 Train Loss 1736.9908 Test MSE 246.12810861642146 Test RE 0.13730174315801433\n",
      "111 Train Loss 1734.7399 Test MSE 244.92668532284188 Test RE 0.1369662282351458\n",
      "112 Train Loss 1732.6096 Test MSE 243.67416834415403 Test RE 0.13661556735886893\n",
      "113 Train Loss 1732.0803 Test MSE 243.75568791632287 Test RE 0.13663841736261573\n",
      "114 Train Loss 1731.8652 Test MSE 243.71950812542906 Test RE 0.13662827660885016\n",
      "115 Train Loss 1731.3085 Test MSE 244.20271092574276 Test RE 0.1367636504163484\n",
      "116 Train Loss 1730.6843 Test MSE 245.15645215173214 Test RE 0.13703045749366527\n",
      "117 Train Loss 1729.3219 Test MSE 244.19354857692628 Test RE 0.13676108474453483\n",
      "118 Train Loss 1728.1155 Test MSE 243.84727303144953 Test RE 0.13666408418920872\n",
      "119 Train Loss 1726.5387 Test MSE 243.62823135775608 Test RE 0.13660268949967452\n",
      "120 Train Loss 1726.1888 Test MSE 242.80534769047952 Test RE 0.1363717983584968\n",
      "121 Train Loss 1724.9895 Test MSE 242.38418935376887 Test RE 0.13625347508524932\n",
      "122 Train Loss 1724.0912 Test MSE 241.33176109464665 Test RE 0.135957348090143\n",
      "123 Train Loss 1722.3651 Test MSE 243.22010996956686 Test RE 0.13648822442604397\n",
      "124 Train Loss 1721.7744 Test MSE 242.61210181046042 Test RE 0.1363175192124876\n",
      "125 Train Loss 1721.4891 Test MSE 243.0868943186726 Test RE 0.1364508408847816\n",
      "126 Train Loss 1721.3315 Test MSE 243.54745852543044 Test RE 0.13658004290251421\n",
      "127 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "128 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "129 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "130 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "131 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "132 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "133 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "134 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "135 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "136 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "137 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "138 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "139 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "140 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "141 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "142 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "143 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "144 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "145 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "146 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "147 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "148 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "149 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "150 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "151 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "152 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "153 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "154 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "155 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "156 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "157 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "158 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "159 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "160 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "161 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "162 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "163 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "164 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "165 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "166 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "167 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "168 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "169 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "170 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "171 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "172 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "173 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "174 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "175 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "176 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "177 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "178 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "179 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "180 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "181 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "182 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "183 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "184 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "185 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "186 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "187 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "188 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "189 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "190 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "191 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "192 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "193 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "194 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "195 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "196 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "197 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "198 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "199 Train Loss 1721.3286 Test MSE 243.52590637222227 Test RE 0.13657399960575364\n",
      "Training time: 432.39\n",
      "Training time: 432.39\n",
      "ES_rowdy_medium\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 18048.908 Test MSE 11057.064636966901 Test RE 0.9202702190511273\n",
      "1 Train Loss 12677.636 Test MSE 5501.291348648586 Test RE 0.6491241497208724\n",
      "2 Train Loss 10047.443 Test MSE 3032.5241663819434 Test RE 0.4819451110486106\n",
      "3 Train Loss 9998.1045 Test MSE 3001.830638622865 Test RE 0.479499917527354\n",
      "4 Train Loss 9996.258 Test MSE 3001.3901418754726 Test RE 0.47946473467916495\n",
      "5 Train Loss 9987.635 Test MSE 2994.7804632636207 Test RE 0.478936503704064\n",
      "6 Train Loss 9962.293 Test MSE 2981.4103731884675 Test RE 0.47786621036599647\n",
      "7 Train Loss 9836.921 Test MSE 2922.48757636744 Test RE 0.4731205157796164\n",
      "8 Train Loss 9603.181 Test MSE 2860.3789195255085 Test RE 0.46806614308400674\n",
      "9 Train Loss 9336.459 Test MSE 2763.2007602422464 Test RE 0.46004642871448276\n",
      "10 Train Loss 8997.735 Test MSE 2612.5161963977616 Test RE 0.4473268220721297\n",
      "11 Train Loss 8671.61 Test MSE 2462.5802356755084 Test RE 0.43430080924965264\n",
      "12 Train Loss 8364.231 Test MSE 2349.004374507872 Test RE 0.424167467846457\n",
      "13 Train Loss 8103.666 Test MSE 2280.194077454163 Test RE 0.4179086404519758\n",
      "14 Train Loss 7414.7026 Test MSE 1902.8650122669367 Test RE 0.3817679221785355\n",
      "15 Train Loss 6964.8423 Test MSE 1850.618083252325 Test RE 0.37649034611433696\n",
      "16 Train Loss 6637.481 Test MSE 1692.5892227839124 Test RE 0.3600569758208469\n",
      "17 Train Loss 6306.7544 Test MSE 1549.8228547592962 Test RE 0.3445374811333815\n",
      "18 Train Loss 5850.0176 Test MSE 1448.3736262616605 Test RE 0.3330701770069607\n",
      "19 Train Loss 5677.7534 Test MSE 1414.2698916044117 Test RE 0.32912554533582994\n",
      "20 Train Loss 5248.003 Test MSE 1194.1810854027801 Test RE 0.3024339473472048\n",
      "21 Train Loss 4924.552 Test MSE 1051.3993026200592 Test RE 0.2837783675846564\n",
      "22 Train Loss 4692.5703 Test MSE 1034.6439138642563 Test RE 0.28150810126427944\n",
      "23 Train Loss 4388.6597 Test MSE 909.8237037436879 Test RE 0.26398184963821175\n",
      "24 Train Loss 4140.688 Test MSE 885.062946026681 Test RE 0.26036495219276135\n",
      "25 Train Loss 3895.196 Test MSE 833.404660724099 Test RE 0.2526523873201592\n",
      "26 Train Loss 3556.2507 Test MSE 724.0189848139962 Test RE 0.23548888907538532\n",
      "27 Train Loss 3161.9873 Test MSE 581.9514138800286 Test RE 0.21112458439210585\n",
      "28 Train Loss 3016.562 Test MSE 522.563232042494 Test RE 0.20006212083711467\n",
      "29 Train Loss 2837.3904 Test MSE 458.0951456969902 Test RE 0.1873153157320551\n",
      "30 Train Loss 2582.9946 Test MSE 438.19081617049886 Test RE 0.18320067918013566\n",
      "31 Train Loss 2523.7974 Test MSE 393.8725807998374 Test RE 0.17368940130101734\n",
      "32 Train Loss 2465.23 Test MSE 369.30365267814585 Test RE 0.1681849945196395\n",
      "33 Train Loss 2360.6729 Test MSE 315.8125929609698 Test RE 0.155528565205533\n",
      "34 Train Loss 2327.115 Test MSE 314.05820991751654 Test RE 0.1550959721438689\n",
      "35 Train Loss 2238.5215 Test MSE 303.9191023346342 Test RE 0.15257186069656767\n",
      "36 Train Loss 2070.7344 Test MSE 294.3333528858493 Test RE 0.15014648951446202\n",
      "37 Train Loss 2021.8239 Test MSE 279.80232421150293 Test RE 0.1463932669306767\n",
      "38 Train Loss 1993.7456 Test MSE 272.99087114656675 Test RE 0.14460040389621467\n",
      "39 Train Loss 1956.4094 Test MSE 272.8455048279771 Test RE 0.14456189926178528\n",
      "40 Train Loss 1944.562 Test MSE 278.7555885053623 Test RE 0.1461191830016133\n",
      "41 Train Loss 1935.1305 Test MSE 280.2779566725659 Test RE 0.14651764013704913\n",
      "42 Train Loss 1911.4865 Test MSE 282.0193287175282 Test RE 0.14697209373012024\n",
      "43 Train Loss 1902.8258 Test MSE 278.1969935225668 Test RE 0.1459727063443537\n",
      "44 Train Loss 1879.8635 Test MSE 281.14469264855785 Test RE 0.14674401199436643\n",
      "45 Train Loss 1851.8748 Test MSE 269.60302364018946 Test RE 0.14370034907072632\n",
      "46 Train Loss 1824.5908 Test MSE 268.2435815572682 Test RE 0.14333759500662166\n",
      "47 Train Loss 1808.3453 Test MSE 271.63415483069264 Test RE 0.14424063713713406\n",
      "48 Train Loss 1799.9474 Test MSE 270.64047018587615 Test RE 0.14397656683293467\n",
      "49 Train Loss 1794.0281 Test MSE 270.2450434238038 Test RE 0.1438713479083748\n",
      "50 Train Loss 1789.1511 Test MSE 265.3846194322933 Test RE 0.14257169685802773\n",
      "51 Train Loss 1778.2948 Test MSE 267.0763775970464 Test RE 0.14302540374815226\n",
      "52 Train Loss 1774.3976 Test MSE 265.46091240246386 Test RE 0.14259218869475035\n",
      "53 Train Loss 1771.8661 Test MSE 264.3918134058914 Test RE 0.1423047660359616\n",
      "54 Train Loss 1764.9176 Test MSE 261.8135526394901 Test RE 0.14160921187505046\n",
      "55 Train Loss 1748.5903 Test MSE 260.00481087174893 Test RE 0.14111920972403885\n",
      "56 Train Loss 1744.8522 Test MSE 258.24027061452813 Test RE 0.14063953696875456\n",
      "57 Train Loss 1741.8278 Test MSE 256.9358011634442 Test RE 0.1402838754609085\n",
      "58 Train Loss 1740.9343 Test MSE 258.2581846040216 Test RE 0.14064441492890098\n",
      "59 Train Loss 1738.7611 Test MSE 255.86607473691504 Test RE 0.1399915419487174\n",
      "60 Train Loss 1730.0203 Test MSE 257.15599317481036 Test RE 0.14034397369050922\n",
      "61 Train Loss 1718.267 Test MSE 253.80412993734404 Test RE 0.13942632686089404\n",
      "62 Train Loss 1715.6755 Test MSE 252.69279033799506 Test RE 0.13912073689338733\n",
      "63 Train Loss 1713.9756 Test MSE 253.3933103936029 Test RE 0.13931344008982877\n",
      "64 Train Loss 1713.0558 Test MSE 253.1792955202291 Test RE 0.13925459590351338\n",
      "65 Train Loss 1711.8589 Test MSE 254.05269219090118 Test RE 0.13949458351432287\n",
      "66 Train Loss 1706.0153 Test MSE 254.17086719424896 Test RE 0.13952702335222725\n",
      "67 Train Loss 1702.7177 Test MSE 254.97358557164938 Test RE 0.13974717567833692\n",
      "68 Train Loss 1701.5869 Test MSE 254.69476782988707 Test RE 0.139670746879605\n",
      "69 Train Loss 1699.9531 Test MSE 254.92262739432755 Test RE 0.139733210276419\n",
      "70 Train Loss 1698.9423 Test MSE 254.84634588526748 Test RE 0.13971230225079137\n",
      "71 Train Loss 1696.4796 Test MSE 255.3143223480096 Test RE 0.13984052084639237\n",
      "72 Train Loss 1694.814 Test MSE 251.78600915415106 Test RE 0.13887089707176642\n",
      "73 Train Loss 1692.4822 Test MSE 250.48458621599428 Test RE 0.13851153652890733\n",
      "74 Train Loss 1690.1272 Test MSE 250.7862999606184 Test RE 0.13859493139580964\n",
      "75 Train Loss 1689.0612 Test MSE 250.1610643498261 Test RE 0.13842205798998916\n",
      "76 Train Loss 1688.3501 Test MSE 249.52387326449002 Test RE 0.13824565656210047\n",
      "77 Train Loss 1685.6039 Test MSE 249.65019285113144 Test RE 0.13828064504712512\n",
      "78 Train Loss 1683.1349 Test MSE 249.73214536058123 Test RE 0.13830333983431806\n",
      "79 Train Loss 1680.7251 Test MSE 248.731280732106 Test RE 0.1380259188189743\n",
      "80 Train Loss 1679.2532 Test MSE 246.34615822682323 Test RE 0.13736254881713597\n",
      "81 Train Loss 1677.1193 Test MSE 246.38980683477098 Test RE 0.13737471750391764\n",
      "82 Train Loss 1676.4447 Test MSE 245.63925764535918 Test RE 0.13716532344578278\n",
      "83 Train Loss 1675.9464 Test MSE 246.21198465408972 Test RE 0.13732513614891176\n",
      "84 Train Loss 1674.052 Test MSE 246.59832360773933 Test RE 0.13743283450811838\n",
      "85 Train Loss 1671.911 Test MSE 243.42069493838494 Test RE 0.13654449412343717\n",
      "86 Train Loss 1670.1047 Test MSE 243.1951920853951 Test RE 0.13648123264218212\n",
      "87 Train Loss 1669.1332 Test MSE 242.00738817162755 Test RE 0.13614752667515173\n",
      "88 Train Loss 1668.0895 Test MSE 242.13075808402675 Test RE 0.13618222472402383\n",
      "89 Train Loss 1666.1875 Test MSE 240.19457393346826 Test RE 0.13563664534704725\n",
      "90 Train Loss 1664.3602 Test MSE 240.27071026449727 Test RE 0.1356581405418448\n",
      "91 Train Loss 1663.708 Test MSE 240.4309754106717 Test RE 0.13570337628332918\n",
      "92 Train Loss 1662.7657 Test MSE 240.41997981626324 Test RE 0.13570027319658406\n",
      "93 Train Loss 1661.4235 Test MSE 240.24469484066196 Test RE 0.13565079611864034\n",
      "94 Train Loss 1660.5029 Test MSE 242.46344114231306 Test RE 0.1362757485039336\n",
      "95 Train Loss 1658.6608 Test MSE 239.4229002361425 Test RE 0.13541858997766082\n",
      "96 Train Loss 1656.4941 Test MSE 237.7626736685187 Test RE 0.13494825767257068\n",
      "97 Train Loss 1654.7163 Test MSE 237.50669871446337 Test RE 0.13487559555982323\n",
      "98 Train Loss 1653.0001 Test MSE 237.66430703054425 Test RE 0.13492033954006613\n",
      "99 Train Loss 1652.3274 Test MSE 236.70383854609275 Test RE 0.1346474379735353\n",
      "100 Train Loss 1651.5586 Test MSE 236.65600793624603 Test RE 0.13463383322067113\n",
      "101 Train Loss 1650.503 Test MSE 235.53022879991826 Test RE 0.13431322305518342\n",
      "102 Train Loss 1649.4867 Test MSE 234.59818438619754 Test RE 0.13404720628424574\n",
      "103 Train Loss 1646.5975 Test MSE 233.4119380828212 Test RE 0.1337078717630043\n",
      "104 Train Loss 1644.8744 Test MSE 233.69812895874944 Test RE 0.13378981755115962\n",
      "105 Train Loss 1644.3955 Test MSE 234.76626502380765 Test RE 0.13409521753762024\n",
      "106 Train Loss 1643.7953 Test MSE 234.54722375137143 Test RE 0.134032646278676\n",
      "107 Train Loss 1642.8055 Test MSE 234.73835043325303 Test RE 0.13408724508815087\n",
      "108 Train Loss 1639.3646 Test MSE 233.8537655288134 Test RE 0.13383436031823492\n",
      "109 Train Loss 1637.0548 Test MSE 233.05120598426086 Test RE 0.13360451078757443\n",
      "110 Train Loss 1635.8754 Test MSE 234.01997182322822 Test RE 0.13388191174865974\n",
      "111 Train Loss 1634.1979 Test MSE 232.72428936096557 Test RE 0.13351076987890334\n",
      "112 Train Loss 1632.8169 Test MSE 233.13622953645043 Test RE 0.13362887988313146\n",
      "113 Train Loss 1631.8213 Test MSE 233.2840077226595 Test RE 0.13367122487857871\n",
      "114 Train Loss 1630.7834 Test MSE 235.76779326410465 Test RE 0.13438094261538083\n",
      "115 Train Loss 1629.5312 Test MSE 235.65265469684144 Test RE 0.1343481257525261\n",
      "116 Train Loss 1626.3064 Test MSE 235.4707475211533 Test RE 0.13429626211822843\n",
      "117 Train Loss 1623.144 Test MSE 236.38415291486135 Test RE 0.13455648170707868\n",
      "118 Train Loss 1619.7385 Test MSE 234.91711670311415 Test RE 0.13413829280715234\n",
      "119 Train Loss 1615.5918 Test MSE 234.75295056410977 Test RE 0.1340914149668285\n",
      "120 Train Loss 1613.4896 Test MSE 234.29191280826558 Test RE 0.13395967735113978\n",
      "121 Train Loss 1610.7686 Test MSE 231.5419018244249 Test RE 0.1331711781437581\n",
      "122 Train Loss 1609.2776 Test MSE 231.80216439700223 Test RE 0.133246002050246\n",
      "123 Train Loss 1606.0629 Test MSE 231.99677646980197 Test RE 0.13330192439307412\n",
      "124 Train Loss 1605.0747 Test MSE 232.5884857356637 Test RE 0.13347180984499551\n",
      "125 Train Loss 1604.1566 Test MSE 231.59733446484734 Test RE 0.13318711821378615\n",
      "126 Train Loss 1602.3434 Test MSE 230.21267873693236 Test RE 0.13278837705378863\n",
      "127 Train Loss 1601.422 Test MSE 229.47420631937743 Test RE 0.13257522780414285\n",
      "128 Train Loss 1599.9648 Test MSE 229.19238564549772 Test RE 0.1324937939928443\n",
      "129 Train Loss 1598.614 Test MSE 229.33155507672566 Test RE 0.13253401410665572\n",
      "130 Train Loss 1596.9567 Test MSE 228.96273013045834 Test RE 0.13242739659358388\n",
      "131 Train Loss 1595.7896 Test MSE 230.23304964266143 Test RE 0.13279425196842923\n",
      "132 Train Loss 1594.7513 Test MSE 229.90168506165983 Test RE 0.13269865501534947\n",
      "133 Train Loss 1593.9342 Test MSE 227.80861014387946 Test RE 0.13209321514959674\n",
      "134 Train Loss 1592.6338 Test MSE 226.19654662061816 Test RE 0.13162501355475992\n",
      "135 Train Loss 1590.3652 Test MSE 226.37734730406856 Test RE 0.13167760750282\n",
      "136 Train Loss 1585.9468 Test MSE 224.21326682069784 Test RE 0.1310467024057608\n",
      "137 Train Loss 1583.6016 Test MSE 223.20847433808424 Test RE 0.13075273542276375\n",
      "138 Train Loss 1581.875 Test MSE 223.55154807164385 Test RE 0.13085318099443072\n",
      "139 Train Loss 1580.2341 Test MSE 225.55855106780407 Test RE 0.13143925591907119\n",
      "140 Train Loss 1577.053 Test MSE 225.49541438434233 Test RE 0.13142085887878868\n",
      "141 Train Loss 1573.2156 Test MSE 225.77216048833984 Test RE 0.13150147927301858\n",
      "142 Train Loss 1567.8174 Test MSE 221.74868862712205 Test RE 0.1303244719470708\n",
      "143 Train Loss 1566.5894 Test MSE 223.5740977222096 Test RE 0.13085978040992774\n",
      "144 Train Loss 1564.564 Test MSE 221.68896686476097 Test RE 0.13030692115272532\n",
      "145 Train Loss 1563.4976 Test MSE 221.73500553611254 Test RE 0.13032045102335935\n",
      "146 Train Loss 1561.8688 Test MSE 220.90544473200632 Test RE 0.13007644344710673\n",
      "147 Train Loss 1560.5951 Test MSE 221.47774101763702 Test RE 0.130244827965135\n",
      "148 Train Loss 1559.4153 Test MSE 220.21322848209527 Test RE 0.12987248361366482\n",
      "149 Train Loss 1557.1265 Test MSE 220.64247868360602 Test RE 0.12999899883262686\n",
      "150 Train Loss 1555.3904 Test MSE 221.45238016833684 Test RE 0.13023737075069367\n",
      "151 Train Loss 1551.9897 Test MSE 220.89600874792845 Test RE 0.13007366530744124\n",
      "152 Train Loss 1548.1663 Test MSE 219.4548548986105 Test RE 0.1296486623547558\n",
      "153 Train Loss 1546.7369 Test MSE 219.385903801923 Test RE 0.12962829342798352\n",
      "154 Train Loss 1544.8188 Test MSE 217.6277669897651 Test RE 0.12910783447881297\n",
      "155 Train Loss 1542.9391 Test MSE 216.64618124113412 Test RE 0.1288163421841058\n",
      "156 Train Loss 1541.4237 Test MSE 217.54772135225056 Test RE 0.1290840887281644\n",
      "157 Train Loss 1540.0034 Test MSE 219.0944345064252 Test RE 0.12954215474662406\n",
      "158 Train Loss 1538.3732 Test MSE 217.9418899513353 Test RE 0.12920097772047923\n",
      "159 Train Loss 1533.78 Test MSE 216.2712584770644 Test RE 0.128704830658842\n",
      "160 Train Loss 1531.1073 Test MSE 215.9469327045516 Test RE 0.1286082899647274\n",
      "161 Train Loss 1530.4896 Test MSE 216.3165273638402 Test RE 0.12871829990018915\n",
      "162 Train Loss 1527.2205 Test MSE 216.57182130342716 Test RE 0.12879423333085346\n",
      "163 Train Loss 1525.3643 Test MSE 217.1944370835199 Test RE 0.12897923378660525\n",
      "164 Train Loss 1524.1782 Test MSE 217.84322506272227 Test RE 0.129171728999289\n",
      "165 Train Loss 1523.4081 Test MSE 216.93361369087356 Test RE 0.12890176654697888\n",
      "166 Train Loss 1522.6423 Test MSE 216.89305779007108 Test RE 0.12888971684310155\n",
      "167 Train Loss 1521.3861 Test MSE 217.14898208232268 Test RE 0.1289657365302301\n",
      "168 Train Loss 1518.7943 Test MSE 215.4188944043297 Test RE 0.1284509557839698\n",
      "169 Train Loss 1515.1847 Test MSE 214.50746614529385 Test RE 0.12817893246029954\n",
      "170 Train Loss 1514.2001 Test MSE 214.28240739966776 Test RE 0.1281116728890189\n",
      "171 Train Loss 1512.7065 Test MSE 214.17957578287863 Test RE 0.12808092955480005\n",
      "172 Train Loss 1510.7902 Test MSE 214.12337684205121 Test RE 0.1280641247665949\n",
      "173 Train Loss 1510.388 Test MSE 214.57262231171217 Test RE 0.128198398015281\n",
      "174 Train Loss 1510.2095 Test MSE 214.60295281779625 Test RE 0.12820745831582678\n",
      "175 Train Loss 1509.477 Test MSE 214.82071915218222 Test RE 0.12827249048048073\n",
      "176 Train Loss 1508.5085 Test MSE 215.20079937705407 Test RE 0.12838591597163432\n",
      "177 Train Loss 1507.0074 Test MSE 216.63108978305297 Test RE 0.12881185546693116\n",
      "178 Train Loss 1503.5958 Test MSE 218.3310367378856 Test RE 0.12931627387064173\n",
      "179 Train Loss 1501.1067 Test MSE 219.57677378813904 Test RE 0.1296846707315233\n",
      "180 Train Loss 1499.965 Test MSE 220.3133780307705 Test RE 0.12990201224879092\n",
      "181 Train Loss 1499.1074 Test MSE 219.79322270812142 Test RE 0.1297485736486642\n",
      "182 Train Loss 1498.1562 Test MSE 219.79167975747612 Test RE 0.12974811822971563\n",
      "183 Train Loss 1497.1354 Test MSE 219.18253816688377 Test RE 0.12956819829011282\n",
      "184 Train Loss 1496.4062 Test MSE 220.05568306133978 Test RE 0.12982601847608827\n",
      "185 Train Loss 1494.0669 Test MSE 219.0298873814478 Test RE 0.12952307121853282\n",
      "186 Train Loss 1490.8209 Test MSE 219.87105326242877 Test RE 0.12977154412395905\n",
      "187 Train Loss 1489.4736 Test MSE 217.69413225191846 Test RE 0.12912751859880975\n",
      "188 Train Loss 1488.789 Test MSE 218.40383159533306 Test RE 0.1293378300711202\n",
      "189 Train Loss 1487.298 Test MSE 218.2395454279986 Test RE 0.12928917613136323\n",
      "190 Train Loss 1485.2858 Test MSE 219.5148532564594 Test RE 0.12966638393864505\n",
      "191 Train Loss 1484.1774 Test MSE 219.8281621933658 Test RE 0.12975888599626922\n",
      "192 Train Loss 1481.7373 Test MSE 220.42672161827454 Test RE 0.12993542299084532\n",
      "193 Train Loss 1479.9801 Test MSE 220.15314734695883 Test RE 0.12985476574407212\n",
      "194 Train Loss 1475.8741 Test MSE 221.36822450814915 Test RE 0.1302126221944289\n",
      "195 Train Loss 1473.1813 Test MSE 220.76683106088515 Test RE 0.13003562688261397\n",
      "196 Train Loss 1471.3677 Test MSE 221.0843324743778 Test RE 0.13012910030319336\n",
      "197 Train Loss 1469.8062 Test MSE 219.92221759620287 Test RE 0.1297866422650711\n",
      "198 Train Loss 1468.616 Test MSE 222.01939727927757 Test RE 0.13040399711037687\n",
      "199 Train Loss 1467.5348 Test MSE 221.8385702126736 Test RE 0.1303508815360061\n",
      "Training time: 639.08\n",
      "Training time: 639.08\n",
      "ES_rowdy_medium\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 19277.43 Test MSE 12296.254451754881 Test RE 0.9704694413201902\n",
      "1 Train Loss 17736.402 Test MSE 10683.979518332577 Test RE 0.9046112140934087\n",
      "2 Train Loss 11657.401 Test MSE 4620.379496347937 Test RE 0.5948867114546986\n",
      "3 Train Loss 10001.496 Test MSE 3003.6138275206577 Test RE 0.4796423159650972\n",
      "4 Train Loss 9961.184 Test MSE 2978.7067062942688 Test RE 0.47764948674667657\n",
      "5 Train Loss 9637.556 Test MSE 2799.741215994018 Test RE 0.4630782561805991\n",
      "6 Train Loss 9130.073 Test MSE 2627.9259366265683 Test RE 0.4486441450528499\n",
      "7 Train Loss 8589.088 Test MSE 2434.957515492604 Test RE 0.43185816780912833\n",
      "8 Train Loss 8148.6665 Test MSE 2284.568059342263 Test RE 0.41830927500276494\n",
      "9 Train Loss 7696.184 Test MSE 2115.1500849782437 Test RE 0.40250014241304627\n",
      "10 Train Loss 7327.5156 Test MSE 1892.9570739394828 Test RE 0.38077272030194276\n",
      "11 Train Loss 6830.4365 Test MSE 1771.1529735364836 Test RE 0.3683184549273725\n",
      "12 Train Loss 6385.4126 Test MSE 1566.9998157361622 Test RE 0.34644150495116444\n",
      "13 Train Loss 6095.123 Test MSE 1503.5785369628295 Test RE 0.33935832114863207\n",
      "14 Train Loss 5808.8516 Test MSE 1383.1424413369612 Test RE 0.32548344024342735\n",
      "15 Train Loss 5445.3877 Test MSE 1362.9948508569107 Test RE 0.323104161305928\n",
      "16 Train Loss 5276.8833 Test MSE 1372.696099170313 Test RE 0.3242519851249262\n",
      "17 Train Loss 4916.8047 Test MSE 1242.7110344310706 Test RE 0.30851800978612415\n",
      "18 Train Loss 4683.5293 Test MSE 1032.7263723193169 Test RE 0.28124711590818197\n",
      "19 Train Loss 4541.007 Test MSE 951.4430675035214 Test RE 0.2699521849006617\n",
      "20 Train Loss 4062.8162 Test MSE 854.1945276996668 Test RE 0.2557842718511166\n",
      "21 Train Loss 3823.2715 Test MSE 787.7371343527208 Test RE 0.24563265543312268\n",
      "22 Train Loss 3581.2625 Test MSE 742.128127927123 Test RE 0.23841572147939066\n",
      "23 Train Loss 3457.963 Test MSE 679.0390986055319 Test RE 0.22805669934620934\n",
      "24 Train Loss 3348.6406 Test MSE 659.773080104029 Test RE 0.2247981535083046\n",
      "25 Train Loss 3182.9805 Test MSE 602.6532719107066 Test RE 0.21484695516902638\n",
      "26 Train Loss 3063.7952 Test MSE 553.6153511788059 Test RE 0.20592046292784189\n",
      "27 Train Loss 2870.16 Test MSE 467.8308290731778 Test RE 0.18929531375020212\n",
      "28 Train Loss 2704.1326 Test MSE 380.95799956409945 Test RE 0.17081814156382277\n",
      "29 Train Loss 2645.9753 Test MSE 343.5950366391661 Test RE 0.16222541156959047\n",
      "30 Train Loss 2589.2236 Test MSE 328.5831163022698 Test RE 0.15864195964389005\n",
      "31 Train Loss 2499.5068 Test MSE 308.9888992444769 Test RE 0.15383915381682753\n",
      "32 Train Loss 2420.6826 Test MSE 331.00759514876194 Test RE 0.15922616066690287\n",
      "33 Train Loss 2289.8577 Test MSE 307.12440382763106 Test RE 0.15337430474276006\n",
      "34 Train Loss 2258.206 Test MSE 309.68080199083334 Test RE 0.15401129950320708\n",
      "35 Train Loss 2189.6172 Test MSE 296.0917180179489 Test RE 0.15059431375235044\n",
      "36 Train Loss 2118.1548 Test MSE 281.46808886427993 Test RE 0.14682838637549958\n",
      "37 Train Loss 2076.4692 Test MSE 275.6370725382204 Test RE 0.1452995462951989\n",
      "38 Train Loss 2044.1595 Test MSE 274.8388144892637 Test RE 0.1450889965518054\n",
      "39 Train Loss 2000.6641 Test MSE 268.3980355579768 Test RE 0.14337885578699983\n",
      "40 Train Loss 1992.624 Test MSE 268.12513623822235 Test RE 0.14330594552288228\n",
      "41 Train Loss 1959.6902 Test MSE 263.6552933922594 Test RE 0.1421064176045085\n",
      "42 Train Loss 1916.239 Test MSE 265.5609703242554 Test RE 0.14261905919285672\n",
      "43 Train Loss 1907.2074 Test MSE 263.1220609634238 Test RE 0.1419626425613529\n",
      "44 Train Loss 1901.6818 Test MSE 263.86588352810435 Test RE 0.1421631588063002\n",
      "45 Train Loss 1886.8567 Test MSE 262.50346101252586 Test RE 0.14179566728144596\n",
      "46 Train Loss 1869.66 Test MSE 264.3522638138681 Test RE 0.14229412216265952\n",
      "47 Train Loss 1859.9069 Test MSE 263.866538462403 Test RE 0.1421633352358365\n",
      "48 Train Loss 1856.3153 Test MSE 264.48715396684315 Test RE 0.1423304215075738\n",
      "49 Train Loss 1851.9806 Test MSE 263.2648305603703 Test RE 0.14200115168634303\n",
      "50 Train Loss 1849.9325 Test MSE 262.4598258376775 Test RE 0.14178388165421013\n",
      "51 Train Loss 1848.2019 Test MSE 262.196825057059 Test RE 0.1417128257932788\n",
      "52 Train Loss 1844.1569 Test MSE 262.74169727745016 Test RE 0.1418599963554398\n",
      "53 Train Loss 1840.7211 Test MSE 261.47803113028067 Test RE 0.14151844467055805\n",
      "54 Train Loss 1837.3484 Test MSE 260.41281417207637 Test RE 0.14122988947091325\n",
      "55 Train Loss 1833.8872 Test MSE 257.8584926594942 Test RE 0.1405355389863635\n",
      "56 Train Loss 1830.8015 Test MSE 256.1323951778995 Test RE 0.14006437871065902\n",
      "57 Train Loss 1828.844 Test MSE 253.67613094340183 Test RE 0.1393911645484727\n",
      "58 Train Loss 1824.8435 Test MSE 251.51742586673262 Test RE 0.13879680964696398\n",
      "59 Train Loss 1817.7206 Test MSE 250.53049914609124 Test RE 0.13852423028224356\n",
      "60 Train Loss 1809.1793 Test MSE 248.45379302986203 Test RE 0.13794890562015186\n",
      "61 Train Loss 1797.1626 Test MSE 248.02041633883277 Test RE 0.13782854132133482\n",
      "62 Train Loss 1793.0068 Test MSE 248.9762194565492 Test RE 0.13809386277317043\n",
      "63 Train Loss 1791.7632 Test MSE 248.05585108442486 Test RE 0.13783838677063268\n",
      "64 Train Loss 1790.0428 Test MSE 248.73411904043323 Test RE 0.138026706333508\n",
      "65 Train Loss 1785.3309 Test MSE 246.7649833882932 Test RE 0.13747926762471865\n",
      "66 Train Loss 1779.4126 Test MSE 245.33473993510904 Test RE 0.13708027551016563\n",
      "67 Train Loss 1775.0182 Test MSE 243.11649662221427 Test RE 0.13645914889385252\n",
      "68 Train Loss 1772.6202 Test MSE 241.9801964140286 Test RE 0.13613987774639827\n",
      "69 Train Loss 1769.3818 Test MSE 240.54911728578398 Test RE 0.13573671284141178\n",
      "70 Train Loss 1766.8444 Test MSE 240.46820800344088 Test RE 0.1357138832343262\n",
      "71 Train Loss 1765.6476 Test MSE 239.98374751997477 Test RE 0.13557710598147857\n",
      "72 Train Loss 1763.8381 Test MSE 239.46386265510205 Test RE 0.1354301737396418\n",
      "73 Train Loss 1760.7649 Test MSE 240.53961133529364 Test RE 0.1357340308128218\n",
      "74 Train Loss 1755.6757 Test MSE 240.83742743458143 Test RE 0.13581803210220217\n",
      "75 Train Loss 1753.5522 Test MSE 239.17267091552986 Test RE 0.1353478061049604\n",
      "76 Train Loss 1751.7327 Test MSE 237.8541080282305 Test RE 0.13497420312755976\n",
      "77 Train Loss 1750.5295 Test MSE 237.1289618922961 Test RE 0.13476829805557677\n",
      "78 Train Loss 1749.1619 Test MSE 236.5980698931449 Test RE 0.134617351707308\n",
      "79 Train Loss 1747.451 Test MSE 236.6058070915983 Test RE 0.1346195528086331\n",
      "80 Train Loss 1745.4985 Test MSE 235.26045846103102 Test RE 0.13423628154086492\n",
      "81 Train Loss 1740.9923 Test MSE 233.912760594009 Test RE 0.13385124067271018\n",
      "82 Train Loss 1738.6194 Test MSE 233.6077831496305 Test RE 0.13376395401982502\n",
      "83 Train Loss 1734.5398 Test MSE 235.42376376843092 Test RE 0.13428286330496544\n",
      "84 Train Loss 1733.1914 Test MSE 234.47814547901535 Test RE 0.13401290734228583\n",
      "85 Train Loss 1732.5435 Test MSE 234.81581773106737 Test RE 0.13410936869869786\n",
      "86 Train Loss 1730.3105 Test MSE 234.42577905555706 Test RE 0.13399794183557012\n",
      "87 Train Loss 1728.796 Test MSE 233.98080006847465 Test RE 0.1338707062757886\n",
      "88 Train Loss 1727.4305 Test MSE 232.80099601530176 Test RE 0.13353277085001483\n",
      "89 Train Loss 1726.5399 Test MSE 233.39651721118602 Test RE 0.13370345483814397\n",
      "90 Train Loss 1725.3794 Test MSE 232.57397496028847 Test RE 0.13346764624752755\n",
      "91 Train Loss 1719.4552 Test MSE 232.9999202687063 Test RE 0.13358980934049117\n",
      "92 Train Loss 1715.2816 Test MSE 233.21932821675486 Test RE 0.13365269298644092\n",
      "93 Train Loss 1712.1208 Test MSE 231.4476157732545 Test RE 0.13314406110159963\n",
      "94 Train Loss 1710.5289 Test MSE 230.80162840146465 Test RE 0.13295812382083583\n",
      "95 Train Loss 1709.2379 Test MSE 232.12194972368937 Test RE 0.1333378808959765\n",
      "96 Train Loss 1706.7316 Test MSE 234.0179029990189 Test RE 0.13388131996422736\n",
      "97 Train Loss 1696.3373 Test MSE 233.8695787102601 Test RE 0.13383888517855339\n",
      "98 Train Loss 1690.6569 Test MSE 234.3044331467034 Test RE 0.13396325664261144\n",
      "99 Train Loss 1688.0315 Test MSE 232.9453416451469 Test RE 0.13357416217861176\n",
      "100 Train Loss 1685.913 Test MSE 233.63139341167278 Test RE 0.13377071347400377\n",
      "101 Train Loss 1684.4011 Test MSE 233.89731007540374 Test RE 0.13384681999632098\n",
      "102 Train Loss 1683.6371 Test MSE 235.23236739006768 Test RE 0.13422826711824268\n",
      "103 Train Loss 1679.8481 Test MSE 234.82122266719233 Test RE 0.13411091213901527\n",
      "104 Train Loss 1676.0765 Test MSE 232.40350139322535 Test RE 0.13341872229412285\n",
      "105 Train Loss 1673.2605 Test MSE 230.99065743694956 Test RE 0.13301255974953194\n",
      "106 Train Loss 1672.0328 Test MSE 230.8306610969281 Test RE 0.13296648600476074\n",
      "107 Train Loss 1669.9504 Test MSE 230.19638948670607 Test RE 0.13278367909063415\n",
      "108 Train Loss 1668.6162 Test MSE 230.04747998781104 Test RE 0.1327407245728762\n",
      "109 Train Loss 1667.2411 Test MSE 229.5454491569186 Test RE 0.13259580593926837\n",
      "110 Train Loss 1665.8551 Test MSE 229.24763139954615 Test RE 0.13250976153577196\n",
      "111 Train Loss 1665.6285 Test MSE 229.51156957884814 Test RE 0.13258602039190323\n",
      "112 Train Loss 1664.5303 Test MSE 229.12998509876104 Test RE 0.13247575620336152\n",
      "113 Train Loss 1662.9393 Test MSE 228.58175616560317 Test RE 0.13231717692058134\n",
      "114 Train Loss 1661.7052 Test MSE 228.3360976827328 Test RE 0.132246056688858\n",
      "115 Train Loss 1660.939 Test MSE 227.6373603537709 Test RE 0.13204355682425264\n",
      "116 Train Loss 1660.3473 Test MSE 228.1446242866232 Test RE 0.1321905969685216\n",
      "117 Train Loss 1657.3005 Test MSE 226.07444024275443 Test RE 0.13158948157269554\n",
      "118 Train Loss 1653.0685 Test MSE 228.13994778425038 Test RE 0.132189242141928\n",
      "119 Train Loss 1651.2704 Test MSE 227.0035388557274 Test RE 0.13185960111715597\n",
      "120 Train Loss 1649.3132 Test MSE 226.00788230868778 Test RE 0.13157010970396016\n",
      "121 Train Loss 1648.478 Test MSE 225.3255976242018 Test RE 0.13137136415216819\n",
      "122 Train Loss 1648.311 Test MSE 225.50220168357222 Test RE 0.13142283671497806\n",
      "123 Train Loss 1647.827 Test MSE 226.3871953900082 Test RE 0.13168047165488164\n",
      "124 Train Loss 1646.614 Test MSE 226.34741363382932 Test RE 0.1316689014100179\n",
      "125 Train Loss 1646.362 Test MSE 227.37003298963958 Test RE 0.13196600095778724\n",
      "126 Train Loss 1646.2463 Test MSE 227.23637509747718 Test RE 0.13192720760803717\n",
      "127 Train Loss 1646.1602 Test MSE 227.27764671329862 Test RE 0.1319391876482483\n",
      "128 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "129 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "130 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "131 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "132 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "133 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "134 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "135 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "136 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "137 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "138 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "139 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "140 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "141 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "142 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "143 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "144 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "145 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "146 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "147 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "148 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "149 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "150 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "151 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "152 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "153 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "154 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "155 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "156 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "157 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "158 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "159 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "160 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "161 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "162 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "163 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "164 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "165 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "166 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "167 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "168 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "169 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "170 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "171 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "172 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "173 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "174 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "175 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "176 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "177 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "178 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "179 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "180 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "181 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "182 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "183 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "184 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "185 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "186 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "187 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "188 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "189 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "190 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "191 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "192 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "193 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "194 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "195 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "196 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "197 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "198 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "199 Train Loss 1646.1514 Test MSE 227.36050437542136 Test RE 0.13196323571587934\n",
      "Training time: 458.18\n",
      "Training time: 458.18\n",
      "ES_rowdy_medium\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 17437.717 Test MSE 10386.608007064542 Test RE 0.8919331688622661\n",
      "1 Train Loss 13767.499 Test MSE 6644.433609575281 Test RE 0.7133857450970162\n",
      "2 Train Loss 9998.9795 Test MSE 3002.297485108485 Test RE 0.4795372021339065\n",
      "3 Train Loss 9861.865 Test MSE 2929.087230822938 Test RE 0.47365442243143524\n",
      "4 Train Loss 9243.7295 Test MSE 2654.408724959984 Test RE 0.4508990724520937\n",
      "5 Train Loss 8701.119 Test MSE 2470.9109763085394 Test RE 0.43503479399647177\n",
      "6 Train Loss 8153.457 Test MSE 2257.229771869253 Test RE 0.41579889296040906\n",
      "7 Train Loss 7418.998 Test MSE 1971.6840991643485 Test RE 0.38861012424108704\n",
      "8 Train Loss 7091.205 Test MSE 1859.8239641818145 Test RE 0.37742560815293363\n",
      "9 Train Loss 6602.583 Test MSE 1686.8739446169086 Test RE 0.3594485689426493\n",
      "10 Train Loss 6238.9624 Test MSE 1557.5175203969852 Test RE 0.34539171368682775\n",
      "11 Train Loss 5836.502 Test MSE 1428.4152510317574 Test RE 0.3307673874733174\n",
      "12 Train Loss 5474.1206 Test MSE 1239.4381581124883 Test RE 0.30811147642261555\n",
      "13 Train Loss 4917.078 Test MSE 1069.586555661639 Test RE 0.286222263247006\n",
      "14 Train Loss 4524.1685 Test MSE 956.2908155972275 Test RE 0.2706390349664206\n",
      "15 Train Loss 4170.766 Test MSE 854.5997684809565 Test RE 0.2558449383175947\n",
      "16 Train Loss 4036.9683 Test MSE 788.9870529812241 Test RE 0.2458274533713428\n",
      "17 Train Loss 3899.2598 Test MSE 756.4762460317944 Test RE 0.24070942281484203\n",
      "18 Train Loss 3777.807 Test MSE 724.357330361247 Test RE 0.23554390649368143\n",
      "19 Train Loss 3628.0552 Test MSE 682.6962803983818 Test RE 0.2286700107303944\n",
      "20 Train Loss 3487.5056 Test MSE 634.4483150254323 Test RE 0.22044160681924949\n",
      "21 Train Loss 3409.2932 Test MSE 611.6604339049005 Test RE 0.21644653514198223\n",
      "22 Train Loss 3346.3945 Test MSE 591.3737179664113 Test RE 0.21282686784073265\n",
      "23 Train Loss 3168.5688 Test MSE 546.8874970727823 Test RE 0.20466540591520374\n",
      "24 Train Loss 3056.662 Test MSE 500.4112648708005 Test RE 0.19577578887987118\n",
      "25 Train Loss 2984.3838 Test MSE 503.25189093077256 Test RE 0.19633067128894968\n",
      "26 Train Loss 2913.4512 Test MSE 475.8606545937964 Test RE 0.19091292987463676\n",
      "27 Train Loss 2868.98 Test MSE 458.09886696205143 Test RE 0.18731607654400742\n",
      "28 Train Loss 2809.2876 Test MSE 438.8453080808773 Test RE 0.18333744451030076\n",
      "29 Train Loss 2712.3154 Test MSE 409.9430934242363 Test RE 0.17719735330999745\n",
      "30 Train Loss 2675.1387 Test MSE 391.7916393286831 Test RE 0.1732299682818352\n",
      "31 Train Loss 2639.205 Test MSE 387.5734031444242 Test RE 0.1722949018466373\n",
      "32 Train Loss 2609.1516 Test MSE 365.5414881753617 Test RE 0.167326135784747\n",
      "33 Train Loss 2554.2202 Test MSE 366.99974880200625 Test RE 0.1676595620226856\n",
      "34 Train Loss 2527.1887 Test MSE 356.0434569236302 Test RE 0.1651379725610843\n",
      "35 Train Loss 2490.3574 Test MSE 352.57970591400897 Test RE 0.16433274109704413\n",
      "36 Train Loss 2422.4504 Test MSE 346.35371246149134 Test RE 0.16287535201952386\n",
      "37 Train Loss 2375.1135 Test MSE 339.01059907829807 Test RE 0.16113952609952037\n",
      "38 Train Loss 2350.8894 Test MSE 322.0031226408401 Test RE 0.15704549598073148\n",
      "39 Train Loss 2286.2275 Test MSE 314.99236553215263 Test RE 0.15532646475728698\n",
      "40 Train Loss 2203.3162 Test MSE 296.0417983681986 Test RE 0.15058161847614943\n",
      "41 Train Loss 2176.6655 Test MSE 289.6501916725097 Test RE 0.14894720357895433\n",
      "42 Train Loss 2141.2998 Test MSE 286.06991583714995 Test RE 0.148023796214508\n",
      "43 Train Loss 2112.5227 Test MSE 285.5084426699948 Test RE 0.14787846074563424\n",
      "44 Train Loss 2080.5422 Test MSE 280.85271581218007 Test RE 0.14666779326528798\n",
      "45 Train Loss 2060.532 Test MSE 276.1308393436883 Test RE 0.14542963037808102\n",
      "46 Train Loss 2032.9467 Test MSE 273.0884033559108 Test RE 0.1446262324841042\n",
      "47 Train Loss 1998.3767 Test MSE 261.1071065390971 Test RE 0.14141803221842897\n",
      "48 Train Loss 1981.3336 Test MSE 257.88686984035706 Test RE 0.14054327170180178\n",
      "49 Train Loss 1964.4467 Test MSE 255.93071407998772 Test RE 0.14000922383538397\n",
      "50 Train Loss 1948.8496 Test MSE 253.09714699551859 Test RE 0.13923200225563664\n",
      "51 Train Loss 1939.2808 Test MSE 251.4045651265413 Test RE 0.13876566574455224\n",
      "52 Train Loss 1936.3818 Test MSE 251.5684113592332 Test RE 0.13881087679376491\n",
      "53 Train Loss 1916.3182 Test MSE 252.029678190949 Test RE 0.13893807783944534\n",
      "54 Train Loss 1885.7866 Test MSE 253.8022578862437 Test RE 0.13942581265789428\n",
      "55 Train Loss 1877.136 Test MSE 250.68119167237165 Test RE 0.13856588474796108\n",
      "56 Train Loss 1862.8174 Test MSE 251.80560431915885 Test RE 0.13887630075803556\n",
      "57 Train Loss 1852.145 Test MSE 252.4766218166893 Test RE 0.1390612180635028\n",
      "58 Train Loss 1837.8274 Test MSE 247.28887472603336 Test RE 0.13762512707658398\n",
      "59 Train Loss 1821.741 Test MSE 247.55589514889354 Test RE 0.1376994102500233\n",
      "60 Train Loss 1814.8425 Test MSE 245.90536239555846 Test RE 0.13723959997732635\n",
      "61 Train Loss 1809.856 Test MSE 247.27818889359196 Test RE 0.13762215351997548\n",
      "62 Train Loss 1802.096 Test MSE 244.25221732463308 Test RE 0.13677751253256162\n",
      "63 Train Loss 1791.1914 Test MSE 244.07629807438374 Test RE 0.13672824761324243\n",
      "64 Train Loss 1785.8569 Test MSE 242.80269453686034 Test RE 0.13637105328364116\n",
      "65 Train Loss 1780.8268 Test MSE 243.1166796221482 Test RE 0.13645920025196862\n",
      "66 Train Loss 1777.561 Test MSE 243.24256673711065 Test RE 0.13649452533097622\n",
      "67 Train Loss 1774.2661 Test MSE 243.0275123880712 Test RE 0.13643417357475368\n",
      "68 Train Loss 1769.5955 Test MSE 242.58526874984545 Test RE 0.13630998059967023\n",
      "69 Train Loss 1765.9054 Test MSE 243.2626205314027 Test RE 0.13650015176543456\n",
      "70 Train Loss 1760.637 Test MSE 241.70542536444765 Test RE 0.13606256166272584\n",
      "71 Train Loss 1754.2485 Test MSE 241.6971584423287 Test RE 0.136060234805112\n",
      "72 Train Loss 1744.9633 Test MSE 242.5783112833138 Test RE 0.13630802586653068\n",
      "73 Train Loss 1740.7311 Test MSE 243.07094371570435 Test RE 0.13644636407228572\n",
      "74 Train Loss 1736.481 Test MSE 243.217249476669 Test RE 0.13648742180995974\n",
      "75 Train Loss 1733.6278 Test MSE 243.6266778532545 Test RE 0.13660225397290868\n",
      "76 Train Loss 1731.4983 Test MSE 243.6223201491948 Test RE 0.13660103227817502\n",
      "77 Train Loss 1727.5486 Test MSE 242.22495763452963 Test RE 0.1362087125957124\n",
      "78 Train Loss 1724.4873 Test MSE 244.8982840698414 Test RE 0.13695828682755326\n",
      "79 Train Loss 1715.1552 Test MSE 244.8236583179766 Test RE 0.1369374181761126\n",
      "80 Train Loss 1711.5221 Test MSE 245.75844927644314 Test RE 0.13719859780105995\n",
      "81 Train Loss 1709.554 Test MSE 245.8874217321784 Test RE 0.13723459355056172\n",
      "82 Train Loss 1707.9333 Test MSE 245.5108441356881 Test RE 0.1371294656122797\n",
      "83 Train Loss 1706.2803 Test MSE 246.60748081650834 Test RE 0.13743538620728604\n",
      "84 Train Loss 1704.5037 Test MSE 244.88462294775678 Test RE 0.1369544668130988\n",
      "85 Train Loss 1702.4404 Test MSE 244.573424251443 Test RE 0.13686741847175252\n",
      "86 Train Loss 1700.2887 Test MSE 244.90916805599502 Test RE 0.1369613302044011\n",
      "87 Train Loss 1698.5887 Test MSE 246.5117204204626 Test RE 0.13740869978065012\n",
      "88 Train Loss 1696.3273 Test MSE 247.2365272702299 Test RE 0.13761055968739566\n",
      "89 Train Loss 1695.134 Test MSE 247.70548855064138 Test RE 0.1377410085570225\n",
      "90 Train Loss 1692.9292 Test MSE 246.95911829888396 Test RE 0.13753333582704427\n",
      "91 Train Loss 1690.9121 Test MSE 246.91003194120523 Test RE 0.13751966687232428\n",
      "92 Train Loss 1688.5106 Test MSE 247.61445906688348 Test RE 0.13771569695587177\n",
      "93 Train Loss 1686.4175 Test MSE 249.20285943545372 Test RE 0.1381567010454389\n",
      "94 Train Loss 1684.0946 Test MSE 249.34681057188652 Test RE 0.13819659814566573\n",
      "95 Train Loss 1682.5812 Test MSE 250.38587613012493 Test RE 0.13848424176970076\n",
      "96 Train Loss 1681.6129 Test MSE 248.79060226250604 Test RE 0.13804237718435663\n",
      "97 Train Loss 1680.8011 Test MSE 248.2562638936424 Test RE 0.13789405770266747\n",
      "98 Train Loss 1680.3875 Test MSE 246.920476165086 Test RE 0.13752257536289722\n",
      "99 Train Loss 1679.2054 Test MSE 245.93316902065624 Test RE 0.137247359186382\n",
      "100 Train Loss 1678.1353 Test MSE 245.2240199710098 Test RE 0.1370493397446399\n",
      "101 Train Loss 1676.9081 Test MSE 244.7752038466741 Test RE 0.13692386646591115\n",
      "102 Train Loss 1676.3837 Test MSE 245.36697174949447 Test RE 0.13708927994404127\n",
      "103 Train Loss 1675.3428 Test MSE 246.43282247378218 Test RE 0.13738670867238498\n",
      "104 Train Loss 1674.9116 Test MSE 246.5361053203559 Test RE 0.13741549583587295\n",
      "105 Train Loss 1674.267 Test MSE 246.1358356545202 Test RE 0.13730389839229382\n",
      "106 Train Loss 1673.8177 Test MSE 245.23591869617738 Test RE 0.13705266464863464\n",
      "107 Train Loss 1672.9576 Test MSE 246.16997129696188 Test RE 0.13731341913980324\n",
      "108 Train Loss 1671.3568 Test MSE 243.87823265912667 Test RE 0.13667275956790326\n",
      "109 Train Loss 1668.8329 Test MSE 242.67620295820288 Test RE 0.1363355264187864\n",
      "110 Train Loss 1667.6403 Test MSE 241.07806556283745 Test RE 0.1358858679810871\n",
      "111 Train Loss 1667.2605 Test MSE 241.42101152351643 Test RE 0.13598248595407586\n",
      "112 Train Loss 1666.7378 Test MSE 240.43036446277276 Test RE 0.1357032038684705\n",
      "113 Train Loss 1665.679 Test MSE 239.40854284008793 Test RE 0.1354145296152726\n",
      "114 Train Loss 1665.202 Test MSE 238.3489683181544 Test RE 0.13511453845474172\n",
      "115 Train Loss 1664.275 Test MSE 237.54941423238563 Test RE 0.13488772368490914\n",
      "116 Train Loss 1663.373 Test MSE 235.94602887894428 Test RE 0.13443172763695546\n",
      "117 Train Loss 1660.8055 Test MSE 235.42059237602786 Test RE 0.13428195884001265\n",
      "118 Train Loss 1659.3882 Test MSE 234.02494694779324 Test RE 0.13388333486576098\n",
      "119 Train Loss 1657.9319 Test MSE 234.0185329858951 Test RE 0.13388150017141814\n",
      "120 Train Loss 1656.3262 Test MSE 233.97829483956747 Test RE 0.13386998959805088\n",
      "121 Train Loss 1655.5475 Test MSE 234.68370914409508 Test RE 0.13407163808074013\n",
      "122 Train Loss 1654.8738 Test MSE 234.81672034188094 Test RE 0.13410962645060576\n",
      "123 Train Loss 1653.2854 Test MSE 234.18048579823335 Test RE 0.13392781858719932\n",
      "124 Train Loss 1651.6007 Test MSE 232.83050605901553 Test RE 0.1335412339431787\n",
      "125 Train Loss 1650.9457 Test MSE 232.5837615739721 Test RE 0.13347045434880492\n",
      "126 Train Loss 1650.2903 Test MSE 232.20570937705543 Test RE 0.13336193576751004\n",
      "127 Train Loss 1648.8368 Test MSE 230.82667536159545 Test RE 0.13296533803874408\n",
      "128 Train Loss 1647.8899 Test MSE 230.2986075455218 Test RE 0.13281315692808607\n",
      "129 Train Loss 1646.5568 Test MSE 231.63517519897164 Test RE 0.13319799850411868\n",
      "130 Train Loss 1645.4729 Test MSE 233.1005238481347 Test RE 0.13361864660912548\n",
      "131 Train Loss 1644.6417 Test MSE 232.06031728592075 Test RE 0.1333201779509721\n",
      "132 Train Loss 1644.1376 Test MSE 231.90128887924473 Test RE 0.13327448868946204\n",
      "133 Train Loss 1643.3855 Test MSE 232.55034133669193 Test RE 0.1334608647401553\n",
      "134 Train Loss 1642.1012 Test MSE 233.66966441117975 Test RE 0.13378166947010256\n",
      "135 Train Loss 1641.1593 Test MSE 232.59817167411438 Test RE 0.1334745889728442\n",
      "136 Train Loss 1640.5837 Test MSE 232.2814931657725 Test RE 0.13338369631833583\n",
      "137 Train Loss 1639.7698 Test MSE 232.74590667137093 Test RE 0.13351697051387768\n",
      "138 Train Loss 1639.0219 Test MSE 231.56399549365045 Test RE 0.13317753157213005\n",
      "139 Train Loss 1638.6104 Test MSE 231.59084619572442 Test RE 0.1331852525620913\n",
      "140 Train Loss 1638.2728 Test MSE 231.43606665433362 Test RE 0.13314073914951727\n",
      "141 Train Loss 1637.819 Test MSE 231.90399013838365 Test RE 0.1332752648987792\n",
      "142 Train Loss 1637.4678 Test MSE 232.11525099755588 Test RE 0.13333595690646888\n",
      "143 Train Loss 1637.1394 Test MSE 231.83378953220398 Test RE 0.13325509122144222\n",
      "144 Train Loss 1636.5529 Test MSE 231.49040199108606 Test RE 0.13315636727189037\n",
      "145 Train Loss 1636.3074 Test MSE 231.36848468147406 Test RE 0.13312129842249687\n",
      "146 Train Loss 1635.9521 Test MSE 230.93073319472836 Test RE 0.13299530538533333\n",
      "147 Train Loss 1635.7308 Test MSE 230.555342940111 Test RE 0.13288716593077798\n",
      "148 Train Loss 1635.5839 Test MSE 230.41537870212144 Test RE 0.1328468236113364\n",
      "149 Train Loss 1635.3959 Test MSE 230.48650201266014 Test RE 0.13286732523090655\n",
      "150 Train Loss 1635.1334 Test MSE 230.9796767582219 Test RE 0.13300939818109478\n",
      "151 Train Loss 1634.6239 Test MSE 231.63611466818847 Test RE 0.1331982686170071\n",
      "152 Train Loss 1634.3949 Test MSE 231.67197842933433 Test RE 0.13320857963005628\n",
      "153 Train Loss 1634.1177 Test MSE 231.5760811769128 Test RE 0.1331810068975152\n",
      "154 Train Loss 1633.5853 Test MSE 231.45999717261253 Test RE 0.1331476223561426\n",
      "155 Train Loss 1632.4905 Test MSE 231.62735428087018 Test RE 0.13319574983996382\n",
      "156 Train Loss 1632.0232 Test MSE 231.3259206600972 Test RE 0.13310905293793987\n",
      "157 Train Loss 1631.6842 Test MSE 232.04276568711862 Test RE 0.13331513610118473\n",
      "158 Train Loss 1631.0985 Test MSE 231.3035905151251 Test RE 0.13310262820141422\n",
      "159 Train Loss 1630.4547 Test MSE 232.25914352835105 Test RE 0.13337727921441225\n",
      "160 Train Loss 1630.1492 Test MSE 232.52200356003237 Test RE 0.13345273295376894\n",
      "161 Train Loss 1629.1669 Test MSE 232.76005934708527 Test RE 0.1335210298627362\n",
      "162 Train Loss 1628.6829 Test MSE 232.4672539200184 Test RE 0.13343702063595936\n",
      "163 Train Loss 1628.2708 Test MSE 232.02858111831276 Test RE 0.13331106132079834\n",
      "164 Train Loss 1627.795 Test MSE 231.89074312966028 Test RE 0.13327145831526815\n",
      "165 Train Loss 1627.393 Test MSE 231.33855669858502 Test RE 0.13311268838904425\n",
      "166 Train Loss 1627.2322 Test MSE 231.31110890769673 Test RE 0.13310479139671336\n",
      "167 Train Loss 1627.1104 Test MSE 231.40855165287638 Test RE 0.13313282448743827\n",
      "168 Train Loss 1626.6733 Test MSE 231.25190138040696 Test RE 0.13308775522846958\n",
      "169 Train Loss 1625.7528 Test MSE 229.7088947926785 Test RE 0.13264300432499487\n",
      "170 Train Loss 1624.6674 Test MSE 229.6453870494032 Test RE 0.13262466711565174\n",
      "171 Train Loss 1623.8827 Test MSE 229.63500571517383 Test RE 0.1326216693707927\n",
      "172 Train Loss 1623.1858 Test MSE 229.54129295930565 Test RE 0.13259460553022362\n",
      "173 Train Loss 1621.9811 Test MSE 228.8012974375482 Test RE 0.1323807036650401\n",
      "174 Train Loss 1621.1628 Test MSE 229.2381626621331 Test RE 0.13250702494677177\n",
      "175 Train Loss 1620.331 Test MSE 230.3040294656221 Test RE 0.13281472032895622\n",
      "176 Train Loss 1619.9474 Test MSE 229.8377331985744 Test RE 0.13268019730709402\n",
      "177 Train Loss 1619.4677 Test MSE 230.13854898055288 Test RE 0.13276699603636669\n",
      "178 Train Loss 1618.1232 Test MSE 228.6564884955069 Test RE 0.13233880498703676\n",
      "179 Train Loss 1615.9388 Test MSE 229.19572955393554 Test RE 0.13249476052909814\n",
      "180 Train Loss 1613.1522 Test MSE 226.1322938656487 Test RE 0.13160631771138273\n",
      "181 Train Loss 1610.767 Test MSE 224.37893361371488 Test RE 0.13109510738737085\n",
      "182 Train Loss 1608.9902 Test MSE 224.317463496546 Test RE 0.13107714896437503\n",
      "183 Train Loss 1607.9464 Test MSE 223.9336049755155 Test RE 0.1309649494428813\n",
      "184 Train Loss 1606.821 Test MSE 224.87345545592555 Test RE 0.131239491960274\n",
      "185 Train Loss 1604.5492 Test MSE 223.96090304914577 Test RE 0.13097293167914895\n",
      "186 Train Loss 1603.7175 Test MSE 224.56511450736772 Test RE 0.13114948491487574\n",
      "187 Train Loss 1603.1407 Test MSE 223.65034987187164 Test RE 0.1308820940164781\n",
      "188 Train Loss 1602.5612 Test MSE 222.9939125023742 Test RE 0.1306898764918813\n",
      "189 Train Loss 1601.8469 Test MSE 223.13436800766283 Test RE 0.13073102833967384\n",
      "190 Train Loss 1600.9193 Test MSE 223.91560400030306 Test RE 0.13095968550719864\n",
      "191 Train Loss 1600.2197 Test MSE 222.25633567771885 Test RE 0.13047356192208762\n",
      "192 Train Loss 1599.2458 Test MSE 222.25944276661525 Test RE 0.13047447391304945\n",
      "193 Train Loss 1598.7885 Test MSE 222.77038874346206 Test RE 0.13062435986809956\n",
      "194 Train Loss 1598.2422 Test MSE 223.1598700754941 Test RE 0.1307384987628302\n",
      "195 Train Loss 1598.0814 Test MSE 223.1149572575056 Test RE 0.13072534198453956\n",
      "196 Train Loss 1597.927 Test MSE 223.27548504688048 Test RE 0.1307723609678785\n",
      "197 Train Loss 1597.4604 Test MSE 222.82355003368022 Test RE 0.13063994485071212\n",
      "198 Train Loss 1595.7992 Test MSE 221.8805865661784 Test RE 0.13036322521773613\n",
      "199 Train Loss 1594.316 Test MSE 222.20094192757475 Test RE 0.1304573017100837\n",
      "Training time: 650.16\n",
      "Training time: 650.16\n",
      "ES_rowdy_medium\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 17772.543 Test MSE 10535.707599307589 Test RE 0.8983122009131456\n",
      "1 Train Loss 12289.65 Test MSE 5041.364369704284 Test RE 0.6213974799234829\n",
      "2 Train Loss 10076.587 Test MSE 3072.6491041164513 Test RE 0.4851230691370958\n",
      "3 Train Loss 9994.793 Test MSE 2995.984938208205 Test RE 0.479032806093297\n",
      "4 Train Loss 9989.915 Test MSE 2997.741334233983 Test RE 0.4791732019992928\n",
      "5 Train Loss 9965.922 Test MSE 2979.4890980353634 Test RE 0.47771221270694886\n",
      "6 Train Loss 9878.36 Test MSE 2948.0565051280396 Test RE 0.47518568114157356\n",
      "7 Train Loss 9701.539 Test MSE 2861.9523378570725 Test RE 0.4681948607506488\n",
      "8 Train Loss 9401.102 Test MSE 2749.1401166113483 Test RE 0.45887445474076355\n",
      "9 Train Loss 9096.3 Test MSE 2610.9769290289746 Test RE 0.4471950225001914\n",
      "10 Train Loss 8782.437 Test MSE 2535.0148916721664 Test RE 0.44064180873982733\n",
      "11 Train Loss 8493.352 Test MSE 2418.4192148943534 Test RE 0.4303890725544862\n",
      "12 Train Loss 8316.563 Test MSE 2356.4791714113762 Test RE 0.42484180618322503\n",
      "13 Train Loss 8005.1826 Test MSE 2139.7141891957485 Test RE 0.4048305953191115\n",
      "14 Train Loss 7825.845 Test MSE 2147.224101893806 Test RE 0.4055404049455752\n",
      "15 Train Loss 7692.7363 Test MSE 2075.3032213519764 Test RE 0.398690808918378\n",
      "16 Train Loss 7583.3887 Test MSE 2129.4697144507427 Test RE 0.40386031321108296\n",
      "17 Train Loss 7176.9062 Test MSE 1865.98489662633 Test RE 0.37805022940354455\n",
      "18 Train Loss 6915.2207 Test MSE 1786.6678604754268 Test RE 0.3699281293989721\n",
      "19 Train Loss 6809.4414 Test MSE 1767.9125465170503 Test RE 0.3679813706976652\n",
      "20 Train Loss 6616.717 Test MSE 1652.2060356633988 Test RE 0.35573577766985875\n",
      "21 Train Loss 6535.0015 Test MSE 1624.0785029568317 Test RE 0.3526947156343629\n",
      "22 Train Loss 6407.699 Test MSE 1604.4865327781529 Test RE 0.3505609054354979\n",
      "23 Train Loss 6223.6904 Test MSE 1554.8128693498552 Test RE 0.34509169460273764\n",
      "24 Train Loss 6108.8115 Test MSE 1532.4939035235557 Test RE 0.3426058873112069\n",
      "25 Train Loss 6006.5547 Test MSE 1512.3953779201256 Test RE 0.3403518491833763\n",
      "26 Train Loss 5859.317 Test MSE 1451.5289065840127 Test RE 0.3334327761376997\n",
      "27 Train Loss 5779.8228 Test MSE 1427.4876250539166 Test RE 0.3306599683416955\n",
      "28 Train Loss 5676.7036 Test MSE 1404.440416894743 Test RE 0.3279798050232959\n",
      "29 Train Loss 5593.589 Test MSE 1366.1827757411131 Test RE 0.3234817967057549\n",
      "30 Train Loss 5492.9604 Test MSE 1323.670311244981 Test RE 0.31840901692067575\n",
      "31 Train Loss 5324.6123 Test MSE 1254.4894034707902 Test RE 0.3099766228871014\n",
      "32 Train Loss 5175.1353 Test MSE 1166.7911633853703 Test RE 0.29894549249095176\n",
      "33 Train Loss 5046.506 Test MSE 1136.3233474301937 Test RE 0.2950165696056392\n",
      "34 Train Loss 4925.365 Test MSE 1117.9998562895385 Test RE 0.29262829552950936\n",
      "35 Train Loss 4847.8647 Test MSE 1099.629660394163 Test RE 0.29021420550167387\n",
      "36 Train Loss 4774.523 Test MSE 1063.736271199213 Test RE 0.28543841941071246\n",
      "37 Train Loss 4637.573 Test MSE 1014.326715501222 Test RE 0.2787304242943135\n",
      "38 Train Loss 4584.238 Test MSE 997.0250346395193 Test RE 0.2763430047428429\n",
      "39 Train Loss 4492.035 Test MSE 927.2578777914201 Test RE 0.26649907727462213\n",
      "40 Train Loss 4397.581 Test MSE 910.3251553992457 Test RE 0.2640545867522442\n",
      "41 Train Loss 4336.2925 Test MSE 890.8646127718911 Test RE 0.2612169160689643\n",
      "42 Train Loss 4270.4585 Test MSE 879.9680575195963 Test RE 0.2596144716901756\n",
      "43 Train Loss 4244.567 Test MSE 872.1800536174292 Test RE 0.2584630821717849\n",
      "44 Train Loss 4168.4014 Test MSE 881.2819393297849 Test RE 0.2598082148032352\n",
      "45 Train Loss 4061.9167 Test MSE 829.9852950282922 Test RE 0.2521335524121032\n",
      "46 Train Loss 4016.858 Test MSE 817.762736883111 Test RE 0.25027017792799416\n",
      "47 Train Loss 3992.0713 Test MSE 812.3667310951276 Test RE 0.2494431076796187\n",
      "48 Train Loss 3941.377 Test MSE 778.2241188546324 Test RE 0.24414497329844764\n",
      "49 Train Loss 3897.1372 Test MSE 764.5274073466436 Test RE 0.24198696534456768\n",
      "50 Train Loss 3860.0583 Test MSE 755.2178415198285 Test RE 0.2405091284212827\n",
      "51 Train Loss 3821.379 Test MSE 733.7644141212645 Test RE 0.23706845351322645\n",
      "52 Train Loss 3729.576 Test MSE 715.5903895108455 Test RE 0.23411416614842234\n",
      "53 Train Loss 3600.6646 Test MSE 678.4286395111286 Test RE 0.2279541643153108\n",
      "54 Train Loss 3490.523 Test MSE 668.3919423518723 Test RE 0.22626170043573862\n",
      "55 Train Loss 3439.1519 Test MSE 626.6299002753024 Test RE 0.21907912653662653\n",
      "56 Train Loss 3359.872 Test MSE 610.4785128021688 Test RE 0.21623731249537345\n",
      "57 Train Loss 3305.8904 Test MSE 581.697490724894 Test RE 0.2110785193182447\n",
      "58 Train Loss 3228.2803 Test MSE 578.4250040747795 Test RE 0.21048394405397453\n",
      "59 Train Loss 3146.2559 Test MSE 561.6823084846795 Test RE 0.2074153130427227\n",
      "60 Train Loss 3088.2195 Test MSE 560.3976646734629 Test RE 0.2071779837524014\n",
      "61 Train Loss 3036.5913 Test MSE 562.4054548137399 Test RE 0.20754879007570323\n",
      "62 Train Loss 2982.0103 Test MSE 533.9348029543868 Test RE 0.20222719537509776\n",
      "63 Train Loss 2953.3853 Test MSE 542.054855519282 Test RE 0.20375912328916654\n",
      "64 Train Loss 2933.588 Test MSE 546.6581061607653 Test RE 0.20462247815720264\n",
      "65 Train Loss 2917.6729 Test MSE 552.2822009325972 Test RE 0.20567237697532448\n",
      "66 Train Loss 2901.1748 Test MSE 549.6653696222089 Test RE 0.20518453854039187\n",
      "67 Train Loss 2887.1484 Test MSE 542.1084651372609 Test RE 0.2037691990025165\n",
      "68 Train Loss 2867.03 Test MSE 539.0015288418966 Test RE 0.20318443813478979\n",
      "69 Train Loss 2832.9277 Test MSE 535.1582380438256 Test RE 0.2024587501271346\n",
      "70 Train Loss 2804.8013 Test MSE 540.4591672703158 Test RE 0.20345899160495803\n",
      "71 Train Loss 2747.4084 Test MSE 492.90555229595805 Test RE 0.19430201253697077\n",
      "72 Train Loss 2716.5679 Test MSE 470.37641642023544 Test RE 0.18980961722431808\n",
      "73 Train Loss 2669.2249 Test MSE 444.3874572995408 Test RE 0.18449149075548055\n",
      "74 Train Loss 2566.688 Test MSE 400.34435999783096 Test RE 0.17511054557403233\n",
      "75 Train Loss 2532.087 Test MSE 401.66698684274553 Test RE 0.17539956542347082\n",
      "76 Train Loss 2467.2424 Test MSE 372.3412290390188 Test RE 0.16887525110559515\n",
      "77 Train Loss 2315.5886 Test MSE 311.4325266145422 Test RE 0.15444627148984758\n",
      "78 Train Loss 2275.5046 Test MSE 307.1108793972206 Test RE 0.15337092773506922\n",
      "79 Train Loss 2236.3867 Test MSE 291.2629728551557 Test RE 0.14936129926179464\n",
      "80 Train Loss 2200.1946 Test MSE 290.40006009109186 Test RE 0.1491398818719804\n",
      "81 Train Loss 2157.1682 Test MSE 287.0037206793701 Test RE 0.14826519300791974\n",
      "82 Train Loss 2126.3252 Test MSE 281.12209229549916 Test RE 0.1467381137269976\n",
      "83 Train Loss 2086.921 Test MSE 266.23373522813597 Test RE 0.14279959853506538\n",
      "84 Train Loss 2060.1445 Test MSE 266.20866050580787 Test RE 0.1427928737212619\n",
      "85 Train Loss 2021.1107 Test MSE 258.13043279772046 Test RE 0.140609624549222\n",
      "86 Train Loss 1999.3243 Test MSE 253.22134434567727 Test RE 0.13926615934727649\n",
      "87 Train Loss 1980.3809 Test MSE 252.7637894910118 Test RE 0.13914027991389263\n",
      "88 Train Loss 1968.5795 Test MSE 251.9309970261392 Test RE 0.13891087486518042\n",
      "89 Train Loss 1964.5532 Test MSE 250.00482681000352 Test RE 0.13837882564358273\n",
      "90 Train Loss 1956.774 Test MSE 253.59413369503434 Test RE 0.13936863460908036\n",
      "91 Train Loss 1947.7074 Test MSE 250.09153415889986 Test RE 0.1384028200222388\n",
      "92 Train Loss 1938.4713 Test MSE 250.93635713981104 Test RE 0.13863638911182144\n",
      "93 Train Loss 1921.472 Test MSE 246.5865395708992 Test RE 0.13742955076127586\n",
      "94 Train Loss 1906.487 Test MSE 246.51656243121 Test RE 0.13741004927253814\n",
      "95 Train Loss 1894.1584 Test MSE 244.6027133886425 Test RE 0.1368756135743069\n",
      "96 Train Loss 1874.6327 Test MSE 240.67779280986204 Test RE 0.13577301240829004\n",
      "97 Train Loss 1860.9744 Test MSE 240.65507778943038 Test RE 0.13576660517086508\n",
      "98 Train Loss 1851.414 Test MSE 237.80826629809476 Test RE 0.13496119568109888\n",
      "99 Train Loss 1844.4717 Test MSE 238.77529235190434 Test RE 0.13523532110525557\n",
      "100 Train Loss 1837.6683 Test MSE 237.84475316270806 Test RE 0.13497154881575238\n",
      "101 Train Loss 1832.2688 Test MSE 237.73626001140371 Test RE 0.1349407615926486\n",
      "102 Train Loss 1825.1663 Test MSE 236.3439092940138 Test RE 0.13454502732137133\n",
      "103 Train Loss 1818.6907 Test MSE 238.1203251019893 Test RE 0.13504971661985937\n",
      "104 Train Loss 1812.3517 Test MSE 238.8476281961215 Test RE 0.1352558040041685\n",
      "105 Train Loss 1804.1184 Test MSE 242.39841023016652 Test RE 0.13625747207740702\n",
      "106 Train Loss 1793.1641 Test MSE 240.9112364230922 Test RE 0.135838842454939\n",
      "107 Train Loss 1789.3295 Test MSE 238.57417509139057 Test RE 0.13517835564913067\n",
      "108 Train Loss 1784.2615 Test MSE 240.38294308836691 Test RE 0.13568982047199668\n",
      "109 Train Loss 1766.4243 Test MSE 237.42393778845374 Test RE 0.13485209432423328\n",
      "110 Train Loss 1740.4833 Test MSE 238.31580349740673 Test RE 0.1351051379413506\n",
      "111 Train Loss 1729.4415 Test MSE 235.7293817134952 Test RE 0.13436999542330547\n",
      "112 Train Loss 1721.0369 Test MSE 236.93807750997684 Test RE 0.13471404415449498\n",
      "113 Train Loss 1718.2527 Test MSE 236.39311761322816 Test RE 0.1345590331616084\n",
      "114 Train Loss 1716.0273 Test MSE 236.76662948780523 Test RE 0.1346652958985708\n",
      "115 Train Loss 1713.4369 Test MSE 236.8774213165998 Test RE 0.13469679964017825\n",
      "116 Train Loss 1713.0875 Test MSE 236.80413001715567 Test RE 0.1346759600279393\n",
      "117 Train Loss 1712.293 Test MSE 236.4052522001676 Test RE 0.134562486725209\n",
      "118 Train Loss 1710.4768 Test MSE 235.78239112601528 Test RE 0.1343851027342262\n",
      "119 Train Loss 1709.4613 Test MSE 236.0366111565431 Test RE 0.13445753007071393\n",
      "120 Train Loss 1708.7004 Test MSE 235.79281473352808 Test RE 0.1343880731894805\n",
      "121 Train Loss 1706.8314 Test MSE 235.2915000119583 Test RE 0.13424513718314987\n",
      "122 Train Loss 1704.7251 Test MSE 235.8199720207342 Test RE 0.13439581199701234\n",
      "123 Train Loss 1703.93 Test MSE 235.56489528531827 Test RE 0.13432310712843604\n",
      "124 Train Loss 1701.7512 Test MSE 235.2000191225628 Test RE 0.13421903751853756\n",
      "125 Train Loss 1698.6393 Test MSE 236.36291354257364 Test RE 0.13455043654844054\n",
      "126 Train Loss 1697.1135 Test MSE 236.91425328468813 Test RE 0.1347072712067836\n",
      "127 Train Loss 1696.0701 Test MSE 237.34195650173018 Test RE 0.13482881044052544\n",
      "128 Train Loss 1695.7681 Test MSE 237.11629979682132 Test RE 0.13476469986195252\n",
      "129 Train Loss 1694.1177 Test MSE 237.92174122158514 Test RE 0.13499339154438564\n",
      "130 Train Loss 1693.0051 Test MSE 237.84698115463075 Test RE 0.13497218098192867\n",
      "131 Train Loss 1692.1748 Test MSE 237.4803543725308 Test RE 0.13486811512376962\n",
      "132 Train Loss 1690.94 Test MSE 237.04962495785134 Test RE 0.13474575125653854\n",
      "133 Train Loss 1689.1609 Test MSE 237.6327213355034 Test RE 0.13491137375476847\n",
      "134 Train Loss 1688.1055 Test MSE 238.43593797520694 Test RE 0.13513918675388442\n",
      "135 Train Loss 1687.2699 Test MSE 237.832177251509 Test RE 0.13496798049518305\n",
      "136 Train Loss 1686.9446 Test MSE 237.77362743384546 Test RE 0.13495136618093803\n",
      "137 Train Loss 1686.5355 Test MSE 238.18877475717025 Test RE 0.13506912580364638\n",
      "138 Train Loss 1686.3499 Test MSE 237.81526796190153 Test RE 0.13496318246225347\n",
      "139 Train Loss 1686.1324 Test MSE 237.3567938824516 Test RE 0.13483302477152254\n",
      "140 Train Loss 1685.486 Test MSE 237.49624060973224 Test RE 0.13487262604645878\n",
      "141 Train Loss 1684.6223 Test MSE 237.18797595771449 Test RE 0.13478506684317637\n",
      "142 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "143 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "144 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "145 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "146 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "147 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "148 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "149 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "150 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "151 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "152 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "153 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "154 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "155 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "156 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "157 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "158 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "159 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "160 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "161 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "162 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "163 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "164 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "165 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "166 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "167 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "168 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "169 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "170 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "171 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "172 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "173 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "174 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "175 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "176 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "177 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "178 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "179 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "180 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "181 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "182 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "183 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "184 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "185 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "186 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "187 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "188 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "189 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "190 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "191 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "192 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "193 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "194 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "195 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "196 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "197 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "198 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "199 Train Loss 1684.423 Test MSE 237.1800390810029 Test RE 0.1347828117090297\n",
      "Training time: 503.52\n",
      "Training time: 503.52\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1541 into shape (500,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13525/3689713413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1541 into shape (500,500)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(75):\n",
    "    label = \"MW_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[31]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
