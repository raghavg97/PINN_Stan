{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_low\"\n",
    "label = \"ES_rowdy\" + level\n",
    "ES_val = 10.0\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA' + level + '.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = ES_val*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_rowdy_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 25.169355 Test MSE 7.659955987868133 Test RE 0.4844384173519763\n",
      "1 Train Loss 25.000095 Test MSE 7.5081466675066 Test RE 0.4796139576175841\n",
      "2 Train Loss 25.000078 Test MSE 7.508115446980206 Test RE 0.4796129604463478\n",
      "3 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "4 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "5 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "6 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "7 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "8 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "9 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "10 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "11 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "12 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "13 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "14 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "15 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "16 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "17 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "18 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "19 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "20 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "21 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "22 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "23 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "24 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "25 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "26 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "27 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "28 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "29 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "30 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "31 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "32 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "33 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "34 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "35 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "36 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "37 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "38 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "39 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "40 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "41 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "42 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "43 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "44 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "45 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "46 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "47 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "48 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "49 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "50 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "51 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "52 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "53 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "54 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "55 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "56 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "57 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "58 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "59 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "60 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "61 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "62 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "63 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "64 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "65 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "66 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "67 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "68 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "69 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "70 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "71 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "72 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "73 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "74 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "75 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "76 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "77 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "78 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "79 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "80 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "81 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "82 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "83 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "84 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "85 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "86 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "87 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "88 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "89 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "90 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "91 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "92 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "93 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "94 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "95 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "96 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "97 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "98 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "99 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "100 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "101 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "102 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "103 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "104 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "105 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "106 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "107 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "108 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "109 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "110 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "111 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "112 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "113 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "114 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "115 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "116 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "117 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "118 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "119 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "120 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "121 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "122 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "123 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "124 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "125 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "126 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "127 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "128 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "129 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "130 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "131 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "132 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "133 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "134 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "135 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "136 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "137 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "138 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "139 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "140 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "141 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "142 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "143 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "144 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "145 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "146 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "147 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "148 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "149 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "150 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "151 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "152 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "153 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "154 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "155 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "156 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "157 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "158 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "159 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "160 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "161 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "162 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "163 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "164 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "165 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "166 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "167 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "168 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "169 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "170 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "171 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "172 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "173 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "174 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "175 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "176 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "177 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "178 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "179 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "180 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "181 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "182 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "183 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "184 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "185 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "186 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "187 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "188 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "189 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "190 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "191 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "192 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "193 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "194 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "195 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "196 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "197 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "198 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "199 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "Training time: 136.83\n",
      "Training time: 136.83\n",
      "ES_rowdy_low\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 38.22128 Test MSE 20.248727503282517 Test RE 0.787633732072572\n",
      "1 Train Loss 25.121239 Test MSE 7.605176236538064 Test RE 0.4827030919984146\n",
      "2 Train Loss 25.002825 Test MSE 7.50792251785695 Test RE 0.47960679832064235\n",
      "3 Train Loss 25.000687 Test MSE 7.508008838940717 Test RE 0.4796095554122025\n",
      "4 Train Loss 25.000254 Test MSE 7.508044235639762 Test RE 0.4796106859766068\n",
      "5 Train Loss 25.00018 Test MSE 7.508062905808763 Test RE 0.47961128229748057\n",
      "6 Train Loss 25.000044 Test MSE 7.508092134909798 Test RE 0.47961221586670877\n",
      "7 Train Loss 25.00003 Test MSE 7.508082871876203 Test RE 0.47961192000822966\n",
      "8 Train Loss 25.000025 Test MSE 7.50807875006352 Test RE 0.4796117883587262\n",
      "9 Train Loss 25.000021 Test MSE 7.508076559993354 Test RE 0.4796117184085044\n",
      "10 Train Loss 25.00002 Test MSE 7.508075332800887 Test RE 0.47961167921231956\n",
      "11 Train Loss 25.000017 Test MSE 7.508074422139174 Test RE 0.47961165012603685\n",
      "12 Train Loss 25.000015 Test MSE 7.5080734595633825 Test RE 0.47961161938163055\n",
      "13 Train Loss 25.000013 Test MSE 7.508072574520898 Test RE 0.4796115911136152\n",
      "14 Train Loss 25.000011 Test MSE 7.508071814506224 Test RE 0.4796115668389527\n",
      "15 Train Loss 25.00001 Test MSE 7.5080712395734395 Test RE 0.47961154847575477\n",
      "16 Train Loss 24.99998 Test MSE 7.508118159166536 Test RE 0.4796130470725863\n",
      "17 Train Loss 24.999966 Test MSE 7.508088594757698 Test RE 0.4796121027953476\n",
      "18 Train Loss 24.999966 Test MSE 7.50808497086698 Test RE 0.47961198704937336\n",
      "19 Train Loss 24.999962 Test MSE 7.508079008912275 Test RE 0.4796117966262814\n",
      "20 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "21 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "22 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "23 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "24 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "25 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "26 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "27 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "28 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "29 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "30 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "31 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "32 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "33 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "34 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "35 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "36 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "37 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "38 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "39 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "40 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "41 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "42 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "43 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "44 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "45 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "46 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "47 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "48 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "49 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "50 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "51 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "52 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "53 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "54 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "55 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "56 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "57 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "58 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "59 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "60 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "61 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "62 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "63 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "64 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "65 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "66 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "67 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "68 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "69 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "70 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "71 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "72 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "73 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "74 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "75 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "76 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "77 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "78 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "79 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "80 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "81 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "82 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "83 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "84 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "85 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "86 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "87 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "88 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "89 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "90 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "91 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "92 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "93 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "94 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "95 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "96 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "97 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "98 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "99 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "100 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "101 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "102 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "103 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "104 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "105 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "106 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "107 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "108 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "109 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "110 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "111 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "112 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "113 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "114 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "115 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "116 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "117 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "118 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "119 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "120 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "121 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "122 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "123 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "124 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "125 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "126 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "127 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "128 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "129 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "130 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "131 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "132 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "133 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "134 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "135 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "136 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "137 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "138 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "139 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "140 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "141 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "142 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "143 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "144 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "145 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "146 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "147 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "148 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "149 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "150 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "151 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "152 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "153 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "154 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "155 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "156 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "157 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "158 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "159 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "160 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "161 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "162 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "163 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "164 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "165 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "166 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "167 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "168 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "169 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "170 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "171 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "172 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "173 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "174 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "175 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "176 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "177 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "178 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "179 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "180 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "181 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "182 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "183 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "184 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "185 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "186 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "187 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "188 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "189 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "190 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "191 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "192 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "193 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "194 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "195 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "196 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "197 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "198 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "199 Train Loss 24.999958 Test MSE 7.5080758038159985 Test RE 0.47961169425641115\n",
      "Training time: 162.95\n",
      "Training time: 162.95\n",
      "ES_rowdy_low\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss nan Test MSE nan Test RE nan\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 1054.57\n",
      "Training time: 1054.57\n",
      "ES_rowdy_low\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 29.528711 Test MSE 11.927118060889288 Test RE 0.6044956758462624\n",
      "1 Train Loss 25.0148 Test MSE 7.508310020266069 Test RE 0.4796191750061616\n",
      "2 Train Loss 25.000225 Test MSE 7.507981281512857 Test RE 0.4796086752309097\n",
      "3 Train Loss 24.999859 Test MSE 7.50796527416204 Test RE 0.4796081639570982\n",
      "4 Train Loss 24.999853 Test MSE 7.507964766318625 Test RE 0.4796081477366015\n",
      "5 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "6 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "7 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "8 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "9 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "10 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "11 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "12 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "13 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "14 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "15 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "16 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "17 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "18 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "19 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "20 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "21 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "22 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "23 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "24 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "25 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "26 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "27 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "28 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "29 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "30 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "31 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "32 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "33 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "34 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "35 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "36 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "37 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "38 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "39 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "40 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "41 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "42 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "43 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "44 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "45 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "46 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "47 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "48 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "49 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "50 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "51 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "52 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "53 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "54 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "55 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "56 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "57 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "58 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "59 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "60 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "61 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "62 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "63 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "64 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "65 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "66 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "67 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "68 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "69 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "70 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "71 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "72 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "73 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "74 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "75 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "76 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "77 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "78 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "79 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "80 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "81 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "82 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "83 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "84 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "85 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "86 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "87 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "88 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "89 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "90 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "91 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "92 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "93 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "94 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "95 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "96 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "97 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "98 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "99 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "100 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "101 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "102 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "103 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "104 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "105 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "106 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "107 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "108 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "109 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "110 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "111 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "112 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "113 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "114 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "115 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "116 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "117 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "118 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "119 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "120 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "121 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "122 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "123 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "124 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "125 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "126 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "127 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "128 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "129 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "130 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "131 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "132 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "133 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "134 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "135 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "136 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "137 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "138 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "139 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "140 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "141 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "142 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "143 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "144 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "145 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "146 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "147 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "148 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "149 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "150 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "151 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "152 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "153 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "154 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "155 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "156 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "157 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "158 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "159 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "160 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "161 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "162 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "163 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "164 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "165 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "166 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "167 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "168 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "169 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "170 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "171 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "172 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "173 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "174 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "175 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "176 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "177 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "178 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "179 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "180 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "181 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "182 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "183 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "184 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "185 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "186 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "187 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "188 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "189 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "190 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "191 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "192 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "193 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "194 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "195 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "196 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "197 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "198 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "199 Train Loss 24.999836 Test MSE 7.507967129831645 Test RE 0.47960822322710056\n",
      "Training time: 122.19\n",
      "Training time: 122.19\n",
      "ES_rowdy_low\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 31.8308 Test MSE 11.5019835232509 Test RE 0.5936244907829137\n",
      "1 Train Loss 24.99884 Test MSE 7.506875136593432 Test RE 0.4795733437469118\n",
      "2 Train Loss 24.994066 Test MSE 7.504878658319336 Test RE 0.47950956744769024\n",
      "3 Train Loss 24.954473 Test MSE 7.478789406010744 Test RE 0.4786753809282058\n",
      "4 Train Loss 24.750706 Test MSE 7.387758832397439 Test RE 0.4757532835523143\n",
      "5 Train Loss 23.909185 Test MSE 7.093020944281694 Test RE 0.4661664998764713\n",
      "6 Train Loss 21.810791 Test MSE 6.3017021294576585 Test RE 0.4393942556787047\n",
      "7 Train Loss 19.269754 Test MSE 5.782843185416684 Test RE 0.4209166907245992\n",
      "8 Train Loss 16.05799 Test MSE 4.459019634568764 Test RE 0.3696112076981579\n",
      "9 Train Loss 14.733232 Test MSE 4.1605293396217355 Test RE 0.357025909333445\n",
      "10 Train Loss 12.986477 Test MSE 3.2812603408415013 Test RE 0.317063160367966\n",
      "11 Train Loss 10.135004 Test MSE 2.234580617464561 Test RE 0.26165163518280005\n",
      "12 Train Loss 8.40649 Test MSE 1.5270849005823572 Test RE 0.21630025477003614\n",
      "13 Train Loss 7.427818 Test MSE 1.3413533652080147 Test RE 0.20272020702918153\n",
      "14 Train Loss 6.4721723 Test MSE 1.0895891791604484 Test RE 0.18270769145881843\n",
      "15 Train Loss 6.0604877 Test MSE 0.9122338355627161 Test RE 0.16717776997320272\n",
      "16 Train Loss 5.6565237 Test MSE 0.8116434251982045 Test RE 0.15769142471264064\n",
      "17 Train Loss 5.277595 Test MSE 0.7507574553608123 Test RE 0.15166147180504647\n",
      "18 Train Loss 5.0196614 Test MSE 0.6830899499430293 Test RE 0.14466530512095882\n",
      "19 Train Loss 4.9493937 Test MSE 0.6809289069756079 Test RE 0.14443629019264495\n",
      "20 Train Loss 4.8685765 Test MSE 0.6627604471598006 Test RE 0.14249634686944165\n",
      "21 Train Loss 4.796193 Test MSE 0.6998585842902779 Test RE 0.1464301766983162\n",
      "22 Train Loss 4.70467 Test MSE 0.6773538825055843 Test RE 0.1440566302851731\n",
      "23 Train Loss 4.5658455 Test MSE 0.63241090200742 Test RE 0.13919547376600744\n",
      "24 Train Loss 4.519979 Test MSE 0.638496289365234 Test RE 0.13986357611031397\n",
      "25 Train Loss 4.4880753 Test MSE 0.6284257757627238 Test RE 0.13875621174949593\n",
      "26 Train Loss 4.449057 Test MSE 0.6334506808723087 Test RE 0.13930985594189108\n",
      "27 Train Loss 4.394209 Test MSE 0.6265176702355253 Test RE 0.1385453970550337\n",
      "28 Train Loss 4.3792725 Test MSE 0.6198811000524495 Test RE 0.13780965231429476\n",
      "29 Train Loss 4.3530087 Test MSE 0.6169313604041399 Test RE 0.13748137376151406\n",
      "30 Train Loss 4.3244014 Test MSE 0.6232318636531442 Test RE 0.13818161495938414\n",
      "31 Train Loss 4.314902 Test MSE 0.6208008910533529 Test RE 0.1379118566661674\n",
      "32 Train Loss 4.307602 Test MSE 0.6229712905353502 Test RE 0.13815272508653942\n",
      "33 Train Loss 4.297849 Test MSE 0.6252330976925287 Test RE 0.13840329177965596\n",
      "34 Train Loss 4.285559 Test MSE 0.6262148519635699 Test RE 0.13851191104943728\n",
      "35 Train Loss 4.2737923 Test MSE 0.6206246361400817 Test RE 0.1378922776286214\n",
      "36 Train Loss 4.265527 Test MSE 0.619334327998452 Test RE 0.1377488607448451\n",
      "37 Train Loss 4.256256 Test MSE 0.6192025265819592 Test RE 0.13773420269976946\n",
      "38 Train Loss 4.2525744 Test MSE 0.621284707135587 Test RE 0.13796558643686768\n",
      "39 Train Loss 4.247829 Test MSE 0.6151857833391214 Test RE 0.1372867375792082\n",
      "40 Train Loss 4.2419395 Test MSE 0.6172926013405938 Test RE 0.1375216186213218\n",
      "41 Train Loss 4.2324147 Test MSE 0.613560138527091 Test RE 0.1371052256576815\n",
      "42 Train Loss 4.22733 Test MSE 0.6186647229978653 Test RE 0.13767437571605942\n",
      "43 Train Loss 4.2217875 Test MSE 0.6231247215581881 Test RE 0.13816973679258313\n",
      "44 Train Loss 4.209977 Test MSE 0.6189405188409279 Test RE 0.13770505937100744\n",
      "45 Train Loss 4.2036614 Test MSE 0.6126674854074625 Test RE 0.13700545389391344\n",
      "46 Train Loss 4.200342 Test MSE 0.6076969692690397 Test RE 0.1364485656425872\n",
      "47 Train Loss 4.1876707 Test MSE 0.6032651072043764 Test RE 0.135950103554306\n",
      "48 Train Loss 4.180802 Test MSE 0.604184982179407 Test RE 0.13605371427231255\n",
      "49 Train Loss 4.1677136 Test MSE 0.609977795437672 Test RE 0.1367043872163934\n",
      "50 Train Loss 4.1377664 Test MSE 0.6063453427553707 Test RE 0.1362967381919599\n",
      "51 Train Loss 4.1313176 Test MSE 0.6060413983340622 Test RE 0.13626257298683084\n",
      "52 Train Loss 4.1268773 Test MSE 0.6025563326116494 Test RE 0.1358702163724935\n",
      "53 Train Loss 4.1251373 Test MSE 0.598177299356365 Test RE 0.13537560275451954\n",
      "54 Train Loss 4.1230273 Test MSE 0.5976764471679963 Test RE 0.13531891607837854\n",
      "55 Train Loss 4.120473 Test MSE 0.5923923796747984 Test RE 0.1347194096718299\n",
      "56 Train Loss 4.1184697 Test MSE 0.5917068046142162 Test RE 0.13464143179078303\n",
      "57 Train Loss 4.1153555 Test MSE 0.5922552622678126 Test RE 0.1347038174344851\n",
      "58 Train Loss 4.1100807 Test MSE 0.5909497468822301 Test RE 0.13455527090754832\n",
      "59 Train Loss 4.1034265 Test MSE 0.5922534789295117 Test RE 0.13470361463117286\n",
      "60 Train Loss 4.1000395 Test MSE 0.5956217571215541 Test RE 0.13508611637311696\n",
      "61 Train Loss 4.099006 Test MSE 0.5948308242584691 Test RE 0.13499639538749728\n",
      "62 Train Loss 4.097458 Test MSE 0.5930787698961888 Test RE 0.13479743508067665\n",
      "63 Train Loss 4.0964813 Test MSE 0.5941671683362673 Test RE 0.1349210662712989\n",
      "64 Train Loss 4.095688 Test MSE 0.5958079836961246 Test RE 0.13510723267584496\n",
      "65 Train Loss 4.0935736 Test MSE 0.5928121807230361 Test RE 0.1347671359216498\n",
      "66 Train Loss 4.0925865 Test MSE 0.591007343138838 Test RE 0.13456182788733204\n",
      "67 Train Loss 4.0911155 Test MSE 0.5909972425423934 Test RE 0.13456067801961605\n",
      "68 Train Loss 4.089765 Test MSE 0.5885633644567543 Test RE 0.13428331448509653\n",
      "69 Train Loss 4.087412 Test MSE 0.587861000202979 Test RE 0.13420316682343542\n",
      "70 Train Loss 4.084907 Test MSE 0.586031821983562 Test RE 0.1339942120141079\n",
      "71 Train Loss 4.076983 Test MSE 0.5836567662817147 Test RE 0.13372241208534413\n",
      "72 Train Loss 4.0688486 Test MSE 0.5785271241737351 Test RE 0.1331334855275799\n",
      "73 Train Loss 4.062299 Test MSE 0.5835961801158492 Test RE 0.133715471414931\n",
      "74 Train Loss 4.0600705 Test MSE 0.5851391733300443 Test RE 0.13389212256195238\n",
      "75 Train Loss 4.0580535 Test MSE 0.5877734324522665 Test RE 0.13419317100203776\n",
      "76 Train Loss 4.055421 Test MSE 0.5855367692285138 Test RE 0.133937603979978\n",
      "77 Train Loss 4.0524592 Test MSE 0.5861208626251714 Test RE 0.13400439104961884\n",
      "78 Train Loss 4.0491962 Test MSE 0.5861420219826747 Test RE 0.1340068098521288\n",
      "79 Train Loss 4.0441575 Test MSE 0.5815053965477809 Test RE 0.1334757329060932\n",
      "80 Train Loss 4.0398993 Test MSE 0.5800765977863634 Test RE 0.13331165252609795\n",
      "81 Train Loss 4.034975 Test MSE 0.5751570563173999 Test RE 0.1327451509326179\n",
      "82 Train Loss 4.0318265 Test MSE 0.5801804340409927 Test RE 0.13332358367737546\n",
      "83 Train Loss 4.030143 Test MSE 0.5793717971202674 Test RE 0.13323064020867117\n",
      "84 Train Loss 4.0288706 Test MSE 0.579899689333757 Test RE 0.13329132266220384\n",
      "85 Train Loss 4.0280485 Test MSE 0.577174656064445 Test RE 0.13297777618959453\n",
      "86 Train Loss 4.0266275 Test MSE 0.5761281579799197 Test RE 0.13285716787509128\n",
      "87 Train Loss 4.023953 Test MSE 0.5763315417258815 Test RE 0.13288061630735545\n",
      "88 Train Loss 4.022356 Test MSE 0.5750570605189743 Test RE 0.13273361101129924\n",
      "89 Train Loss 4.0214305 Test MSE 0.5755981149298336 Test RE 0.13279603892163966\n",
      "90 Train Loss 4.0210395 Test MSE 0.5749867372650642 Test RE 0.13272549482122212\n",
      "91 Train Loss 4.0189614 Test MSE 0.5738567991439151 Test RE 0.13259501759594974\n",
      "92 Train Loss 4.0178494 Test MSE 0.5738613949498226 Test RE 0.1325955485470041\n",
      "93 Train Loss 4.0159707 Test MSE 0.575958195249552 Test RE 0.1328375694295773\n",
      "94 Train Loss 4.0073233 Test MSE 0.5744069899652335 Test RE 0.13265856575301999\n",
      "95 Train Loss 4.0034804 Test MSE 0.5752324748953759 Test RE 0.13275385387926122\n",
      "96 Train Loss 4.002249 Test MSE 0.5733049483490593 Test RE 0.1325312470994641\n",
      "97 Train Loss 4.001217 Test MSE 0.5716834119149504 Test RE 0.13234368861022597\n",
      "98 Train Loss 3.9991906 Test MSE 0.5708499215965961 Test RE 0.13224717766028943\n",
      "99 Train Loss 3.9968452 Test MSE 0.5712284637549397 Test RE 0.1322910182861574\n",
      "100 Train Loss 3.995357 Test MSE 0.5690474799300699 Test RE 0.1320382293257603\n",
      "101 Train Loss 3.994674 Test MSE 0.5707194726860824 Test RE 0.13223206643291646\n",
      "102 Train Loss 3.9922755 Test MSE 0.5691052418699882 Test RE 0.13204493051591248\n",
      "103 Train Loss 3.9881132 Test MSE 0.5657359591213938 Test RE 0.13165347642943695\n",
      "104 Train Loss 3.9851801 Test MSE 0.5623371521499558 Test RE 0.13125740929058496\n",
      "105 Train Loss 3.9796648 Test MSE 0.5610335865610253 Test RE 0.13110518573960125\n",
      "106 Train Loss 3.9769318 Test MSE 0.5588440983674071 Test RE 0.13084911027965948\n",
      "107 Train Loss 3.974081 Test MSE 0.5588959204581816 Test RE 0.13085517701392288\n",
      "108 Train Loss 3.9729538 Test MSE 0.5580551474240157 Test RE 0.13075671421593388\n",
      "109 Train Loss 3.9712024 Test MSE 0.5560163685965387 Test RE 0.13051764469954913\n",
      "110 Train Loss 3.968607 Test MSE 0.555888934404488 Test RE 0.13050268707900875\n",
      "111 Train Loss 3.9665883 Test MSE 0.5558172843037893 Test RE 0.1304942763773241\n",
      "112 Train Loss 3.9654346 Test MSE 0.5562348355341402 Test RE 0.1305432833240842\n",
      "113 Train Loss 3.965322 Test MSE 0.555730713432394 Test RE 0.1304841134663637\n",
      "114 Train Loss 3.9650083 Test MSE 0.5547451413237402 Test RE 0.1303683572483713\n",
      "115 Train Loss 3.9642258 Test MSE 0.5550069845671637 Test RE 0.1303991209665172\n",
      "116 Train Loss 3.9634473 Test MSE 0.5547245020414687 Test RE 0.13036593204970667\n",
      "117 Train Loss 3.9622684 Test MSE 0.5547209509967613 Test RE 0.1303655147831246\n",
      "118 Train Loss 3.9612162 Test MSE 0.5549563646281476 Test RE 0.13039317424309804\n",
      "119 Train Loss 3.958421 Test MSE 0.5549856188423444 Test RE 0.13039661099941177\n",
      "120 Train Loss 3.946624 Test MSE 0.5467780834992447 Test RE 0.12942881909331116\n",
      "121 Train Loss 3.937358 Test MSE 0.5511070916598126 Test RE 0.12994017258968807\n",
      "122 Train Loss 3.9335692 Test MSE 0.5504578217738576 Test RE 0.1298636075215398\n",
      "123 Train Loss 3.9296904 Test MSE 0.5527118547469492 Test RE 0.13012922079568554\n",
      "124 Train Loss 3.9257977 Test MSE 0.5507795892594882 Test RE 0.12990155755164529\n",
      "125 Train Loss 3.920367 Test MSE 0.5509843092235245 Test RE 0.12992569694608758\n",
      "126 Train Loss 3.9183512 Test MSE 0.5497852429181133 Test RE 0.12978424611576744\n",
      "127 Train Loss 3.9166667 Test MSE 0.5517473634953225 Test RE 0.13001563240837238\n",
      "128 Train Loss 3.9158475 Test MSE 0.5515772992017461 Test RE 0.12999559359874976\n",
      "129 Train Loss 3.9151945 Test MSE 0.549923050842908 Test RE 0.12980051080932462\n",
      "130 Train Loss 3.9142334 Test MSE 0.5508748664678239 Test RE 0.1299127926471374\n",
      "131 Train Loss 3.912102 Test MSE 0.547081352344994 Test RE 0.12946470777333746\n",
      "132 Train Loss 3.90789 Test MSE 0.5426006443303668 Test RE 0.1289334465527734\n",
      "133 Train Loss 3.905281 Test MSE 0.540817593116607 Test RE 0.12872142679444815\n",
      "134 Train Loss 3.8959153 Test MSE 0.5394099737892447 Test RE 0.12855380205064396\n",
      "135 Train Loss 3.890102 Test MSE 0.5372335458692192 Test RE 0.12829419350351964\n",
      "136 Train Loss 3.8888116 Test MSE 0.535726062149917 Test RE 0.1281140695408399\n",
      "137 Train Loss 3.8870978 Test MSE 0.5351224195602579 Test RE 0.1280418713492002\n",
      "138 Train Loss 3.8833292 Test MSE 0.5369861746708657 Test RE 0.12826465333170115\n",
      "139 Train Loss 3.8803654 Test MSE 0.5350788032781953 Test RE 0.12803665308140255\n",
      "140 Train Loss 3.8795428 Test MSE 0.5328158379337394 Test RE 0.12776561870048164\n",
      "141 Train Loss 3.8781402 Test MSE 0.5328689811893571 Test RE 0.12777199023746727\n",
      "142 Train Loss 3.8749108 Test MSE 0.5322040644504742 Test RE 0.12769224807072901\n",
      "143 Train Loss 3.8672688 Test MSE 0.5312073311823587 Test RE 0.12757261843277745\n",
      "144 Train Loss 3.8569043 Test MSE 0.5263398745150336 Test RE 0.1269867989588084\n",
      "145 Train Loss 3.8533566 Test MSE 0.5245004901973844 Test RE 0.1267647162662928\n",
      "146 Train Loss 3.8523204 Test MSE 0.5247130083428803 Test RE 0.1267903950545787\n",
      "147 Train Loss 3.8493726 Test MSE 0.5249871786591739 Test RE 0.1268235156580441\n",
      "148 Train Loss 3.841342 Test MSE 0.5243940471449762 Test RE 0.12675185268633113\n",
      "149 Train Loss 3.8367512 Test MSE 0.5271505101277518 Test RE 0.12708454988137516\n",
      "150 Train Loss 3.8284123 Test MSE 0.5252765481952598 Test RE 0.1268584629936475\n",
      "151 Train Loss 3.8251896 Test MSE 0.524900798084416 Test RE 0.12681308155670687\n",
      "152 Train Loss 3.8225424 Test MSE 0.5242569603180973 Test RE 0.12673528390076258\n",
      "153 Train Loss 3.8199089 Test MSE 0.5246867840353795 Test RE 0.1267872266255512\n",
      "154 Train Loss 3.8158782 Test MSE 0.5251518466115653 Test RE 0.12684340388783208\n",
      "155 Train Loss 3.813604 Test MSE 0.5234277300855057 Test RE 0.12663501406483618\n",
      "156 Train Loss 3.8120294 Test MSE 0.5221731411303019 Test RE 0.1264831590944329\n",
      "157 Train Loss 3.809926 Test MSE 0.5224135918708316 Test RE 0.12651227728008027\n",
      "158 Train Loss 3.8039396 Test MSE 0.5242509395621109 Test RE 0.12673455616182178\n",
      "159 Train Loss 3.799837 Test MSE 0.5258046953333615 Test RE 0.12692222283724314\n",
      "160 Train Loss 3.798708 Test MSE 0.5236715813650205 Test RE 0.12666450859933934\n",
      "161 Train Loss 3.7964687 Test MSE 0.5227158143249492 Test RE 0.12654886641489688\n",
      "162 Train Loss 3.7954683 Test MSE 0.5202636107747697 Test RE 0.12625167970225712\n",
      "163 Train Loss 3.794658 Test MSE 0.5201918611482813 Test RE 0.1262429737087846\n",
      "164 Train Loss 3.793208 Test MSE 0.5197374337158245 Test RE 0.1261878202076787\n",
      "165 Train Loss 3.7916396 Test MSE 0.5189091566243347 Test RE 0.12608723080572398\n",
      "166 Train Loss 3.7905588 Test MSE 0.5199381589015286 Test RE 0.12621218503780288\n",
      "167 Train Loss 3.7903259 Test MSE 0.5201778343354172 Test RE 0.1262412716459139\n",
      "168 Train Loss 3.789725 Test MSE 0.5203968245558965 Test RE 0.12626784207343017\n",
      "169 Train Loss 3.7889493 Test MSE 0.5217360754474962 Test RE 0.12643021399193377\n",
      "170 Train Loss 3.7883835 Test MSE 0.5214381378427577 Test RE 0.12639410982350552\n",
      "171 Train Loss 3.788058 Test MSE 0.5215734840203947 Test RE 0.12641051239215031\n",
      "172 Train Loss 3.7876236 Test MSE 0.5207578053121819 Test RE 0.1263116282356095\n",
      "173 Train Loss 3.7872913 Test MSE 0.5201661375407637 Test RE 0.12623985229811133\n",
      "174 Train Loss 3.7864385 Test MSE 0.5190536741653847 Test RE 0.12610478739322312\n",
      "175 Train Loss 3.7851188 Test MSE 0.5196563874326424 Test RE 0.1261779811506162\n",
      "176 Train Loss 3.7813268 Test MSE 0.5248897222283722 Test RE 0.1268117436173759\n",
      "177 Train Loss 3.7770734 Test MSE 0.5264742488349248 Test RE 0.12700300775907\n",
      "178 Train Loss 3.7729409 Test MSE 0.5273043068064283 Test RE 0.1271030870504889\n",
      "179 Train Loss 3.7704587 Test MSE 0.5272213891902369 Test RE 0.12709309329622007\n",
      "180 Train Loss 3.7689033 Test MSE 0.5292428160492237 Test RE 0.1273365049005587\n",
      "181 Train Loss 3.767629 Test MSE 0.5293304206213392 Test RE 0.12734704335105732\n",
      "182 Train Loss 3.7668705 Test MSE 0.5314648785410329 Test RE 0.12760354045467748\n",
      "183 Train Loss 3.7656624 Test MSE 0.5279602099168992 Test RE 0.12718211295683604\n",
      "184 Train Loss 3.7647798 Test MSE 0.5288192512563614 Test RE 0.12728553958331343\n",
      "185 Train Loss 3.7635908 Test MSE 0.5265598754454671 Test RE 0.12701333532491768\n",
      "186 Train Loss 3.7610364 Test MSE 0.5291787788302819 Test RE 0.12732880094873017\n",
      "187 Train Loss 3.754657 Test MSE 0.5250181403855964 Test RE 0.12682725538470982\n",
      "188 Train Loss 3.7491336 Test MSE 0.5215347532103454 Test RE 0.12640581883257715\n",
      "189 Train Loss 3.7461495 Test MSE 0.5164674129262932 Test RE 0.12579022724726002\n",
      "190 Train Loss 3.7433364 Test MSE 0.5144162811064038 Test RE 0.1255401930602848\n",
      "191 Train Loss 3.7395477 Test MSE 0.5182433812933779 Test RE 0.12600631807731913\n",
      "192 Train Loss 3.7346835 Test MSE 0.5157279953758205 Test RE 0.12570014913780767\n",
      "193 Train Loss 3.7304277 Test MSE 0.5087112944981735 Test RE 0.12484211841576552\n",
      "194 Train Loss 3.7261028 Test MSE 0.5055631700485033 Test RE 0.12445523054806819\n",
      "195 Train Loss 3.7210922 Test MSE 0.5076598666468467 Test RE 0.12471303697335069\n",
      "196 Train Loss 3.714659 Test MSE 0.5075860103990348 Test RE 0.12470396478486355\n",
      "197 Train Loss 3.7070172 Test MSE 0.5044101484077775 Test RE 0.12431322901910209\n",
      "198 Train Loss 3.7021506 Test MSE 0.5092619252588231 Test RE 0.12490966490019702\n",
      "199 Train Loss 3.7004511 Test MSE 0.5094714863979589 Test RE 0.12493536240284281\n",
      "Training time: 636.18\n",
      "Training time: 636.18\n",
      "ES_rowdy_low\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 25.002502 Test MSE 7.509620920652625 Test RE 0.47966104231847806\n",
      "1 Train Loss 25.00011 Test MSE 7.508099957091083 Test RE 0.4796124657046605\n",
      "2 Train Loss 25.000107 Test MSE 7.508093119934939 Test RE 0.4796122473281016\n",
      "3 Train Loss 25.000101 Test MSE 7.508085394998427 Test RE 0.479612000596005\n",
      "4 Train Loss 25.000097 Test MSE 7.508081298252664 Test RE 0.4796118697471539\n",
      "5 Train Loss 25.000095 Test MSE 7.508077355085855 Test RE 0.4796117438035353\n",
      "6 Train Loss 25.000088 Test MSE 7.508071181744598 Test RE 0.4796115466287171\n",
      "7 Train Loss 25.000063 Test MSE 7.508048988217446 Test RE 0.4796108377729077\n",
      "8 Train Loss 25.000057 Test MSE 7.508046011482582 Test RE 0.47961074269664716\n",
      "9 Train Loss 25.000011 Test MSE 7.508053891215618 Test RE 0.4796109943735559\n",
      "10 Train Loss 25.0 Test MSE 7.508060278423157 Test RE 0.47961119837943755\n",
      "11 Train Loss 24.999987 Test MSE 7.508064908266899 Test RE 0.47961134625548535\n",
      "12 Train Loss 24.999975 Test MSE 7.5080652366247325 Test RE 0.4796113567431507\n",
      "13 Train Loss 24.999973 Test MSE 7.508063919334793 Test RE 0.4796113146692459\n",
      "14 Train Loss 24.999968 Test MSE 7.508061616921654 Test RE 0.4796112411307486\n",
      "15 Train Loss 24.999968 Test MSE 7.508060827501016 Test RE 0.4796112159168491\n",
      "16 Train Loss 24.99996 Test MSE 7.5080568117185935 Test RE 0.47961108765373356\n",
      "17 Train Loss 24.99996 Test MSE 7.508056495402081 Test RE 0.4796110775506594\n",
      "18 Train Loss 24.999956 Test MSE 7.508050900303169 Test RE 0.4796108988444976\n",
      "19 Train Loss 24.999954 Test MSE 7.50804873037439 Test RE 0.47961082953745754\n",
      "20 Train Loss 24.99995 Test MSE 7.508046479320501 Test RE 0.47961075763928923\n",
      "21 Train Loss 24.999947 Test MSE 7.508043857924251 Test RE 0.4796106739124532\n",
      "22 Train Loss 24.999943 Test MSE 7.508040665622817 Test RE 0.4796105719510081\n",
      "23 Train Loss 24.99994 Test MSE 7.508037581507418 Test RE 0.47961047344498176\n",
      "24 Train Loss 24.99994 Test MSE 7.508036308378703 Test RE 0.47961043278150056\n",
      "25 Train Loss 24.999937 Test MSE 7.508032702058656 Test RE 0.4796103175963272\n",
      "26 Train Loss 24.999937 Test MSE 7.508031670494783 Test RE 0.47961028464836913\n",
      "27 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "28 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "29 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "30 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "31 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "32 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "33 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "34 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "35 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "36 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "37 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "38 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "39 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "40 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "41 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "42 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "43 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "44 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "45 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "46 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "47 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "48 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "49 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "50 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "51 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "52 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "53 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "54 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "55 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "56 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "57 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "58 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "59 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "60 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "61 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "62 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "63 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "64 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "65 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "66 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "67 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "68 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "69 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "70 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "71 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "72 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "73 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "74 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "75 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "76 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "77 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "78 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "79 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "80 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "81 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "82 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "83 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "84 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "85 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "86 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "87 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "88 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "89 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "90 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "91 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "92 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "93 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "94 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "95 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "96 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "97 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "98 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "99 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "100 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "101 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "102 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "103 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "104 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "105 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "106 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "107 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "108 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "109 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "110 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "111 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "112 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "113 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "114 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "115 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "116 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "117 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "118 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "119 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "120 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "121 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "122 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "123 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "124 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "125 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "126 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "127 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "128 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "129 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "130 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "131 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "132 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "133 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "134 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "135 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "136 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "137 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "138 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "139 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "140 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "141 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "142 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "143 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "144 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "145 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "146 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "147 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "148 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "149 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "150 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "151 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "152 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "153 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "154 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "155 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "156 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "157 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "158 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "159 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "160 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "161 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "162 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "163 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "164 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "165 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "166 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "167 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "168 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "169 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "170 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "171 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "172 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "173 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "174 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "175 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "176 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "177 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "178 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "179 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "180 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "181 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "182 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "183 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "184 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "185 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "186 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "187 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "188 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "189 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "190 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "191 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "192 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "193 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "194 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "195 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "196 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "197 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "198 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "199 Train Loss 24.999933 Test MSE 7.508029448066611 Test RE 0.47961021366441925\n",
      "Training time: 155.67\n",
      "Training time: 155.67\n",
      "ES_rowdy_low\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 34.71543 Test MSE 17.096544269140846 Test RE 0.7237350522299338\n",
      "1 Train Loss 25.051146 Test MSE 7.508950651963874 Test RE 0.4796396358483325\n",
      "2 Train Loss 25.001268 Test MSE 7.508092059349044 Test RE 0.47961221345332194\n",
      "3 Train Loss 25.00003 Test MSE 7.508047934047548 Test RE 0.47961080410295376\n",
      "4 Train Loss 25.000004 Test MSE 7.508046239164546 Test RE 0.47961074996876\n",
      "5 Train Loss 24.999987 Test MSE 7.508048410164787 Test RE 0.4796108193100337\n",
      "6 Train Loss 24.99998 Test MSE 7.508049575324337 Test RE 0.47961085652497115\n",
      "7 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "8 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "9 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "10 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "11 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "12 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "13 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "14 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "15 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "16 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "17 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "18 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "19 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "20 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "21 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "22 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "23 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "24 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "25 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "26 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "27 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "28 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "29 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "30 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "31 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "32 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "33 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "34 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "35 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "36 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "37 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "38 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "39 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "40 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "41 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "42 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "43 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "44 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "45 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "46 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "47 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "48 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "49 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "50 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "51 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "52 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "53 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "54 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "55 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "56 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "57 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "58 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "59 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "60 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "61 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "62 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "63 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "64 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "65 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "66 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "67 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "68 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "69 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "70 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "71 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "72 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "73 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "74 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "75 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "76 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "77 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "78 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "79 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "80 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "81 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "82 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "83 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "84 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "85 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "86 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "87 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "88 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "89 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "90 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "91 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "92 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "93 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "94 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "95 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "96 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "97 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "98 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "99 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "100 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "101 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "102 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "103 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "104 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "105 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "106 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "107 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "108 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "109 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "110 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "111 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "112 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "113 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "114 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "115 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "116 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "117 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "118 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "119 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "120 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "121 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "122 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "123 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "124 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "125 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "126 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "127 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "128 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "129 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "130 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "131 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "132 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "133 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "134 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "135 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "136 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "137 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "138 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "139 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "140 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "141 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "142 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "143 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "144 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "145 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "146 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "147 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "148 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "149 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "150 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "151 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "152 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "153 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "154 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "155 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "156 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "157 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "158 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "159 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "160 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "161 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "162 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "163 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "164 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "165 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "166 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "167 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "168 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "169 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "170 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "171 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "172 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "173 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "174 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "175 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "176 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "177 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "178 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "179 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "180 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "181 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "182 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "183 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "184 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "185 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "186 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "187 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "188 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "189 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "190 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "191 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "192 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "193 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "194 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "195 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "196 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "197 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "198 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "199 Train Loss 24.999945 Test MSE 7.508061027975874 Test RE 0.4796112223199665\n",
      "Training time: 205.39\n",
      "Training time: 205.39\n",
      "ES_rowdy_low\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 38.993526 Test MSE 21.396603235129117 Test RE 0.8096509992707022\n",
      "1 Train Loss 25.01847 Test MSE 7.508088717158802 Test RE 0.4796121067048014\n",
      "2 Train Loss 25.000378 Test MSE 7.508062077670437 Test RE 0.47961125584694997\n",
      "3 Train Loss 25.00004 Test MSE 7.50806351991511 Test RE 0.4796113019118824\n",
      "4 Train Loss 25.00003 Test MSE 7.508062211724316 Test RE 0.47961126012859734\n",
      "5 Train Loss 25.000029 Test MSE 7.5080617951208035 Test RE 0.47961124682238504\n",
      "6 Train Loss 25.000025 Test MSE 7.5080613884816065 Test RE 0.4796112338344303\n",
      "7 Train Loss 25.000023 Test MSE 7.508061114683142 Test RE 0.47961122508937515\n",
      "8 Train Loss 25.000021 Test MSE 7.508061040258043 Test RE 0.4796112227122559\n",
      "9 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "10 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "11 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "12 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "13 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "14 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "15 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "16 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "17 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "18 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "19 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "20 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "21 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "22 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "23 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "24 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "25 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "26 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "27 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "28 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "29 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "30 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "31 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "32 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "33 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "34 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "35 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "36 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "37 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "38 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "39 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "40 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "41 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "42 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "43 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "44 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "45 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "46 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "47 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "48 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "49 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "50 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "51 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "52 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "53 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "54 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "55 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "56 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "57 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "58 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "59 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "60 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "61 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "62 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "63 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "64 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "65 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "66 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "67 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "68 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "69 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "70 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "71 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "72 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "73 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "74 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "75 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "76 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "77 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "78 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "79 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "80 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "81 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "82 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "83 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "84 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "85 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "86 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "87 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "88 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "89 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "90 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "91 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "92 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "93 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "94 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "95 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "96 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "97 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "98 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "99 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "100 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "101 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "102 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "103 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "104 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "105 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "106 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "107 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "108 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "109 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "110 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "111 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "112 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "113 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "114 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "115 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "116 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "117 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "118 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "119 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "120 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "121 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "122 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "123 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "124 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "125 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "126 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "127 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "128 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "129 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "130 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "131 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "132 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "133 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "134 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "135 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "136 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "137 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "138 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "139 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "140 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "141 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "142 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "143 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "144 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "145 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "146 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "147 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "148 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "149 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "150 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "151 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "152 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "153 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "154 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "155 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "156 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "157 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "158 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "159 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "160 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "161 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "162 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "163 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "164 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "165 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "166 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "167 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "168 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "169 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "170 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "171 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "172 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "173 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "174 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "175 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "176 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "177 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "178 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "179 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "180 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "181 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "182 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "183 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "184 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "185 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "186 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "187 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "188 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "189 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "190 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "191 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "192 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "193 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "194 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "195 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "196 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "197 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "198 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "199 Train Loss 25.000017 Test MSE 7.5080608701298805 Test RE 0.4796112172784045\n",
      "Training time: 127.87\n",
      "Training time: 127.87\n",
      "ES_rowdy_low\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 30.606487 Test MSE 12.560312313424845 Test RE 0.6203341049792827\n",
      "1 Train Loss 25.002619 Test MSE 7.507922601045217 Test RE 0.47960680097767955\n",
      "2 Train Loss 25.000135 Test MSE 7.508051065444237 Test RE 0.4796109041190659\n",
      "3 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "4 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "5 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "6 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "7 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "8 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "9 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "10 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "11 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "12 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "13 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "14 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "15 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "16 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "17 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "18 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "19 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "20 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "21 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "22 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "23 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "24 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "25 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "26 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "27 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "28 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "29 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "30 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "31 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "32 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "33 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "34 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "35 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "36 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "37 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "38 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "39 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "40 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "41 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "42 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "43 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "44 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "45 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "46 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "47 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "48 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "49 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "50 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "51 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "52 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "53 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "54 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "55 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "56 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "57 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "58 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "59 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "60 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "61 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "62 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "63 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "64 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "65 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "66 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "67 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "68 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "69 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "70 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "71 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "72 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "73 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "74 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "75 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "76 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "77 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "78 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "79 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "80 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "81 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "82 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "83 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "84 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "85 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "86 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "87 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "88 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "89 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "90 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "91 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "92 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "93 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "94 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "95 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "96 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "97 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "98 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "99 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "100 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "101 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "102 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "103 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "104 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "105 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "106 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "107 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "108 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "109 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "110 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "111 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "112 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "113 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "114 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "115 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "116 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "117 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "118 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "119 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "120 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "121 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "122 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "123 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "124 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "125 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "126 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "127 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "128 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "129 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "130 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "131 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "132 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "133 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "134 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "135 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "136 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "137 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "138 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "139 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "140 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "141 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "142 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "143 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "144 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "145 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "146 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "147 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "148 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "149 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "150 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "151 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "152 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "153 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "154 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "155 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "156 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "157 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "158 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "159 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "160 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "161 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "162 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "163 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "164 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "165 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "166 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "167 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "168 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "169 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "170 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "171 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "172 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "173 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "174 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "175 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "176 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "177 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "178 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "179 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "180 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "181 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "182 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "183 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "184 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "185 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "186 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "187 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "188 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "189 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "190 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "191 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "192 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "193 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "194 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "195 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "196 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "197 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "198 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "199 Train Loss 25.000057 Test MSE 7.508044598344439 Test RE 0.4796106975613173\n",
      "Training time: 139.55\n",
      "Training time: 139.55\n",
      "ES_rowdy_low\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 28.01592 Test MSE 10.225144008838969 Test RE 0.5597062494685918\n",
      "1 Train Loss 25.02629 Test MSE 7.50788042049737 Test RE 0.47960545372710794\n",
      "2 Train Loss 25.000917 Test MSE 7.507971450891803 Test RE 0.4796083612415359\n",
      "3 Train Loss 25.000004 Test MSE 7.508025210407129 Test RE 0.47961007831432034\n",
      "4 Train Loss 24.999987 Test MSE 7.508028750866025 Test RE 0.47961019139595773\n",
      "5 Train Loss 24.999987 Test MSE 7.508029082951154 Test RE 0.4796102020026974\n",
      "6 Train Loss 24.999987 Test MSE 7.508029276192911 Test RE 0.4796102081748044\n",
      "7 Train Loss 24.999983 Test MSE 7.508029813967113 Test RE 0.4796102253512153\n",
      "8 Train Loss 24.99998 Test MSE 7.508030482525282 Test RE 0.4796102467048416\n",
      "9 Train Loss 24.999977 Test MSE 7.508031324631896 Test RE 0.4796102736015724\n",
      "10 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "11 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "12 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "13 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "14 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "15 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "16 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "17 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "18 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "19 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "20 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "21 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "22 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "23 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "24 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "25 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "26 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "27 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "28 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "29 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "30 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "31 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "32 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "33 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "34 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "35 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "36 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "37 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "38 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "39 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "40 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "41 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "42 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "43 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "44 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "45 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "46 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "47 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "48 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "49 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "50 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "51 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "52 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "53 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "54 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "55 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "56 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "57 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "58 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "59 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "60 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "61 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "62 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "63 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "64 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "65 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "66 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "67 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "68 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "69 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "70 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "71 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "72 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "73 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "74 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "75 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "76 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "77 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "78 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "79 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "80 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "81 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "82 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "83 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "84 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "85 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "86 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "87 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "88 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "89 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "90 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "91 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "92 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "93 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "94 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "95 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "96 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "97 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "98 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "99 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "100 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "101 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "102 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "103 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "104 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "105 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "106 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "107 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "108 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "109 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "110 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "111 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "112 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "113 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "114 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "115 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "116 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "117 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "118 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "119 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "120 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "121 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "122 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "123 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "124 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "125 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "126 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "127 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "128 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "129 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "130 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "131 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "132 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "133 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "134 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "135 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "136 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "137 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "138 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "139 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "140 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "141 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "142 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "143 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "144 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "145 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "146 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "147 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "148 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "149 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "150 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "151 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "152 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "153 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "154 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "155 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "156 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "157 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "158 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "159 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "160 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "161 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "162 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "163 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "164 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "165 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "166 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "167 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "168 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "169 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "170 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "171 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "172 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "173 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "174 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "175 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "176 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "177 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "178 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "179 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "180 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "181 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "182 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "183 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "184 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "185 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "186 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "187 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "188 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "189 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "190 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "191 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "192 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "193 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "194 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "195 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "196 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "197 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "198 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "199 Train Loss 24.999973 Test MSE 7.508031988216997 Test RE 0.47961029479635764\n",
      "Training time: 183.31\n",
      "Training time: 183.31\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1541 into shape (500,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13146/3689713413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1541 into shape (500,500)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(75):\n",
    "    label = \"MW_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[31]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
