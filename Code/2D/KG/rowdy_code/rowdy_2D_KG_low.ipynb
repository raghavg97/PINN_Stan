{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "    y = x*np.cos(5*np.pi*t) + np.power(x*t,3)\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "\n",
    "loss_thresh = 0.1\n",
    "level = \"low\"\n",
    "label = \"KG_rowdy_\" + level\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "t = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "    \n",
    "    x_NBC = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_NBC = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_NBC,t_NBC))\n",
    "    xt_NBC = lb_xt + (ub_xt - lb_xt)*samples\n",
    "\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC,xt_NBC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC,xt_NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,rowdy_terms,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "\n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                    \n",
    "        \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_NBC(self,xt_NBC,N_hat):\n",
    "        g = xt_NBC.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_NBC.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]        \n",
    "        dy_dt = y_x_t[:,[1]]\n",
    "        \n",
    "        loss_nbc = self.loss_function(dy_dt, N_hat)\n",
    "                \n",
    "        return loss_nbc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        term1 = -25.0*(pi**2)*(g[:,0].reshape(-1,1))*torch.cos(5.0*pi*g[:,1].reshape(-1,1)) + 6*torch.pow(g[:,0].reshape(-1,1),3)*g[:,1].reshape(-1,1)  \n",
    "        term1 = term1.reshape(-1,1)\n",
    "        term2 = 6*torch.pow(g[:,1].reshape(-1,1),3)*g[:,0].reshape(-1,1)\n",
    "        term2 = term2.reshape(-1,1)\n",
    "        term3 = torch.pow(g[:,0]*torch.cos(5.0*pi*g[:,1]) + torch.pow(g[:,0]*g[:,1],3),3)\n",
    "        term3 = term3.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,3) - (term1 - term2 + term3)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_NBC = self.loss_NBC(xt_NBC,N_hat)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f + loss_NBC\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*123)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "#     f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_BC, y_BC, xt_NBC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "    y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "    xt_NBC = torch.from_numpy(xt_NBC).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    N_hat = torch.zeros(xt_NBC.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        if(np.isnan(loss_np) == False):\n",
    "            train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,i)\n",
    "            loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG_rowdy_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 9846.65 Test MSE 0.4133816575348056 Test RE 1.5046568451019167\n",
      "1 Train Loss 2085.0242 Test MSE 0.4114579148063863 Test RE 1.5011516724986942\n",
      "2 Train Loss 147.91435 Test MSE 0.27985637716518125 Test RE 1.2380251517543945\n",
      "3 Train Loss 22.023539 Test MSE 0.23615445005848218 Test RE 1.1372604363356724\n",
      "4 Train Loss 7.694083 Test MSE 0.24276750904646327 Test RE 1.153073908521846\n",
      "5 Train Loss 4.1264067 Test MSE 0.2727625866582828 Test RE 1.2222337283241973\n",
      "6 Train Loss 2.985764 Test MSE 0.2974988712656516 Test RE 1.2764521089377971\n",
      "7 Train Loss 2.2577946 Test MSE 0.297232665432426 Test RE 1.2758808882068615\n",
      "8 Train Loss 1.9598656 Test MSE 0.29504581838598365 Test RE 1.2711786672061463\n",
      "9 Train Loss 1.5067393 Test MSE 0.2674465560118847 Test RE 1.210264707174134\n",
      "10 Train Loss 1.299573 Test MSE 0.25427123796871637 Test RE 1.1800773718549065\n",
      "11 Train Loss 0.9083609 Test MSE 0.23765289691685343 Test RE 1.140862802537651\n",
      "12 Train Loss 0.79027474 Test MSE 0.25009275515156604 Test RE 1.170340999475011\n",
      "13 Train Loss 0.7051436 Test MSE 0.2713007505540918 Test RE 1.2189541258046044\n",
      "14 Train Loss 0.61916745 Test MSE 0.28638156090749667 Test RE 1.252375001022191\n",
      "15 Train Loss 0.5598307 Test MSE 0.27380709156346594 Test RE 1.2245716759656384\n",
      "16 Train Loss 0.47464737 Test MSE 0.2743691625372748 Test RE 1.2258279314539606\n",
      "17 Train Loss 0.38006473 Test MSE 0.23343485289613997 Test RE 1.1306930257483458\n",
      "18 Train Loss 0.33900076 Test MSE 0.21288098098367192 Test RE 1.0797676093620576\n",
      "19 Train Loss 0.31493008 Test MSE 0.19920561431566652 Test RE 1.04451011768145\n",
      "20 Train Loss 0.2836887 Test MSE 0.16155239851046982 Test RE 0.9406294441019072\n",
      "21 Train Loss 0.25048882 Test MSE 0.14185353039178106 Test RE 0.8814180470336672\n",
      "22 Train Loss 0.22934327 Test MSE 0.12487376477052496 Test RE 0.826984684609804\n",
      "23 Train Loss 0.20560992 Test MSE 0.09809298505327596 Test RE 0.732960953471526\n",
      "24 Train Loss 0.1874091 Test MSE 0.09291114475613077 Test RE 0.713338673043778\n",
      "25 Train Loss 0.16266881 Test MSE 0.07720800922582693 Test RE 0.6502689861219404\n",
      "26 Train Loss 0.15855223 Test MSE 0.07121647611326858 Test RE 0.6245282732179661\n",
      "27 Train Loss 0.14613761 Test MSE 0.06884548718808216 Test RE 0.6140441565547415\n",
      "28 Train Loss 0.13103314 Test MSE 0.06310175746444191 Test RE 0.587871751048387\n",
      "29 Train Loss 0.12971103 Test MSE 0.058551834488464106 Test RE 0.5662811655701135\n",
      "30 Train Loss 0.114634186 Test MSE 0.04947210935129166 Test RE 0.520525581460827\n",
      "31 Train Loss 0.110047504 Test MSE 0.0476931308094289 Test RE 0.5110810523080126\n",
      "32 Train Loss 0.10837361 Test MSE 0.04686951452070743 Test RE 0.5066488854167012\n",
      "33 Train Loss 0.09334052 Test MSE 0.037125607704688965 Test RE 0.4509191188738088\n",
      "34 Train Loss 0.092156895 Test MSE 0.035595864432076504 Test RE 0.4415314424244742\n",
      "35 Train Loss 0.0878574 Test MSE 0.02918111152044418 Test RE 0.3997723740107251\n",
      "36 Train Loss 0.07590905 Test MSE 0.021921616638332923 Test RE 0.3464959424971075\n",
      "37 Train Loss 0.07392022 Test MSE 0.02295387111795179 Test RE 0.3545600773458012\n",
      "38 Train Loss 0.07373953 Test MSE 0.022793678756536002 Test RE 0.3533206946860313\n",
      "39 Train Loss 0.07262226 Test MSE 0.020282303189636922 Test RE 0.33328863087085775\n",
      "40 Train Loss 0.0719943 Test MSE 0.020670454029459464 Test RE 0.3364626584782975\n",
      "41 Train Loss 0.07034448 Test MSE 0.017340639627671944 Test RE 0.30817288242737456\n",
      "42 Train Loss 0.066010006 Test MSE 0.014764292229737769 Test RE 0.28435978706158394\n",
      "43 Train Loss 0.063810766 Test MSE 0.014051178089788173 Test RE 0.2774075221349922\n",
      "44 Train Loss 0.06334782 Test MSE 0.01480272325408629 Test RE 0.2847296366791362\n",
      "45 Train Loss 0.062789194 Test MSE 0.015448950793539334 Test RE 0.2908783238582637\n",
      "46 Train Loss 0.062439915 Test MSE 0.01522577971211693 Test RE 0.28876970891616566\n",
      "47 Train Loss 0.061525438 Test MSE 0.01570496773263712 Test RE 0.29327860931317995\n",
      "48 Train Loss 0.057675745 Test MSE 0.013509723614707576 Test RE 0.27201014135868895\n",
      "49 Train Loss 0.054027334 Test MSE 0.008733084724130603 Test RE 0.2186984478598235\n",
      "50 Train Loss 0.052408133 Test MSE 0.006848216205074911 Test RE 0.19366475712854161\n",
      "51 Train Loss 0.05115089 Test MSE 0.007555350645594148 Test RE 0.20341790465508866\n",
      "52 Train Loss 0.050150078 Test MSE 0.00741248338373472 Test RE 0.20148546894613747\n",
      "53 Train Loss 0.04913088 Test MSE 0.006744150572691402 Test RE 0.19218765734817944\n",
      "54 Train Loss 0.04800717 Test MSE 0.007349652849962753 Test RE 0.2006297246867881\n",
      "55 Train Loss 0.046930462 Test MSE 0.006599060153301891 Test RE 0.19010910077746176\n",
      "56 Train Loss 0.045937937 Test MSE 0.006291463229091711 Test RE 0.1856255251770169\n",
      "57 Train Loss 0.043829378 Test MSE 0.004651897943646246 Test RE 0.15961619481920233\n",
      "58 Train Loss 0.042916607 Test MSE 0.004213129017689438 Test RE 0.15190226280416336\n",
      "59 Train Loss 0.04259563 Test MSE 0.003727305609968461 Test RE 0.14287602712544872\n",
      "60 Train Loss 0.042166628 Test MSE 0.003485125571272895 Test RE 0.13815642266247274\n",
      "61 Train Loss 0.041829094 Test MSE 0.0029067071328930734 Test RE 0.12617185907988654\n",
      "62 Train Loss 0.04133932 Test MSE 0.002259224215734658 Test RE 0.11123501846911876\n",
      "63 Train Loss 0.040580705 Test MSE 0.0022561479285411737 Test RE 0.11115926073193359\n",
      "64 Train Loss 0.036343142 Test MSE 0.001651728015458594 Test RE 0.0951110840265082\n",
      "65 Train Loss 0.03502194 Test MSE 0.0015297155709415743 Test RE 0.09153078936120913\n",
      "66 Train Loss 0.034251526 Test MSE 0.001355836879695913 Test RE 0.08617188346640492\n",
      "67 Train Loss 0.033609435 Test MSE 0.0013171815807598165 Test RE 0.08493460835420646\n",
      "68 Train Loss 0.0327666 Test MSE 0.0015750379366004956 Test RE 0.09287682773881206\n",
      "69 Train Loss 0.032273866 Test MSE 0.0017395064781943975 Test RE 0.09760563456275843\n",
      "70 Train Loss 0.031102804 Test MSE 0.0015307026987533589 Test RE 0.0915603170780162\n",
      "71 Train Loss 0.030238194 Test MSE 0.0019786350839298646 Test RE 0.10409855876034323\n",
      "72 Train Loss 0.029661415 Test MSE 0.00163311561758343 Test RE 0.09457368906964952\n",
      "73 Train Loss 0.029070057 Test MSE 0.0013974141206185065 Test RE 0.0874831531191971\n",
      "74 Train Loss 0.02855982 Test MSE 0.0015242421178576651 Test RE 0.09136689012393438\n",
      "75 Train Loss 0.028065078 Test MSE 0.0015444371670966825 Test RE 0.09197016936713918\n",
      "76 Train Loss 0.027934127 Test MSE 0.001526577848085312 Test RE 0.09143686808577964\n",
      "77 Train Loss 0.02772806 Test MSE 0.0014044988792170437 Test RE 0.08770463843950105\n",
      "78 Train Loss 0.02743321 Test MSE 0.0011352657947163271 Test RE 0.07885162366239615\n",
      "79 Train Loss 0.027140232 Test MSE 0.0011218571842054749 Test RE 0.07838458272785558\n",
      "80 Train Loss 0.026651094 Test MSE 0.0010194812759613588 Test RE 0.07472251673163433\n",
      "81 Train Loss 0.026038 Test MSE 0.0008164789443301449 Test RE 0.06687046740060025\n",
      "82 Train Loss 0.025210617 Test MSE 0.0006870389633430188 Test RE 0.06134123955726735\n",
      "83 Train Loss 0.02480771 Test MSE 0.0004996504314029347 Test RE 0.05231123790605793\n",
      "84 Train Loss 0.024551526 Test MSE 0.000505599688853397 Test RE 0.05262174710006118\n",
      "85 Train Loss 0.024181996 Test MSE 0.0005200835533800462 Test RE 0.05337015009412131\n",
      "86 Train Loss 0.023351768 Test MSE 0.0005188825341710491 Test RE 0.05330849113256316\n",
      "87 Train Loss 0.023069778 Test MSE 0.000565630744454767 Test RE 0.055658098832997875\n",
      "88 Train Loss 0.02284965 Test MSE 0.0005989448277501732 Test RE 0.05727370436921202\n",
      "89 Train Loss 0.022639602 Test MSE 0.0005958833352132052 Test RE 0.05712714023652073\n",
      "90 Train Loss 0.022563765 Test MSE 0.0006177119463803122 Test RE 0.05816408016210819\n",
      "91 Train Loss 0.022520445 Test MSE 0.0006246229249659242 Test RE 0.0584885458339274\n",
      "92 Train Loss 0.022468228 Test MSE 0.0006306239082541925 Test RE 0.05876883477130847\n",
      "93 Train Loss 0.02235587 Test MSE 0.0006182772248647527 Test RE 0.05819068753525958\n",
      "94 Train Loss 0.021984562 Test MSE 0.0005636765312459291 Test RE 0.055561868295819626\n",
      "95 Train Loss 0.02161767 Test MSE 0.0005609704907441737 Test RE 0.05542833998527375\n",
      "96 Train Loss 0.021423878 Test MSE 0.0005105806279075874 Test RE 0.05288031464334715\n",
      "97 Train Loss 0.021228028 Test MSE 0.0004906740288888107 Test RE 0.05183921302424041\n",
      "98 Train Loss 0.020951223 Test MSE 0.0004570941214711087 Test RE 0.05003393753766997\n",
      "99 Train Loss 0.020789726 Test MSE 0.00046175564433139656 Test RE 0.05028841763798793\n",
      "100 Train Loss 0.020622302 Test MSE 0.00040304650037246845 Test RE 0.046982859042439276\n",
      "101 Train Loss 0.020494772 Test MSE 0.0003804227028892664 Test RE 0.0456451961274108\n",
      "102 Train Loss 0.020366654 Test MSE 0.00038777024095416126 Test RE 0.04608388692121077\n",
      "103 Train Loss 0.020219052 Test MSE 0.00041543007120910453 Test RE 0.0476991707690671\n",
      "104 Train Loss 0.020025894 Test MSE 0.0004705552804903673 Test RE 0.05076532720155093\n",
      "105 Train Loss 0.019754546 Test MSE 0.000564621113261832 Test RE 0.05560840276199294\n",
      "106 Train Loss 0.019211441 Test MSE 0.0006475847671381186 Test RE 0.059553895739638645\n",
      "107 Train Loss 0.018704409 Test MSE 0.001052348409458966 Test RE 0.07591745462211195\n",
      "108 Train Loss 0.018247427 Test MSE 0.0008879829687318324 Test RE 0.06973714797332461\n",
      "109 Train Loss 0.01766249 Test MSE 0.0008339260821257575 Test RE 0.06758116010227264\n",
      "110 Train Loss 0.017332107 Test MSE 0.000832550960631288 Test RE 0.06752541736307621\n",
      "111 Train Loss 0.017087776 Test MSE 0.0012085434320091403 Test RE 0.08135663835126845\n",
      "112 Train Loss 0.016916411 Test MSE 0.0012766206223826981 Test RE 0.08361665535394264\n",
      "113 Train Loss 0.016762488 Test MSE 0.0013370291521174122 Test RE 0.0855721221294767\n",
      "114 Train Loss 0.01649322 Test MSE 0.001259473627262254 Test RE 0.08305320626954889\n",
      "115 Train Loss 0.016334502 Test MSE 0.0012113050951298306 Test RE 0.08144954002672597\n",
      "116 Train Loss 0.015989428 Test MSE 0.0010093345049047872 Test RE 0.0743497348652357\n",
      "117 Train Loss 0.015770257 Test MSE 0.0013220462719331365 Test RE 0.08509130650911613\n",
      "118 Train Loss 0.015644135 Test MSE 0.001191636668146659 Test RE 0.08078556994841805\n",
      "119 Train Loss 0.015402782 Test MSE 0.0011294010576774643 Test RE 0.07864768779580464\n",
      "120 Train Loss 0.015206286 Test MSE 0.0011911425899243727 Test RE 0.08076882049363335\n",
      "121 Train Loss 0.014905183 Test MSE 0.0012570439522635952 Test RE 0.08297305782149672\n",
      "122 Train Loss 0.014683898 Test MSE 0.0010862016239926834 Test RE 0.07712889118412712\n",
      "123 Train Loss 0.014574958 Test MSE 0.0010487867749784086 Test RE 0.07578887582109907\n",
      "124 Train Loss 0.014436978 Test MSE 0.0011175172316748184 Test RE 0.07823281875276546\n",
      "125 Train Loss 0.014279317 Test MSE 0.0011668969166685177 Test RE 0.07994257079249019\n",
      "126 Train Loss 0.014075498 Test MSE 0.0013108346353291037 Test RE 0.08472972912720729\n",
      "127 Train Loss 0.0138568925 Test MSE 0.0013550196845888683 Test RE 0.08614591063124134\n",
      "128 Train Loss 0.013529259 Test MSE 0.0012535577928301988 Test RE 0.08285792336832723\n",
      "129 Train Loss 0.013108412 Test MSE 0.0010748317815024314 Test RE 0.07672415495871433\n",
      "130 Train Loss 0.012311093 Test MSE 0.0009272512904995719 Test RE 0.07126242341081727\n",
      "131 Train Loss 0.0117471805 Test MSE 0.000838079753707922 Test RE 0.06774925729552919\n",
      "132 Train Loss 0.011286814 Test MSE 0.0008041123886399149 Test RE 0.06636211833566621\n",
      "133 Train Loss 0.010944373 Test MSE 0.0008614446174816423 Test RE 0.06868715761170296\n",
      "134 Train Loss 0.010717817 Test MSE 0.000981364747466166 Test RE 0.07331234152151746\n",
      "135 Train Loss 0.010480097 Test MSE 0.0011312625637014395 Test RE 0.07871247561717953\n",
      "136 Train Loss 0.010376878 Test MSE 0.0012290769459405153 Test RE 0.08204486416763936\n",
      "137 Train Loss 0.010282734 Test MSE 0.001303059328444428 Test RE 0.084478065238577\n",
      "138 Train Loss 0.0102025345 Test MSE 0.0013309015185303426 Test RE 0.08537580749728016\n",
      "139 Train Loss 0.010150703 Test MSE 0.0013813954378050926 Test RE 0.08698029428967505\n",
      "140 Train Loss 0.010104526 Test MSE 0.0013098235158954122 Test RE 0.08469704445415083\n",
      "141 Train Loss 0.010059421 Test MSE 0.0011321397051533733 Test RE 0.07874298515553313\n",
      "142 Train Loss 0.010001561 Test MSE 0.0009613674809625119 Test RE 0.07256155445885602\n",
      "143 Train Loss 0.009948448 Test MSE 0.0008469646536304826 Test RE 0.06810743227161832\n",
      "144 Train Loss 0.009932259 Test MSE 0.0008207992924605109 Test RE 0.06704715446006561\n",
      "145 Train Loss 0.009890215 Test MSE 0.0007876295138805592 Test RE 0.06567844392987901\n",
      "146 Train Loss 0.0098621035 Test MSE 0.0007209949648818109 Test RE 0.06283881403397139\n",
      "147 Train Loss 0.009844032 Test MSE 0.0006802489097929844 Test RE 0.06103736704007113\n",
      "148 Train Loss 0.009807825 Test MSE 0.0007225458304808239 Test RE 0.06290636111101736\n",
      "149 Train Loss 0.009747909 Test MSE 0.0007083947484897253 Test RE 0.06228730356323984\n",
      "150 Train Loss 0.009607334 Test MSE 0.0004868745979879445 Test RE 0.051638119978565336\n",
      "151 Train Loss 0.0094465995 Test MSE 0.00029136808586039396 Test RE 0.039946879036162654\n",
      "152 Train Loss 0.009067446 Test MSE 0.00015037020994307176 Test RE 0.02869741426485867\n",
      "153 Train Loss 0.008672615 Test MSE 0.00022990350409576326 Test RE 0.035484170665385874\n",
      "154 Train Loss 0.008439599 Test MSE 0.00024260905049033565 Test RE 0.036451496526146265\n",
      "155 Train Loss 0.00823916 Test MSE 0.00023466606572187422 Test RE 0.03584982253671201\n",
      "156 Train Loss 0.008060422 Test MSE 0.00025972467497753385 Test RE 0.03771537843488928\n",
      "157 Train Loss 0.007958315 Test MSE 0.00024039447274300214 Test RE 0.03628474731432406\n",
      "158 Train Loss 0.007887129 Test MSE 0.00020163658021480106 Test RE 0.03323123836534083\n",
      "159 Train Loss 0.007827017 Test MSE 0.00020655774644942246 Test RE 0.03363431656522381\n",
      "160 Train Loss 0.0077676564 Test MSE 0.00024526095328347285 Test RE 0.036650176462673385\n",
      "161 Train Loss 0.0076586795 Test MSE 0.00022184313611856558 Test RE 0.03485658711117781\n",
      "162 Train Loss 0.0075471736 Test MSE 0.00023020496885037872 Test RE 0.03550742764533708\n",
      "163 Train Loss 0.0074693426 Test MSE 0.00019897729707519393 Test RE 0.033011376026403846\n",
      "164 Train Loss 0.007317047 Test MSE 0.000169834826080116 Test RE 0.03049827242692649\n",
      "165 Train Loss 0.0071980692 Test MSE 0.00015923967892209672 Test RE 0.029531636264323712\n",
      "166 Train Loss 0.007030631 Test MSE 0.0001485341401501413 Test RE 0.02852167371436604\n",
      "167 Train Loss 0.006896868 Test MSE 0.00017783363855950745 Test RE 0.03120820723937613\n",
      "168 Train Loss 0.0068144198 Test MSE 0.00015655362542336883 Test RE 0.029281507295094846\n",
      "169 Train Loss 0.0067630755 Test MSE 0.00013264123314473287 Test RE 0.026952629379217285\n",
      "170 Train Loss 0.0066647036 Test MSE 0.00020853639013768181 Test RE 0.03379502637770118\n",
      "171 Train Loss 0.0066327946 Test MSE 0.0002035502297498952 Test RE 0.03338855796467912\n",
      "172 Train Loss 0.0065289233 Test MSE 0.0002085774900991082 Test RE 0.03379835650595579\n",
      "173 Train Loss 0.006373138 Test MSE 0.0001796766704530692 Test RE 0.03136950817070384\n",
      "174 Train Loss 0.006304061 Test MSE 0.00016987188378253054 Test RE 0.03050159958392016\n",
      "175 Train Loss 0.006208553 Test MSE 0.00018159909624693223 Test RE 0.031536878539314556\n",
      "176 Train Loss 0.006166032 Test MSE 0.00018029461699202006 Test RE 0.03142340509715996\n",
      "177 Train Loss 0.006132855 Test MSE 0.00016607145704376216 Test RE 0.030158473952780047\n",
      "178 Train Loss 0.0060805143 Test MSE 0.0001606527200711463 Test RE 0.02966237394437285\n",
      "179 Train Loss 0.0060060583 Test MSE 0.00014807156284177747 Test RE 0.028477226804296294\n",
      "180 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "181 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "182 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "183 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "184 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "185 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "186 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "187 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "188 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "189 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "190 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "191 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "192 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "193 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "194 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "195 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "196 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "197 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "198 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "199 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "200 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "201 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "202 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "203 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "204 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "205 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "206 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "207 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "208 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "209 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "210 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "211 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "212 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "213 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "214 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "215 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "216 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "217 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "218 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "219 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "220 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "221 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "222 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "223 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "224 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "225 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "226 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "227 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "228 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "229 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "230 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "231 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "232 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "233 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "234 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "235 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "236 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "237 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "238 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "239 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "240 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "241 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "242 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "243 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "244 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "245 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "246 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "247 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "248 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "249 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "250 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "251 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "252 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "253 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "254 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "255 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "256 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "257 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "258 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "259 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "260 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "261 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "262 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "263 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "264 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "265 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "266 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "267 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "268 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "269 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "270 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "271 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "272 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "273 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "274 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "275 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "276 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "277 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "278 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "279 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "280 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "281 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "282 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "283 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "284 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "285 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "286 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "287 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "288 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "289 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "290 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "291 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "292 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "293 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "294 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "295 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "296 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "297 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "298 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "299 Train Loss 0.0059640743 Test MSE 0.00013889463889641646 Test RE 0.027580656887971206\n",
      "Training time: 378.00\n",
      "KG_rowdy_low\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 11605.874 Test MSE 0.7275953959622219 Test RE 1.9962127961801874\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "200 Train Loss nan Test MSE nan Test RE nan\n",
      "201 Train Loss nan Test MSE nan Test RE nan\n",
      "202 Train Loss nan Test MSE nan Test RE nan\n",
      "203 Train Loss nan Test MSE nan Test RE nan\n",
      "204 Train Loss nan Test MSE nan Test RE nan\n",
      "205 Train Loss nan Test MSE nan Test RE nan\n",
      "206 Train Loss nan Test MSE nan Test RE nan\n",
      "207 Train Loss nan Test MSE nan Test RE nan\n",
      "208 Train Loss nan Test MSE nan Test RE nan\n",
      "209 Train Loss nan Test MSE nan Test RE nan\n",
      "210 Train Loss nan Test MSE nan Test RE nan\n",
      "211 Train Loss nan Test MSE nan Test RE nan\n",
      "212 Train Loss nan Test MSE nan Test RE nan\n",
      "213 Train Loss nan Test MSE nan Test RE nan\n",
      "214 Train Loss nan Test MSE nan Test RE nan\n",
      "215 Train Loss nan Test MSE nan Test RE nan\n",
      "216 Train Loss nan Test MSE nan Test RE nan\n",
      "217 Train Loss nan Test MSE nan Test RE nan\n",
      "218 Train Loss nan Test MSE nan Test RE nan\n",
      "219 Train Loss nan Test MSE nan Test RE nan\n",
      "220 Train Loss nan Test MSE nan Test RE nan\n",
      "221 Train Loss nan Test MSE nan Test RE nan\n",
      "222 Train Loss nan Test MSE nan Test RE nan\n",
      "223 Train Loss nan Test MSE nan Test RE nan\n",
      "224 Train Loss nan Test MSE nan Test RE nan\n",
      "225 Train Loss nan Test MSE nan Test RE nan\n",
      "226 Train Loss nan Test MSE nan Test RE nan\n",
      "227 Train Loss nan Test MSE nan Test RE nan\n",
      "228 Train Loss nan Test MSE nan Test RE nan\n",
      "229 Train Loss nan Test MSE nan Test RE nan\n",
      "230 Train Loss nan Test MSE nan Test RE nan\n",
      "231 Train Loss nan Test MSE nan Test RE nan\n",
      "232 Train Loss nan Test MSE nan Test RE nan\n",
      "233 Train Loss nan Test MSE nan Test RE nan\n",
      "234 Train Loss nan Test MSE nan Test RE nan\n",
      "235 Train Loss nan Test MSE nan Test RE nan\n",
      "236 Train Loss nan Test MSE nan Test RE nan\n",
      "237 Train Loss nan Test MSE nan Test RE nan\n",
      "238 Train Loss nan Test MSE nan Test RE nan\n",
      "239 Train Loss nan Test MSE nan Test RE nan\n",
      "240 Train Loss nan Test MSE nan Test RE nan\n",
      "241 Train Loss nan Test MSE nan Test RE nan\n",
      "242 Train Loss nan Test MSE nan Test RE nan\n",
      "243 Train Loss nan Test MSE nan Test RE nan\n",
      "244 Train Loss nan Test MSE nan Test RE nan\n",
      "245 Train Loss nan Test MSE nan Test RE nan\n",
      "246 Train Loss nan Test MSE nan Test RE nan\n",
      "247 Train Loss nan Test MSE nan Test RE nan\n",
      "248 Train Loss nan Test MSE nan Test RE nan\n",
      "249 Train Loss nan Test MSE nan Test RE nan\n",
      "250 Train Loss nan Test MSE nan Test RE nan\n",
      "251 Train Loss nan Test MSE nan Test RE nan\n",
      "252 Train Loss nan Test MSE nan Test RE nan\n",
      "253 Train Loss nan Test MSE nan Test RE nan\n",
      "254 Train Loss nan Test MSE nan Test RE nan\n",
      "255 Train Loss nan Test MSE nan Test RE nan\n",
      "256 Train Loss nan Test MSE nan Test RE nan\n",
      "257 Train Loss nan Test MSE nan Test RE nan\n",
      "258 Train Loss nan Test MSE nan Test RE nan\n",
      "259 Train Loss nan Test MSE nan Test RE nan\n",
      "260 Train Loss nan Test MSE nan Test RE nan\n",
      "261 Train Loss nan Test MSE nan Test RE nan\n",
      "262 Train Loss nan Test MSE nan Test RE nan\n",
      "263 Train Loss nan Test MSE nan Test RE nan\n",
      "264 Train Loss nan Test MSE nan Test RE nan\n",
      "265 Train Loss nan Test MSE nan Test RE nan\n",
      "266 Train Loss nan Test MSE nan Test RE nan\n",
      "267 Train Loss nan Test MSE nan Test RE nan\n",
      "268 Train Loss nan Test MSE nan Test RE nan\n",
      "269 Train Loss nan Test MSE nan Test RE nan\n",
      "270 Train Loss nan Test MSE nan Test RE nan\n",
      "271 Train Loss nan Test MSE nan Test RE nan\n",
      "272 Train Loss nan Test MSE nan Test RE nan\n",
      "273 Train Loss nan Test MSE nan Test RE nan\n",
      "274 Train Loss nan Test MSE nan Test RE nan\n",
      "275 Train Loss nan Test MSE nan Test RE nan\n",
      "276 Train Loss nan Test MSE nan Test RE nan\n",
      "277 Train Loss nan Test MSE nan Test RE nan\n",
      "278 Train Loss nan Test MSE nan Test RE nan\n",
      "279 Train Loss nan Test MSE nan Test RE nan\n",
      "280 Train Loss nan Test MSE nan Test RE nan\n",
      "281 Train Loss nan Test MSE nan Test RE nan\n",
      "282 Train Loss nan Test MSE nan Test RE nan\n",
      "283 Train Loss nan Test MSE nan Test RE nan\n",
      "284 Train Loss nan Test MSE nan Test RE nan\n",
      "285 Train Loss nan Test MSE nan Test RE nan\n",
      "286 Train Loss nan Test MSE nan Test RE nan\n",
      "287 Train Loss nan Test MSE nan Test RE nan\n",
      "288 Train Loss nan Test MSE nan Test RE nan\n",
      "289 Train Loss nan Test MSE nan Test RE nan\n",
      "290 Train Loss nan Test MSE nan Test RE nan\n",
      "291 Train Loss nan Test MSE nan Test RE nan\n",
      "292 Train Loss nan Test MSE nan Test RE nan\n",
      "293 Train Loss nan Test MSE nan Test RE nan\n",
      "294 Train Loss nan Test MSE nan Test RE nan\n",
      "295 Train Loss nan Test MSE nan Test RE nan\n",
      "296 Train Loss nan Test MSE nan Test RE nan\n",
      "297 Train Loss nan Test MSE nan Test RE nan\n",
      "298 Train Loss nan Test MSE nan Test RE nan\n",
      "299 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 27.18\n",
      "KG_rowdy_low\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 7495.2007 Test MSE 0.9200375435602665 Test RE 2.244732729756734\n",
      "1 Train Loss 1221.7345 Test MSE 2.3249027631012 Test RE 3.5683238347942843\n",
      "2 Train Loss 408.69855 Test MSE 4.308855606993406 Test RE 4.857835837305784\n",
      "3 Train Loss 150.73483 Test MSE 5.384693411549952 Test RE 5.430531716452243\n",
      "4 Train Loss 64.56987 Test MSE 4.132150865354951 Test RE 4.757183990647907\n",
      "5 Train Loss 28.422401 Test MSE 3.786523562234305 Test RE 4.553886433913631\n",
      "6 Train Loss 17.766567 Test MSE 3.1808687833191347 Test RE 4.173829853639179\n",
      "7 Train Loss 12.206936 Test MSE 2.6902621323188614 Test RE 3.838478950404359\n",
      "8 Train Loss 9.3189125 Test MSE 2.169756258013629 Test RE 3.4472068085888394\n",
      "9 Train Loss 7.0915794 Test MSE 1.6547361512302767 Test RE 3.01041411487792\n",
      "10 Train Loss 5.8309703 Test MSE 1.2380875305985035 Test RE 2.6039793678107492\n",
      "11 Train Loss 4.4590735 Test MSE 1.0098857595099764 Test RE 2.3517870149800486\n",
      "12 Train Loss 3.5337558 Test MSE 0.8369939469768662 Test RE 2.1410313311519884\n",
      "13 Train Loss 2.6809566 Test MSE 0.5930760828776978 Test RE 1.802258445300886\n",
      "14 Train Loss 2.2624006 Test MSE 0.47918878480926 Test RE 1.6200006416584523\n",
      "15 Train Loss 1.9214045 Test MSE 0.3841195324205775 Test RE 1.450424275328228\n",
      "16 Train Loss 1.6556586 Test MSE 0.36650341847819734 Test RE 1.4167749808021133\n",
      "17 Train Loss 1.4430166 Test MSE 0.2869814344118548 Test RE 1.2536859680704002\n",
      "18 Train Loss 1.2424014 Test MSE 0.2749261810033792 Test RE 1.2270716255031109\n",
      "19 Train Loss 1.1040795 Test MSE 0.2327490658914126 Test RE 1.1290309240595149\n",
      "20 Train Loss 0.93619907 Test MSE 0.22102633484109294 Test RE 1.1002309940587098\n",
      "21 Train Loss 0.85177606 Test MSE 0.18957428092585735 Test RE 1.018946947723189\n",
      "22 Train Loss 0.77149147 Test MSE 0.1773785097909951 Test RE 0.985626485012069\n",
      "23 Train Loss 0.7131127 Test MSE 0.1582140461157173 Test RE 0.9308600300176535\n",
      "24 Train Loss 0.6840619 Test MSE 0.14631646740131146 Test RE 0.8951760774039637\n",
      "25 Train Loss 0.66974914 Test MSE 0.1363308134923387 Test RE 0.864089797048924\n",
      "26 Train Loss 0.58719116 Test MSE 0.12025057655534982 Test RE 0.8115316230620346\n",
      "27 Train Loss 0.5641778 Test MSE 0.12027499932004031 Test RE 0.8116140294844875\n",
      "28 Train Loss 0.49561158 Test MSE 0.10191431381090095 Test RE 0.7471012385726314\n",
      "29 Train Loss 0.48363405 Test MSE 0.1017594635121242 Test RE 0.7465334438047121\n",
      "30 Train Loss 0.4357833 Test MSE 0.09473387609026256 Test RE 0.7203018290545599\n",
      "31 Train Loss 0.37275574 Test MSE 0.08504898550972982 Test RE 0.6824902207231127\n",
      "32 Train Loss 0.34935153 Test MSE 0.0774204642852585 Test RE 0.6511630515716217\n",
      "33 Train Loss 0.3125059 Test MSE 0.07379306140320581 Test RE 0.6357254941340348\n",
      "34 Train Loss 0.288725 Test MSE 0.06239971705038833 Test RE 0.5845924124222286\n",
      "35 Train Loss 0.28133515 Test MSE 0.06297521290422933 Test RE 0.5872819948424172\n",
      "36 Train Loss 0.25826123 Test MSE 0.057153668077636464 Test RE 0.5594791651944433\n",
      "37 Train Loss 0.24162254 Test MSE 0.04794441275176863 Test RE 0.5124256560481874\n",
      "38 Train Loss 0.23492579 Test MSE 0.042418173805402574 Test RE 0.4819897999930127\n",
      "39 Train Loss 0.23007667 Test MSE 0.042734966369161305 Test RE 0.4837862794782823\n",
      "40 Train Loss 0.20257516 Test MSE 0.04267266431621137 Test RE 0.48343350194792883\n",
      "41 Train Loss 0.1901131 Test MSE 0.0424765062177872 Test RE 0.48232109629740655\n",
      "42 Train Loss 0.1832344 Test MSE 0.04387031797471886 Test RE 0.49017059553897896\n",
      "43 Train Loss 0.18194532 Test MSE 0.04398898292250521 Test RE 0.49083307977120516\n",
      "44 Train Loss 0.16658132 Test MSE 0.042848114145237946 Test RE 0.4844263075551224\n",
      "45 Train Loss 0.12589402 Test MSE 0.03194573147337268 Test RE 0.41828113946187107\n",
      "46 Train Loss 0.11889261 Test MSE 0.027092593980909064 Test RE 0.38520078040345757\n",
      "47 Train Loss 0.11740019 Test MSE 0.026652040527611266 Test RE 0.3820560634141632\n",
      "48 Train Loss 0.11274879 Test MSE 0.02571989032357572 Test RE 0.3753154305731698\n",
      "49 Train Loss 0.106909566 Test MSE 0.027922537320941705 Test RE 0.3910563155105702\n",
      "50 Train Loss 0.10052734 Test MSE 0.02345151089980069 Test RE 0.3583828988849944\n",
      "51 Train Loss 0.09681489 Test MSE 0.020404241447794517 Test RE 0.3342890038132865\n",
      "52 Train Loss 0.095376045 Test MSE 0.019959616751884797 Test RE 0.33062673095040723\n",
      "53 Train Loss 0.09256443 Test MSE 0.019679914558785122 Test RE 0.3283019545658149\n",
      "54 Train Loss 0.08928432 Test MSE 0.017354640376295475 Test RE 0.3082972659736677\n",
      "55 Train Loss 0.087147854 Test MSE 0.01619118061553319 Test RE 0.2977838378097361\n",
      "56 Train Loss 0.08619105 Test MSE 0.01545980416686326 Test RE 0.29098048149999206\n",
      "57 Train Loss 0.0841638 Test MSE 0.014169283187144766 Test RE 0.2785709364545364\n",
      "58 Train Loss 0.081334695 Test MSE 0.014620509972468065 Test RE 0.2829717787155011\n",
      "59 Train Loss 0.07809592 Test MSE 0.011639436600264614 Test RE 0.2524805011557951\n",
      "60 Train Loss 0.07488141 Test MSE 0.009484442251393516 Test RE 0.22791229918457823\n",
      "61 Train Loss 0.0734499 Test MSE 0.008610685340485619 Test RE 0.21716044533905635\n",
      "62 Train Loss 0.07311883 Test MSE 0.008853468104766588 Test RE 0.2202006403433759\n",
      "63 Train Loss 0.072677225 Test MSE 0.009208094624114059 Test RE 0.22456742066705251\n",
      "64 Train Loss 0.06963529 Test MSE 0.009640739665363125 Test RE 0.22978254846783547\n",
      "65 Train Loss 0.06618914 Test MSE 0.009142463090668355 Test RE 0.22376567711087644\n",
      "66 Train Loss 0.06217606 Test MSE 0.007538257882103464 Test RE 0.20318767424962575\n",
      "67 Train Loss 0.060477544 Test MSE 0.006772491602913112 Test RE 0.19259105032206744\n",
      "68 Train Loss 0.05958047 Test MSE 0.006335029528389511 Test RE 0.18626711403942367\n",
      "69 Train Loss 0.059431802 Test MSE 0.0060154459973063585 Test RE 0.1815080036791701\n",
      "70 Train Loss 0.059073895 Test MSE 0.005721423781627464 Test RE 0.17701657047359842\n",
      "71 Train Loss 0.05654867 Test MSE 0.005334789685853437 Test RE 0.17093087486818986\n",
      "72 Train Loss 0.049601723 Test MSE 0.005301321349256041 Test RE 0.1703938553128811\n",
      "73 Train Loss 0.044116728 Test MSE 0.005392623910720769 Test RE 0.17185490439859738\n",
      "74 Train Loss 0.042750902 Test MSE 0.005614005811603879 Test RE 0.17534698102931873\n",
      "75 Train Loss 0.042595234 Test MSE 0.005658573819883202 Test RE 0.17604162022470685\n",
      "76 Train Loss 0.041842368 Test MSE 0.005158682219799572 Test RE 0.16808588762009957\n",
      "77 Train Loss 0.03942179 Test MSE 0.005158936753439682 Test RE 0.16809003431718947\n",
      "78 Train Loss 0.037206825 Test MSE 0.004297284368548323 Test RE 0.15341185131433788\n",
      "79 Train Loss 0.03600348 Test MSE 0.0037090426762973953 Test RE 0.14252556759526264\n",
      "80 Train Loss 0.034592707 Test MSE 0.0034139999183239963 Test RE 0.13673938341069747\n",
      "81 Train Loss 0.034074374 Test MSE 0.0033490425713681286 Test RE 0.13543228256633816\n",
      "82 Train Loss 0.033839777 Test MSE 0.0033033366934860525 Test RE 0.13450495539904206\n",
      "83 Train Loss 0.033693537 Test MSE 0.003258783526601537 Test RE 0.13359482031282296\n",
      "84 Train Loss 0.033384338 Test MSE 0.0032352482071665066 Test RE 0.13311152734812123\n",
      "85 Train Loss 0.0325074 Test MSE 0.002867699852993908 Test RE 0.1253224022450476\n",
      "86 Train Loss 0.031957556 Test MSE 0.0028687769872583267 Test RE 0.12534593615498868\n",
      "87 Train Loss 0.030966194 Test MSE 0.002263120925062034 Test RE 0.11133090620530553\n",
      "88 Train Loss 0.03003141 Test MSE 0.0018728217503314471 Test RE 0.10127682695858078\n",
      "89 Train Loss 0.029660903 Test MSE 0.001694456355096242 Test RE 0.09633343747572458\n",
      "90 Train Loss 0.029073162 Test MSE 0.0016340458734765194 Test RE 0.09460062078401546\n",
      "91 Train Loss 0.028527912 Test MSE 0.0016824823697688361 Test RE 0.09599246079386592\n",
      "92 Train Loss 0.02759891 Test MSE 0.0016614324489418503 Test RE 0.09539007893930357\n",
      "93 Train Loss 0.027445996 Test MSE 0.0015794904498336146 Test RE 0.09300801297856323\n",
      "94 Train Loss 0.027349778 Test MSE 0.0014890709341026202 Test RE 0.09030661365875584\n",
      "95 Train Loss 0.02727108 Test MSE 0.0014792817701595982 Test RE 0.09000928600289232\n",
      "96 Train Loss 0.027270947 Test MSE 0.0014792818671209417 Test RE 0.09000928895277695\n",
      "97 Train Loss 0.027270941 Test MSE 0.0014792819114729115 Test RE 0.09000929030211048\n",
      "98 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "99 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "100 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "101 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "102 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "103 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "104 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "105 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "106 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "107 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "108 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "109 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "110 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "111 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "112 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "113 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "114 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "115 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "116 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "117 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "118 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "119 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "120 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "121 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "122 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "123 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "124 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "125 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "126 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "127 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "128 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "129 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "130 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "131 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "132 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "133 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "134 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "135 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "136 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "137 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "138 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "139 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "140 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "141 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "142 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "143 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "144 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "145 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "146 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "147 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "148 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "149 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "150 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "151 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "152 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "153 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "154 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "155 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "156 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "157 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "158 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "159 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "160 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "161 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "162 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "163 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "164 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "165 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "166 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "167 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "168 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "169 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "170 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "171 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "172 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "173 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "174 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "175 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "176 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "177 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "178 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "179 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "180 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "181 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "182 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "183 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "184 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "185 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "186 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "187 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "188 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "189 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "190 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "191 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "192 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "193 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "194 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "195 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "196 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "197 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "198 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "199 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "200 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "201 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "202 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "203 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "204 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "205 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "206 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "207 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "208 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "209 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "210 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "211 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "212 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "213 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "214 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "215 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "216 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "217 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "218 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "219 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "220 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "221 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "222 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "223 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "224 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "225 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "226 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "227 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "228 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "229 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "230 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "231 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "232 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "233 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "234 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "235 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "236 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "237 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "238 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "239 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "240 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "241 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "242 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "243 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "244 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "245 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "246 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "247 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "248 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "249 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "250 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "251 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "252 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "253 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "254 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "255 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "256 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "257 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "258 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "259 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "260 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "261 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "262 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "263 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "264 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "265 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "266 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "267 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "268 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "269 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "270 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "271 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "272 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "273 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "274 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "275 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "276 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "277 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "278 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "279 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "280 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "281 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "282 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "283 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "284 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "285 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "286 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "287 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "288 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "289 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "290 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "291 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "292 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "293 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "294 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "295 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "296 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "297 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "298 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "299 Train Loss 0.027270861 Test MSE 0.001479281897043738 Test RE 0.0900092898631273\n",
      "Training time: 247.97\n",
      "KG_rowdy_low\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 9625.803 Test MSE 1.2238241262351262 Test RE 2.5889363264151912\n",
      "1 Train Loss 2817.223 Test MSE 3.5863518774973433 Test RE 4.431883271362371\n",
      "2 Train Loss 1339.9955 Test MSE 2.603934232053785 Test RE 3.7763902660935234\n",
      "3 Train Loss 659.5148 Test MSE 1.5912081230504747 Test RE 2.9520612061532048\n",
      "4 Train Loss 338.04196 Test MSE 1.864818559739579 Test RE 3.195804143381944\n",
      "5 Train Loss 192.16034 Test MSE 2.3449741275751226 Test RE 3.5836937704915184\n",
      "6 Train Loss 108.16931 Test MSE 2.2389171789906954 Test RE 3.5017156598745656\n",
      "7 Train Loss 68.698586 Test MSE 2.336741466941326 Test RE 3.577397480336669\n",
      "8 Train Loss 50.09483 Test MSE 2.3566878953792063 Test RE 3.592633370983529\n",
      "9 Train Loss 38.770374 Test MSE 2.264907330732111 Test RE 3.5219816001821806\n",
      "10 Train Loss 30.615982 Test MSE 1.9048192645465336 Test RE 3.2298975822908416\n",
      "11 Train Loss 24.857315 Test MSE 2.0092076864740926 Test RE 3.3172200333564406\n",
      "12 Train Loss 20.53754 Test MSE 1.9371676553947148 Test RE 3.2572078184493614\n",
      "13 Train Loss 16.726685 Test MSE 1.8658466732843209 Test RE 3.1966849788851976\n",
      "14 Train Loss 14.096483 Test MSE 1.8044895631831528 Test RE 3.143685205907695\n",
      "15 Train Loss 12.918078 Test MSE 1.7234231759985228 Test RE 3.072259023678903\n",
      "16 Train Loss 10.17744 Test MSE 1.5244236109003735 Test RE 2.8894467644857724\n",
      "17 Train Loss 8.766326 Test MSE 1.2526444956552996 Test RE 2.6192429355190705\n",
      "18 Train Loss 7.7763557 Test MSE 1.2230199640871395 Test RE 2.5880856050297445\n",
      "19 Train Loss 6.109914 Test MSE 1.0872541051421072 Test RE 2.440211065091334\n",
      "20 Train Loss 4.9012475 Test MSE 0.8427517191615724 Test RE 2.148382903182081\n",
      "21 Train Loss 4.2035813 Test MSE 0.6393539148720191 Test RE 1.8712530810725376\n",
      "22 Train Loss 3.8155618 Test MSE 0.5846889617396275 Test RE 1.7894695449129594\n",
      "23 Train Loss 3.4531713 Test MSE 0.5495723181284067 Test RE 1.7348993674142248\n",
      "24 Train Loss 3.017298 Test MSE 0.47045749460684494 Test RE 1.6051737899812168\n",
      "25 Train Loss 2.7035391 Test MSE 0.4521924479628237 Test RE 1.573705695165469\n",
      "26 Train Loss 2.3332305 Test MSE 0.36712863797479045 Test RE 1.4179829066319374\n",
      "27 Train Loss 1.6186136 Test MSE 0.302355293879005 Test RE 1.2868284456901298\n",
      "28 Train Loss 1.3698424 Test MSE 0.25720714334614886 Test RE 1.1868706135767775\n",
      "29 Train Loss 1.1549766 Test MSE 0.2614724883867099 Test RE 1.1966712687511714\n",
      "30 Train Loss 0.9750099 Test MSE 0.24675254205181543 Test RE 1.1624992510922485\n",
      "31 Train Loss 0.9117781 Test MSE 0.2339853433791183 Test RE 1.1320254515070487\n",
      "32 Train Loss 0.8543777 Test MSE 0.2430957409899662 Test RE 1.153853147574687\n",
      "33 Train Loss 0.67039824 Test MSE 0.1609489868463788 Test RE 0.9388711360499206\n",
      "34 Train Loss 0.63984627 Test MSE 0.1573084665127346 Test RE 0.9281921962235941\n",
      "35 Train Loss 0.6264528 Test MSE 0.1551265231435363 Test RE 0.9217324842183612\n",
      "36 Train Loss 0.5026439 Test MSE 0.16796760844410727 Test RE 0.9591237238952387\n",
      "37 Train Loss 0.49014413 Test MSE 0.16403622616301908 Test RE 0.9478328331596242\n",
      "38 Train Loss 0.42038044 Test MSE 0.11155219448521402 Test RE 0.7816294660826539\n",
      "39 Train Loss 0.40849996 Test MSE 0.10071241062422394 Test RE 0.7426827890186287\n",
      "40 Train Loss 0.37649807 Test MSE 0.10354652702248326 Test RE 0.7530600912807061\n",
      "41 Train Loss 0.36098692 Test MSE 0.12069424659901 Test RE 0.8130273363668368\n",
      "42 Train Loss 0.31485894 Test MSE 0.0914833141353447 Test RE 0.7078362639069495\n",
      "43 Train Loss 0.31136763 Test MSE 0.08963060643814152 Test RE 0.7006321007983957\n",
      "44 Train Loss 0.28260663 Test MSE 0.06686194704098346 Test RE 0.6051337475369868\n",
      "45 Train Loss 0.2764487 Test MSE 0.07162778317142081 Test RE 0.6263291420545082\n",
      "46 Train Loss 0.2483152 Test MSE 0.04342303866787094 Test RE 0.4876654295845668\n",
      "47 Train Loss 0.23228762 Test MSE 0.0454756646679159 Test RE 0.49905842509128123\n",
      "48 Train Loss 0.23074876 Test MSE 0.041909897142864874 Test RE 0.479093370739392\n",
      "49 Train Loss 0.2222833 Test MSE 0.029069597425905227 Test RE 0.39900778822498284\n",
      "50 Train Loss 0.20979469 Test MSE 0.021634716418708188 Test RE 0.34422108368654364\n",
      "51 Train Loss 0.1928699 Test MSE 0.01839377827490817 Test RE 0.3173929950750758\n",
      "52 Train Loss 0.1820614 Test MSE 0.01870086349939088 Test RE 0.3200314757444683\n",
      "53 Train Loss 0.18082884 Test MSE 0.018257204746484304 Test RE 0.3162124805473037\n",
      "54 Train Loss 0.17936292 Test MSE 0.019608131702899545 Test RE 0.32770266383029933\n",
      "55 Train Loss 0.16842268 Test MSE 0.01691482111863078 Test RE 0.3043656018481335\n",
      "56 Train Loss 0.14604017 Test MSE 0.012300672523418196 Test RE 0.2595531430555192\n",
      "57 Train Loss 0.14442003 Test MSE 0.012188159839891523 Test RE 0.25836336636521673\n",
      "58 Train Loss 0.1428642 Test MSE 0.012795562536418328 Test RE 0.26472292687086946\n",
      "59 Train Loss 0.13757046 Test MSE 0.015007670485127005 Test RE 0.28669393584012814\n",
      "60 Train Loss 0.12855648 Test MSE 0.018463694713810708 Test RE 0.31799564288631904\n",
      "61 Train Loss 0.12285071 Test MSE 0.032662219977028735 Test RE 0.4229457972023143\n",
      "62 Train Loss 0.12065824 Test MSE 0.036774865641675404 Test RE 0.44878404782754255\n",
      "63 Train Loss 0.1166929 Test MSE 0.033093471815008955 Test RE 0.4257287989861235\n",
      "64 Train Loss 0.1142247 Test MSE 0.03162859407288025 Test RE 0.41619974306488716\n",
      "65 Train Loss 0.11228669 Test MSE 0.029722446832988 Test RE 0.40346339949570525\n",
      "66 Train Loss 0.11066597 Test MSE 0.024593517073737668 Test RE 0.36700517190940846\n",
      "67 Train Loss 0.110053025 Test MSE 0.027665769818532585 Test RE 0.38925414301626843\n",
      "68 Train Loss 0.107084416 Test MSE 0.028181787467646394 Test RE 0.3928675257559692\n",
      "69 Train Loss 0.098704785 Test MSE 0.020068227704442804 Test RE 0.33152506898331213\n",
      "70 Train Loss 0.09571811 Test MSE 0.02750959101567758 Test RE 0.38815387927881434\n",
      "71 Train Loss 0.09508792 Test MSE 0.03226717744287629 Test RE 0.4203802970301653\n",
      "72 Train Loss 0.094880946 Test MSE 0.03614809613477981 Test RE 0.4449432045510587\n",
      "73 Train Loss 0.08940337 Test MSE 0.030585210392229712 Test RE 0.40927724574080876\n",
      "74 Train Loss 0.076737456 Test MSE 0.0226595522935709 Test RE 0.35227962588738315\n",
      "75 Train Loss 0.074098796 Test MSE 0.02357770542534324 Test RE 0.3593458492481303\n",
      "76 Train Loss 0.073843 Test MSE 0.023788065221476286 Test RE 0.3609453276289455\n",
      "77 Train Loss 0.07372732 Test MSE 0.02278127561164632 Test RE 0.3532245521517007\n",
      "78 Train Loss 0.07163562 Test MSE 0.023601451658503424 Test RE 0.35952676089163027\n",
      "79 Train Loss 0.06678547 Test MSE 0.02217477031817753 Test RE 0.34849088965514563\n",
      "80 Train Loss 0.05985192 Test MSE 0.017539453172131743 Test RE 0.3099344762638414\n",
      "81 Train Loss 0.058286365 Test MSE 0.017685380145258982 Test RE 0.3112211217442483\n",
      "82 Train Loss 0.057769325 Test MSE 0.017091383827826837 Test RE 0.30595001423775164\n",
      "83 Train Loss 0.057455014 Test MSE 0.019770804902118275 Test RE 0.329059201344271\n",
      "84 Train Loss 0.057160057 Test MSE 0.020195035312220375 Test RE 0.3325708439226743\n",
      "85 Train Loss 0.05696152 Test MSE 0.0209648674156412 Test RE 0.3388503387554741\n",
      "86 Train Loss 0.056807898 Test MSE 0.020101680599749233 Test RE 0.3318012731304075\n",
      "87 Train Loss 0.05671053 Test MSE 0.02065207460851299 Test RE 0.33631303999191897\n",
      "88 Train Loss 0.056562506 Test MSE 0.021715345840958977 Test RE 0.3448619179721554\n",
      "89 Train Loss 0.056205735 Test MSE 0.020860177782318783 Test RE 0.33800324266861465\n",
      "90 Train Loss 0.05604458 Test MSE 0.02151811178755458 Test RE 0.34329220621488504\n",
      "91 Train Loss 0.05499942 Test MSE 0.024477295813936498 Test RE 0.3661369692270575\n",
      "92 Train Loss 0.053624213 Test MSE 0.020013601011146045 Test RE 0.33107354781541143\n",
      "93 Train Loss 0.05231777 Test MSE 0.017424519833279445 Test RE 0.308917330749982\n",
      "94 Train Loss 0.050853405 Test MSE 0.013273925916006471 Test RE 0.26962586957405515\n",
      "95 Train Loss 0.04874931 Test MSE 0.011650459107511892 Test RE 0.25260002195803666\n",
      "96 Train Loss 0.04724635 Test MSE 0.010603904415908654 Test RE 0.2409876386465337\n",
      "97 Train Loss 0.046362635 Test MSE 0.011148084598821112 Test RE 0.24709388168019214\n",
      "98 Train Loss 0.04582325 Test MSE 0.010430665531576776 Test RE 0.23901099168982534\n",
      "99 Train Loss 0.04549237 Test MSE 0.010826846108180502 Test RE 0.2435077825740306\n",
      "100 Train Loss 0.045367043 Test MSE 0.011135797308302156 Test RE 0.24695767212165046\n",
      "101 Train Loss 0.04527512 Test MSE 0.011098371550261594 Test RE 0.24654232885359947\n",
      "102 Train Loss 0.045153882 Test MSE 0.010737020519213571 Test RE 0.2424955400845408\n",
      "103 Train Loss 0.04489648 Test MSE 0.010720990652580076 Test RE 0.24231445525356862\n",
      "104 Train Loss 0.044756677 Test MSE 0.010238207418187427 Test RE 0.23679570774685785\n",
      "105 Train Loss 0.044611968 Test MSE 0.010063934292295589 Test RE 0.23477170843697015\n",
      "106 Train Loss 0.044431034 Test MSE 0.009987735704597869 Test RE 0.23388123842621794\n",
      "107 Train Loss 0.04401231 Test MSE 0.010293967732822793 Test RE 0.23743966197942587\n",
      "108 Train Loss 0.04184879 Test MSE 0.00807202085460736 Test RE 0.2102582288203213\n",
      "109 Train Loss 0.039723624 Test MSE 0.006240387555343209 Test RE 0.1848705124636496\n",
      "110 Train Loss 0.038581334 Test MSE 0.007173579613603144 Test RE 0.19821194598095385\n",
      "111 Train Loss 0.038212318 Test MSE 0.007205169651995618 Test RE 0.19864789602577412\n",
      "112 Train Loss 0.03742295 Test MSE 0.00672477484146576 Test RE 0.1919113842448928\n",
      "113 Train Loss 0.037005108 Test MSE 0.006174787528706873 Test RE 0.18389624999343784\n",
      "114 Train Loss 0.03676142 Test MSE 0.005968979269820024 Test RE 0.18080560907923837\n",
      "115 Train Loss 0.03665086 Test MSE 0.006171017080591997 Test RE 0.18384009606548501\n",
      "116 Train Loss 0.03647157 Test MSE 0.00562500683634971 Test RE 0.17551869909187984\n",
      "117 Train Loss 0.03632998 Test MSE 0.004818867211765858 Test RE 0.16245547173215097\n",
      "118 Train Loss 0.03620429 Test MSE 0.0048460810177103795 Test RE 0.1629135469683254\n",
      "119 Train Loss 0.036062922 Test MSE 0.004775565190532108 Test RE 0.16172391755078797\n",
      "120 Train Loss 0.035918385 Test MSE 0.004628347312281807 Test RE 0.15921164683316888\n",
      "121 Train Loss 0.03569945 Test MSE 0.004695830933309437 Test RE 0.16036813928927254\n",
      "122 Train Loss 0.035463348 Test MSE 0.0046427846896657365 Test RE 0.15945977091919303\n",
      "123 Train Loss 0.03533727 Test MSE 0.004456896432795824 Test RE 0.15623492928689472\n",
      "124 Train Loss 0.034271296 Test MSE 0.004184814526715247 Test RE 0.1513909698989492\n",
      "125 Train Loss 0.03377288 Test MSE 0.004281569270840677 Test RE 0.1531310820884107\n",
      "126 Train Loss 0.03354326 Test MSE 0.004573679554089556 Test RE 0.1582685890612441\n",
      "127 Train Loss 0.03318562 Test MSE 0.004208546721472722 Test RE 0.15181963412993224\n",
      "128 Train Loss 0.032727234 Test MSE 0.0029064598666216045 Test RE 0.12616649240425573\n",
      "129 Train Loss 0.032441556 Test MSE 0.002854632424216366 Test RE 0.1250365439791158\n",
      "130 Train Loss 0.031761866 Test MSE 0.0022316367569321484 Test RE 0.11055378526594165\n",
      "131 Train Loss 0.030596362 Test MSE 0.0026351430019591927 Test RE 0.12013345242549829\n",
      "132 Train Loss 0.030328663 Test MSE 0.0026219878571517676 Test RE 0.1198332124987784\n",
      "133 Train Loss 0.029701782 Test MSE 0.002723520609208472 Test RE 0.12213136106755598\n",
      "134 Train Loss 0.028380584 Test MSE 0.003260975532463693 Test RE 0.13363974373270746\n",
      "135 Train Loss 0.027927512 Test MSE 0.0024767509241775602 Test RE 0.11646703808132824\n",
      "136 Train Loss 0.027670933 Test MSE 0.0027039824532050544 Test RE 0.12169249590858179\n",
      "137 Train Loss 0.026998043 Test MSE 0.002177677454202595 Test RE 0.10920905307453921\n",
      "138 Train Loss 0.026486915 Test MSE 0.002392378752163724 Test RE 0.11446608582148622\n",
      "139 Train Loss 0.025749432 Test MSE 0.0027603707036675507 Test RE 0.12295482295539878\n",
      "140 Train Loss 0.025032785 Test MSE 0.003519049002245111 Test RE 0.13882718617216916\n",
      "141 Train Loss 0.024705794 Test MSE 0.003975914777529809 Test RE 0.14756399265638392\n",
      "142 Train Loss 0.024302755 Test MSE 0.004771661985801734 Test RE 0.16165781327093318\n",
      "143 Train Loss 0.023578081 Test MSE 0.006137700158833902 Test RE 0.18334315402904758\n",
      "144 Train Loss 0.02331772 Test MSE 0.006156313949107634 Test RE 0.1836209557637878\n",
      "145 Train Loss 0.023035098 Test MSE 0.006052233950166726 Test RE 0.1820621712403237\n",
      "146 Train Loss 0.022665093 Test MSE 0.005260649821707692 Test RE 0.16973896937310845\n",
      "147 Train Loss 0.022369027 Test MSE 0.005485222818765692 Test RE 0.17332411887186622\n",
      "148 Train Loss 0.021756295 Test MSE 0.006755651055425254 Test RE 0.1923514518360489\n",
      "149 Train Loss 0.020645063 Test MSE 0.005854537791637762 Test RE 0.17906395432420938\n",
      "150 Train Loss 0.020408707 Test MSE 0.0055314429694030935 Test RE 0.17405282780096495\n",
      "151 Train Loss 0.02018131 Test MSE 0.005128387975215367 Test RE 0.16759162062323116\n",
      "152 Train Loss 0.020042423 Test MSE 0.005296297701279606 Test RE 0.17031310170666164\n",
      "153 Train Loss 0.01979271 Test MSE 0.005680674786140868 Test RE 0.1763850723399494\n",
      "154 Train Loss 0.019724494 Test MSE 0.005905304958402993 Test RE 0.17983864806575087\n",
      "155 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "156 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "157 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "158 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "159 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "160 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "161 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "162 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "163 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "164 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "165 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "166 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "167 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "168 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "169 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "170 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "171 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "172 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "173 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "174 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "175 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "176 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "177 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "178 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "179 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "180 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "181 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "182 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "183 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "184 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "185 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "186 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "187 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "188 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "189 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "190 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "191 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "192 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "193 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "194 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "195 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "196 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "197 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "198 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "199 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "200 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "201 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "202 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "203 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "204 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "205 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "206 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "207 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "208 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "209 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "210 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "211 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "212 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "213 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "214 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "215 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "216 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "217 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "218 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "219 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "220 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "221 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "222 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "223 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "224 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "225 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "226 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "227 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "228 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "229 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "230 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "231 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "232 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "233 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "234 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "235 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "236 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "237 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "238 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "239 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "240 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "241 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "242 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "243 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "244 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "245 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "246 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "247 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "248 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "249 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "250 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "251 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "252 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "253 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "254 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "255 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "256 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "257 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "258 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "259 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "260 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "261 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "262 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "263 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "264 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "265 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "266 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "267 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "268 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "269 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "270 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "271 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "272 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "273 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "274 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "275 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "276 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "277 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "278 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "279 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "280 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "281 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "282 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "283 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "284 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "285 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "286 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "287 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "288 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "289 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "290 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "291 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "292 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "293 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "294 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "295 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "296 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "297 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "298 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "299 Train Loss 0.019663319 Test MSE 0.005653414272966854 Test RE 0.1759613436240677\n",
      "Training time: 312.23\n",
      "KG_rowdy_low\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10944.42 Test MSE 0.8003747400924394 Test RE 2.093671546609215\n",
      "1 Train Loss 7414.2275 Test MSE 0.499791784814118 Test RE 1.6544605676790356\n",
      "2 Train Loss 2323.0168 Test MSE 0.4495576109887947 Test RE 1.5691141592140965\n",
      "3 Train Loss 271.5308 Test MSE 1.4059530198515315 Test RE 2.774899562758693\n",
      "4 Train Loss 56.295452 Test MSE 1.5509692326871434 Test RE 2.914495944492175\n",
      "5 Train Loss 25.28034 Test MSE 0.9970808833602156 Test RE 2.336829674450097\n",
      "6 Train Loss 13.619981 Test MSE 1.3985570957695879 Test RE 2.767591349987823\n",
      "7 Train Loss 9.353199 Test MSE 0.9263047766432959 Test RE 2.252365237020421\n",
      "8 Train Loss 6.5239744 Test MSE 0.57795554447025 Test RE 1.7791357284543334\n",
      "9 Train Loss 4.57887 Test MSE 0.31933590441878934 Test RE 1.3224697278618271\n",
      "10 Train Loss 3.2865605 Test MSE 0.28534568687688744 Test RE 1.2501079591346975\n",
      "11 Train Loss 2.3394861 Test MSE 0.34009517019346797 Test RE 1.3647782612187704\n",
      "12 Train Loss 1.9251474 Test MSE 0.3633088757903356 Test RE 1.4105869706909675\n",
      "13 Train Loss 1.603905 Test MSE 0.3852491496383882 Test RE 1.4525554106863745\n",
      "14 Train Loss 1.326591 Test MSE 0.3383011242676096 Test RE 1.361173810693931\n",
      "15 Train Loss 1.0854613 Test MSE 0.31873745974689555 Test RE 1.3212299735975062\n",
      "16 Train Loss 0.9499948 Test MSE 0.2836191136655625 Test RE 1.2463201362391385\n",
      "17 Train Loss 0.8167173 Test MSE 0.2809282818124073 Test RE 1.2403938250080213\n",
      "18 Train Loss 0.70498365 Test MSE 0.23539271623780322 Test RE 1.1354247956425785\n",
      "19 Train Loss 0.63725793 Test MSE 0.21038227062853374 Test RE 1.073411967852917\n",
      "20 Train Loss 0.555703 Test MSE 0.20368720728207604 Test RE 1.05619410908284\n",
      "21 Train Loss 0.5294923 Test MSE 0.19748015134667865 Test RE 1.0399766530904526\n",
      "22 Train Loss 0.48669788 Test MSE 0.19310151541489587 Test RE 1.0283825649258223\n",
      "23 Train Loss 0.46400666 Test MSE 0.18926945380869734 Test RE 1.0181274071757065\n",
      "24 Train Loss 0.4160131 Test MSE 0.17823683155306738 Test RE 0.9880082947261697\n",
      "25 Train Loss 0.40190217 Test MSE 0.18001142294290484 Test RE 0.9929145995476387\n",
      "26 Train Loss 0.3620077 Test MSE 0.17561539207585386 Test RE 0.9807157564632947\n",
      "27 Train Loss 0.33654207 Test MSE 0.1616616318290694 Test RE 0.9409473926880736\n",
      "28 Train Loss 0.28301337 Test MSE 0.1388221407355043 Test RE 0.8719492985991859\n",
      "29 Train Loss 0.2659654 Test MSE 0.12304278394473783 Test RE 0.8208994004211968\n",
      "30 Train Loss 0.21462865 Test MSE 0.10404141094365528 Test RE 0.7548575108305569\n",
      "31 Train Loss 0.20804399 Test MSE 0.1076306254414627 Test RE 0.7677676268724486\n",
      "32 Train Loss 0.19467798 Test MSE 0.10135439599706687 Test RE 0.7450461227751426\n",
      "33 Train Loss 0.18700744 Test MSE 0.08846844723603992 Test RE 0.6960750486628428\n",
      "34 Train Loss 0.16809072 Test MSE 0.07912356850265086 Test RE 0.6582862699284009\n",
      "35 Train Loss 0.15745232 Test MSE 0.07503145894470253 Test RE 0.6410376824300621\n",
      "36 Train Loss 0.13870353 Test MSE 0.05589154087992567 Test RE 0.5532671761869853\n",
      "37 Train Loss 0.12505567 Test MSE 0.04400179681245897 Test RE 0.49090456384165737\n",
      "38 Train Loss 0.11234399 Test MSE 0.028997651299476297 Test RE 0.39851371793241197\n",
      "39 Train Loss 0.09370638 Test MSE 0.024904357411699357 Test RE 0.36931719999056056\n",
      "40 Train Loss 0.08998695 Test MSE 0.031375944399524375 Test RE 0.41453410528483065\n",
      "41 Train Loss 0.0837519 Test MSE 0.0255399380520759 Test RE 0.3740001563938177\n",
      "42 Train Loss 0.0737451 Test MSE 0.012857908656934577 Test RE 0.2653670717409654\n",
      "43 Train Loss 0.07241648 Test MSE 0.010634245308400634 Test RE 0.24133216067021412\n",
      "44 Train Loss 0.06811656 Test MSE 0.00647115492849484 Test RE 0.18825770621695984\n",
      "45 Train Loss 0.05878655 Test MSE 0.004479794138100268 Test RE 0.15663575061599053\n",
      "46 Train Loss 0.057562143 Test MSE 0.005556895633027895 Test RE 0.17445281597435638\n",
      "47 Train Loss 0.05518954 Test MSE 0.004422299328075732 Test RE 0.15562735325803362\n",
      "48 Train Loss 0.052871138 Test MSE 0.0038373368701579364 Test RE 0.1449695621740589\n",
      "49 Train Loss 0.052204885 Test MSE 0.0037686141157669216 Test RE 0.1436655696708196\n",
      "50 Train Loss 0.051558077 Test MSE 0.00269489244002966 Test RE 0.12148777608392257\n",
      "51 Train Loss 0.04996833 Test MSE 0.002318587193567285 Test RE 0.1126869385949952\n",
      "52 Train Loss 0.048871692 Test MSE 0.0018858652210920763 Test RE 0.10162889176170459\n",
      "53 Train Loss 0.047917087 Test MSE 0.0015694994546837026 Test RE 0.09271338734687856\n",
      "54 Train Loss 0.047621567 Test MSE 0.0014571309328022478 Test RE 0.08933284245822395\n",
      "55 Train Loss 0.046277322 Test MSE 0.0015600351523250109 Test RE 0.09243342727918354\n",
      "56 Train Loss 0.044357143 Test MSE 0.0023918181484712774 Test RE 0.11445267367468814\n",
      "57 Train Loss 0.042121526 Test MSE 0.0016556029622585332 Test RE 0.09522258376678347\n",
      "58 Train Loss 0.041328944 Test MSE 0.0019094488452207653 Test RE 0.10226237575669377\n",
      "59 Train Loss 0.04096341 Test MSE 0.002078568149186853 Test RE 0.10669498288754233\n",
      "60 Train Loss 0.040542345 Test MSE 0.0019003701775906282 Test RE 0.10201897769332618\n",
      "61 Train Loss 0.038556956 Test MSE 0.0017289361922627347 Test RE 0.09730862745304654\n",
      "62 Train Loss 0.03708094 Test MSE 0.0018959769501326626 Test RE 0.10190098702255686\n",
      "63 Train Loss 0.036938183 Test MSE 0.001941304911450057 Test RE 0.10311188841249108\n",
      "64 Train Loss 0.036332853 Test MSE 0.001988166822009215 Test RE 0.10434899606613282\n",
      "65 Train Loss 0.034304418 Test MSE 0.0025625437081714763 Test RE 0.1184670311385284\n",
      "66 Train Loss 0.03279986 Test MSE 0.003094987377356276 Test RE 0.13019410039827153\n",
      "67 Train Loss 0.03253081 Test MSE 0.0030714074302034813 Test RE 0.129697193731166\n",
      "68 Train Loss 0.032220416 Test MSE 0.0025219028640741624 Test RE 0.11752385846346002\n",
      "69 Train Loss 0.031207716 Test MSE 0.0018743132330734384 Test RE 0.10131714648647758\n",
      "70 Train Loss 0.030649642 Test MSE 0.0018713091142650471 Test RE 0.10123591918755141\n",
      "71 Train Loss 0.02942527 Test MSE 0.0020090599955034983 Test RE 0.10489585254750221\n",
      "72 Train Loss 0.027799558 Test MSE 0.0024648949954654623 Test RE 0.11618794636529917\n",
      "73 Train Loss 0.026746567 Test MSE 0.0023676371742226573 Test RE 0.11387265305249496\n",
      "74 Train Loss 0.02644177 Test MSE 0.0025997648535353857 Test RE 0.11932430077480201\n",
      "75 Train Loss 0.026217364 Test MSE 0.00281208159401173 Test RE 0.12410115485221146\n",
      "76 Train Loss 0.026096648 Test MSE 0.00283316688432045 Test RE 0.12456554786552652\n",
      "77 Train Loss 0.025676293 Test MSE 0.0030844099565841055 Test RE 0.12997143448120255\n",
      "78 Train Loss 0.024688935 Test MSE 0.003138523935984696 Test RE 0.13110660956386702\n",
      "79 Train Loss 0.024124425 Test MSE 0.0037016530366843596 Test RE 0.14238351777509287\n",
      "80 Train Loss 0.023849588 Test MSE 0.0032472100083117132 Test RE 0.13335737935987116\n",
      "81 Train Loss 0.023754539 Test MSE 0.0031165939552904036 Test RE 0.13064776241955423\n",
      "82 Train Loss 0.023591563 Test MSE 0.0031370566529461616 Test RE 0.13107595932871022\n",
      "83 Train Loss 0.023437176 Test MSE 0.003082822105280848 Test RE 0.12993797558925488\n",
      "84 Train Loss 0.023358889 Test MSE 0.0030059445039182604 Test RE 0.1283075886710002\n",
      "85 Train Loss 0.023261782 Test MSE 0.0030743495860989455 Test RE 0.12975929848814807\n",
      "86 Train Loss 0.023081932 Test MSE 0.0033731415860570758 Test RE 0.1359186804171612\n",
      "87 Train Loss 0.0228003 Test MSE 0.0034158302443265355 Test RE 0.1367760331059323\n",
      "88 Train Loss 0.021989651 Test MSE 0.0031766358201193315 Test RE 0.13190023776561147\n",
      "89 Train Loss 0.02110197 Test MSE 0.0034496136545783284 Test RE 0.1374507434315017\n",
      "90 Train Loss 0.020198662 Test MSE 0.0037208694216482965 Test RE 0.14275261702305758\n",
      "91 Train Loss 0.019839559 Test MSE 0.003361594782089684 Test RE 0.13568584528092145\n",
      "92 Train Loss 0.019683566 Test MSE 0.0032466711709081375 Test RE 0.13334631433403255\n",
      "93 Train Loss 0.019564418 Test MSE 0.003263536466197209 Test RE 0.13369220891647102\n",
      "94 Train Loss 0.019178376 Test MSE 0.0032705464034026477 Test RE 0.13383571447958886\n",
      "95 Train Loss 0.018273758 Test MSE 0.0033575627476487083 Test RE 0.13560444728595236\n",
      "96 Train Loss 0.017160056 Test MSE 0.002981928067260235 Test RE 0.12779399455932275\n",
      "97 Train Loss 0.016435526 Test MSE 0.002649660511750238 Test RE 0.12046391705370393\n",
      "98 Train Loss 0.015192521 Test MSE 0.002742931817541006 Test RE 0.12256581860376967\n",
      "99 Train Loss 0.014374783 Test MSE 0.0030450058566317143 Test RE 0.12913855726935541\n",
      "100 Train Loss 0.014100506 Test MSE 0.0027785348830429903 Test RE 0.12335870193543252\n",
      "101 Train Loss 0.01398531 Test MSE 0.0026096870535637427 Test RE 0.11955178905352253\n",
      "102 Train Loss 0.013860949 Test MSE 0.0024793402718838484 Test RE 0.11652790307894165\n",
      "103 Train Loss 0.013694471 Test MSE 0.002490091614632562 Test RE 0.11678028396246098\n",
      "104 Train Loss 0.013621091 Test MSE 0.0024500193834283653 Test RE 0.11583681936325017\n",
      "105 Train Loss 0.013516412 Test MSE 0.0024410239852268034 Test RE 0.11562397277847333\n",
      "106 Train Loss 0.013365189 Test MSE 0.0024688785391960926 Test RE 0.11628179476797414\n",
      "107 Train Loss 0.013202273 Test MSE 0.0026569394200159063 Test RE 0.12062926737414867\n",
      "108 Train Loss 0.013007462 Test MSE 0.0027113385865759044 Test RE 0.12185791455275753\n",
      "109 Train Loss 0.012654952 Test MSE 0.0023732227711165997 Test RE 0.11400689491491107\n",
      "110 Train Loss 0.012059791 Test MSE 0.001842849389415258 Test RE 0.10046314868922865\n",
      "111 Train Loss 0.011477347 Test MSE 0.0017398375892141 Test RE 0.09761492362371105\n",
      "112 Train Loss 0.011223976 Test MSE 0.0016145583168016109 Test RE 0.0940348274702388\n",
      "113 Train Loss 0.011037128 Test MSE 0.001521257806549212 Test RE 0.09127740275354623\n",
      "114 Train Loss 0.0108358525 Test MSE 0.001262705697863802 Test RE 0.08315970386780552\n",
      "115 Train Loss 0.010667456 Test MSE 0.0012844996748097187 Test RE 0.08387429125516924\n",
      "116 Train Loss 0.010479171 Test MSE 0.001102470329495798 Test RE 0.07770434771989726\n",
      "117 Train Loss 0.010348847 Test MSE 0.00112721887619376 Test RE 0.0785716711726515\n",
      "118 Train Loss 0.010146147 Test MSE 0.0010917397619086473 Test RE 0.0773252669492676\n",
      "119 Train Loss 0.009968826 Test MSE 0.0008756989189200575 Test RE 0.06925310832005599\n",
      "120 Train Loss 0.009875894 Test MSE 0.0009004217409053866 Test RE 0.0702238846845923\n",
      "121 Train Loss 0.009792627 Test MSE 0.0008557447880449854 Test RE 0.06845954295496286\n",
      "122 Train Loss 0.009755369 Test MSE 0.0008044355943094981 Test RE 0.06637545382107778\n",
      "123 Train Loss 0.00973288 Test MSE 0.0008000193681095203 Test RE 0.0661930076211858\n",
      "124 Train Loss 0.009707609 Test MSE 0.0007956178475053333 Test RE 0.06601066720572417\n",
      "125 Train Loss 0.009688832 Test MSE 0.0007678098219378126 Test RE 0.06484682179151952\n",
      "126 Train Loss 0.009660341 Test MSE 0.0007997372857062285 Test RE 0.06618133694819578\n",
      "127 Train Loss 0.009589981 Test MSE 0.0007931833505618337 Test RE 0.06590959739602678\n",
      "128 Train Loss 0.00951928 Test MSE 0.0008149346760173385 Test RE 0.06680719888676503\n",
      "129 Train Loss 0.009446842 Test MSE 0.000741203861615525 Test RE 0.06371338816218644\n",
      "130 Train Loss 0.009199148 Test MSE 0.0006344325301216957 Test RE 0.05894603336230326\n",
      "131 Train Loss 0.008756513 Test MSE 0.0005394622434524315 Test RE 0.054355361834446114\n",
      "132 Train Loss 0.008151235 Test MSE 0.0003827745686995583 Test RE 0.045786073588420684\n",
      "133 Train Loss 0.007878906 Test MSE 0.0003295622730205245 Test RE 0.042484509529464524\n",
      "134 Train Loss 0.0076959203 Test MSE 0.000282118923432621 Test RE 0.039307730789133265\n",
      "135 Train Loss 0.007639904 Test MSE 0.0002735360215081412 Test RE 0.03870518316555203\n",
      "136 Train Loss 0.00761526 Test MSE 0.0002738956625471223 Test RE 0.038730619303311585\n",
      "137 Train Loss 0.0075523877 Test MSE 0.00028201897826917875 Test RE 0.039300767474627296\n",
      "138 Train Loss 0.0074983058 Test MSE 0.00028050479927604785 Test RE 0.039195121237280146\n",
      "139 Train Loss 0.0074656797 Test MSE 0.00028676704790328177 Test RE 0.039630220325311875\n",
      "140 Train Loss 0.0074451976 Test MSE 0.00029540690921258533 Test RE 0.040222789707307645\n",
      "141 Train Loss 0.0074069393 Test MSE 0.0003024633641527677 Test RE 0.04070036023937123\n",
      "142 Train Loss 0.0073379725 Test MSE 0.00030221193818302615 Test RE 0.04068344041330444\n",
      "143 Train Loss 0.00729281 Test MSE 0.0003252769154825608 Test RE 0.04220738885819493\n",
      "144 Train Loss 0.007252206 Test MSE 0.0003321130777693035 Test RE 0.04264860719836003\n",
      "145 Train Loss 0.007204164 Test MSE 0.0003311072530958229 Test RE 0.042583976275010184\n",
      "146 Train Loss 0.0070973597 Test MSE 0.0003196194277757685 Test RE 0.041838725728163835\n",
      "147 Train Loss 0.007024124 Test MSE 0.0002927719130011755 Test RE 0.040042996508544\n",
      "148 Train Loss 0.006861984 Test MSE 0.0002970189544783549 Test RE 0.040332388935416484\n",
      "149 Train Loss 0.0067619737 Test MSE 0.0002655117307713422 Test RE 0.03813324124046141\n",
      "150 Train Loss 0.0066120615 Test MSE 0.00023362756498083106 Test RE 0.03577040894869798\n",
      "151 Train Loss 0.0064045675 Test MSE 0.000269877171121119 Test RE 0.038445449114222874\n",
      "152 Train Loss 0.0062771793 Test MSE 0.00025584080084917595 Test RE 0.0374323219040054\n",
      "153 Train Loss 0.0061878213 Test MSE 0.00026902419604159705 Test RE 0.038384645596933484\n",
      "154 Train Loss 0.0061326977 Test MSE 0.0002625398681276363 Test RE 0.03791922874175069\n",
      "155 Train Loss 0.0060512163 Test MSE 0.0002774502003511301 Test RE 0.03898112649599308\n",
      "156 Train Loss 0.0060196067 Test MSE 0.00026407752185836824 Test RE 0.038030110033605374\n",
      "157 Train Loss 0.00598748 Test MSE 0.00027185166745471155 Test RE 0.03858583160419166\n",
      "158 Train Loss 0.005940949 Test MSE 0.00028654321919168926 Test RE 0.03961475112690897\n",
      "159 Train Loss 0.0059041954 Test MSE 0.00031616393901622703 Test RE 0.041611946447449935\n",
      "160 Train Loss 0.005875769 Test MSE 0.0002906502884291966 Test RE 0.039897643292629834\n",
      "161 Train Loss 0.00584252 Test MSE 0.0002797024332427257 Test RE 0.03913902352415608\n",
      "162 Train Loss 0.0058133802 Test MSE 0.0002599841218826108 Test RE 0.03773421125422199\n",
      "163 Train Loss 0.0057744374 Test MSE 0.0002396403468517244 Test RE 0.03622778934706209\n",
      "164 Train Loss 0.0056909905 Test MSE 0.00023117204126369526 Test RE 0.0355819313939106\n",
      "165 Train Loss 0.005665835 Test MSE 0.00022837873798972638 Test RE 0.03536630584951857\n",
      "166 Train Loss 0.005650577 Test MSE 0.00023744711309146822 Test RE 0.036061626496088024\n",
      "167 Train Loss 0.005648697 Test MSE 0.00023611137449652264 Test RE 0.035960052635248034\n",
      "168 Train Loss 0.00563252 Test MSE 0.00024406104553417144 Test RE 0.036560413396252586\n",
      "169 Train Loss 0.0056058727 Test MSE 0.00024755612394557344 Test RE 0.036821264700533644\n",
      "170 Train Loss 0.0055785906 Test MSE 0.00024923893867259645 Test RE 0.03694620287549208\n",
      "171 Train Loss 0.0055634747 Test MSE 0.000263212198926441 Test RE 0.037967750828157384\n",
      "172 Train Loss 0.005528499 Test MSE 0.00024069386468330116 Test RE 0.03630733514781761\n",
      "173 Train Loss 0.0054678484 Test MSE 0.00023150026475745857 Test RE 0.03560718246502983\n",
      "174 Train Loss 0.005390667 Test MSE 0.00022545277371342846 Test RE 0.03513902082672321\n",
      "175 Train Loss 0.0053049284 Test MSE 0.0002557565389974828 Test RE 0.03742615717872837\n",
      "176 Train Loss 0.005204288 Test MSE 0.00023862978748147342 Test RE 0.036151322653628595\n",
      "177 Train Loss 0.005145844 Test MSE 0.00023930935462272097 Test RE 0.03620276171646425\n",
      "178 Train Loss 0.005110229 Test MSE 0.0002403984689160336 Test RE 0.03628504890097125\n",
      "179 Train Loss 0.005071163 Test MSE 0.00023700314539137923 Test RE 0.03602789753649821\n",
      "180 Train Loss 0.005009607 Test MSE 0.00023467274953764268 Test RE 0.035850333074744584\n",
      "181 Train Loss 0.004968469 Test MSE 0.00024734464052893917 Test RE 0.03680553341797107\n",
      "182 Train Loss 0.0049204812 Test MSE 0.00025714879034590657 Test RE 0.03752788653762864\n",
      "183 Train Loss 0.0048779333 Test MSE 0.0002522200586148337 Test RE 0.037166500858373266\n",
      "184 Train Loss 0.004850174 Test MSE 0.00025567822354665516 Test RE 0.037420426590572806\n",
      "185 Train Loss 0.004822505 Test MSE 0.0002658914859116902 Test RE 0.0381605020322422\n",
      "186 Train Loss 0.0048079914 Test MSE 0.00026962455798736524 Test RE 0.038427451854500555\n",
      "187 Train Loss 0.0047976165 Test MSE 0.00028024329177333234 Test RE 0.0391768466688496\n",
      "188 Train Loss 0.0047872635 Test MSE 0.0002803072881852971 Test RE 0.03918131962961967\n",
      "189 Train Loss 0.0047790725 Test MSE 0.0002874272481334259 Test RE 0.03967581280070513\n",
      "190 Train Loss 0.0047716917 Test MSE 0.00028793541050324556 Test RE 0.03971087010950096\n",
      "191 Train Loss 0.0047646095 Test MSE 0.0003039034339468675 Test RE 0.040797135200775346\n",
      "192 Train Loss 0.004739597 Test MSE 0.00028723248805089804 Test RE 0.039662368400315576\n",
      "193 Train Loss 0.004714203 Test MSE 0.0002947495413619604 Test RE 0.040178010972893696\n",
      "194 Train Loss 0.0046959775 Test MSE 0.00029404124797338476 Test RE 0.04012970735847364\n",
      "195 Train Loss 0.0046705045 Test MSE 0.0003034028383464024 Test RE 0.04076352043819019\n",
      "196 Train Loss 0.004630173 Test MSE 0.0003326999936582835 Test RE 0.04268627523641199\n",
      "197 Train Loss 0.004568411 Test MSE 0.0003112527373276913 Test RE 0.0412874873667502\n",
      "198 Train Loss 0.0045172567 Test MSE 0.0002943695498958671 Test RE 0.0401521038500972\n",
      "199 Train Loss 0.004478893 Test MSE 0.00030746797597437924 Test RE 0.041035696443490706\n",
      "200 Train Loss 0.004355548 Test MSE 0.0002928731456399298 Test RE 0.04004991880482537\n",
      "201 Train Loss 0.004237796 Test MSE 0.0002546219695340403 Test RE 0.03734305124453694\n",
      "202 Train Loss 0.0041228496 Test MSE 0.0002044691245106476 Test RE 0.033463836738206604\n",
      "203 Train Loss 0.00407205 Test MSE 0.0002064196134268162 Test RE 0.0336230684109158\n",
      "204 Train Loss 0.0040277657 Test MSE 0.00021215941898279427 Test RE 0.03408733295753814\n",
      "205 Train Loss 0.0039983112 Test MSE 0.0001949212877584378 Test RE 0.03267318712095467\n",
      "206 Train Loss 0.003947888 Test MSE 0.0001854459817012922 Test RE 0.031869157147101035\n",
      "207 Train Loss 0.0039195023 Test MSE 0.0001909400977384431 Test RE 0.03233779728853053\n",
      "208 Train Loss 0.0038194729 Test MSE 0.0001699256453516976 Test RE 0.030506425821483538\n",
      "209 Train Loss 0.0037551273 Test MSE 0.00018982237849102961 Test RE 0.0322430093687349\n",
      "210 Train Loss 0.0037096003 Test MSE 0.00019598639164521042 Test RE 0.03276233317772218\n",
      "211 Train Loss 0.0036933795 Test MSE 0.00021474703026755433 Test RE 0.03429457675393223\n",
      "212 Train Loss 0.0036561373 Test MSE 0.00021884106187178976 Test RE 0.03461993680235358\n",
      "213 Train Loss 0.0036239405 Test MSE 0.00022594655416503378 Test RE 0.03517748003766399\n",
      "214 Train Loss 0.0036064642 Test MSE 0.0002200206281840002 Test RE 0.03471311317161716\n",
      "215 Train Loss 0.0036043052 Test MSE 0.0002200932130281032 Test RE 0.0347188386305322\n",
      "216 Train Loss 0.003578183 Test MSE 0.00026107711252546667 Test RE 0.03781344663822715\n",
      "217 Train Loss 0.0035459343 Test MSE 0.0002754208881219125 Test RE 0.038838307975332644\n",
      "218 Train Loss 0.003512404 Test MSE 0.0002611607869736617 Test RE 0.037819505702112725\n",
      "219 Train Loss 0.0034598904 Test MSE 0.00021448521833025488 Test RE 0.03427366501449165\n",
      "220 Train Loss 0.0034071503 Test MSE 0.00021405996976086686 Test RE 0.03423967185989367\n",
      "221 Train Loss 0.0033686084 Test MSE 0.0002424245109946378 Test RE 0.03643763055448143\n",
      "222 Train Loss 0.003300306 Test MSE 0.00021039967498014437 Test RE 0.03394567089027097\n",
      "223 Train Loss 0.0032734254 Test MSE 0.0001928842521599689 Test RE 0.032502012262159076\n",
      "224 Train Loss 0.0032561067 Test MSE 0.00019490664917599836 Test RE 0.03267196022026839\n",
      "225 Train Loss 0.0032361462 Test MSE 0.00021703349151068322 Test RE 0.03447666449547536\n",
      "226 Train Loss 0.0032179132 Test MSE 0.00022473514259043333 Test RE 0.035083051336324125\n",
      "227 Train Loss 0.0032092107 Test MSE 0.00021348251658920696 Test RE 0.03419345780274876\n",
      "228 Train Loss 0.0032000444 Test MSE 0.0002126197176730175 Test RE 0.03412429066981881\n",
      "229 Train Loss 0.0031949528 Test MSE 0.00020932720355948316 Test RE 0.03385904463163193\n",
      "230 Train Loss 0.0031878029 Test MSE 0.00021405618267672947 Test RE 0.03423936897960308\n",
      "231 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "232 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "233 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "234 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "235 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "236 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "237 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "238 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "239 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "240 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "241 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "242 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "243 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "244 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "245 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "246 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "247 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "248 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "249 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "250 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "251 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "252 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "253 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "254 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "255 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "256 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "257 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "258 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "259 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "260 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "261 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "262 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "263 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "264 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "265 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "266 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "267 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "268 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "269 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "270 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "271 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "272 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "273 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "274 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "275 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "276 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "277 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "278 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "279 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "280 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "281 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "282 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "283 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "284 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "285 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "286 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "287 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "288 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "289 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "290 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "291 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "292 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "293 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "294 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "295 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "296 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "297 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "298 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "299 Train Loss 0.0031868303 Test MSE 0.0002141124848555575 Test RE 0.034243871592669464\n",
      "Training time: 405.39\n",
      "KG_rowdy_low\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 5551.033 Test MSE 0.6296253382641589 Test RE 1.8569617691190516\n",
      "1 Train Loss 462.47522 Test MSE 0.5411392744744431 Test RE 1.7215371204125196\n",
      "2 Train Loss 54.565914 Test MSE 0.8595262622261859 Test RE 2.1696587863086116\n",
      "3 Train Loss 10.1832905 Test MSE 0.7554431263941103 Test RE 2.034055277060921\n",
      "4 Train Loss 5.039163 Test MSE 0.674469142554167 Test RE 1.921953635194187\n",
      "5 Train Loss 3.1894763 Test MSE 0.5824928843666393 Test RE 1.7861057815689778\n",
      "6 Train Loss 1.9243907 Test MSE 0.3722261966894506 Test RE 1.4277932729689446\n",
      "7 Train Loss 1.547461 Test MSE 0.4035317601242935 Test RE 1.486622579914417\n",
      "8 Train Loss 1.070199 Test MSE 0.35796245751306244 Test RE 1.4001694722043132\n",
      "9 Train Loss 0.96444577 Test MSE 0.35784619552485647 Test RE 1.3999420744658393\n",
      "10 Train Loss 0.72288954 Test MSE 0.3401437660735718 Test RE 1.3648757636791677\n",
      "11 Train Loss 0.48525444 Test MSE 0.2893445814525155 Test RE 1.258837120625585\n",
      "12 Train Loss 0.43747994 Test MSE 0.2878819786569805 Test RE 1.2556514530087768\n",
      "13 Train Loss 0.38189113 Test MSE 0.29005664257742825 Test RE 1.2603851333281717\n",
      "14 Train Loss 0.34644067 Test MSE 0.28225309830630146 Test RE 1.2433151424328917\n",
      "15 Train Loss 0.32008815 Test MSE 0.27502379599704335 Test RE 1.2272894475346598\n",
      "16 Train Loss 0.24790482 Test MSE 0.2404200685510977 Test RE 1.1474855422915626\n",
      "17 Train Loss 0.24131733 Test MSE 0.2316277121712244 Test RE 1.1263078806547386\n",
      "18 Train Loss 0.22379324 Test MSE 0.22058902935901342 Test RE 1.0991420395085911\n",
      "19 Train Loss 0.18868145 Test MSE 0.20572943852446884 Test RE 1.0614757684039149\n",
      "20 Train Loss 0.17907903 Test MSE 0.20966448639870622 Test RE 1.0715792647334719\n",
      "21 Train Loss 0.17084633 Test MSE 0.22040055012485074 Test RE 1.0986723658313213\n",
      "22 Train Loss 0.16768888 Test MSE 0.22326796384118724 Test RE 1.1057961407257655\n",
      "23 Train Loss 0.16446902 Test MSE 0.22057946778954823 Test RE 1.0991182177508154\n",
      "24 Train Loss 0.1449643 Test MSE 0.19927907658839963 Test RE 1.044702695120758\n",
      "25 Train Loss 0.143106 Test MSE 0.19243322296241316 Test RE 1.0266014914844552\n",
      "26 Train Loss 0.13901122 Test MSE 0.1710771824127564 Test RE 0.9679611091259084\n",
      "27 Train Loss 0.12888941 Test MSE 0.14544739136102058 Test RE 0.8925135788673679\n",
      "28 Train Loss 0.10391058 Test MSE 0.12244621850040337 Test RE 0.8189069420344233\n",
      "29 Train Loss 0.08099109 Test MSE 0.09015488097667901 Test RE 0.7026782098593028\n",
      "30 Train Loss 0.07802844 Test MSE 0.09239490075064963 Test RE 0.7113541437441834\n",
      "31 Train Loss 0.07534415 Test MSE 0.09591459698548774 Test RE 0.7247766898709056\n",
      "32 Train Loss 0.071112625 Test MSE 0.07178834995695658 Test RE 0.6270307647730744\n",
      "33 Train Loss 0.066155665 Test MSE 0.06975561930195112 Test RE 0.6180896387771576\n",
      "34 Train Loss 0.06325054 Test MSE 0.06689977784091286 Test RE 0.6053049170696155\n",
      "35 Train Loss 0.055302884 Test MSE 0.06165262671155492 Test RE 0.5810823125910973\n",
      "36 Train Loss 0.049870554 Test MSE 0.05883713429531478 Test RE 0.5676591205248407\n",
      "37 Train Loss 0.04913231 Test MSE 0.05443573110581561 Test RE 0.5460141455801641\n",
      "38 Train Loss 0.048766144 Test MSE 0.05270156057021483 Test RE 0.5372465085539604\n",
      "39 Train Loss 0.045911904 Test MSE 0.05034317427603757 Test RE 0.5250880827938543\n",
      "40 Train Loss 0.041443016 Test MSE 0.04301656807608956 Test RE 0.48537761510029354\n",
      "41 Train Loss 0.039670236 Test MSE 0.04290788116413547 Test RE 0.4847640426884117\n",
      "42 Train Loss 0.03810362 Test MSE 0.04655543007163707 Test RE 0.5049484408082402\n",
      "43 Train Loss 0.03776076 Test MSE 0.04633581122553451 Test RE 0.5037560204233117\n",
      "44 Train Loss 0.036686223 Test MSE 0.04523783428335727 Test RE 0.4977517168861583\n",
      "45 Train Loss 0.03316576 Test MSE 0.04105967690046614 Test RE 0.4742088205387058\n",
      "46 Train Loss 0.027385157 Test MSE 0.0314634858454085 Test RE 0.4151119944708046\n",
      "47 Train Loss 0.024343215 Test MSE 0.025959272038758165 Test RE 0.3770579646203733\n",
      "48 Train Loss 0.023352973 Test MSE 0.020590436599154792 Test RE 0.3358107864115437\n",
      "49 Train Loss 0.023258213 Test MSE 0.02055120009727038 Test RE 0.33549067849536707\n",
      "50 Train Loss 0.022583742 Test MSE 0.018025667742421647 Test RE 0.314200986946915\n",
      "51 Train Loss 0.02152794 Test MSE 0.016727078559931816 Test RE 0.3026717671166994\n",
      "52 Train Loss 0.02081492 Test MSE 0.015876700348488714 Test RE 0.29487773914157017\n",
      "53 Train Loss 0.01871348 Test MSE 0.014528732135739407 Test RE 0.28208222620946377\n",
      "54 Train Loss 0.015888482 Test MSE 0.00949795412177678 Test RE 0.22807458734795363\n",
      "55 Train Loss 0.012971748 Test MSE 0.004688100867278057 Test RE 0.16023608950824073\n",
      "56 Train Loss 0.011266402 Test MSE 0.001982871153440033 Test RE 0.10420993173878633\n",
      "57 Train Loss 0.010537927 Test MSE 0.0011232079581228775 Test RE 0.07843175807215884\n",
      "58 Train Loss 0.010040043 Test MSE 0.001214160123077794 Test RE 0.0815454712052374\n",
      "59 Train Loss 0.009920293 Test MSE 0.0009825169644625778 Test RE 0.0733553667810958\n",
      "60 Train Loss 0.009861861 Test MSE 0.0009654555328167693 Test RE 0.0727156686367957\n",
      "61 Train Loss 0.009757555 Test MSE 0.0009724622012999805 Test RE 0.07297905390305544\n",
      "62 Train Loss 0.009642524 Test MSE 0.0009683164021937285 Test RE 0.0728233256650728\n",
      "63 Train Loss 0.009593472 Test MSE 0.000988671764683274 Test RE 0.07358476880888558\n",
      "64 Train Loss 0.009467593 Test MSE 0.0009397354068807279 Test RE 0.07174054294802186\n",
      "65 Train Loss 0.009329591 Test MSE 0.0009844390071564915 Test RE 0.07342708221476701\n",
      "66 Train Loss 0.009142645 Test MSE 0.0008653428067010729 Test RE 0.06884239295999392\n",
      "67 Train Loss 0.0089956755 Test MSE 0.0008899869538538223 Test RE 0.06981579444129823\n",
      "68 Train Loss 0.008745985 Test MSE 0.0008033436994085446 Test RE 0.06633039140081944\n",
      "69 Train Loss 0.008426111 Test MSE 0.0008203913624928368 Test RE 0.06703049146894714\n",
      "70 Train Loss 0.008164834 Test MSE 0.0006965102536892181 Test RE 0.06176260726936934\n",
      "71 Train Loss 0.00808127 Test MSE 0.0007817502580423782 Test RE 0.0654328565974143\n",
      "72 Train Loss 0.008032437 Test MSE 0.0008013119031767389 Test RE 0.06624645773616496\n",
      "73 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "74 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "75 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "76 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "77 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "78 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "79 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "80 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "81 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "82 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "83 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "84 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "85 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "86 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "87 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "88 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "89 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "90 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "91 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "92 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "93 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "94 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "95 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "96 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "97 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "98 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "99 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "100 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "101 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "102 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "103 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "104 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "105 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "106 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "107 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "108 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "109 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "110 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "111 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "112 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "113 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "114 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "115 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "116 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "117 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "118 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "119 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "120 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "121 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "122 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "123 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "124 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "125 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "126 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "127 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "128 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "129 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "130 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "131 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "132 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "133 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "134 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "135 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "136 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "137 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "138 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "139 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "140 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "141 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "142 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "143 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "144 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "145 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "146 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "147 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "148 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "149 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "150 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "151 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "152 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "153 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "154 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "155 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "156 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "157 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "158 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "159 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "160 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "161 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "162 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "163 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "164 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "165 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "166 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "167 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "168 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "169 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "170 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "171 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "172 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "173 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "174 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "175 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "176 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "177 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "178 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "179 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "180 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "181 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "182 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "183 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "184 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "185 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "186 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "187 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "188 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "189 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "190 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "191 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "192 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "193 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "194 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "195 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "196 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "197 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "198 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "199 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "200 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "201 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "202 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "203 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "204 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "205 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "206 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "207 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "208 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "209 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "210 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "211 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "212 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "213 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "214 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "215 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "216 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "217 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "218 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "219 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "220 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "221 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "222 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "223 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "224 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "225 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "226 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "227 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "228 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "229 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "230 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "231 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "232 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "233 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "234 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "235 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "236 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "237 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "238 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "239 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "240 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "241 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "242 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "243 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "244 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "245 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "246 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "247 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "248 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "249 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "250 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "251 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "252 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "253 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "254 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "255 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "256 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "257 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "258 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "259 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "260 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "261 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "262 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "263 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "264 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "265 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "266 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "267 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "268 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "269 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "270 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "271 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "272 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "273 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "274 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "275 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "276 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "277 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "278 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "279 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "280 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "281 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "282 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "283 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "284 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "285 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "286 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "287 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "288 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "289 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "290 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "291 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "292 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "293 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "294 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "295 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "296 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "297 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "298 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "299 Train Loss 0.007991652 Test MSE 0.0007766029701117619 Test RE 0.06521708564993217\n",
      "Training time: 235.11\n",
      "KG_rowdy_low\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 7955.2466 Test MSE 0.17242633231676774 Test RE 0.9717703840506372\n",
      "1 Train Loss 2666.0525 Test MSE 0.1808834383319259 Test RE 0.995316643685206\n",
      "2 Train Loss 563.32806 Test MSE 0.21872388139395332 Test RE 1.0944853835562582\n",
      "3 Train Loss 88.51511 Test MSE 0.6123068685810291 Test RE 1.8312449060616434\n",
      "4 Train Loss 42.02868 Test MSE 0.8098897131294968 Test RE 2.106079716201007\n",
      "5 Train Loss 21.775686 Test MSE 0.7393191767397982 Test RE 2.0122310662379905\n",
      "6 Train Loss 13.715298 Test MSE 0.709807288971417 Test RE 1.9716602980005473\n",
      "7 Train Loss 8.842464 Test MSE 0.6704061283725369 Test RE 1.9161559491218558\n",
      "8 Train Loss 6.5255566 Test MSE 0.6176740646806208 Test RE 1.8392533137438838\n",
      "9 Train Loss 4.609091 Test MSE 0.5801617359065567 Test RE 1.7825281826045094\n",
      "10 Train Loss 3.3285644 Test MSE 0.4688221344982099 Test RE 1.6023814841788129\n",
      "11 Train Loss 2.6065242 Test MSE 0.4263515124022689 Test RE 1.528078861456461\n",
      "12 Train Loss 1.9226804 Test MSE 0.3472948907170943 Test RE 1.3791485943585846\n",
      "13 Train Loss 1.5876839 Test MSE 0.302138290517856 Test RE 1.2863665781025673\n",
      "14 Train Loss 1.2409362 Test MSE 0.2689799384554506 Test RE 1.2137292240918205\n",
      "15 Train Loss 0.97305447 Test MSE 0.1930965262190498 Test RE 1.028369279594653\n",
      "16 Train Loss 0.7876801 Test MSE 0.15621445544017418 Test RE 0.924958981143357\n",
      "17 Train Loss 0.61541396 Test MSE 0.11059093601993206 Test RE 0.7782544828755643\n",
      "18 Train Loss 0.52744454 Test MSE 0.10586965973644867 Test RE 0.7614609260723464\n",
      "19 Train Loss 0.43274555 Test MSE 0.09601561924199871 Test RE 0.7251582757281068\n",
      "20 Train Loss 0.37693274 Test MSE 0.08412082508239804 Test RE 0.6787559128787422\n",
      "21 Train Loss 0.34576398 Test MSE 0.08069372174043606 Test RE 0.6647858045888683\n",
      "22 Train Loss 0.29141048 Test MSE 0.0704524188258728 Test RE 0.6211690635278709\n",
      "23 Train Loss 0.25529355 Test MSE 0.06863145936580965 Test RE 0.613088939008066\n",
      "24 Train Loss 0.22924654 Test MSE 0.06348503939335012 Test RE 0.5896544232900772\n",
      "25 Train Loss 0.20358163 Test MSE 0.06012386370883519 Test RE 0.5738327156871459\n",
      "26 Train Loss 0.18609972 Test MSE 0.05814336350395065 Test RE 0.5643024551042081\n",
      "27 Train Loss 0.17353785 Test MSE 0.05779684562064985 Test RE 0.5626184012940129\n",
      "28 Train Loss 0.16280383 Test MSE 0.05620397688859743 Test RE 0.5548114142232637\n",
      "29 Train Loss 0.15398163 Test MSE 0.05617602881206937 Test RE 0.5546734538793104\n",
      "30 Train Loss 0.14209208 Test MSE 0.057230716628045235 Test RE 0.5598561535995435\n",
      "31 Train Loss 0.1386687 Test MSE 0.05535649947406572 Test RE 0.5506126358459947\n",
      "32 Train Loss 0.13410974 Test MSE 0.05447237062402641 Test RE 0.5461978698510227\n",
      "33 Train Loss 0.12514153 Test MSE 0.05297015157966925 Test RE 0.538613794394416\n",
      "34 Train Loss 0.11995698 Test MSE 0.051308341399014634 Test RE 0.530097617053578\n",
      "35 Train Loss 0.11621754 Test MSE 0.050404003643325965 Test RE 0.5254052174766297\n",
      "36 Train Loss 0.11373692 Test MSE 0.050349125658573836 Test RE 0.5251191188552456\n",
      "37 Train Loss 0.10534649 Test MSE 0.04825621179847044 Test RE 0.514089196146863\n",
      "38 Train Loss 0.10195389 Test MSE 0.04644501355285778 Test RE 0.5043492867804652\n",
      "39 Train Loss 0.10027869 Test MSE 0.04495798651732259 Test RE 0.4962097465264452\n",
      "40 Train Loss 0.09611882 Test MSE 0.04339687127071086 Test RE 0.48751847006224225\n",
      "41 Train Loss 0.090175115 Test MSE 0.04173596373554558 Test RE 0.4780981764435816\n",
      "42 Train Loss 0.08779646 Test MSE 0.04238999220816166 Test RE 0.4818296622732322\n",
      "43 Train Loss 0.08105741 Test MSE 0.040249436060578125 Test RE 0.46950666733141355\n",
      "44 Train Loss 0.0770658 Test MSE 0.038138009969998124 Test RE 0.4570259688777419\n",
      "45 Train Loss 0.0752531 Test MSE 0.038580544215561105 Test RE 0.4596698708691324\n",
      "46 Train Loss 0.07333817 Test MSE 0.038185599035993514 Test RE 0.45731102122366596\n",
      "47 Train Loss 0.070147544 Test MSE 0.036329093829826 Test RE 0.4460557546473206\n",
      "48 Train Loss 0.06925825 Test MSE 0.0351832717786266 Test RE 0.4389650824543083\n",
      "49 Train Loss 0.06785559 Test MSE 0.03322986798793705 Test RE 0.42660522666921175\n",
      "50 Train Loss 0.061525654 Test MSE 0.029389308384142943 Test RE 0.4011959562671875\n",
      "51 Train Loss 0.05988684 Test MSE 0.029208490908574695 Test RE 0.39995987469518357\n",
      "52 Train Loss 0.05857293 Test MSE 0.029494179570587524 Test RE 0.40191112160310155\n",
      "53 Train Loss 0.0564594 Test MSE 0.0277841800195029 Test RE 0.3900862624276176\n",
      "54 Train Loss 0.05458534 Test MSE 0.02726067762957296 Test RE 0.38639383414546374\n",
      "55 Train Loss 0.053343114 Test MSE 0.02598665972719582 Test RE 0.37725681502985026\n",
      "56 Train Loss 0.05260349 Test MSE 0.025558382832369345 Test RE 0.37413518228275394\n",
      "57 Train Loss 0.051835883 Test MSE 0.025038404220549773 Test RE 0.3703097844243995\n",
      "58 Train Loss 0.051268283 Test MSE 0.024460447924394165 Test RE 0.36601094025094666\n",
      "59 Train Loss 0.050623916 Test MSE 0.023309017083734746 Test RE 0.3572924540883279\n",
      "60 Train Loss 0.048797827 Test MSE 0.02210861974621033 Test RE 0.34797070185712314\n",
      "61 Train Loss 0.046918415 Test MSE 0.02101407836203471 Test RE 0.3392477982666271\n",
      "62 Train Loss 0.04449949 Test MSE 0.020498625649482686 Test RE 0.3350612745641611\n",
      "63 Train Loss 0.04237008 Test MSE 0.02042262086662935 Test RE 0.3344395277783293\n",
      "64 Train Loss 0.04125808 Test MSE 0.020419068029708297 Test RE 0.3344104359984021\n",
      "65 Train Loss 0.04056396 Test MSE 0.02068481391756463 Test RE 0.336579509498655\n",
      "66 Train Loss 0.039562102 Test MSE 0.019954889439493043 Test RE 0.33058757517858145\n",
      "67 Train Loss 0.03823275 Test MSE 0.0188491434585241 Test RE 0.32129774236705483\n",
      "68 Train Loss 0.03620335 Test MSE 0.018068798632474257 Test RE 0.3145766643311456\n",
      "69 Train Loss 0.03518478 Test MSE 0.017895322345670855 Test RE 0.3130629165414157\n",
      "70 Train Loss 0.034318212 Test MSE 0.01777150834552969 Test RE 0.3119780282673969\n",
      "71 Train Loss 0.03332139 Test MSE 0.01731079088091409 Test RE 0.3079075365015582\n",
      "72 Train Loss 0.03249704 Test MSE 0.017175020015661322 Test RE 0.30669767941629955\n",
      "73 Train Loss 0.031850725 Test MSE 0.016987200779257982 Test RE 0.3050161071732285\n",
      "74 Train Loss 0.030610662 Test MSE 0.016494093316399415 Test RE 0.3005564743833539\n",
      "75 Train Loss 0.029523913 Test MSE 0.01632757859031373 Test RE 0.29903550468854273\n",
      "76 Train Loss 0.028678004 Test MSE 0.016173073200446936 Test RE 0.2976172778752861\n",
      "77 Train Loss 0.02774831 Test MSE 0.015969832375865453 Test RE 0.29574134446312317\n",
      "78 Train Loss 0.027212083 Test MSE 0.015780315058019294 Test RE 0.2939812951800007\n",
      "79 Train Loss 0.026809394 Test MSE 0.015436817297666064 Test RE 0.29076407453974035\n",
      "80 Train Loss 0.026071768 Test MSE 0.015097292556849535 Test RE 0.2875486940425597\n",
      "81 Train Loss 0.025377098 Test MSE 0.01491080110928256 Test RE 0.28576718233982135\n",
      "82 Train Loss 0.025029989 Test MSE 0.014860807831069854 Test RE 0.28528771670822534\n",
      "83 Train Loss 0.024600733 Test MSE 0.01488958772475874 Test RE 0.28556383153640386\n",
      "84 Train Loss 0.024161618 Test MSE 0.014970337320247096 Test RE 0.2863371230819625\n",
      "85 Train Loss 0.023935495 Test MSE 0.015023047492411493 Test RE 0.2868407729550403\n",
      "86 Train Loss 0.023683906 Test MSE 0.015071208402872027 Test RE 0.28730018236761085\n",
      "87 Train Loss 0.02331521 Test MSE 0.014830059364582095 Test RE 0.2849924197697551\n",
      "88 Train Loss 0.023022983 Test MSE 0.014750495131454497 Test RE 0.2842268901750003\n",
      "89 Train Loss 0.022717603 Test MSE 0.014794677216454522 Test RE 0.2846522435965115\n",
      "90 Train Loss 0.022200594 Test MSE 0.014906055831451544 Test RE 0.285721706829458\n",
      "91 Train Loss 0.021880353 Test MSE 0.014892829673722184 Test RE 0.28559491812513066\n",
      "92 Train Loss 0.021545954 Test MSE 0.014924598421856226 Test RE 0.2858993652934814\n",
      "93 Train Loss 0.021318935 Test MSE 0.014911036374603465 Test RE 0.28576943677411276\n",
      "94 Train Loss 0.021106463 Test MSE 0.01490375687988171 Test RE 0.28569967264071555\n",
      "95 Train Loss 0.020873686 Test MSE 0.014792204716826681 Test RE 0.28462845693447275\n",
      "96 Train Loss 0.020651694 Test MSE 0.01473783210175791 Test RE 0.2841048621919851\n",
      "97 Train Loss 0.02041569 Test MSE 0.014560720980788663 Test RE 0.2823925947641587\n",
      "98 Train Loss 0.020210706 Test MSE 0.01440880243792067 Test RE 0.2809155676425934\n",
      "99 Train Loss 0.01982462 Test MSE 0.014482677990652334 Test RE 0.2816347898131184\n",
      "100 Train Loss 0.019589556 Test MSE 0.014530066030259515 Test RE 0.2820951750096852\n",
      "101 Train Loss 0.019323872 Test MSE 0.01448935489789209 Test RE 0.2816997029695006\n",
      "102 Train Loss 0.019015446 Test MSE 0.01444405587324154 Test RE 0.28125901008940546\n",
      "103 Train Loss 0.0187367 Test MSE 0.014321285532893222 Test RE 0.28006114874990073\n",
      "104 Train Loss 0.018424405 Test MSE 0.014187491616087807 Test RE 0.2787498696620875\n",
      "105 Train Loss 0.018248077 Test MSE 0.014231891589277942 Test RE 0.27918570494203476\n",
      "106 Train Loss 0.017988954 Test MSE 0.014049593512139046 Test RE 0.2773918798112649\n",
      "107 Train Loss 0.017686551 Test MSE 0.013655078226438404 Test RE 0.27346954009294683\n",
      "108 Train Loss 0.017482921 Test MSE 0.013410564576971826 Test RE 0.27101004912682913\n",
      "109 Train Loss 0.017248183 Test MSE 0.013218163367255377 Test RE 0.26905893673000447\n",
      "110 Train Loss 0.016991356 Test MSE 0.012907085952444462 Test RE 0.2658740585702339\n",
      "111 Train Loss 0.016666804 Test MSE 0.012722942995262861 Test RE 0.2639706578319052\n",
      "112 Train Loss 0.016367154 Test MSE 0.012463801218815518 Test RE 0.2612685415835778\n",
      "113 Train Loss 0.016071023 Test MSE 0.012369983760841965 Test RE 0.2602833746103762\n",
      "114 Train Loss 0.015903253 Test MSE 0.01221376146381315 Test RE 0.25863457432305303\n",
      "115 Train Loss 0.015727524 Test MSE 0.012163459549939774 Test RE 0.2581014364816807\n",
      "116 Train Loss 0.015393588 Test MSE 0.011987999325361954 Test RE 0.25623309297222685\n",
      "117 Train Loss 0.014857371 Test MSE 0.01186676373131242 Test RE 0.25493414768180195\n",
      "118 Train Loss 0.014650036 Test MSE 0.011648453267136928 Test RE 0.2525782761569345\n",
      "119 Train Loss 0.014406962 Test MSE 0.011411359295340843 Test RE 0.24999455744289162\n",
      "120 Train Loss 0.014229368 Test MSE 0.011379332043255561 Test RE 0.24964349214548434\n",
      "121 Train Loss 0.014102777 Test MSE 0.011128457688981795 Test RE 0.24687627363631487\n",
      "122 Train Loss 0.014018102 Test MSE 0.011057310605269195 Test RE 0.2460858365815558\n",
      "123 Train Loss 0.013960891 Test MSE 0.010988402147915416 Test RE 0.24531784253768832\n",
      "124 Train Loss 0.013856163 Test MSE 0.01090949105108304 Test RE 0.24443540396301727\n",
      "125 Train Loss 0.01374175 Test MSE 0.010949029879599838 Test RE 0.2448779520375357\n",
      "126 Train Loss 0.013570681 Test MSE 0.010526358937475465 Test RE 0.24010486040100634\n",
      "127 Train Loss 0.013269845 Test MSE 0.010148725255527116 Test RE 0.23575863686754897\n",
      "128 Train Loss 0.012860986 Test MSE 0.009841502291272945 Test RE 0.23216276261598776\n",
      "129 Train Loss 0.012488519 Test MSE 0.009409666184281032 Test RE 0.22701208234174447\n",
      "130 Train Loss 0.0120398635 Test MSE 0.008788185289190724 Test RE 0.21938729151805858\n",
      "131 Train Loss 0.011828311 Test MSE 0.008725822196354904 Test RE 0.21860749294799972\n",
      "132 Train Loss 0.011644029 Test MSE 0.008712202512322768 Test RE 0.21843681976527404\n",
      "133 Train Loss 0.011519142 Test MSE 0.008598565999889169 Test RE 0.21700756737329052\n",
      "134 Train Loss 0.011443168 Test MSE 0.008538593245252456 Test RE 0.21624945757119673\n",
      "135 Train Loss 0.011367871 Test MSE 0.008538471852858765 Test RE 0.2162479203664232\n",
      "136 Train Loss 0.01130151 Test MSE 0.008449013530844334 Test RE 0.21511211328656885\n",
      "137 Train Loss 0.011257753 Test MSE 0.008416513616004196 Test RE 0.2146979903062935\n",
      "138 Train Loss 0.011194717 Test MSE 0.0083416771674139 Test RE 0.21374135247120363\n",
      "139 Train Loss 0.011119553 Test MSE 0.00811904532590503 Test RE 0.2108697810044528\n",
      "140 Train Loss 0.011060025 Test MSE 0.008115889248650626 Test RE 0.21082879182521144\n",
      "141 Train Loss 0.010887822 Test MSE 0.007971168781710985 Test RE 0.2089406139666878\n",
      "142 Train Loss 0.010639707 Test MSE 0.007727895258288049 Test RE 0.20572756112599513\n",
      "143 Train Loss 0.0103305215 Test MSE 0.007012157681696291 Test RE 0.19596914632857648\n",
      "144 Train Loss 0.009794382 Test MSE 0.006705012400313611 Test RE 0.19162918684546654\n",
      "145 Train Loss 0.009484114 Test MSE 0.006331555048458622 Test RE 0.1862160274494992\n",
      "146 Train Loss 0.009288101 Test MSE 0.006347323156466167 Test RE 0.18644775950892997\n",
      "147 Train Loss 0.009156098 Test MSE 0.006200429297537595 Test RE 0.1842776833514851\n",
      "148 Train Loss 0.009102546 Test MSE 0.006176278398353449 Test RE 0.18391844903988414\n",
      "149 Train Loss 0.008983255 Test MSE 0.006236694487239438 Test RE 0.1848158010832883\n",
      "150 Train Loss 0.008919004 Test MSE 0.006262400241230875 Test RE 0.1851962866290311\n",
      "151 Train Loss 0.008839213 Test MSE 0.006184265894016105 Test RE 0.18403733723300128\n",
      "152 Train Loss 0.008738687 Test MSE 0.0062146999321025164 Test RE 0.18448962416684134\n",
      "153 Train Loss 0.008585813 Test MSE 0.00601503181373013 Test RE 0.1815017548550419\n",
      "154 Train Loss 0.008408725 Test MSE 0.005755732858708374 Test RE 0.17754652576464405\n",
      "155 Train Loss 0.008291126 Test MSE 0.00580548608201255 Test RE 0.1783122409868924\n",
      "156 Train Loss 0.008165385 Test MSE 0.005690091411743392 Test RE 0.17653120501759284\n",
      "157 Train Loss 0.008068862 Test MSE 0.005888236310617842 Test RE 0.1795785578702514\n",
      "158 Train Loss 0.008015123 Test MSE 0.005897251908461857 Test RE 0.1797159834696813\n",
      "159 Train Loss 0.007978703 Test MSE 0.005829628673634957 Test RE 0.17868261938811406\n",
      "160 Train Loss 0.00791165 Test MSE 0.005829056095795363 Test RE 0.1786738441962269\n",
      "161 Train Loss 0.007864667 Test MSE 0.005753411269880666 Test RE 0.17751071524202428\n",
      "162 Train Loss 0.007793259 Test MSE 0.005690958419248681 Test RE 0.17654465366356145\n",
      "163 Train Loss 0.0077483514 Test MSE 0.005586055545421192 Test RE 0.17490993925408685\n",
      "164 Train Loss 0.007696987 Test MSE 0.005527136562383846 Test RE 0.17398506173467346\n",
      "165 Train Loss 0.0076575615 Test MSE 0.005475910442698504 Test RE 0.17317692840509585\n",
      "166 Train Loss 0.0076130377 Test MSE 0.005449440411840678 Test RE 0.17275786094864967\n",
      "167 Train Loss 0.0075754942 Test MSE 0.005425686373697197 Test RE 0.17238092512851677\n",
      "168 Train Loss 0.0074098813 Test MSE 0.005102961528018667 Test RE 0.16717564639725951\n",
      "169 Train Loss 0.007278608 Test MSE 0.004798095908529704 Test RE 0.16210496861059212\n",
      "170 Train Loss 0.007149462 Test MSE 0.004765718206766133 Test RE 0.16155709807651888\n",
      "171 Train Loss 0.0070794933 Test MSE 0.004775107533471318 Test RE 0.16171616811557482\n",
      "172 Train Loss 0.0069611585 Test MSE 0.004789944060013527 Test RE 0.16196720386699373\n",
      "173 Train Loss 0.0067524957 Test MSE 0.004706103850462991 Test RE 0.16054345954665697\n",
      "174 Train Loss 0.006600246 Test MSE 0.004654059043428624 Test RE 0.1596532664053612\n",
      "175 Train Loss 0.0064725415 Test MSE 0.004480219437484685 Test RE 0.15664318572408092\n",
      "176 Train Loss 0.0063538 Test MSE 0.004243378053630073 Test RE 0.15244659445864867\n",
      "177 Train Loss 0.0062608304 Test MSE 0.004119229171412882 Test RE 0.15019996838015487\n",
      "178 Train Loss 0.006168083 Test MSE 0.004094437798005033 Test RE 0.14974730078866028\n",
      "179 Train Loss 0.0061157253 Test MSE 0.003963327599678157 Test RE 0.14733022423669956\n",
      "180 Train Loss 0.006091007 Test MSE 0.003909543711739638 Test RE 0.14632714551714915\n",
      "181 Train Loss 0.006054435 Test MSE 0.0038425804968945576 Test RE 0.1450685770361962\n",
      "182 Train Loss 0.0060219425 Test MSE 0.0037247774979258176 Test RE 0.14282756478936962\n",
      "183 Train Loss 0.0059936047 Test MSE 0.0036356958872888864 Test RE 0.14110930033616684\n",
      "184 Train Loss 0.005967429 Test MSE 0.003588917324529112 Test RE 0.140198572279662\n",
      "185 Train Loss 0.0059334794 Test MSE 0.0036127463538731266 Test RE 0.14066323453365462\n",
      "186 Train Loss 0.0059000254 Test MSE 0.0036248407446582954 Test RE 0.14089848696107368\n",
      "187 Train Loss 0.005870612 Test MSE 0.003684968007981323 Test RE 0.1420622618163759\n",
      "188 Train Loss 0.0058285287 Test MSE 0.003715260276291717 Test RE 0.142644977909222\n",
      "189 Train Loss 0.0057247593 Test MSE 0.0037654941781382466 Test RE 0.14360608886296733\n",
      "190 Train Loss 0.005665954 Test MSE 0.003664923458430584 Test RE 0.14167535791587277\n",
      "191 Train Loss 0.0055981656 Test MSE 0.0035718551371531197 Test RE 0.13986491397062303\n",
      "192 Train Loss 0.0055085407 Test MSE 0.0034140130601575135 Test RE 0.13673964659237958\n",
      "193 Train Loss 0.005437668 Test MSE 0.0032106495173724286 Test RE 0.13260451571116397\n",
      "194 Train Loss 0.0053243483 Test MSE 0.003021151272686509 Test RE 0.12863172678998291\n",
      "195 Train Loss 0.005248129 Test MSE 0.002942902268481984 Test RE 0.12695499242945316\n",
      "196 Train Loss 0.0051254365 Test MSE 0.0028402370133260133 Test RE 0.12472087682658635\n",
      "197 Train Loss 0.0049963356 Test MSE 0.0027562810091466016 Test RE 0.12286370583573941\n",
      "198 Train Loss 0.0049201697 Test MSE 0.002723985333372165 Test RE 0.12214178048084547\n",
      "199 Train Loss 0.004853891 Test MSE 0.002632736420856862 Test RE 0.1200785831195539\n",
      "200 Train Loss 0.0047787987 Test MSE 0.0024588412088102436 Test RE 0.11604517974735365\n",
      "201 Train Loss 0.0047111753 Test MSE 0.0024396358988481807 Test RE 0.11559109336278404\n",
      "202 Train Loss 0.004670824 Test MSE 0.0024184589870893517 Test RE 0.11508831390155794\n",
      "203 Train Loss 0.004650506 Test MSE 0.002375149042731705 Test RE 0.11405315346763557\n",
      "204 Train Loss 0.004624907 Test MSE 0.0023570968833679954 Test RE 0.11361890013310173\n",
      "205 Train Loss 0.0046019075 Test MSE 0.002355540489888054 Test RE 0.11358138260095112\n",
      "206 Train Loss 0.0045825834 Test MSE 0.002405522019207513 Test RE 0.11478008244255815\n",
      "207 Train Loss 0.004565013 Test MSE 0.0023935990431229997 Test RE 0.11449527520507469\n",
      "208 Train Loss 0.004533834 Test MSE 0.0023466722702791777 Test RE 0.11336737344176663\n",
      "209 Train Loss 0.0045028185 Test MSE 0.0023488700887226506 Test RE 0.11342044915149749\n",
      "210 Train Loss 0.0044772634 Test MSE 0.002330764844543003 Test RE 0.11298247742221992\n",
      "211 Train Loss 0.0044314866 Test MSE 0.002229312075386111 Test RE 0.11049618868174049\n",
      "212 Train Loss 0.004404983 Test MSE 0.002192295022890551 Test RE 0.1095749706469361\n",
      "213 Train Loss 0.0043637203 Test MSE 0.0022057236071503297 Test RE 0.1099100510803574\n",
      "214 Train Loss 0.004299608 Test MSE 0.002095869886831257 Test RE 0.1071381203960651\n",
      "215 Train Loss 0.0042618597 Test MSE 0.002029745291687046 Test RE 0.1054344739121737\n",
      "216 Train Loss 0.0042243144 Test MSE 0.001977266944759499 Test RE 0.10406256274849383\n",
      "217 Train Loss 0.0041859825 Test MSE 0.0019089511483709877 Test RE 0.10224904757057697\n",
      "218 Train Loss 0.0041279555 Test MSE 0.0018673551825242616 Test RE 0.10112891079034278\n",
      "219 Train Loss 0.0040688184 Test MSE 0.0018158227382336718 Test RE 0.09972374720311795\n",
      "220 Train Loss 0.0040168646 Test MSE 0.0017806287720861414 Test RE 0.09875260422957449\n",
      "221 Train Loss 0.0039778585 Test MSE 0.0017115682408939696 Test RE 0.09681863907055704\n",
      "222 Train Loss 0.003950365 Test MSE 0.0016679704751429247 Test RE 0.0955775829466449\n",
      "223 Train Loss 0.0039294227 Test MSE 0.0016395889685872798 Test RE 0.09476093950342084\n",
      "224 Train Loss 0.0039021443 Test MSE 0.001615349381512568 Test RE 0.09405786117654044\n",
      "225 Train Loss 0.0038704555 Test MSE 0.0015785551778779152 Test RE 0.09298047223998179\n",
      "226 Train Loss 0.003838372 Test MSE 0.0015312378918380576 Test RE 0.0915763221997266\n",
      "227 Train Loss 0.0038065785 Test MSE 0.0014544048321409096 Test RE 0.08924923832718545\n",
      "228 Train Loss 0.0037708108 Test MSE 0.0013906911653093735 Test RE 0.0872724588023989\n",
      "229 Train Loss 0.0037282766 Test MSE 0.001332994749518977 Test RE 0.08544292030380253\n",
      "230 Train Loss 0.003674203 Test MSE 0.0012667347680368442 Test RE 0.08329227214253385\n",
      "231 Train Loss 0.0036348945 Test MSE 0.0012321774351843216 Test RE 0.0821482828333512\n",
      "232 Train Loss 0.0036061201 Test MSE 0.0011915538977675382 Test RE 0.0807827642407105\n",
      "233 Train Loss 0.003572473 Test MSE 0.0011474707660584134 Test RE 0.07927434803164472\n",
      "234 Train Loss 0.0035400032 Test MSE 0.0011124186168223224 Test RE 0.07805414812138008\n",
      "235 Train Loss 0.0035186212 Test MSE 0.0010739255830078996 Test RE 0.07669180479199914\n",
      "236 Train Loss 0.0034927872 Test MSE 0.0010626291982851787 Test RE 0.07628738645195376\n",
      "237 Train Loss 0.0034700925 Test MSE 0.001073832758893238 Test RE 0.07668849031526569\n",
      "238 Train Loss 0.0034521092 Test MSE 0.0010435897919842702 Test RE 0.0756008668498668\n",
      "239 Train Loss 0.0034416174 Test MSE 0.0010297035574782852 Test RE 0.07509620157886858\n",
      "240 Train Loss 0.0034260175 Test MSE 0.00102035426970939 Test RE 0.07475450276838863\n",
      "241 Train Loss 0.0034066436 Test MSE 0.0010118398571358944 Test RE 0.07444195247298786\n",
      "242 Train Loss 0.0033831296 Test MSE 0.0010200787362819304 Test RE 0.07474440884528866\n",
      "243 Train Loss 0.0033593092 Test MSE 0.001003755285403758 Test RE 0.07414396150096804\n",
      "244 Train Loss 0.0033356005 Test MSE 0.0010339599206002567 Test RE 0.07525124963641448\n",
      "245 Train Loss 0.0033247934 Test MSE 0.0010185554216876132 Test RE 0.07468857894514253\n",
      "246 Train Loss 0.0033217936 Test MSE 0.0010151296299568853 Test RE 0.0745628700154443\n",
      "247 Train Loss 0.0033138192 Test MSE 0.0010134186148833203 Test RE 0.07450000513847901\n",
      "248 Train Loss 0.0033077425 Test MSE 0.0010092494292096006 Test RE 0.07434660137051974\n",
      "249 Train Loss 0.0033013576 Test MSE 0.0010019660517934445 Test RE 0.07407784974996998\n",
      "250 Train Loss 0.003294149 Test MSE 0.0009854753527181496 Test RE 0.07346572138575964\n",
      "251 Train Loss 0.00327602 Test MSE 0.0009688053439531786 Test RE 0.07284170905260154\n",
      "252 Train Loss 0.0032667115 Test MSE 0.0009562659056147093 Test RE 0.07236877144538148\n",
      "253 Train Loss 0.0032610737 Test MSE 0.0009476502634324987 Test RE 0.07204202436120449\n",
      "254 Train Loss 0.0032606241 Test MSE 0.0009455698590037858 Test RE 0.07196290292799709\n",
      "255 Train Loss 0.0032433576 Test MSE 0.0009402489701133194 Test RE 0.07176014329113661\n",
      "256 Train Loss 0.003232382 Test MSE 0.0009294438530399934 Test RE 0.07134662661755109\n",
      "257 Train Loss 0.0032059369 Test MSE 0.0008730260633400607 Test RE 0.06914733850879726\n",
      "258 Train Loss 0.00318977 Test MSE 0.0008577835893255823 Test RE 0.06854104644205135\n",
      "259 Train Loss 0.003179877 Test MSE 0.0008499749502794135 Test RE 0.06822835923278794\n",
      "260 Train Loss 0.0031638613 Test MSE 0.0008229244248771135 Test RE 0.06713389428517576\n",
      "261 Train Loss 0.0031556876 Test MSE 0.0008196622371880312 Test RE 0.06700069807171453\n",
      "262 Train Loss 0.00313378 Test MSE 0.0008425481475853489 Test RE 0.06792962671745362\n",
      "263 Train Loss 0.0031134947 Test MSE 0.0008483772696388878 Test RE 0.06816420534127274\n",
      "264 Train Loss 0.0030813734 Test MSE 0.0008344693971912211 Test RE 0.06760317157583307\n",
      "265 Train Loss 0.0030487068 Test MSE 0.0007573245144633022 Test RE 0.06440252154607477\n",
      "266 Train Loss 0.0030382778 Test MSE 0.0007490252674989395 Test RE 0.06404866740054002\n",
      "267 Train Loss 0.0030060671 Test MSE 0.0007409714816088883 Test RE 0.06370339976427479\n",
      "268 Train Loss 0.002990563 Test MSE 0.0007069084060155586 Test RE 0.06222192414117023\n",
      "269 Train Loss 0.0029818525 Test MSE 0.0006503545988749603 Test RE 0.05968112098533302\n",
      "270 Train Loss 0.0029610742 Test MSE 0.0006311536680104743 Test RE 0.058793514162207114\n",
      "271 Train Loss 0.002926234 Test MSE 0.0006278679192126135 Test RE 0.05864027668081067\n",
      "272 Train Loss 0.0028759465 Test MSE 0.0006087611039993664 Test RE 0.057741134415019754\n",
      "273 Train Loss 0.0028271636 Test MSE 0.000558123767385537 Test RE 0.05528752166853452\n",
      "274 Train Loss 0.0028117478 Test MSE 0.0005342690685010157 Test RE 0.054093101095387466\n",
      "275 Train Loss 0.0027914473 Test MSE 0.0005102860503273357 Test RE 0.05286505789274638\n",
      "276 Train Loss 0.0027703203 Test MSE 0.00047804013683282247 Test RE 0.051167481953081975\n",
      "277 Train Loss 0.0027533958 Test MSE 0.0004613098742166338 Test RE 0.05026413803594714\n",
      "278 Train Loss 0.0027354015 Test MSE 0.0004638262826715045 Test RE 0.05040104501141894\n",
      "279 Train Loss 0.002696333 Test MSE 0.00044100500522651335 Test RE 0.04914548477276775\n",
      "280 Train Loss 0.0026712453 Test MSE 0.0004044496814995037 Test RE 0.047064571924036236\n",
      "281 Train Loss 0.002635975 Test MSE 0.0003875495359533346 Test RE 0.04607077040158982\n",
      "282 Train Loss 0.0025996342 Test MSE 0.0003267000428937935 Test RE 0.04229961940530855\n",
      "283 Train Loss 0.0025699954 Test MSE 0.0003134413278408728 Test RE 0.04143239069303618\n",
      "284 Train Loss 0.0025173686 Test MSE 0.0002981169498114406 Test RE 0.04040686890133799\n",
      "285 Train Loss 0.002490948 Test MSE 0.00028018118654537215 Test RE 0.03917250540202946\n",
      "286 Train Loss 0.002469788 Test MSE 0.00026083489583324725 Test RE 0.03779590168146847\n",
      "287 Train Loss 0.0024425222 Test MSE 0.00023516871429367454 Test RE 0.03588819668945625\n",
      "288 Train Loss 0.0024103334 Test MSE 0.00024524092667549955 Test RE 0.03664868011013524\n",
      "289 Train Loss 0.0023923109 Test MSE 0.00023053819391676475 Test RE 0.0355331171131923\n",
      "290 Train Loss 0.0023650045 Test MSE 0.00021930854762742516 Test RE 0.034656894430880374\n",
      "291 Train Loss 0.0023174214 Test MSE 0.00024089375034647554 Test RE 0.03632240784125461\n",
      "292 Train Loss 0.0023022979 Test MSE 0.0002445555570496318 Test RE 0.03659743363551185\n",
      "293 Train Loss 0.0022861066 Test MSE 0.00025458435309759795 Test RE 0.0373402927150958\n",
      "294 Train Loss 0.0022601353 Test MSE 0.0002622544315091086 Test RE 0.03789861000681782\n",
      "295 Train Loss 0.0022482246 Test MSE 0.00025789973561603333 Test RE 0.03758264247190379\n",
      "296 Train Loss 0.0022415929 Test MSE 0.00026240624301091686 Test RE 0.03790957762418788\n",
      "297 Train Loss 0.0022248295 Test MSE 0.00026841148118362325 Test RE 0.038340909290679674\n",
      "298 Train Loss 0.0022071532 Test MSE 0.00025939823163533747 Test RE 0.03769166908712386\n",
      "299 Train Loss 0.0021962747 Test MSE 0.00025849518993039213 Test RE 0.03762600398232845\n",
      "Training time: 489.81\n",
      "KG_rowdy_low\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 3581.3628 Test MSE 2.323313198501358 Test RE 3.5671037727796295\n",
      "1 Train Loss 437.24713 Test MSE 3.3281184220562254 Test RE 4.269344986335132\n",
      "2 Train Loss 92.02939 Test MSE 3.720320341583763 Test RE 4.513901027611439\n",
      "3 Train Loss 30.74926 Test MSE 5.048593715835561 Test RE 5.258320765874085\n",
      "4 Train Loss 17.352283 Test MSE 5.063240772834718 Test RE 5.265943001597342\n",
      "5 Train Loss 11.698639 Test MSE 4.717451181100794 Test RE 5.082946882700528\n",
      "6 Train Loss 8.995594 Test MSE 4.364601625715842 Test RE 4.889159089300908\n",
      "7 Train Loss 7.551285 Test MSE 4.01697714840847 Test RE 4.690417965218311\n",
      "8 Train Loss 6.4108586 Test MSE 3.665181559495914 Test RE 4.480325947767684\n",
      "9 Train Loss 5.8470435 Test MSE 3.338351313778375 Test RE 4.275903379025258\n",
      "10 Train Loss 5.205312 Test MSE 2.792085276553666 Test RE 3.910445191978225\n",
      "11 Train Loss 4.7979083 Test MSE 2.4423734891685256 Test RE 3.6573616160552764\n",
      "12 Train Loss 4.3814964 Test MSE 2.1469097605365226 Test RE 3.4290100601222573\n",
      "13 Train Loss 3.6795132 Test MSE 1.6092047500517448 Test RE 2.968708233541477\n",
      "14 Train Loss 3.1954732 Test MSE 1.3787267099029847 Test RE 2.747900218690878\n",
      "15 Train Loss 2.8868144 Test MSE 1.1601054895412841 Test RE 2.5206387299495394\n",
      "16 Train Loss 2.5353847 Test MSE 0.9581066918783216 Test RE 2.290703085411349\n",
      "17 Train Loss 2.2468503 Test MSE 0.7760014428585543 Test RE 2.0615464680994644\n",
      "18 Train Loss 1.9199703 Test MSE 0.5765648686938758 Test RE 1.7769939624204762\n",
      "19 Train Loss 1.6764233 Test MSE 0.451718974197499 Test RE 1.5728815952603301\n",
      "20 Train Loss 1.3318647 Test MSE 0.33736905985936344 Test RE 1.359297410295775\n",
      "21 Train Loss 1.170595 Test MSE 0.2869497399542799 Test RE 1.2536167371217892\n",
      "22 Train Loss 1.1012346 Test MSE 0.277348735002702 Test RE 1.232466032599556\n",
      "23 Train Loss 0.95509547 Test MSE 0.260464677965961 Test RE 1.1943628382629368\n",
      "24 Train Loss 0.8006856 Test MSE 0.21799709756847857 Test RE 1.0926654721677742\n",
      "25 Train Loss 0.6821118 Test MSE 0.1854175849477804 Test RE 1.0077140740588804\n",
      "26 Train Loss 0.6592114 Test MSE 0.17891248102843146 Test RE 0.9898791646576566\n",
      "27 Train Loss 0.5452589 Test MSE 0.1620621295850502 Test RE 0.9421122151784079\n",
      "28 Train Loss 0.4932322 Test MSE 0.15134125350380934 Test RE 0.9104173538785861\n",
      "29 Train Loss 0.44878894 Test MSE 0.14073783041041205 Test RE 0.8779449597502356\n",
      "30 Train Loss 0.41177428 Test MSE 0.128342413080592 Test RE 0.8383916887071392\n",
      "31 Train Loss 0.38773826 Test MSE 0.1248496081733647 Test RE 0.8269046914176833\n",
      "32 Train Loss 0.317625 Test MSE 0.10922039678872753 Test RE 0.7734170449128626\n",
      "33 Train Loss 0.309607 Test MSE 0.1068286381257786 Test RE 0.7649018477153808\n",
      "34 Train Loss 0.2917923 Test MSE 0.105786301909034 Test RE 0.761161094053663\n",
      "35 Train Loss 0.27453095 Test MSE 0.10327447411092448 Test RE 0.7520701646212359\n",
      "36 Train Loss 0.26515228 Test MSE 0.10181193819946852 Test RE 0.7467259028583487\n",
      "37 Train Loss 0.24118392 Test MSE 0.09394359748166708 Test RE 0.7172911253359516\n",
      "38 Train Loss 0.23681612 Test MSE 0.09255662047045468 Test RE 0.7119764167103736\n",
      "39 Train Loss 0.20298032 Test MSE 0.07044471756200747 Test RE 0.6211351121200922\n",
      "40 Train Loss 0.20100495 Test MSE 0.06910232704925214 Test RE 0.6151884886244056\n",
      "41 Train Loss 0.1945997 Test MSE 0.06535327112643406 Test RE 0.5982676626397574\n",
      "42 Train Loss 0.16564903 Test MSE 0.04430369678756957 Test RE 0.4925857535267162\n",
      "43 Train Loss 0.15780862 Test MSE 0.041934300440036855 Test RE 0.47923283369761716\n",
      "44 Train Loss 0.1552121 Test MSE 0.038494119270109754 Test RE 0.4591547249901463\n",
      "45 Train Loss 0.15283199 Test MSE 0.03420622750844579 Test RE 0.4328271091474264\n",
      "46 Train Loss 0.14374124 Test MSE 0.030950243238603525 Test RE 0.4117123525674127\n",
      "47 Train Loss 0.13174294 Test MSE 0.02753640909324586 Test RE 0.38834303158399125\n",
      "48 Train Loss 0.1258877 Test MSE 0.025308229697403248 Test RE 0.37229975272746346\n",
      "49 Train Loss 0.12051948 Test MSE 0.022980752400874042 Test RE 0.3547676292852575\n",
      "50 Train Loss 0.11746184 Test MSE 0.023631990309575322 Test RE 0.35975928711692046\n",
      "51 Train Loss 0.1141929 Test MSE 0.02483318455743272 Test RE 0.36878909630195644\n",
      "52 Train Loss 0.11057278 Test MSE 0.022130327700689286 Test RE 0.348141492230801\n",
      "53 Train Loss 0.10226699 Test MSE 0.01916594588018986 Test RE 0.32398655860034353\n",
      "54 Train Loss 0.09049505 Test MSE 0.01760460953496647 Test RE 0.31050962199841936\n",
      "55 Train Loss 0.087306544 Test MSE 0.01796768130626401 Test RE 0.31369520619707925\n",
      "56 Train Loss 0.08623721 Test MSE 0.016844236081592648 Test RE 0.30372988372029164\n",
      "57 Train Loss 0.084896736 Test MSE 0.015220590842480005 Test RE 0.28872049908633857\n",
      "58 Train Loss 0.08385391 Test MSE 0.014308137241335906 Test RE 0.27993255794714117\n",
      "59 Train Loss 0.08093924 Test MSE 0.01316095711029545 Test RE 0.26847608176762194\n",
      "60 Train Loss 0.0791737 Test MSE 0.012854930618309694 Test RE 0.2653363389377199\n",
      "61 Train Loss 0.07786244 Test MSE 0.012073866701541605 Test RE 0.25714912592690176\n",
      "62 Train Loss 0.07738755 Test MSE 0.012271603874423779 Test RE 0.2592462768281667\n",
      "63 Train Loss 0.075613625 Test MSE 0.01170459935824508 Test RE 0.25318626397421506\n",
      "64 Train Loss 0.07404202 Test MSE 0.011900211949010342 Test RE 0.25529317954594094\n",
      "65 Train Loss 0.07294489 Test MSE 0.011507313875458912 Test RE 0.2510434205058515\n",
      "66 Train Loss 0.07088193 Test MSE 0.011662833700367225 Test RE 0.25273413653720106\n",
      "67 Train Loss 0.069115624 Test MSE 0.011086235394431047 Test RE 0.24640749400326073\n",
      "68 Train Loss 0.06575359 Test MSE 0.01074350380552701 Test RE 0.24256874152604607\n",
      "69 Train Loss 0.06360564 Test MSE 0.010190365338974921 Test RE 0.2362417990335683\n",
      "70 Train Loss 0.06224514 Test MSE 0.009440746389392652 Test RE 0.22738668465943435\n",
      "71 Train Loss 0.061077967 Test MSE 0.008581103447591635 Test RE 0.21678709852144767\n",
      "72 Train Loss 0.059552275 Test MSE 0.008472648685773661 Test RE 0.21541277900613068\n",
      "73 Train Loss 0.059050914 Test MSE 0.008769264836685641 Test RE 0.21915100022091907\n",
      "74 Train Loss 0.05778271 Test MSE 0.008871813364523227 Test RE 0.22042866101454286\n",
      "75 Train Loss 0.057437904 Test MSE 0.009074217452176484 Test RE 0.22292894216358045\n",
      "76 Train Loss 0.05692317 Test MSE 0.00907775219494339 Test RE 0.2229723574633851\n",
      "77 Train Loss 0.05626218 Test MSE 0.008472230171529734 Test RE 0.21540745868464622\n",
      "78 Train Loss 0.055817343 Test MSE 0.008156358828489028 Test RE 0.21135378314057712\n",
      "79 Train Loss 0.05443851 Test MSE 0.007741824162645413 Test RE 0.20591288127815688\n",
      "80 Train Loss 0.052185144 Test MSE 0.006935787981776082 Test RE 0.19489907074561397\n",
      "81 Train Loss 0.051138483 Test MSE 0.007001677175394867 Test RE 0.19582264193053972\n",
      "82 Train Loss 0.050875742 Test MSE 0.006927307242441287 Test RE 0.19477987781493214\n",
      "83 Train Loss 0.050219458 Test MSE 0.006710917910506311 Test RE 0.19171355798276735\n",
      "84 Train Loss 0.04934153 Test MSE 0.006764148045817926 Test RE 0.1924723798431537\n",
      "85 Train Loss 0.049118236 Test MSE 0.006288993203845899 Test RE 0.185589083356785\n",
      "86 Train Loss 0.048489843 Test MSE 0.0063143076567582215 Test RE 0.185962224793501\n",
      "87 Train Loss 0.046510827 Test MSE 0.00622311820033728 Test RE 0.18461453426501978\n",
      "88 Train Loss 0.043884534 Test MSE 0.006408540831841581 Test RE 0.1873447131779452\n",
      "89 Train Loss 0.042116944 Test MSE 0.006745812514692959 Test RE 0.19221133602063842\n",
      "90 Train Loss 0.04098787 Test MSE 0.006221990820783846 Test RE 0.18459781113108734\n",
      "91 Train Loss 0.040884234 Test MSE 0.006154629858617729 Test RE 0.18359583883026673\n",
      "92 Train Loss 0.040776394 Test MSE 0.00610812292613395 Test RE 0.18290086035220007\n",
      "93 Train Loss 0.040280234 Test MSE 0.005812950608718456 Test RE 0.17842683853266342\n",
      "94 Train Loss 0.03929439 Test MSE 0.00564704249951171 Test RE 0.17586215560642351\n",
      "95 Train Loss 0.0377392 Test MSE 0.005048713767169322 Test RE 0.1662846798992404\n",
      "96 Train Loss 0.035909064 Test MSE 0.005028086973532815 Test RE 0.16594464970629902\n",
      "97 Train Loss 0.03535302 Test MSE 0.005156015537637703 Test RE 0.16804243761094523\n",
      "98 Train Loss 0.03513461 Test MSE 0.0051944549381896955 Test RE 0.168667673898735\n",
      "99 Train Loss 0.035046708 Test MSE 0.005069121883370597 Test RE 0.16662042230278706\n",
      "100 Train Loss 0.034997296 Test MSE 0.005081788465914293 Test RE 0.16682846568819015\n",
      "101 Train Loss 0.034716003 Test MSE 0.004881636830187985 Test RE 0.1635101050256493\n",
      "102 Train Loss 0.034270756 Test MSE 0.004710758759667794 Test RE 0.16062283842845324\n",
      "103 Train Loss 0.033758312 Test MSE 0.004143739755012163 Test RE 0.15064617184267642\n",
      "104 Train Loss 0.03246396 Test MSE 0.003761121229410519 Test RE 0.14352267823184472\n",
      "105 Train Loss 0.030935798 Test MSE 0.0031416608024822865 Test RE 0.13117211188743946\n",
      "106 Train Loss 0.030284029 Test MSE 0.0031310030981769018 Test RE 0.13094943010924284\n",
      "107 Train Loss 0.029885715 Test MSE 0.0031838388352189834 Test RE 0.13204969484725715\n",
      "108 Train Loss 0.029685307 Test MSE 0.0030900503776302864 Test RE 0.13009021875131044\n",
      "109 Train Loss 0.029571436 Test MSE 0.002921757455843347 Test RE 0.126498083123212\n",
      "110 Train Loss 0.029515125 Test MSE 0.0029664351436160985 Test RE 0.1274615785928383\n",
      "111 Train Loss 0.029445412 Test MSE 0.002984263064432024 Test RE 0.12784401927768935\n",
      "112 Train Loss 0.029254338 Test MSE 0.002824050656691846 Test RE 0.12436498028834773\n",
      "113 Train Loss 0.02912973 Test MSE 0.0027654632151839415 Test RE 0.12306818822317048\n",
      "114 Train Loss 0.02889475 Test MSE 0.0026219393891960575 Test RE 0.11983210492353519\n",
      "115 Train Loss 0.028373195 Test MSE 0.0024744123097368734 Test RE 0.11641203944986586\n",
      "116 Train Loss 0.027516413 Test MSE 0.0024053588599826053 Test RE 0.11477618978501782\n",
      "117 Train Loss 0.026902897 Test MSE 0.0023329176656439613 Test RE 0.11303464375778326\n",
      "118 Train Loss 0.026669195 Test MSE 0.0022529062010823017 Test RE 0.11107937289366096\n",
      "119 Train Loss 0.026451146 Test MSE 0.002168776922777494 Test RE 0.1089856467472154\n",
      "120 Train Loss 0.026212828 Test MSE 0.002045796050062531 Test RE 0.10585052878591548\n",
      "121 Train Loss 0.026012635 Test MSE 0.001944387699505598 Test RE 0.10319372666498669\n",
      "122 Train Loss 0.025876839 Test MSE 0.001948974815170563 Test RE 0.10331538005374179\n",
      "123 Train Loss 0.0257347 Test MSE 0.0019611198045136648 Test RE 0.1036367837703813\n",
      "124 Train Loss 0.0254701 Test MSE 0.0018839502614793962 Test RE 0.10157728026514995\n",
      "125 Train Loss 0.025380172 Test MSE 0.001853516702798542 Test RE 0.10075349404484789\n",
      "126 Train Loss 0.025315266 Test MSE 0.001806898551911074 Test RE 0.09947839025386788\n",
      "127 Train Loss 0.025241867 Test MSE 0.001780792732333456 Test RE 0.09875715069342901\n",
      "128 Train Loss 0.025171239 Test MSE 0.00169969471060582 Test RE 0.09648222837917443\n",
      "129 Train Loss 0.02508232 Test MSE 0.0017417648052926614 Test RE 0.0976689726291584\n",
      "130 Train Loss 0.024939649 Test MSE 0.001754879227221622 Test RE 0.0980359768599578\n",
      "131 Train Loss 0.024763703 Test MSE 0.0016884566450927323 Test RE 0.09616273812009897\n",
      "132 Train Loss 0.0246594 Test MSE 0.0016396496183561727 Test RE 0.09476269213046434\n",
      "133 Train Loss 0.024521142 Test MSE 0.0017626507566634979 Test RE 0.0982528145258666\n",
      "134 Train Loss 0.024303868 Test MSE 0.0017405033218962835 Test RE 0.09763359755913982\n",
      "135 Train Loss 0.023877995 Test MSE 0.001571295290816989 Test RE 0.09276641395055366\n",
      "136 Train Loss 0.02343779 Test MSE 0.001486853040109387 Test RE 0.09023933508673933\n",
      "137 Train Loss 0.022895377 Test MSE 0.001357957526241915 Test RE 0.08623924728806041\n",
      "138 Train Loss 0.02234952 Test MSE 0.0013090560555412767 Test RE 0.08467222769552779\n",
      "139 Train Loss 0.021950226 Test MSE 0.0013187771851828382 Test RE 0.08498603673032502\n",
      "140 Train Loss 0.021787219 Test MSE 0.0013334420119849413 Test RE 0.08545725352085454\n",
      "141 Train Loss 0.021696415 Test MSE 0.0013201109589121823 Test RE 0.08502900209954202\n",
      "142 Train Loss 0.021599282 Test MSE 0.0013154352103447934 Test RE 0.08487828488045576\n",
      "143 Train Loss 0.021521667 Test MSE 0.0013328858143066427 Test RE 0.0854394289424003\n",
      "144 Train Loss 0.021427121 Test MSE 0.0013953704292155127 Test RE 0.08741915849446272\n",
      "145 Train Loss 0.021279039 Test MSE 0.0014676255893743054 Test RE 0.0896539651069597\n",
      "146 Train Loss 0.02115289 Test MSE 0.001553233512781225 Test RE 0.09223170566366327\n",
      "147 Train Loss 0.02108767 Test MSE 0.0015614604395285223 Test RE 0.0924756423867801\n",
      "148 Train Loss 0.020960443 Test MSE 0.0015500945102747737 Test RE 0.09213846083770653\n",
      "149 Train Loss 0.020883344 Test MSE 0.001529294932487234 Test RE 0.09151820400952146\n",
      "150 Train Loss 0.020781111 Test MSE 0.001439224887046254 Test RE 0.08878225961533794\n",
      "151 Train Loss 0.02058559 Test MSE 0.0014625310653766455 Test RE 0.08949822329511525\n",
      "152 Train Loss 0.02029724 Test MSE 0.0014804286596122656 Test RE 0.09004417141210332\n",
      "153 Train Loss 0.01990646 Test MSE 0.0014311776684270713 Test RE 0.08853370508182277\n",
      "154 Train Loss 0.019375427 Test MSE 0.00142435559551406 Test RE 0.08832244379490277\n",
      "155 Train Loss 0.018891556 Test MSE 0.0014078835121427184 Test RE 0.08781025240142375\n",
      "156 Train Loss 0.018794406 Test MSE 0.001432288523255494 Test RE 0.08856805756625816\n",
      "157 Train Loss 0.018606815 Test MSE 0.0013822400017357796 Test RE 0.08700687943412966\n",
      "158 Train Loss 0.018104928 Test MSE 0.001402401487968259 Test RE 0.0876391276475694\n",
      "159 Train Loss 0.018004695 Test MSE 0.0013963682122758161 Test RE 0.08745040817779032\n",
      "160 Train Loss 0.017964922 Test MSE 0.0014005887743830977 Test RE 0.08758246912000499\n",
      "161 Train Loss 0.017933512 Test MSE 0.0013472644239427296 Test RE 0.08589903500861419\n",
      "162 Train Loss 0.017912526 Test MSE 0.0013185695840273052 Test RE 0.08497934724023229\n",
      "163 Train Loss 0.017824803 Test MSE 0.0013159602280633019 Test RE 0.08489522154048608\n",
      "164 Train Loss 0.017768849 Test MSE 0.0012564017298478543 Test RE 0.08295185969021149\n",
      "165 Train Loss 0.01770591 Test MSE 0.0012398783586543146 Test RE 0.08240459018681949\n",
      "166 Train Loss 0.017648634 Test MSE 0.0012753174524125013 Test RE 0.08357396665897353\n",
      "167 Train Loss 0.017578682 Test MSE 0.0012805786892887635 Test RE 0.08374617863518373\n",
      "168 Train Loss 0.017543422 Test MSE 0.0012888454803340987 Test RE 0.08401605601560448\n",
      "169 Train Loss 0.017482521 Test MSE 0.00132573037817624 Test RE 0.0852097847079848\n",
      "170 Train Loss 0.017429102 Test MSE 0.0013108647536921579 Test RE 0.08473070251699127\n",
      "171 Train Loss 0.017288245 Test MSE 0.001436008206931515 Test RE 0.0886829895565235\n",
      "172 Train Loss 0.017160568 Test MSE 0.0015783959829247943 Test RE 0.09297578365027134\n",
      "173 Train Loss 0.017063126 Test MSE 0.001528239802513953 Test RE 0.09148662728100743\n",
      "174 Train Loss 0.01693001 Test MSE 0.001565151376184298 Test RE 0.09258487354818222\n",
      "175 Train Loss 0.016832612 Test MSE 0.0015837783735620605 Test RE 0.09313417422387359\n",
      "176 Train Loss 0.016651845 Test MSE 0.0015989263756388595 Test RE 0.09357850385473437\n",
      "177 Train Loss 0.016484603 Test MSE 0.0015483456110158428 Test RE 0.09208646840679256\n",
      "178 Train Loss 0.016361302 Test MSE 0.0015735308968602203 Test RE 0.09283238354966139\n",
      "179 Train Loss 0.016210753 Test MSE 0.0014728756129841361 Test RE 0.0898141780561911\n",
      "180 Train Loss 0.016113704 Test MSE 0.0014655047328915403 Test RE 0.08958916249727458\n",
      "181 Train Loss 0.016033608 Test MSE 0.0014077673479146763 Test RE 0.08780662972223416\n",
      "182 Train Loss 0.015876653 Test MSE 0.0014443253006970137 Test RE 0.08893943650058034\n",
      "183 Train Loss 0.015814587 Test MSE 0.0014471116218630537 Test RE 0.08902518395369374\n",
      "184 Train Loss 0.015646199 Test MSE 0.0014118480733255631 Test RE 0.08793380111091871\n",
      "185 Train Loss 0.015521522 Test MSE 0.0013724888634569265 Test RE 0.08669943726777629\n",
      "186 Train Loss 0.015426937 Test MSE 0.0013986790497607074 Test RE 0.0875227387214076\n",
      "187 Train Loss 0.015385298 Test MSE 0.0013868150450526205 Test RE 0.08715075148356562\n",
      "188 Train Loss 0.015340755 Test MSE 0.0013734656680135238 Test RE 0.08673028390877345\n",
      "189 Train Loss 0.01531534 Test MSE 0.0013923481770582475 Test RE 0.08732443599280869\n",
      "190 Train Loss 0.0152912885 Test MSE 0.0013776360104932627 Test RE 0.08686185648909468\n",
      "191 Train Loss 0.01522313 Test MSE 0.0013481427659055322 Test RE 0.08592703115865659\n",
      "192 Train Loss 0.015201315 Test MSE 0.0013379475871464577 Test RE 0.08560150777984203\n",
      "193 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "194 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "195 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "196 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "197 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "198 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "199 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "200 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "201 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "202 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "203 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "204 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "205 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "206 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "207 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "208 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "209 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "210 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "211 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "212 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "213 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "214 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "215 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "216 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "217 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "218 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "219 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "220 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "221 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "222 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "223 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "224 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "225 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "226 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "227 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "228 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "229 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "230 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "231 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "232 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "233 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "234 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "235 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "236 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "237 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "238 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "239 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "240 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "241 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "242 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "243 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "244 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "245 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "246 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "247 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "248 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "249 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "250 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "251 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "252 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "253 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "254 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "255 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "256 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "257 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "258 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "259 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "260 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "261 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "262 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "263 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "264 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "265 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "266 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "267 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "268 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "269 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "270 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "271 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "272 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "273 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "274 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "275 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "276 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "277 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "278 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "279 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "280 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "281 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "282 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "283 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "284 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "285 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "286 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "287 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "288 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "289 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "290 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "291 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "292 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "293 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "294 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "295 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "296 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "297 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "298 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "299 Train Loss 0.015198589 Test MSE 0.001333634339224486 Test RE 0.08546341620543933\n",
      "Training time: 360.36\n",
      "KG_rowdy_low\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 5152.8774 Test MSE 1.2099406063191465 Test RE 2.5742095082789094\n",
      "1 Train Loss 368.91095 Test MSE 1.0628391092895264 Test RE 2.4126572412819116\n",
      "2 Train Loss 58.05275 Test MSE 0.5229760352987417 Test RE 1.6923989928516678\n",
      "3 Train Loss 21.092308 Test MSE 0.3295298109667431 Test RE 1.3434119856075617\n",
      "4 Train Loss 10.925236 Test MSE 0.29926543480308815 Test RE 1.2802363185506525\n",
      "5 Train Loss 6.530452 Test MSE 0.2351751796598383 Test RE 1.1349000268223508\n",
      "6 Train Loss 4.1489196 Test MSE 0.19012731043521333 Test RE 1.0204321105771212\n",
      "7 Train Loss 2.7935984 Test MSE 0.13552094552699182 Test RE 0.8615194279297759\n",
      "8 Train Loss 1.9759392 Test MSE 0.09181116666132647 Test RE 0.7091034805845505\n",
      "9 Train Loss 1.7486286 Test MSE 0.08315647559722787 Test RE 0.6748541163542837\n",
      "10 Train Loss 1.3738323 Test MSE 0.06679453074535013 Test RE 0.6048285951585175\n",
      "11 Train Loss 1.0160965 Test MSE 0.059579727892130674 Test RE 0.5712301499331206\n",
      "12 Train Loss 0.82270956 Test MSE 0.06268552988085692 Test RE 0.5859297031253639\n",
      "13 Train Loss 0.67218053 Test MSE 0.0636579174751066 Test RE 0.5904567305977946\n",
      "14 Train Loss 0.5356 Test MSE 0.05742872636637039 Test RE 0.5608238266899426\n",
      "15 Train Loss 0.48778537 Test MSE 0.05230137817220965 Test RE 0.5352028661279353\n",
      "16 Train Loss 0.38394135 Test MSE 0.045420815301497865 Test RE 0.49875737069840775\n",
      "17 Train Loss 0.3479634 Test MSE 0.04476692756677087 Test RE 0.4951542471654705\n",
      "18 Train Loss 0.295916 Test MSE 0.03520528177712597 Test RE 0.4391023651707494\n",
      "19 Train Loss 0.24772681 Test MSE 0.02738478098772274 Test RE 0.3872723581691191\n",
      "20 Train Loss 0.21109915 Test MSE 0.023312224901972812 Test RE 0.3573170387762231\n",
      "21 Train Loss 0.17909577 Test MSE 0.020827452707568205 Test RE 0.3377380118769626\n",
      "22 Train Loss 0.15930042 Test MSE 0.019197255646947135 Test RE 0.3242510851779205\n",
      "23 Train Loss 0.14287218 Test MSE 0.017428869492587984 Test RE 0.3089558856516668\n",
      "24 Train Loss 0.13093126 Test MSE 0.01628887857039418 Test RE 0.29868090386625423\n",
      "25 Train Loss 0.12023183 Test MSE 0.017446764765033155 Test RE 0.3091144567878154\n",
      "26 Train Loss 0.105091624 Test MSE 0.01796392380740416 Test RE 0.3136624036615015\n",
      "27 Train Loss 0.09750722 Test MSE 0.017593214129426545 Test RE 0.3104091098046952\n",
      "28 Train Loss 0.093543366 Test MSE 0.018382295283209504 Test RE 0.3172939074903241\n",
      "29 Train Loss 0.08436105 Test MSE 0.018320380494913312 Test RE 0.3167591060732615\n",
      "30 Train Loss 0.079753585 Test MSE 0.01755226020667982 Test RE 0.31004761027043887\n",
      "31 Train Loss 0.0736729 Test MSE 0.016880877314846617 Test RE 0.3040600557682315\n",
      "32 Train Loss 0.071954235 Test MSE 0.016460479991660056 Test RE 0.30025006606767696\n",
      "33 Train Loss 0.06586068 Test MSE 0.015242704870430198 Test RE 0.28893016426370555\n",
      "34 Train Loss 0.06455694 Test MSE 0.014952989363404884 Test RE 0.28617116810531745\n",
      "35 Train Loss 0.062330853 Test MSE 0.014175622812383621 Test RE 0.2786332486343549\n",
      "36 Train Loss 0.059918728 Test MSE 0.013508993471519772 Test RE 0.2720027907628914\n",
      "37 Train Loss 0.053649195 Test MSE 0.012900564999045922 Test RE 0.26580688727415974\n",
      "38 Train Loss 0.051995307 Test MSE 0.012491662149959212 Test RE 0.26156039161319705\n",
      "39 Train Loss 0.04764021 Test MSE 0.012148434037462323 Test RE 0.2579419709639081\n",
      "40 Train Loss 0.046653472 Test MSE 0.011871769248014344 Test RE 0.25498790886833644\n",
      "41 Train Loss 0.03975427 Test MSE 0.011119995741076704 Test RE 0.24678239488424522\n",
      "42 Train Loss 0.03847981 Test MSE 0.011391921967458729 Test RE 0.24978155489699083\n",
      "43 Train Loss 0.035527352 Test MSE 0.01116323636193719 Test RE 0.2472617417584508\n",
      "44 Train Loss 0.033398103 Test MSE 0.01109547977327892 Test RE 0.24651020738984755\n",
      "45 Train Loss 0.03224756 Test MSE 0.011300333639766057 Test RE 0.24877543660133\n",
      "46 Train Loss 0.03168154 Test MSE 0.01106151115909733 Test RE 0.24613257482948325\n",
      "47 Train Loss 0.02941449 Test MSE 0.009962815598075237 Test RE 0.23358928108855542\n",
      "48 Train Loss 0.028642815 Test MSE 0.009582027832676452 Test RE 0.2290817953267456\n",
      "49 Train Loss 0.028192576 Test MSE 0.009557758504041532 Test RE 0.22879150258754205\n",
      "50 Train Loss 0.02652628 Test MSE 0.008970541805876047 Test RE 0.22165176876751275\n",
      "51 Train Loss 0.023868851 Test MSE 0.008664787529942278 Test RE 0.21784160240732714\n",
      "52 Train Loss 0.022880342 Test MSE 0.008884190340510535 Test RE 0.22058236630907427\n",
      "53 Train Loss 0.022531448 Test MSE 0.008716946913618599 Test RE 0.21849628870416926\n",
      "54 Train Loss 0.021952273 Test MSE 0.008604147884158309 Test RE 0.21707799275615902\n",
      "55 Train Loss 0.021107163 Test MSE 0.008532927067934815 Test RE 0.21617769451368324\n",
      "56 Train Loss 0.020257892 Test MSE 0.008526602099341826 Test RE 0.2160975596086398\n",
      "57 Train Loss 0.020070631 Test MSE 0.00836810627391829 Test RE 0.21407968524738322\n",
      "58 Train Loss 0.019943058 Test MSE 0.00831800274960623 Test RE 0.21343782852699328\n",
      "59 Train Loss 0.019183518 Test MSE 0.008198871239666089 Test RE 0.21190387426996377\n",
      "60 Train Loss 0.018156894 Test MSE 0.007918179721082374 Test RE 0.20824498021545984\n",
      "61 Train Loss 0.017819501 Test MSE 0.007724043794059665 Test RE 0.2056762890095488\n",
      "62 Train Loss 0.017698891 Test MSE 0.007563341076732161 Test RE 0.20352544216814378\n",
      "63 Train Loss 0.017332371 Test MSE 0.00721042307794045 Test RE 0.19872030180570135\n",
      "64 Train Loss 0.016754167 Test MSE 0.006953775157255893 Test RE 0.19515163136599512\n",
      "65 Train Loss 0.01592806 Test MSE 0.0066311474576334194 Test RE 0.1905707340350132\n",
      "66 Train Loss 0.01540213 Test MSE 0.006419356761287736 Test RE 0.1875027408011954\n",
      "67 Train Loss 0.015157472 Test MSE 0.006186103590913972 Test RE 0.18406467917853558\n",
      "68 Train Loss 0.015043581 Test MSE 0.006043446091541258 Test RE 0.18192994586411612\n",
      "69 Train Loss 0.014630832 Test MSE 0.005941654665272544 Test RE 0.1803912912727418\n",
      "70 Train Loss 0.014168204 Test MSE 0.0054733795091753135 Test RE 0.17313690310202748\n",
      "71 Train Loss 0.013473897 Test MSE 0.005169381715309792 Test RE 0.16826010872610533\n",
      "72 Train Loss 0.012775467 Test MSE 0.004829359175976151 Test RE 0.16263223010224628\n",
      "73 Train Loss 0.012596768 Test MSE 0.004653080276779256 Test RE 0.15963647767237357\n",
      "74 Train Loss 0.012439657 Test MSE 0.004459796995980983 Test RE 0.15628576012668605\n",
      "75 Train Loss 0.012304294 Test MSE 0.00428622645589032 Test RE 0.15321434197694417\n",
      "76 Train Loss 0.012017793 Test MSE 0.003946576890974538 Test RE 0.14701855446982015\n",
      "77 Train Loss 0.011773561 Test MSE 0.0038761501601743595 Test RE 0.1457008752206687\n",
      "78 Train Loss 0.011494343 Test MSE 0.0036256260569876627 Test RE 0.14091374878007082\n",
      "79 Train Loss 0.011043568 Test MSE 0.0033910087796581168 Test RE 0.13627817881695486\n",
      "80 Train Loss 0.010643765 Test MSE 0.00318898272271356 Test RE 0.13215632314690073\n",
      "81 Train Loss 0.010408152 Test MSE 0.0031623309146296525 Test RE 0.13160291860424278\n",
      "82 Train Loss 0.010255068 Test MSE 0.003204741627222864 Test RE 0.13248245728843527\n",
      "83 Train Loss 0.010071531 Test MSE 0.003349957890629718 Test RE 0.13545078864760238\n",
      "84 Train Loss 0.009927716 Test MSE 0.0033482745096715862 Test RE 0.1354167518122303\n",
      "85 Train Loss 0.009776695 Test MSE 0.003293232656534185 Test RE 0.13429909023952788\n",
      "86 Train Loss 0.009507576 Test MSE 0.0031840863184892836 Test RE 0.13205482693103623\n",
      "87 Train Loss 0.009325283 Test MSE 0.0031387502321014757 Test RE 0.13111133605033812\n",
      "88 Train Loss 0.009121496 Test MSE 0.0030898135326780972 Test RE 0.13008523310410425\n",
      "89 Train Loss 0.008794945 Test MSE 0.002966945016867664 Test RE 0.12747253222135527\n",
      "90 Train Loss 0.008293781 Test MSE 0.0028754336702972515 Test RE 0.1254912776550514\n",
      "91 Train Loss 0.008057987 Test MSE 0.002841731936413481 Test RE 0.12475369514248402\n",
      "92 Train Loss 0.007834752 Test MSE 0.0026826617090415575 Test RE 0.12121177730223373\n",
      "93 Train Loss 0.0077804145 Test MSE 0.0026050458750361888 Test RE 0.11944543375081829\n",
      "94 Train Loss 0.0076981606 Test MSE 0.0023796502930314746 Test RE 0.11416117590729331\n",
      "95 Train Loss 0.0076068 Test MSE 0.002288329072597152 Test RE 0.11194922802935892\n",
      "96 Train Loss 0.0074691204 Test MSE 0.0021946188256523488 Test RE 0.10963302925133224\n",
      "97 Train Loss 0.0073000453 Test MSE 0.001957286067893817 Test RE 0.10353543593388473\n",
      "98 Train Loss 0.007199707 Test MSE 0.0018065943868478753 Test RE 0.09947001703216685\n",
      "99 Train Loss 0.007056927 Test MSE 0.0016450956317842023 Test RE 0.09491993641782574\n",
      "100 Train Loss 0.0068639023 Test MSE 0.0014884033697701545 Test RE 0.09028636874260275\n",
      "101 Train Loss 0.006718141 Test MSE 0.0014540482039355096 Test RE 0.08923829544993991\n",
      "102 Train Loss 0.006560289 Test MSE 0.0014453784683064357 Test RE 0.0889718568510225\n",
      "103 Train Loss 0.0064207693 Test MSE 0.0013591090127460848 Test RE 0.08627580302901768\n",
      "104 Train Loss 0.006338371 Test MSE 0.0013471642145290123 Test RE 0.08589584036773538\n",
      "105 Train Loss 0.0062635997 Test MSE 0.0013252934042032054 Test RE 0.0851957405525908\n",
      "106 Train Loss 0.0062039266 Test MSE 0.0012603458387414836 Test RE 0.08308195932206479\n",
      "107 Train Loss 0.00612845 Test MSE 0.001255709036330966 Test RE 0.0829289895619146\n",
      "108 Train Loss 0.0061003184 Test MSE 0.0012374573970256057 Test RE 0.08232410010132438\n",
      "109 Train Loss 0.006016533 Test MSE 0.0012202958318806923 Test RE 0.08175125491412673\n",
      "110 Train Loss 0.005926584 Test MSE 0.00118049816694419 Test RE 0.08040712289026991\n",
      "111 Train Loss 0.005841834 Test MSE 0.0012037492357872418 Test RE 0.0811951103245725\n",
      "112 Train Loss 0.005769026 Test MSE 0.0012355036370361866 Test RE 0.08225908571389064\n",
      "113 Train Loss 0.00565025 Test MSE 0.0012220414745665214 Test RE 0.08180970692082243\n",
      "114 Train Loss 0.005591256 Test MSE 0.0012035203050444541 Test RE 0.08118738905662708\n",
      "115 Train Loss 0.0055220127 Test MSE 0.0011539234855006545 Test RE 0.0794969323499623\n",
      "116 Train Loss 0.0054308022 Test MSE 0.001119468902824858 Test RE 0.07830110321661041\n",
      "117 Train Loss 0.0053544063 Test MSE 0.0010885852812515659 Test RE 0.07721347404804856\n",
      "118 Train Loss 0.005283222 Test MSE 0.0010544349951840323 Test RE 0.07599268152930587\n",
      "119 Train Loss 0.0052466956 Test MSE 0.0010554919782176507 Test RE 0.07603076014782494\n",
      "120 Train Loss 0.005217092 Test MSE 0.0010640882005032797 Test RE 0.07633974022088566\n",
      "121 Train Loss 0.0051900474 Test MSE 0.001014882204044111 Test RE 0.07455378255019166\n",
      "122 Train Loss 0.005162806 Test MSE 0.000984028359096914 Test RE 0.07341176596168501\n",
      "123 Train Loss 0.0051288307 Test MSE 0.0009297955163895014 Test RE 0.07136012265650153\n",
      "124 Train Loss 0.0050867377 Test MSE 0.0008988872820448314 Test RE 0.07016402295289176\n",
      "125 Train Loss 0.004949128 Test MSE 0.0008135410345342411 Test RE 0.06675005005876207\n",
      "126 Train Loss 0.004838506 Test MSE 0.0008318190572968949 Test RE 0.06749572972406573\n",
      "127 Train Loss 0.0047258935 Test MSE 0.0007839526573119064 Test RE 0.06552496268771309\n",
      "128 Train Loss 0.004610501 Test MSE 0.0007148023192572822 Test RE 0.06256836991584024\n",
      "129 Train Loss 0.0045216456 Test MSE 0.0006983003682981313 Test RE 0.061841924979793066\n",
      "130 Train Loss 0.00443885 Test MSE 0.0006658534013858226 Test RE 0.06038807355148322\n",
      "131 Train Loss 0.0043885 Test MSE 0.0006803326219336038 Test RE 0.061041122585571886\n",
      "132 Train Loss 0.004366289 Test MSE 0.0006827858702074224 Test RE 0.06115107929616193\n",
      "133 Train Loss 0.0043266662 Test MSE 0.0006663666027307573 Test RE 0.060411340888734746\n",
      "134 Train Loss 0.004276605 Test MSE 0.00065189220129816 Test RE 0.05975162995154045\n",
      "135 Train Loss 0.004212897 Test MSE 0.0006453200080501216 Test RE 0.059449667410891714\n",
      "136 Train Loss 0.004132928 Test MSE 0.0005975108496341762 Test RE 0.05720510167683039\n",
      "137 Train Loss 0.0040708478 Test MSE 0.0005859600224252588 Test RE 0.05664947084292224\n",
      "138 Train Loss 0.004006965 Test MSE 0.0005458258253787766 Test RE 0.05467501415238694\n",
      "139 Train Loss 0.0039335997 Test MSE 0.0005386039933332546 Test RE 0.05431210666172005\n",
      "140 Train Loss 0.0038605605 Test MSE 0.0005428861284652187 Test RE 0.05452758162087623\n",
      "141 Train Loss 0.0036380838 Test MSE 0.00045875745730043825 Test RE 0.05012488999530588\n",
      "142 Train Loss 0.0035672628 Test MSE 0.0004439443061715468 Test RE 0.049308990256017954\n",
      "143 Train Loss 0.0034894312 Test MSE 0.00041825952783144883 Test RE 0.04786133248830869\n",
      "144 Train Loss 0.003456419 Test MSE 0.00042005900461169316 Test RE 0.04796417882566154\n",
      "145 Train Loss 0.0034427396 Test MSE 0.00042044457789317197 Test RE 0.047986187000360066\n",
      "146 Train Loss 0.003433253 Test MSE 0.0004291531338534376 Test RE 0.048480602469146476\n",
      "147 Train Loss 0.0034153124 Test MSE 0.000409466369048153 Test RE 0.0473555606580905\n",
      "148 Train Loss 0.0033522842 Test MSE 0.0003903903989926204 Test RE 0.046239318886705905\n",
      "149 Train Loss 0.0032937273 Test MSE 0.00039201482292359894 Test RE 0.04633542048462704\n",
      "150 Train Loss 0.003186185 Test MSE 0.00042823135086871776 Test RE 0.048428508457112794\n",
      "151 Train Loss 0.0031404078 Test MSE 0.000452605348200933 Test RE 0.049787658818214994\n",
      "152 Train Loss 0.003083301 Test MSE 0.00043391714787268494 Test RE 0.048748950535205275\n",
      "153 Train Loss 0.0030170768 Test MSE 0.0004351776474402113 Test RE 0.04881970536765231\n",
      "154 Train Loss 0.0029530027 Test MSE 0.00045371809934368777 Test RE 0.04984882387582216\n",
      "155 Train Loss 0.0029319052 Test MSE 0.00045340105170288745 Test RE 0.04983140423360359\n",
      "156 Train Loss 0.0029108294 Test MSE 0.0004560149917954967 Test RE 0.04997484138077682\n",
      "157 Train Loss 0.002894807 Test MSE 0.00046329060283303636 Test RE 0.050371932146862454\n",
      "158 Train Loss 0.0028827281 Test MSE 0.00046343305481549277 Test RE 0.05037967569923628\n",
      "159 Train Loss 0.0028605342 Test MSE 0.0004555424168697907 Test RE 0.04994893984335732\n",
      "160 Train Loss 0.0028487495 Test MSE 0.0004482855316811288 Test RE 0.049549494157714764\n",
      "161 Train Loss 0.0028368377 Test MSE 0.00042681681865543576 Test RE 0.04834845786177536\n",
      "162 Train Loss 0.0028241368 Test MSE 0.0004267827636098155 Test RE 0.04834652899942671\n",
      "163 Train Loss 0.0027965382 Test MSE 0.00041523638218554014 Test RE 0.04768804990472471\n",
      "164 Train Loss 0.0027789206 Test MSE 0.00041530033023318987 Test RE 0.047691721837648346\n",
      "165 Train Loss 0.0027568091 Test MSE 0.0004218313987574676 Test RE 0.048065262177463194\n",
      "166 Train Loss 0.002726275 Test MSE 0.00042219914386190953 Test RE 0.0480862088332071\n",
      "167 Train Loss 0.0026784407 Test MSE 0.0004237660205713605 Test RE 0.04817535560841616\n",
      "168 Train Loss 0.0026136606 Test MSE 0.00043716626949130553 Test RE 0.048931123411842255\n",
      "169 Train Loss 0.0025643378 Test MSE 0.0004191435410654495 Test RE 0.04791188449725799\n",
      "170 Train Loss 0.0024952556 Test MSE 0.0003875157500760679 Test RE 0.04606876217402719\n",
      "171 Train Loss 0.002460696 Test MSE 0.0003899340893025965 Test RE 0.046212287460630046\n",
      "172 Train Loss 0.0024293317 Test MSE 0.0003780500349182789 Test RE 0.04550263064844647\n",
      "173 Train Loss 0.0024054882 Test MSE 0.0003759056033882068 Test RE 0.04537339373036326\n",
      "174 Train Loss 0.002393082 Test MSE 0.0003728481666002338 Test RE 0.04518849422592629\n",
      "175 Train Loss 0.0023706192 Test MSE 0.00035608049569053104 Test RE 0.044160700823585086\n",
      "176 Train Loss 0.0023574785 Test MSE 0.0003594869923951152 Test RE 0.0443714329597673\n",
      "177 Train Loss 0.002340246 Test MSE 0.00036109984057871344 Test RE 0.044470858387161444\n",
      "178 Train Loss 0.0023198782 Test MSE 0.0003557977895975861 Test RE 0.04414316689753653\n",
      "179 Train Loss 0.0022926403 Test MSE 0.0003463308503190548 Test RE 0.04355193482739695\n",
      "180 Train Loss 0.002278367 Test MSE 0.0003485462093536905 Test RE 0.043691006139300125\n",
      "181 Train Loss 0.002261267 Test MSE 0.00032613430226668174 Test RE 0.04226297878838269\n",
      "182 Train Loss 0.0022448571 Test MSE 0.0003187486236939759 Test RE 0.04178169198816436\n",
      "183 Train Loss 0.0022222453 Test MSE 0.00030899467846562124 Test RE 0.04113744967895234\n",
      "184 Train Loss 0.0022047788 Test MSE 0.0003064637318348371 Test RE 0.040968626755584034\n",
      "185 Train Loss 0.0021811284 Test MSE 0.00030051421242907036 Test RE 0.04056900648245354\n",
      "186 Train Loss 0.0021614828 Test MSE 0.00028942989470282155 Test RE 0.039813793294299696\n",
      "187 Train Loss 0.0021470503 Test MSE 0.0002766792399502048 Test RE 0.03892692972267983\n",
      "188 Train Loss 0.0021158296 Test MSE 0.0002449663273479411 Test RE 0.03662815637100584\n",
      "189 Train Loss 0.00209356 Test MSE 0.0002247868433509131 Test RE 0.03508708656665547\n",
      "190 Train Loss 0.002077928 Test MSE 0.0002156546590369072 Test RE 0.03436697338839015\n",
      "191 Train Loss 0.0020502815 Test MSE 0.00021050949025104652 Test RE 0.033954528477050556\n",
      "192 Train Loss 0.0020118095 Test MSE 0.00020425137915716884 Test RE 0.033446013666045424\n",
      "193 Train Loss 0.0019819979 Test MSE 0.00016857946365657017 Test RE 0.030385346830871976\n",
      "194 Train Loss 0.0019575737 Test MSE 0.00017406018031881687 Test RE 0.030875327902328106\n",
      "195 Train Loss 0.0019397842 Test MSE 0.0001691939827918902 Test RE 0.030440677981819833\n",
      "196 Train Loss 0.0019393717 Test MSE 0.00016996861212343866 Test RE 0.030510282448845513\n",
      "197 Train Loss 0.0019260388 Test MSE 0.00015518408721030112 Test RE 0.02915314797847474\n",
      "198 Train Loss 0.0019058945 Test MSE 0.00015212155374611677 Test RE 0.02886404814951076\n",
      "199 Train Loss 0.0018976415 Test MSE 0.00015142198859745593 Test RE 0.028797602764696926\n",
      "200 Train Loss 0.0018816172 Test MSE 0.00014292356816612403 Test RE 0.027977814674542092\n",
      "201 Train Loss 0.0018564244 Test MSE 0.00013478813787099373 Test RE 0.027169878779161785\n",
      "202 Train Loss 0.0018392806 Test MSE 0.00013115698642405605 Test RE 0.02680140606466689\n",
      "203 Train Loss 0.0018221762 Test MSE 0.00012941548562926404 Test RE 0.02662287709378793\n",
      "204 Train Loss 0.0018120558 Test MSE 0.00012796561023379667 Test RE 0.026473325534348537\n",
      "205 Train Loss 0.0018001982 Test MSE 0.00012800170011495476 Test RE 0.02647705838029212\n",
      "206 Train Loss 0.0017836939 Test MSE 0.00012614882884416631 Test RE 0.0262847272914808\n",
      "207 Train Loss 0.0017718574 Test MSE 0.00012879467345733593 Test RE 0.026558944733137486\n",
      "208 Train Loss 0.001771348 Test MSE 0.000127466047883415 Test RE 0.026421600663755394\n",
      "209 Train Loss 0.001762956 Test MSE 0.00013139942731745028 Test RE 0.026826165541986472\n",
      "210 Train Loss 0.0017528238 Test MSE 0.0001326154332998465 Test RE 0.02695000799422981\n",
      "211 Train Loss 0.0017367586 Test MSE 0.00014537949430716484 Test RE 0.028217169109930103\n",
      "212 Train Loss 0.0017253101 Test MSE 0.00014723413589181868 Test RE 0.028396585355440998\n",
      "213 Train Loss 0.001704631 Test MSE 0.00014841978991887445 Test RE 0.028510692777797313\n",
      "214 Train Loss 0.001690901 Test MSE 0.00014496644199058067 Test RE 0.028177055276914612\n",
      "215 Train Loss 0.0016724848 Test MSE 0.0001468205814937557 Test RE 0.02835667684283332\n",
      "216 Train Loss 0.0016578973 Test MSE 0.00014849850665168995 Test RE 0.028518252319200927\n",
      "217 Train Loss 0.0016463447 Test MSE 0.00015007472268326513 Test RE 0.028669204255021404\n",
      "218 Train Loss 0.0016358377 Test MSE 0.00014665399926273398 Test RE 0.02834058557318656\n",
      "219 Train Loss 0.0016133568 Test MSE 0.00014951764938544364 Test RE 0.028615945130884977\n",
      "220 Train Loss 0.0015993138 Test MSE 0.00014598711547143754 Test RE 0.02827607518743306\n",
      "221 Train Loss 0.0015867738 Test MSE 0.00013928508318081036 Test RE 0.027619395433291484\n",
      "222 Train Loss 0.0015750519 Test MSE 0.0001378679767342567 Test RE 0.027478534383648296\n",
      "223 Train Loss 0.001562751 Test MSE 0.00013463587593746216 Test RE 0.027154528365860863\n",
      "224 Train Loss 0.001554198 Test MSE 0.0001278411308418346 Test RE 0.026460446350436404\n",
      "225 Train Loss 0.0015450806 Test MSE 0.0001259328991617411 Test RE 0.026262221796536592\n",
      "226 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "227 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "228 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "229 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "230 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "231 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "232 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "233 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "234 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "235 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "236 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "237 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "238 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "239 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "240 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "241 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "242 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "243 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "244 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "245 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "246 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "247 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "248 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "249 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "250 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "251 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "252 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "253 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "254 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "255 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "256 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "257 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "258 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "259 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "260 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "261 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "262 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "263 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "264 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "265 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "266 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "267 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "268 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "269 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "270 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "271 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "272 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "273 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "274 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "275 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "276 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "277 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "278 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "279 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "280 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "281 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "282 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "283 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "284 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "285 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "286 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "287 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "288 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "289 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "290 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "291 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "292 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "293 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "294 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "295 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "296 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "297 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "298 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "299 Train Loss 0.0015401987 Test MSE 0.0001274293058805946 Test RE 0.02641779238496601\n",
      "Training time: 400.64\n",
      "KG_rowdy_low\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 11199.575 Test MSE 0.33637880118022107 Test RE 1.3573010131456273\n",
      "1 Train Loss 3825.7266 Test MSE 1.6319439993659248 Test RE 2.989609672212273\n",
      "2 Train Loss 1704.153 Test MSE 1.6161612320809033 Test RE 2.9751180749694877\n",
      "3 Train Loss 487.16348 Test MSE 2.025995991539691 Test RE 3.3310500253226554\n",
      "4 Train Loss 143.62788 Test MSE 1.6843954445307854 Test RE 3.0372734452192183\n",
      "5 Train Loss 52.42227 Test MSE 1.9490473306414284 Test RE 3.267179962214501\n",
      "6 Train Loss 24.75757 Test MSE 1.6168384612710676 Test RE 2.97574134996726\n",
      "7 Train Loss 16.143078 Test MSE 1.5347136986854648 Test RE 2.8991824620631896\n",
      "8 Train Loss 8.539793 Test MSE 1.1201900711613888 Test RE 2.4768957246929935\n",
      "9 Train Loss 6.1271834 Test MSE 0.8573042242256138 Test RE 2.166852482209302\n",
      "10 Train Loss 4.571027 Test MSE 0.7662792918883001 Test RE 2.0485916992793265\n",
      "11 Train Loss 3.14957 Test MSE 0.6058495988144399 Test RE 1.8215633361838255\n",
      "12 Train Loss 2.5413702 Test MSE 0.5607137821453745 Test RE 1.7523969133956703\n",
      "13 Train Loss 2.194915 Test MSE 0.5222916814811259 Test RE 1.691291314124875\n",
      "14 Train Loss 1.7949558 Test MSE 0.402606684068874 Test RE 1.4849175988458045\n",
      "15 Train Loss 1.4042808 Test MSE 0.3332204301355813 Test RE 1.3509139126382057\n",
      "16 Train Loss 1.2628747 Test MSE 0.2861714684120075 Test RE 1.2519155390686547\n",
      "17 Train Loss 1.052827 Test MSE 0.2469786084991684 Test RE 1.163031650694891\n",
      "18 Train Loss 0.88843316 Test MSE 0.14781667153166275 Test RE 0.8997535595224389\n",
      "19 Train Loss 0.82614183 Test MSE 0.07185806958497264 Test RE 0.6273351717047488\n",
      "20 Train Loss 0.75911087 Test MSE 0.06204723817876105 Test RE 0.5829389731317003\n",
      "21 Train Loss 0.6722606 Test MSE 0.08867150702649766 Test RE 0.6968734341008631\n",
      "22 Train Loss 0.60615396 Test MSE 0.07550891249561649 Test RE 0.6430740306638826\n",
      "23 Train Loss 0.51300716 Test MSE 0.05925954176308537 Test RE 0.5696931643745463\n",
      "24 Train Loss 0.4341313 Test MSE 0.0725319801157414 Test RE 0.630269992979876\n",
      "25 Train Loss 0.42009553 Test MSE 0.0758143715777615 Test RE 0.6443734437940978\n",
      "26 Train Loss 0.35976222 Test MSE 0.06892122540655546 Test RE 0.6143818244884828\n",
      "27 Train Loss 0.3391667 Test MSE 0.07011131962032756 Test RE 0.61966352718303\n",
      "28 Train Loss 0.29711497 Test MSE 0.07065419672107166 Test RE 0.6220579511875651\n",
      "29 Train Loss 0.24829113 Test MSE 0.06212210724736263 Test RE 0.5832905676703195\n",
      "30 Train Loss 0.2119473 Test MSE 0.035720586067286994 Test RE 0.44230429021307477\n",
      "31 Train Loss 0.1982316 Test MSE 0.04009016176743502 Test RE 0.46857678512108697\n",
      "32 Train Loss 0.15699922 Test MSE 0.026781866378673774 Test RE 0.38298545749070195\n",
      "33 Train Loss 0.12705666 Test MSE 0.017568590199286124 Test RE 0.3101918053212577\n",
      "34 Train Loss 0.1132895 Test MSE 0.015844733019520774 Test RE 0.2945807251761647\n",
      "35 Train Loss 0.10874925 Test MSE 0.01930226954397345 Test RE 0.32513674380761315\n",
      "36 Train Loss 0.094721474 Test MSE 0.01476600201654501 Test RE 0.2843762518040067\n",
      "37 Train Loss 0.09039328 Test MSE 0.011479579105713825 Test RE 0.2507407072961406\n",
      "38 Train Loss 0.085964866 Test MSE 0.011982464879705838 Test RE 0.2561739391551239\n",
      "39 Train Loss 0.06897381 Test MSE 0.008140273069222798 Test RE 0.2111452670585505\n",
      "40 Train Loss 0.067972176 Test MSE 0.007773531658305413 Test RE 0.20633411988198644\n",
      "41 Train Loss 0.06311836 Test MSE 0.008375586297808192 Test RE 0.21417534389600765\n",
      "42 Train Loss 0.059328392 Test MSE 0.009128431596615973 Test RE 0.22359389777966915\n",
      "43 Train Loss 0.05850649 Test MSE 0.009579533123830829 Test RE 0.229051972329285\n",
      "44 Train Loss 0.054931752 Test MSE 0.008731449592136245 Test RE 0.21867797298863823\n",
      "45 Train Loss 0.050993834 Test MSE 0.007515122869020564 Test RE 0.20287564182871998\n",
      "46 Train Loss 0.050078038 Test MSE 0.007079364066122727 Test RE 0.1969060170897104\n",
      "47 Train Loss 0.049090166 Test MSE 0.007502365688719422 Test RE 0.20270337449017378\n",
      "48 Train Loss 0.040864848 Test MSE 0.006577613420827108 Test RE 0.18979992515000793\n",
      "49 Train Loss 0.038581103 Test MSE 0.007606311065332153 Test RE 0.20410277297076762\n",
      "50 Train Loss 0.038284764 Test MSE 0.007520077045677368 Test RE 0.20294250142613796\n",
      "51 Train Loss 0.037493274 Test MSE 0.006905943383022086 Test RE 0.19447929469737468\n",
      "52 Train Loss 0.034809437 Test MSE 0.006432468803423129 Test RE 0.18769413767843787\n",
      "53 Train Loss 0.034582507 Test MSE 0.006296082355505734 Test RE 0.18569365483258124\n",
      "54 Train Loss 0.033095118 Test MSE 0.005945818421039941 Test RE 0.18045448694626193\n",
      "55 Train Loss 0.030420285 Test MSE 0.005244300100738405 Test RE 0.1694749958615729\n",
      "56 Train Loss 0.026828904 Test MSE 0.005406385889659817 Test RE 0.17207405157309738\n",
      "57 Train Loss 0.026556132 Test MSE 0.005227159353915104 Test RE 0.16919780867900647\n",
      "58 Train Loss 0.025963435 Test MSE 0.00440161223638048 Test RE 0.15526292165563607\n",
      "59 Train Loss 0.024320057 Test MSE 0.0031781053632865906 Test RE 0.13193074341562144\n",
      "60 Train Loss 0.023071589 Test MSE 0.0031836297162909206 Test RE 0.13204535817296006\n",
      "61 Train Loss 0.022880582 Test MSE 0.0032061275458880804 Test RE 0.13251110079313652\n",
      "62 Train Loss 0.02276012 Test MSE 0.0032600277401907247 Test RE 0.13362032133593113\n",
      "63 Train Loss 0.022312436 Test MSE 0.003326909883017229 Test RE 0.13498402769883772\n",
      "64 Train Loss 0.021041602 Test MSE 0.0031818108200197294 Test RE 0.13200763218945344\n",
      "65 Train Loss 0.020235332 Test MSE 0.0030743894037918128 Test RE 0.12976013877956905\n",
      "66 Train Loss 0.019963853 Test MSE 0.002814841353163352 Test RE 0.12416203596436651\n",
      "67 Train Loss 0.019753534 Test MSE 0.0025908367066707237 Test RE 0.11911923201450555\n",
      "68 Train Loss 0.019639969 Test MSE 0.0025091930170180574 Test RE 0.11722733692813257\n",
      "69 Train Loss 0.01944233 Test MSE 0.0024772591997601493 Test RE 0.11647898807481892\n",
      "70 Train Loss 0.019237986 Test MSE 0.0023184098979057905 Test RE 0.11268263009037602\n",
      "71 Train Loss 0.018549329 Test MSE 0.002538293497711161 Test RE 0.1179051520442413\n",
      "72 Train Loss 0.018107561 Test MSE 0.0024766572600199424 Test RE 0.11646483582311101\n",
      "73 Train Loss 0.017962603 Test MSE 0.0025770920275557043 Test RE 0.1188028413960873\n",
      "74 Train Loss 0.017881358 Test MSE 0.002524864852969043 Test RE 0.11759285442202053\n",
      "75 Train Loss 0.017853588 Test MSE 0.0025363118141804026 Test RE 0.11785911790365362\n",
      "76 Train Loss 0.017707547 Test MSE 0.0026467252401810907 Test RE 0.12039717410857091\n",
      "77 Train Loss 0.017216166 Test MSE 0.002592589801865919 Test RE 0.11915952634082115\n",
      "78 Train Loss 0.016676817 Test MSE 0.0024343352569146815 Test RE 0.11546545162892954\n",
      "79 Train Loss 0.015761286 Test MSE 0.002449112344127446 Test RE 0.11581537498719481\n",
      "80 Train Loss 0.0149625335 Test MSE 0.0025081256046351986 Test RE 0.11720239998214717\n",
      "81 Train Loss 0.014639446 Test MSE 0.0023646597194923124 Test RE 0.11380102946617912\n",
      "82 Train Loss 0.0144764315 Test MSE 0.0022545852292844963 Test RE 0.11112075736481752\n",
      "83 Train Loss 0.014396958 Test MSE 0.0022407307320397598 Test RE 0.11077881096050643\n",
      "84 Train Loss 0.014362776 Test MSE 0.0022418513144915075 Test RE 0.11080650756715063\n",
      "85 Train Loss 0.014330249 Test MSE 0.0023659637592553613 Test RE 0.11383240408896701\n",
      "86 Train Loss 0.014273892 Test MSE 0.0023119962107102207 Test RE 0.11252665860643245\n",
      "87 Train Loss 0.014050246 Test MSE 0.0021858205236221623 Test RE 0.10941304726667249\n",
      "88 Train Loss 0.013613337 Test MSE 0.002169742914385522 Test RE 0.10900991561013852\n",
      "89 Train Loss 0.012889159 Test MSE 0.0024723980075410973 Test RE 0.11636464703131054\n",
      "90 Train Loss 0.012096627 Test MSE 0.002589557224103515 Test RE 0.11908981491499358\n",
      "91 Train Loss 0.011643051 Test MSE 0.0025825283073301207 Test RE 0.11892808047155126\n",
      "92 Train Loss 0.011305179 Test MSE 0.002761936264566675 Test RE 0.12298968529784252\n",
      "93 Train Loss 0.011172523 Test MSE 0.002599733633626366 Test RE 0.11932358430519645\n",
      "94 Train Loss 0.01108228 Test MSE 0.0025980018072340747 Test RE 0.11928383366378681\n",
      "95 Train Loss 0.011014697 Test MSE 0.0025820529098328306 Test RE 0.11891713369629084\n",
      "96 Train Loss 0.010953476 Test MSE 0.0025933499690163147 Test RE 0.11917699430253204\n",
      "97 Train Loss 0.010880483 Test MSE 0.002514389289838774 Test RE 0.11734865684611416\n",
      "98 Train Loss 0.0108519085 Test MSE 0.0024731867108023775 Test RE 0.11638320590779233\n",
      "99 Train Loss 0.010695426 Test MSE 0.0023886494987606266 Test RE 0.11437683583911891\n",
      "100 Train Loss 0.010461381 Test MSE 0.0023252375780002726 Test RE 0.11284843238115634\n",
      "101 Train Loss 0.0100051565 Test MSE 0.0026551385337697173 Test RE 0.12058837889719398\n",
      "102 Train Loss 0.009824035 Test MSE 0.0026916467340827757 Test RE 0.12141459461107378\n",
      "103 Train Loss 0.009711135 Test MSE 0.0025789055519527897 Test RE 0.11884463539460545\n",
      "104 Train Loss 0.009641355 Test MSE 0.0026611000062916647 Test RE 0.12072367903882623\n",
      "105 Train Loss 0.009626902 Test MSE 0.0026307576123828385 Test RE 0.12003344810696591\n",
      "106 Train Loss 0.009552919 Test MSE 0.0026722250297804766 Test RE 0.12097576520470196\n",
      "107 Train Loss 0.009519227 Test MSE 0.0026649796282699344 Test RE 0.12081164861065703\n",
      "108 Train Loss 0.009500847 Test MSE 0.0026437492972601504 Test RE 0.1203294685748069\n",
      "109 Train Loss 0.009478323 Test MSE 0.002662993994154084 Test RE 0.12076663280090746\n",
      "110 Train Loss 0.00945327 Test MSE 0.002638608557024229 Test RE 0.12021242201129692\n",
      "111 Train Loss 0.009426612 Test MSE 0.002710735846714727 Test RE 0.12184436908387207\n",
      "112 Train Loss 0.009368332 Test MSE 0.0028245594219937125 Test RE 0.1243761822363608\n",
      "113 Train Loss 0.00933871 Test MSE 0.0027806179228428846 Test RE 0.12340493366444878\n",
      "114 Train Loss 0.00917493 Test MSE 0.0024547548255154797 Test RE 0.11594871107987853\n",
      "115 Train Loss 0.00896278 Test MSE 0.002232206010116181 Test RE 0.11056788457697221\n",
      "116 Train Loss 0.008642502 Test MSE 0.0017828279313313214 Test RE 0.09881356743896501\n",
      "117 Train Loss 0.008498865 Test MSE 0.0016673114868544622 Test RE 0.09555870049908038\n",
      "118 Train Loss 0.008441364 Test MSE 0.0016199482760479932 Test RE 0.09419165723032881\n",
      "119 Train Loss 0.00839147 Test MSE 0.001649331902867339 Test RE 0.09504207158176203\n",
      "120 Train Loss 0.008351506 Test MSE 0.001625344209128591 Test RE 0.09434839968696647\n",
      "121 Train Loss 0.008317806 Test MSE 0.0016259402684380853 Test RE 0.09436569820338386\n",
      "122 Train Loss 0.008280595 Test MSE 0.0016578202044246807 Test RE 0.09528632516230434\n",
      "123 Train Loss 0.008258127 Test MSE 0.0016136744642022492 Test RE 0.09400908535132557\n",
      "124 Train Loss 0.008245912 Test MSE 0.0016123502370201477 Test RE 0.0939705041692728\n",
      "125 Train Loss 0.008216374 Test MSE 0.0015465956120625352 Test RE 0.09203441388397278\n",
      "126 Train Loss 0.008140215 Test MSE 0.0014491963192621755 Test RE 0.08908928535909723\n",
      "127 Train Loss 0.00784692 Test MSE 0.0013281650508706248 Test RE 0.0852879917020275\n",
      "128 Train Loss 0.007488304 Test MSE 0.001219404537837098 Test RE 0.08172139423837527\n",
      "129 Train Loss 0.0071643414 Test MSE 0.000997788796392181 Test RE 0.07392327101346248\n",
      "130 Train Loss 0.007090358 Test MSE 0.0010132746356208513 Test RE 0.07449471273677657\n",
      "131 Train Loss 0.0070508504 Test MSE 0.0009431218194892986 Test RE 0.07186968813325695\n",
      "132 Train Loss 0.0070235804 Test MSE 0.0008871343467028292 Test RE 0.06970381702658195\n",
      "133 Train Loss 0.007014263 Test MSE 0.0008685632414280304 Test RE 0.06897037492497705\n",
      "134 Train Loss 0.0070042275 Test MSE 0.0008605851347547135 Test RE 0.06865288369732407\n",
      "135 Train Loss 0.0070014033 Test MSE 0.0008575565388639049 Test RE 0.06853197462814622\n",
      "136 Train Loss 0.0070013953 Test MSE 0.0008572090736877119 Test RE 0.06851808931205565\n",
      "137 Train Loss 0.0070010046 Test MSE 0.0008567596994279279 Test RE 0.06850012735682157\n",
      "138 Train Loss 0.0070007783 Test MSE 0.000855231627407793 Test RE 0.06843901346332647\n",
      "139 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "140 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "141 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "142 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "143 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "144 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "145 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "146 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "147 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "148 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "149 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "150 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "151 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "152 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "153 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "154 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "155 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "156 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "157 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "158 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "159 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "160 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "161 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "162 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "163 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "164 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "165 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "166 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "167 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "168 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "169 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "170 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "171 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "172 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "173 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "174 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "175 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "176 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "177 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "178 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "179 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "180 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "181 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "182 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "183 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "184 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "185 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "186 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "187 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "188 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "189 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "190 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "191 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "192 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "193 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "194 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "195 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "196 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "197 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "198 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "199 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "200 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "201 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "202 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "203 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "204 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "205 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "206 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "207 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "208 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "209 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "210 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "211 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "212 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "213 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "214 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "215 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "216 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "217 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "218 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "219 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "220 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "221 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "222 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "223 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "224 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "225 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "226 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "227 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "228 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "229 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "230 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "231 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "232 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "233 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "234 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "235 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "236 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "237 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "238 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "239 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "240 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "241 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "242 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "243 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "244 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "245 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "246 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "247 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "248 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "249 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "250 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "251 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "252 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "253 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "254 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "255 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "256 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "257 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "258 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "259 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "260 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "261 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "262 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "263 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "264 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "265 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "266 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "267 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "268 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "269 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "270 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "271 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "272 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "273 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "274 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "275 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "276 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "277 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "278 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "279 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "280 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "281 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "282 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "283 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "284 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "285 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "286 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "287 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "288 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "289 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "290 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "291 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "292 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "293 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "294 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "295 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "296 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "297 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "298 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "299 Train Loss 0.0070004105 Test MSE 0.0008526966107843603 Test RE 0.06833750716560803\n",
      "Training time: 291.78\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 300 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_I = 1000  #Total number of data points for 'y'\n",
    "N_B = 5000\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "    # layers = np.array([2,50,50,50,50,50,50,50,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers,rowdy_terms,n_val)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f30b0352c50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.imshow(u_pred.reshape(500,500),cmap = 'jet')\n",
    "plt.figure()\n",
    "plt.imshow(y_true.reshape(500,500),cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  0.06787299637290108\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((10,1))\n",
    "for i in range(10):\n",
    "    a[i] = test_re_full[i][-1]    \n",
    "print(\"a = \",np.nanmean(a))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
