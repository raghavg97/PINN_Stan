{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    y = np.zeros((x.shape[0],1))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        k = x[i]\n",
    "        if(k<0.25):\n",
    "            y[i] = 4.0\n",
    "        elif(k<=0.5):\n",
    "            y[i] = 0.0\n",
    "        elif(k<=0.75):\n",
    "            y[i] = 4.0\n",
    "        elif(k<=1.0):\n",
    "            y[i] = 1.5\n",
    "        elif(k<=1.25):\n",
    "            y[i] = 3.0\n",
    "        elif(k<=1.50):\n",
    "            y[i] = 0.0\n",
    "        elif(k<=1.75):\n",
    "            y[i] = 4.0\n",
    "        else:\n",
    "            y[i] = 0.0\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_adv(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    q = g(xt[:,0].reshape(-1,1) - ubar*xt[:,1].reshape(-1,1))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubar = 0.5\n",
    "loss_thresh = 0.1\n",
    "label = \"Adv_stan\"\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(2.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,500).reshape(-1,1)\n",
    "t = np.linspace(0,0.2,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_adv(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_D,N_f,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_IC = np.random.uniform(x_ll,x_ul,(N_I,1))\n",
    "    t_IC = np.zeros((N_I,1))\n",
    "    xt_IC = np.hstack((x_IC,t_IC))\n",
    "    u_IC = g(x_IC)\n",
    "    \n",
    "    x_BC1 = x_ll*np.ones((N_D,1))\n",
    "    t_BC1 = np.random.uniform(0,0.2,(N_D,1))\n",
    "    xt_BC1 = np.hstack((x_BC1,t_BC1))\n",
    "    \n",
    "    x_BC2 = x_ul*np.ones((N_D,1))\n",
    "    t_BC2 = np.random.uniform(0,0.2,(N_D,1))\n",
    "    xt_BC2 = np.hstack((x_BC2,t_BC2))\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2))\n",
    "    u_BC = true_adv(xt_BC)\n",
    "    \n",
    "    xt_train = np.vstack((xt_IC,xt_BC))\n",
    "    u_train = np.vstack((u_IC,u_BC))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll,xt_train)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        self.m_lambda = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_BC = Parameter(torch.ones(2*N_D+N_I,1))\n",
    "        self.lambdas_BC.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+2*N_D+N_I,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "    def loss_BC(self,xt,u,lambda_ind):\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_BC)\n",
    "        u_pred = self.forward(xt)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - u))/2.0\n",
    "        \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat,lambda_ind):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt + ubar*du_dx\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        # loss_f = self.loss_function(f,f_hat)\n",
    "        loss_f = (N_f+2*N_D+N_I)*self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        lambda_ind = False\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_train = self.loss_BC(xt_train,u_train,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "    \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        lambda_ind = True\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_train = self.loss_BC(xt_train,u_train,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "    \n",
    "        return -1.0*loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(30):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    # lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_I,N_D,N_f,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 6394.032 Test MSE 5.366476682971549 Test RE 0.829213790755026\n",
      "1 Train Loss 1625.8591 Test MSE 3.2966016019413282 Test RE 0.649912852577705\n",
      "2 Train Loss 1495.9044 Test MSE 3.1021903054875644 Test RE 0.6304579290795116\n",
      "3 Train Loss 1365.2189 Test MSE 2.8024521021703697 Test RE 0.5992264738218335\n",
      "4 Train Loss 1330.3815 Test MSE 2.694221657235594 Test RE 0.5875415090212246\n",
      "5 Train Loss 1321.5166 Test MSE 2.6618122095483514 Test RE 0.5839969773961537\n",
      "6 Train Loss 1268.1334 Test MSE 2.427263521605277 Test RE 0.5576739490047015\n",
      "7 Train Loss 1052.7397 Test MSE 2.0934424597114196 Test RE 0.5179077467191209\n",
      "8 Train Loss 846.9308 Test MSE 1.6635425080117003 Test RE 0.46167764320761645\n",
      "9 Train Loss 708.1273 Test MSE 1.4151579982395373 Test RE 0.42581833557934956\n",
      "10 Train Loss 608.29913 Test MSE 1.1859860826150284 Test RE 0.38981782075144344\n",
      "11 Train Loss 532.4169 Test MSE 1.0147621447751942 Test RE 0.36058198217063603\n",
      "12 Train Loss 448.38263 Test MSE 0.8940206031802538 Test RE 0.33845088703030235\n",
      "13 Train Loss 392.89594 Test MSE 0.7950854047999372 Test RE 0.3191749386459166\n",
      "14 Train Loss 339.9363 Test MSE 0.7132189435674479 Test RE 0.30229664540851137\n",
      "15 Train Loss 277.30353 Test MSE 0.5939909880429827 Test RE 0.27587466634343405\n",
      "16 Train Loss 241.87383 Test MSE 0.5190293204385131 Test RE 0.25788010540065487\n",
      "17 Train Loss 230.62033 Test MSE 0.49367531197589687 Test RE 0.25150266861305154\n",
      "18 Train Loss 223.21005 Test MSE 0.4766144736357651 Test RE 0.24711864061563918\n",
      "19 Train Loss 217.89449 Test MSE 0.46791365211326014 Test RE 0.24485261760977858\n",
      "20 Train Loss 209.4604 Test MSE 0.4531879079599005 Test RE 0.24096893023492705\n",
      "21 Train Loss 199.56552 Test MSE 0.42335542858564673 Test RE 0.23290266576585322\n",
      "22 Train Loss 191.65654 Test MSE 0.4108682963830991 Test RE 0.2294421526409708\n",
      "23 Train Loss 183.35379 Test MSE 0.3900160779148542 Test RE 0.22354406717233472\n",
      "24 Train Loss 172.742 Test MSE 0.3659524584492024 Test RE 0.21653805297896306\n",
      "25 Train Loss 163.4885 Test MSE 0.34514565601906333 Test RE 0.21029216820884325\n",
      "26 Train Loss 156.2819 Test MSE 0.3334898460470443 Test RE 0.20671081560559854\n",
      "27 Train Loss 143.2918 Test MSE 0.3156653281055304 Test RE 0.20111077252963505\n",
      "28 Train Loss 135.3935 Test MSE 0.2953725053225189 Test RE 0.19453911070561522\n",
      "29 Train Loss 130.8405 Test MSE 0.28542245269157873 Test RE 0.19123437457110581\n",
      "30 Train Loss 127.58449 Test MSE 0.28129845596018255 Test RE 0.18984779925584183\n",
      "31 Train Loss 121.729225 Test MSE 0.2669467808539944 Test RE 0.18494144089558012\n",
      "32 Train Loss 118.69095 Test MSE 0.2632144887332565 Test RE 0.18364401902424968\n",
      "33 Train Loss 115.50612 Test MSE 0.25896767747893135 Test RE 0.1821565003460938\n",
      "34 Train Loss 110.8024 Test MSE 0.2474767819318634 Test RE 0.17806932930580785\n",
      "35 Train Loss 106.42612 Test MSE 0.23642675194079021 Test RE 0.17404846637784468\n",
      "36 Train Loss 101.32412 Test MSE 0.2242214241844176 Test RE 0.16949638743796064\n",
      "37 Train Loss 96.35447 Test MSE 0.214246296377435 Test RE 0.16568323073795585\n",
      "38 Train Loss 93.05811 Test MSE 0.21027186504035028 Test RE 0.16413926195692677\n",
      "39 Train Loss 88.69608 Test MSE 0.201389895456164 Test RE 0.16063520473893675\n",
      "40 Train Loss 85.580025 Test MSE 0.19383383099459978 Test RE 0.15759291277409496\n",
      "41 Train Loss 81.383316 Test MSE 0.18723824638223543 Test RE 0.15488850054090414\n",
      "42 Train Loss 79.01139 Test MSE 0.1792877193874667 Test RE 0.15156438595482968\n",
      "43 Train Loss 75.28226 Test MSE 0.17108585471787632 Test RE 0.14805699985375048\n",
      "44 Train Loss 73.40829 Test MSE 0.17116616145464988 Test RE 0.1480917443380035\n",
      "45 Train Loss 70.60574 Test MSE 0.16693843676615333 Test RE 0.14625141079000034\n",
      "46 Train Loss 68.76593 Test MSE 0.16449436495961578 Test RE 0.14517686218551037\n",
      "47 Train Loss 66.426895 Test MSE 0.15835826673766146 Test RE 0.1424433775014807\n",
      "48 Train Loss 64.42489 Test MSE 0.15569619678134397 Test RE 0.14124103617526812\n",
      "49 Train Loss 62.84877 Test MSE 0.15123367004008798 Test RE 0.1392022126890918\n",
      "50 Train Loss 61.726162 Test MSE 0.14629991077753893 Test RE 0.13691275941653544\n",
      "51 Train Loss 60.52428 Test MSE 0.1448585202663224 Test RE 0.1362366372116322\n",
      "52 Train Loss 59.636375 Test MSE 0.1413169826308863 Test RE 0.1345609581474833\n",
      "53 Train Loss 58.471222 Test MSE 0.13993326093972097 Test RE 0.13390055291664338\n",
      "54 Train Loss 57.315678 Test MSE 0.13768452574837917 Test RE 0.13282030079066617\n",
      "55 Train Loss 55.886124 Test MSE 0.13223567449973248 Test RE 0.13016559572077205\n",
      "56 Train Loss 54.248028 Test MSE 0.1284379748844294 Test RE 0.12828285522924388\n",
      "57 Train Loss 53.5642 Test MSE 0.1283956381287814 Test RE 0.12826171067470943\n",
      "58 Train Loss 52.324284 Test MSE 0.1262903117945106 Test RE 0.12720579914032143\n",
      "59 Train Loss 50.979706 Test MSE 0.12031740515011777 Test RE 0.12416126293170972\n",
      "60 Train Loss 49.847527 Test MSE 0.11889977659456727 Test RE 0.12342763633200192\n",
      "61 Train Loss 49.115738 Test MSE 0.11738450009999347 Test RE 0.12263862434244878\n",
      "62 Train Loss 48.214104 Test MSE 0.11822912972652443 Test RE 0.12307905110249594\n",
      "63 Train Loss 47.50238 Test MSE 0.11577430372939213 Test RE 0.12179458552888438\n",
      "64 Train Loss 46.43677 Test MSE 0.11303314350708095 Test RE 0.1203440978963311\n",
      "65 Train Loss 45.664627 Test MSE 0.1123005199531051 Test RE 0.11995345915908355\n",
      "66 Train Loss 44.496853 Test MSE 0.1095183728780064 Test RE 0.11845826960683001\n",
      "67 Train Loss 43.02417 Test MSE 0.10694223242001015 Test RE 0.11705676448721673\n",
      "68 Train Loss 41.86604 Test MSE 0.10326439591954235 Test RE 0.11502631247057261\n",
      "69 Train Loss 40.256954 Test MSE 0.0995502193422425 Test RE 0.11293875698969293\n",
      "70 Train Loss 38.71104 Test MSE 0.09660752908636816 Test RE 0.11125700894820055\n",
      "71 Train Loss 36.79083 Test MSE 0.09444268973617533 Test RE 0.11000338936009245\n",
      "72 Train Loss 36.008507 Test MSE 0.09224198210806576 Test RE 0.10871418294485811\n",
      "73 Train Loss 34.953957 Test MSE 0.0909092547570991 Test RE 0.1079259653006284\n",
      "74 Train Loss 34.11501 Test MSE 0.0873101611261359 Test RE 0.10576799891625452\n",
      "75 Train Loss 33.342926 Test MSE 0.08798892410569917 Test RE 0.10617833152519392\n",
      "76 Train Loss 32.339485 Test MSE 0.08725287011943626 Test RE 0.10573329190495502\n",
      "77 Train Loss 31.834175 Test MSE 0.08617581238656159 Test RE 0.10507867455807665\n",
      "78 Train Loss 31.192488 Test MSE 0.08654597775425889 Test RE 0.10530411372107715\n",
      "79 Train Loss 30.45925 Test MSE 0.08745945216292898 Test RE 0.10585838627715534\n",
      "80 Train Loss 29.903471 Test MSE 0.08633898380535988 Test RE 0.10517810923311172\n",
      "81 Train Loss 29.427341 Test MSE 0.08515466331449685 Test RE 0.10445424901500523\n",
      "82 Train Loss 28.970993 Test MSE 0.08416691240504842 Test RE 0.10384667382685088\n",
      "83 Train Loss 28.455648 Test MSE 0.08290554976138123 Test RE 0.1030655902293322\n",
      "84 Train Loss 28.224644 Test MSE 0.08306321464519846 Test RE 0.10316354570971942\n",
      "85 Train Loss 27.893389 Test MSE 0.08249154685978811 Test RE 0.10280793042103982\n",
      "86 Train Loss 27.555147 Test MSE 0.08190105345890245 Test RE 0.10243930820005097\n",
      "87 Train Loss 27.310432 Test MSE 0.08174493829642225 Test RE 0.102341629618647\n",
      "88 Train Loss 27.043846 Test MSE 0.08151204038377703 Test RE 0.1021957358562533\n",
      "89 Train Loss 26.771103 Test MSE 0.0790161606265007 Test RE 0.10061896708895246\n",
      "90 Train Loss 26.48047 Test MSE 0.07808164074826364 Test RE 0.10002218977368638\n",
      "91 Train Loss 26.251331 Test MSE 0.07736962315450231 Test RE 0.09956509986814355\n",
      "92 Train Loss 25.943766 Test MSE 0.07865273589745181 Test RE 0.10038730837300817\n",
      "93 Train Loss 25.66788 Test MSE 0.0772425967365669 Test RE 0.09948333267772873\n",
      "94 Train Loss 25.478678 Test MSE 0.0778355443285263 Test RE 0.09986444123923592\n",
      "95 Train Loss 25.285978 Test MSE 0.07714566632572128 Test RE 0.09942089311839045\n",
      "96 Train Loss 25.050138 Test MSE 0.07594637155631898 Test RE 0.09864507509596254\n",
      "97 Train Loss 24.931522 Test MSE 0.07569885306228434 Test RE 0.09848419573801472\n",
      "98 Train Loss 24.74513 Test MSE 0.07635422389993013 Test RE 0.09890959566357718\n",
      "99 Train Loss 24.592691 Test MSE 0.07464871880633699 Test RE 0.09779869778869169\n",
      "100 Train Loss 24.362299 Test MSE 0.07454533011795783 Test RE 0.0977309485847168\n",
      "101 Train Loss 24.103233 Test MSE 0.07345919714454485 Test RE 0.09701636124578254\n",
      "102 Train Loss 23.951841 Test MSE 0.07220235518703409 Test RE 0.09618283526658913\n",
      "103 Train Loss 23.781555 Test MSE 0.07208120000345475 Test RE 0.09610210423240963\n",
      "104 Train Loss 23.627855 Test MSE 0.07430972431791355 Test RE 0.09757638356426929\n",
      "105 Train Loss 23.425962 Test MSE 0.07427850023477894 Test RE 0.09755588117876\n",
      "106 Train Loss 23.17835 Test MSE 0.07414384584552675 Test RE 0.09746741489009318\n",
      "107 Train Loss 22.882685 Test MSE 0.0741205437586816 Test RE 0.09745209755244122\n",
      "108 Train Loss 22.712593 Test MSE 0.07554882527198692 Test RE 0.09838655426943645\n",
      "109 Train Loss 22.61326 Test MSE 0.07445208460883786 Test RE 0.09766980576341697\n",
      "110 Train Loss 22.46572 Test MSE 0.07566603845354414 Test RE 0.0984628475238473\n",
      "111 Train Loss 22.292147 Test MSE 0.0761702749856721 Test RE 0.09879037970382287\n",
      "112 Train Loss 22.135206 Test MSE 0.07479768532536951 Test RE 0.09789623108083255\n",
      "113 Train Loss 22.046223 Test MSE 0.07650372371578562 Test RE 0.09900637966926489\n",
      "114 Train Loss 21.809622 Test MSE 0.07540503334613466 Test RE 0.09829288021123922\n",
      "115 Train Loss 21.610758 Test MSE 0.07684912112748214 Test RE 0.09922962394444826\n",
      "116 Train Loss 21.458796 Test MSE 0.07728173778785608 Test RE 0.09950853502222899\n",
      "117 Train Loss 21.34938 Test MSE 0.07524684829976074 Test RE 0.0981897264421909\n",
      "118 Train Loss 21.161688 Test MSE 0.07506478740457204 Test RE 0.09807086873504882\n",
      "119 Train Loss 20.989271 Test MSE 0.0756021072149127 Test RE 0.09842124245145911\n",
      "120 Train Loss 20.814854 Test MSE 0.0760560023630973 Test RE 0.09871624794454281\n",
      "121 Train Loss 20.629036 Test MSE 0.07639547180682155 Test RE 0.09893630841590487\n",
      "122 Train Loss 20.511826 Test MSE 0.07595683925071205 Test RE 0.09865187299098048\n",
      "123 Train Loss 20.40012 Test MSE 0.07463328203654827 Test RE 0.09778858526389589\n",
      "124 Train Loss 20.329842 Test MSE 0.07416685316299351 Test RE 0.0974825361034301\n",
      "125 Train Loss 20.227352 Test MSE 0.0745943264171912 Test RE 0.09776306104720749\n",
      "126 Train Loss 20.152754 Test MSE 0.07573346460438332 Test RE 0.09850670797038034\n",
      "127 Train Loss 20.086973 Test MSE 0.07519235368158733 Test RE 0.09815416494757408\n",
      "128 Train Loss 19.959469 Test MSE 0.07488398892746612 Test RE 0.09795269246482614\n",
      "129 Train Loss 19.809282 Test MSE 0.07439923127127404 Test RE 0.09763513184045049\n",
      "130 Train Loss 19.69663 Test MSE 0.07395793749677532 Test RE 0.09734514325213664\n",
      "131 Train Loss 19.571651 Test MSE 0.07474713985810938 Test RE 0.0978631481922531\n",
      "132 Train Loss 19.4712 Test MSE 0.0767712223997991 Test RE 0.09917931874126128\n",
      "133 Train Loss 19.272732 Test MSE 0.07582307290597318 Test RE 0.09856496761385719\n",
      "134 Train Loss 19.122375 Test MSE 0.07371174106296648 Test RE 0.09718298347491298\n",
      "135 Train Loss 19.015211 Test MSE 0.07450103457001814 Test RE 0.0977019079379541\n",
      "136 Train Loss 18.862816 Test MSE 0.07474164617159391 Test RE 0.09785955180490247\n",
      "137 Train Loss 18.729977 Test MSE 0.07484751060455473 Test RE 0.09792883165525205\n",
      "138 Train Loss 18.641676 Test MSE 0.07413316501479515 Test RE 0.09746039427737266\n",
      "139 Train Loss 18.524351 Test MSE 0.07513291266892681 Test RE 0.09811536089250833\n",
      "140 Train Loss 18.382994 Test MSE 0.07445977427902835 Test RE 0.09767484947177073\n",
      "141 Train Loss 18.260939 Test MSE 0.07367379210363077 Test RE 0.09715796393373323\n",
      "142 Train Loss 18.142754 Test MSE 0.07460405253110992 Test RE 0.09776943434476097\n",
      "143 Train Loss 18.044418 Test MSE 0.07568875231099212 Test RE 0.09847762498090926\n",
      "144 Train Loss 17.919542 Test MSE 0.07612972423553904 Test RE 0.09876407969822863\n",
      "145 Train Loss 17.824512 Test MSE 0.07594698015654928 Test RE 0.09864547034391195\n",
      "146 Train Loss 17.716269 Test MSE 0.07550943577376168 Test RE 0.09836090263118011\n",
      "147 Train Loss 17.547588 Test MSE 0.07817685016589285 Test RE 0.10008315258846731\n",
      "148 Train Loss 17.184738 Test MSE 0.0762439610417342 Test RE 0.0988381523670732\n",
      "149 Train Loss 16.994001 Test MSE 0.07881470581663809 Test RE 0.10049061922238357\n",
      "150 Train Loss 16.848995 Test MSE 0.07707000912225298 Test RE 0.0993721298359065\n",
      "151 Train Loss 16.736572 Test MSE 0.07611896732439712 Test RE 0.09875710191149002\n",
      "152 Train Loss 16.636238 Test MSE 0.07720482680323955 Test RE 0.09945900712133438\n",
      "153 Train Loss 16.547932 Test MSE 0.07682429008822138 Test RE 0.09921359140093752\n",
      "154 Train Loss 16.478739 Test MSE 0.077505385945066 Test RE 0.0996524165091744\n",
      "155 Train Loss 16.380058 Test MSE 0.07570799723234946 Test RE 0.09849014384028916\n",
      "156 Train Loss 16.291191 Test MSE 0.07511793896074742 Test RE 0.09810558339330035\n",
      "157 Train Loss 16.201914 Test MSE 0.07589884825886219 Test RE 0.09861420678226206\n",
      "158 Train Loss 16.096273 Test MSE 0.07617576788213194 Test RE 0.09879394169401128\n",
      "159 Train Loss 15.9619255 Test MSE 0.07755689269780697 Test RE 0.09968552336905792\n",
      "160 Train Loss 15.806645 Test MSE 0.07635418280421855 Test RE 0.09890956904579137\n",
      "161 Train Loss 15.723351 Test MSE 0.07805720900411882 Test RE 0.10000654007860114\n",
      "162 Train Loss 15.665465 Test MSE 0.07754731828583432 Test RE 0.09967937008149497\n",
      "163 Train Loss 15.590131 Test MSE 0.07709948663949628 Test RE 0.09939113180350297\n",
      "164 Train Loss 15.518476 Test MSE 0.07799222717124334 Test RE 0.0999649041975188\n",
      "165 Train Loss 15.453687 Test MSE 0.0779793240390792 Test RE 0.09995663469573043\n",
      "166 Train Loss 15.408415 Test MSE 0.0773870659277908 Test RE 0.09957632257753234\n",
      "167 Train Loss 15.375391 Test MSE 0.07779232620834704 Test RE 0.0998367125663389\n",
      "168 Train Loss 15.352977 Test MSE 0.0780195755136058 Test RE 0.09998242924394894\n",
      "169 Train Loss 15.31093 Test MSE 0.0775556859501091 Test RE 0.09968474783680255\n",
      "170 Train Loss 15.288302 Test MSE 0.07840250627095828 Test RE 0.10022749263591402\n",
      "171 Train Loss 15.248435 Test MSE 0.07878934922228464 Test RE 0.10047445279239539\n",
      "172 Train Loss 15.192634 Test MSE 0.07837741151707432 Test RE 0.10021145114901321\n",
      "173 Train Loss 15.152195 Test MSE 0.07785928934653581 Test RE 0.09987967272604545\n",
      "174 Train Loss 15.114044 Test MSE 0.07828133809517979 Test RE 0.10015001374635916\n",
      "175 Train Loss 15.052587 Test MSE 0.07865137473537868 Test RE 0.10038643971925827\n",
      "176 Train Loss 15.051122 Test MSE 0.07873666142469586 Test RE 0.1004408526739148\n",
      "177 Train Loss 15.05118 Test MSE 0.07873666142469586 Test RE 0.1004408526739148\n",
      "178 Train Loss 15.014736 Test MSE 0.07898108320034683 Test RE 0.10059663085955334\n",
      "179 Train Loss 14.965076 Test MSE 0.07789674421110077 Test RE 0.09990369381490276\n",
      "180 Train Loss 14.889074 Test MSE 0.07814991303454724 Test RE 0.10006590844717124\n",
      "181 Train Loss 14.809573 Test MSE 0.07936458048552236 Test RE 0.1008405615359588\n",
      "182 Train Loss 14.75099 Test MSE 0.07989699327758953 Test RE 0.10117823775926446\n",
      "183 Train Loss 14.71368 Test MSE 0.08104108982433365 Test RE 0.1019000810118544\n",
      "184 Train Loss 14.6259775 Test MSE 0.08080266838097169 Test RE 0.10175007649090674\n",
      "185 Train Loss 14.552815 Test MSE 0.08049917479567603 Test RE 0.10155881085953285\n",
      "186 Train Loss 14.487789 Test MSE 0.07914381818946421 Test RE 0.10070021368614399\n",
      "187 Train Loss 14.425396 Test MSE 0.07970894877285109 Test RE 0.10105910173884354\n",
      "188 Train Loss 14.35482 Test MSE 0.08033018042634116 Test RE 0.10145215209976945\n",
      "189 Train Loss 14.262348 Test MSE 0.07958820476028754 Test RE 0.100982529997655\n",
      "190 Train Loss 14.176482 Test MSE 0.08018354244581169 Test RE 0.10135951235976083\n",
      "191 Train Loss 14.11501 Test MSE 0.08056721822651225 Test RE 0.10160172403391442\n",
      "192 Train Loss 14.005337 Test MSE 0.07988332278201271 Test RE 0.1011695815147356\n",
      "193 Train Loss 13.958748 Test MSE 0.08036751510674302 Test RE 0.1014757250810629\n",
      "194 Train Loss 13.894995 Test MSE 0.08110483841512318 Test RE 0.10194015148482663\n",
      "195 Train Loss 13.818219 Test MSE 0.07867366283828452 Test RE 0.10040066238745314\n",
      "196 Train Loss 13.751309 Test MSE 0.07888939053355341 Test RE 0.10053822034107357\n",
      "197 Train Loss 13.695257 Test MSE 0.07777894944609265 Test RE 0.09982812849738676\n",
      "198 Train Loss 13.639374 Test MSE 0.07900584810233548 Test RE 0.10061240090439268\n",
      "199 Train Loss 13.604113 Test MSE 0.07806005512054266 Test RE 0.10000836327767053\n",
      "200 Train Loss 13.538509 Test MSE 0.07723959737411581 Test RE 0.09948140116919692\n",
      "201 Train Loss 13.466059 Test MSE 0.0792681254412555 Test RE 0.10077926506186995\n",
      "202 Train Loss 13.341766 Test MSE 0.07935536980343832 Test RE 0.1008347098241646\n",
      "203 Train Loss 13.171549 Test MSE 0.07704759604893731 Test RE 0.0993576793589401\n",
      "204 Train Loss 13.082635 Test MSE 0.07676331043417535 Test RE 0.09917420794873599\n",
      "205 Train Loss 13.028295 Test MSE 0.07684481201145768 Test RE 0.09922684188298403\n",
      "206 Train Loss 12.996553 Test MSE 0.07622093753339004 Test RE 0.09882322808446553\n",
      "207 Train Loss 12.951496 Test MSE 0.07495140342632817 Test RE 0.09799677362455653\n",
      "208 Train Loss 12.872886 Test MSE 0.07441073185914705 Test RE 0.09764267773846984\n",
      "209 Train Loss 12.824633 Test MSE 0.07564106529578572 Test RE 0.09844659762364573\n",
      "210 Train Loss 12.761423 Test MSE 0.07541896500004908 Test RE 0.09830195997079513\n",
      "211 Train Loss 12.686101 Test MSE 0.07527155359229615 Test RE 0.09820584410619332\n",
      "212 Train Loss 12.640687 Test MSE 0.07463885132404283 Test RE 0.09779223378752029\n",
      "213 Train Loss 12.560065 Test MSE 0.07365439262708291 Test RE 0.0971451714760581\n",
      "214 Train Loss 12.492987 Test MSE 0.07517600760596448 Test RE 0.09814349549429188\n",
      "215 Train Loss 12.420937 Test MSE 0.07354915101314596 Test RE 0.09707574337451373\n",
      "216 Train Loss 12.336427 Test MSE 0.07598858772200032 Test RE 0.09867248811249513\n",
      "217 Train Loss 12.275133 Test MSE 0.07630557066923287 Test RE 0.09887807783801171\n",
      "218 Train Loss 12.235767 Test MSE 0.07590321146237594 Test RE 0.09861704126315232\n",
      "219 Train Loss 12.194905 Test MSE 0.07633281613688006 Test RE 0.09889572883658025\n",
      "220 Train Loss 12.159791 Test MSE 0.0763199437447476 Test RE 0.09888738983941094\n",
      "221 Train Loss 12.0933695 Test MSE 0.07498992032548672 Test RE 0.09802195025150746\n",
      "222 Train Loss 12.04701 Test MSE 0.07590754831552532 Test RE 0.09861985854524566\n",
      "223 Train Loss 11.988009 Test MSE 0.07515590997019794 Test RE 0.09813037572287135\n",
      "224 Train Loss 11.940122 Test MSE 0.07441093174650608 Test RE 0.09764280888570753\n",
      "225 Train Loss 11.905059 Test MSE 0.07299686417143574 Test RE 0.09671058156081494\n",
      "226 Train Loss 11.881642 Test MSE 0.07364750560100389 Test RE 0.09714062960865635\n",
      "227 Train Loss 11.857468 Test MSE 0.07457167726516531 Test RE 0.09774821797104531\n",
      "228 Train Loss 11.798258 Test MSE 0.07423127668317535 Test RE 0.09752486501905352\n",
      "229 Train Loss 11.668688 Test MSE 0.07659225914638595 Test RE 0.09906365163894386\n",
      "230 Train Loss 11.621094 Test MSE 0.07680328263305325 Test RE 0.0992000255900962\n",
      "231 Train Loss 11.563381 Test MSE 0.07434838955482764 Test RE 0.09760176599824617\n",
      "232 Train Loss 11.498494 Test MSE 0.07311850272387733 Test RE 0.0967911250110366\n",
      "233 Train Loss 11.42584 Test MSE 0.07364895613356466 Test RE 0.09714158622593251\n",
      "234 Train Loss 11.332282 Test MSE 0.072228778854701 Test RE 0.09620043352071657\n",
      "235 Train Loss 11.216055 Test MSE 0.0659603930064042 Test RE 0.09193132301224337\n",
      "236 Train Loss 11.103165 Test MSE 0.059728315140166605 Test RE 0.08748065476615785\n",
      "237 Train Loss 11.055273 Test MSE 0.0591748670277369 Test RE 0.08707440956576788\n",
      "238 Train Loss 10.981811 Test MSE 0.05771792861962906 Test RE 0.0859958040207568\n",
      "239 Train Loss 10.922007 Test MSE 0.05471432682339917 Test RE 0.08372832883649335\n",
      "240 Train Loss 10.893094 Test MSE 0.052822217207369467 Test RE 0.08226786112885406\n",
      "241 Train Loss 10.893023 Test MSE 0.05280912025236549 Test RE 0.08225766158282916\n",
      "242 Train Loss 10.867445 Test MSE 0.05279748058056709 Test RE 0.08224859586706774\n",
      "243 Train Loss 10.833879 Test MSE 0.05372572569092747 Test RE 0.08296846171922051\n",
      "244 Train Loss 10.79807 Test MSE 0.05365165618283972 Test RE 0.08291124934428913\n",
      "245 Train Loss 10.784027 Test MSE 0.05316412159280267 Test RE 0.08253368084278466\n",
      "246 Train Loss 10.752654 Test MSE 0.053198239122507096 Test RE 0.08256015916696594\n",
      "247 Train Loss 10.746289 Test MSE 0.053393848004787206 Test RE 0.0827118059376392\n",
      "248 Train Loss 10.746306 Test MSE 0.053393848004787206 Test RE 0.0827118059376392\n",
      "249 Train Loss 10.740398 Test MSE 0.05332987017180475 Test RE 0.08266223741596167\n",
      "250 Train Loss 10.740414 Test MSE 0.05332987017180475 Test RE 0.08266223741596167\n",
      "251 Train Loss 10.727289 Test MSE 0.05248907050070103 Test RE 0.08200802143183279\n",
      "252 Train Loss 10.690283 Test MSE 0.052534990947625176 Test RE 0.08204388624783598\n",
      "253 Train Loss 10.610038 Test MSE 0.05134735753226123 Test RE 0.0811112215907822\n",
      "254 Train Loss 10.551383 Test MSE 0.05142920043060184 Test RE 0.08117583771368733\n",
      "255 Train Loss 10.529947 Test MSE 0.0510724038705008 Test RE 0.0808937638286709\n",
      "256 Train Loss 10.475787 Test MSE 0.053014875056902416 Test RE 0.08241775187913697\n",
      "257 Train Loss 10.417709 Test MSE 0.05421883222630419 Test RE 0.08334834345209839\n",
      "258 Train Loss 10.372624 Test MSE 0.053986441297172366 Test RE 0.08316952921084877\n",
      "259 Train Loss 10.335192 Test MSE 0.05296218914938455 Test RE 0.08237678853241164\n",
      "260 Train Loss 10.312459 Test MSE 0.052871642148159224 Test RE 0.08230634051955044\n",
      "261 Train Loss 10.312478 Test MSE 0.052871642148159224 Test RE 0.08230634051955044\n",
      "262 Train Loss 10.312475 Test MSE 0.052871654075336415 Test RE 0.08230634980318728\n",
      "263 Train Loss 10.311003 Test MSE 0.05277710845109865 Test RE 0.08223272635313758\n",
      "264 Train Loss 10.31102 Test MSE 0.05277710845109865 Test RE 0.08223272635313758\n",
      "265 Train Loss 10.295802 Test MSE 0.052727952784500036 Test RE 0.08219442237773233\n",
      "266 Train Loss 10.271944 Test MSE 0.05390896043795187 Test RE 0.08310982569826778\n",
      "267 Train Loss 10.251466 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "268 Train Loss 10.251484 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "269 Train Loss 10.251501 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "270 Train Loss 10.251518 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "271 Train Loss 10.251535 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "272 Train Loss 10.251551 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "273 Train Loss 10.251567 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "274 Train Loss 10.251582 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "275 Train Loss 10.251597 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "276 Train Loss 10.251614 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "277 Train Loss 10.251628 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "278 Train Loss 10.251643 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "279 Train Loss 10.2516575 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "280 Train Loss 10.251671 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "281 Train Loss 10.251686 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "282 Train Loss 10.2517 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "283 Train Loss 10.251714 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "284 Train Loss 10.251726 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "285 Train Loss 10.25174 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "286 Train Loss 10.251754 Test MSE 0.054019134972637654 Test RE 0.08319470873730021\n",
      "287 Train Loss 10.2517395 Test MSE 0.05401908477143602 Test RE 0.08319467007993074\n",
      "288 Train Loss 10.251753 Test MSE 0.05401908477143602 Test RE 0.08319467007993074\n",
      "289 Train Loss 10.251766 Test MSE 0.05401908477143602 Test RE 0.08319467007993074\n",
      "290 Train Loss 10.236981 Test MSE 0.05419894778920323 Test RE 0.08333305829372148\n",
      "291 Train Loss 10.196072 Test MSE 0.05513176538125926 Test RE 0.08404712121849615\n",
      "292 Train Loss 10.168408 Test MSE 0.054039955895743105 Test RE 0.08321074031346513\n",
      "293 Train Loss 10.168424 Test MSE 0.054039955895743105 Test RE 0.08321074031346513\n",
      "294 Train Loss 10.168438 Test MSE 0.054039955895743105 Test RE 0.08321074031346513\n",
      "295 Train Loss 10.137273 Test MSE 0.055246966188046365 Test RE 0.08413488589882726\n",
      "296 Train Loss 10.087361 Test MSE 0.054461981887463616 Test RE 0.08353502628988368\n",
      "297 Train Loss 10.063615 Test MSE 0.05489387943929204 Test RE 0.08386559934490904\n",
      "298 Train Loss 10.057882 Test MSE 0.054492625417867424 Test RE 0.08355852385695361\n",
      "299 Train Loss 10.057898 Test MSE 0.054492625417867424 Test RE 0.08355852385695361\n",
      "300 Train Loss 10.028511 Test MSE 0.05364474850730612 Test RE 0.08290591174163993\n",
      "301 Train Loss 9.99974 Test MSE 0.054865349287347105 Test RE 0.08384380265929986\n",
      "302 Train Loss 9.987615 Test MSE 0.05428414595701919 Test RE 0.08339853037592176\n",
      "303 Train Loss 9.984424 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "304 Train Loss 9.98444 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "305 Train Loss 9.984456 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "306 Train Loss 9.984469 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "307 Train Loss 9.984485 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "308 Train Loss 9.9845 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "309 Train Loss 9.984513 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "310 Train Loss 9.984529 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "311 Train Loss 9.984543 Test MSE 0.054436280053678204 Test RE 0.08351531293525448\n",
      "312 Train Loss 9.984159 Test MSE 0.054291081243258245 Test RE 0.08340385766068723\n",
      "313 Train Loss 9.984175 Test MSE 0.054291081243258245 Test RE 0.08340385766068723\n",
      "314 Train Loss 9.984187 Test MSE 0.054291081243258245 Test RE 0.08340385766068723\n",
      "315 Train Loss 9.984033 Test MSE 0.05429094619075884 Test RE 0.08340375392443643\n",
      "316 Train Loss 9.983034 Test MSE 0.05429483884429614 Test RE 0.08340674388980952\n",
      "317 Train Loss 9.983047 Test MSE 0.05429483884429614 Test RE 0.08340674388980952\n",
      "318 Train Loss 9.983062 Test MSE 0.05429483884429614 Test RE 0.08340674388980952\n",
      "319 Train Loss 9.983074 Test MSE 0.05429483884429614 Test RE 0.08340674388980952\n",
      "320 Train Loss 9.983088 Test MSE 0.05429483884429614 Test RE 0.08340674388980952\n",
      "321 Train Loss 9.983046 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "322 Train Loss 9.983059 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "323 Train Loss 9.983072 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "324 Train Loss 9.983084 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "325 Train Loss 9.983097 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "326 Train Loss 9.9831085 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "327 Train Loss 9.983122 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "328 Train Loss 9.983133 Test MSE 0.05429489316458499 Test RE 0.08340678561271894\n",
      "329 Train Loss 9.968655 Test MSE 0.054510915399571876 Test RE 0.0835725455309285\n",
      "330 Train Loss 9.94124 Test MSE 0.05397756626297557 Test RE 0.08316269265395887\n",
      "331 Train Loss 9.90778 Test MSE 0.0534069568978868 Test RE 0.08272195873352071\n",
      "332 Train Loss 9.863268 Test MSE 0.05408913689976198 Test RE 0.08324859616478178\n",
      "333 Train Loss 9.827243 Test MSE 0.053164067385770904 Test RE 0.08253363876640984\n",
      "334 Train Loss 9.822149 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "335 Train Loss 9.8221655 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "336 Train Loss 9.822179 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "337 Train Loss 9.822193 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "338 Train Loss 9.8222065 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "339 Train Loss 9.822219 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "340 Train Loss 9.822231 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "341 Train Loss 9.822243 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "342 Train Loss 9.822255 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "343 Train Loss 9.822267 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "344 Train Loss 9.822277 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "345 Train Loss 9.822288 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "346 Train Loss 9.822298 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "347 Train Loss 9.822309 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "348 Train Loss 9.82232 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "349 Train Loss 9.8223295 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "350 Train Loss 9.822339 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "351 Train Loss 9.82235 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "352 Train Loss 9.822359 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "353 Train Loss 9.822368 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "354 Train Loss 9.822377 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "355 Train Loss 9.822387 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "356 Train Loss 9.822396 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "357 Train Loss 9.822405 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "358 Train Loss 9.822413 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "359 Train Loss 9.822423 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "360 Train Loss 9.822431 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "361 Train Loss 9.82244 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "362 Train Loss 9.822447 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "363 Train Loss 9.822456 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "364 Train Loss 9.822465 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "365 Train Loss 9.822473 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "366 Train Loss 9.822481 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "367 Train Loss 9.822489 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "368 Train Loss 9.822496 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "369 Train Loss 9.822504 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "370 Train Loss 9.822512 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "371 Train Loss 9.82252 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "372 Train Loss 9.822528 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "373 Train Loss 9.8225355 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "374 Train Loss 9.822543 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "375 Train Loss 9.82255 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "376 Train Loss 9.822557 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "377 Train Loss 9.822564 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "378 Train Loss 9.822571 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "379 Train Loss 9.822578 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "380 Train Loss 9.822586 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "381 Train Loss 9.822592 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "382 Train Loss 9.822599 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "383 Train Loss 9.822605 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "384 Train Loss 9.822613 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "385 Train Loss 9.822619 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "386 Train Loss 9.822626 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "387 Train Loss 9.822633 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "388 Train Loss 9.822639 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "389 Train Loss 9.822646 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "390 Train Loss 9.822651 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "391 Train Loss 9.822658 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "392 Train Loss 9.822663 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "393 Train Loss 9.822671 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "394 Train Loss 9.822677 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "395 Train Loss 9.822682 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "396 Train Loss 9.822689 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "397 Train Loss 9.822695 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "398 Train Loss 9.8227005 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "399 Train Loss 9.822705 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "400 Train Loss 9.822712 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "401 Train Loss 9.822718 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "402 Train Loss 9.822723 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "403 Train Loss 9.82273 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "404 Train Loss 9.822736 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "405 Train Loss 9.8227415 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "406 Train Loss 9.822746 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "407 Train Loss 9.822752 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "408 Train Loss 9.822757 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "409 Train Loss 9.8227625 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "410 Train Loss 9.822768 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "411 Train Loss 9.822773 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "412 Train Loss 9.82278 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "413 Train Loss 9.822783 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "414 Train Loss 9.822789 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "415 Train Loss 9.822795 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "416 Train Loss 9.8228 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "417 Train Loss 9.822804 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "418 Train Loss 9.822809 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "419 Train Loss 9.822814 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "420 Train Loss 9.82282 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "421 Train Loss 9.8228245 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "422 Train Loss 9.822828 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "423 Train Loss 9.822834 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "424 Train Loss 9.822839 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "425 Train Loss 9.822843 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "426 Train Loss 9.822848 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "427 Train Loss 9.822853 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "428 Train Loss 9.822858 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "429 Train Loss 9.822862 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "430 Train Loss 9.822866 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "431 Train Loss 9.822871 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "432 Train Loss 9.822875 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "433 Train Loss 9.82288 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "434 Train Loss 9.822885 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "435 Train Loss 9.822888 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "436 Train Loss 9.822893 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "437 Train Loss 9.822897 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "438 Train Loss 9.822901 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "439 Train Loss 9.822906 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "440 Train Loss 9.822908 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "441 Train Loss 9.822913 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "442 Train Loss 9.822918 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "443 Train Loss 9.822922 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "444 Train Loss 9.822926 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "445 Train Loss 9.822929 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "446 Train Loss 9.822934 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "447 Train Loss 9.822937 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "448 Train Loss 9.822942 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "449 Train Loss 9.822947 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "450 Train Loss 9.822948 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "451 Train Loss 9.822953 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "452 Train Loss 9.822957 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "453 Train Loss 9.822961 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "454 Train Loss 9.822964 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "455 Train Loss 9.8229685 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "456 Train Loss 9.822971 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "457 Train Loss 9.822975 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "458 Train Loss 9.822979 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "459 Train Loss 9.822982 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "460 Train Loss 9.822986 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "461 Train Loss 9.8229885 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "462 Train Loss 9.822992 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "463 Train Loss 9.822995 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "464 Train Loss 9.823 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "465 Train Loss 9.823004 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "466 Train Loss 9.823007 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "467 Train Loss 9.823009 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "468 Train Loss 9.823013 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "469 Train Loss 9.823016 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "470 Train Loss 9.82302 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "471 Train Loss 9.823022 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "472 Train Loss 9.823025 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "473 Train Loss 9.823029 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "474 Train Loss 9.82303 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "475 Train Loss 9.823035 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "476 Train Loss 9.823037 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "477 Train Loss 9.82304 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "478 Train Loss 9.823044 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "479 Train Loss 9.823048 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "480 Train Loss 9.82305 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "481 Train Loss 9.823052 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "482 Train Loss 9.823055 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "483 Train Loss 9.823058 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "484 Train Loss 9.823062 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "485 Train Loss 9.823064 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "486 Train Loss 9.823067 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "487 Train Loss 9.823071 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "488 Train Loss 9.823072 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "489 Train Loss 9.823075 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "490 Train Loss 9.823077 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "491 Train Loss 9.82308 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "492 Train Loss 9.823083 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "493 Train Loss 9.823086 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "494 Train Loss 9.823088 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "495 Train Loss 9.823091 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "496 Train Loss 9.823092 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "497 Train Loss 9.823095 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "498 Train Loss 9.823097 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "499 Train Loss 9.8231 Test MSE 0.053353139440203305 Test RE 0.08268026933701013\n",
      "Training time: 228.47\n",
      "Training time: 228.47\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1 #10\n",
    "max_iter = 500 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "N_I = 1000\n",
    "N_D = 1000\n",
    "N_f = 5000\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr= 0.5, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f37e0363990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred= PINN.test()\n",
    "plt.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = 'jet',vmax = 4,vmin = 0.2)\n",
    "plt.figure()\n",
    "plt.imshow(np.flip(u_true.reshape(500,500),axis = 0),cmap = 'jet',vmax = 4,vmin = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
