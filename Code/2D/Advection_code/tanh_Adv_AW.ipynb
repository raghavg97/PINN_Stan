{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    y = np.zeros((x.shape[0],1))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        k = x[i]\n",
    "        if(k<0.25):\n",
    "            y[i] = 4.0\n",
    "        elif(k<=0.5):\n",
    "            y[i] = 0.0\n",
    "        elif(k<=0.75):\n",
    "            y[i] = 4.0\n",
    "        elif(k<=1.0):\n",
    "            y[i] = 1.5\n",
    "        elif(k<=1.25):\n",
    "            y[i] = 3.0\n",
    "        elif(k<=1.50):\n",
    "            y[i] = 0.0\n",
    "        elif(k<=1.75):\n",
    "            y[i] = 4.0\n",
    "        else:\n",
    "            y[i] = 0.0\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_adv(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    q = g(xt[:,0].reshape(-1,1) - ubar*xt[:,1].reshape(-1,1))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubar = 0.5\n",
    "loss_thresh = 0.1\n",
    "label = \"Adv_tanh\"\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(2.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,500).reshape(-1,1)\n",
    "t = np.linspace(0,0.2,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_adv(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_D,N_f,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_IC = np.random.uniform(x_ll,x_ul,(N_I,1))\n",
    "    t_IC = np.zeros((N_I,1))\n",
    "    xt_IC = np.hstack((x_IC,t_IC))\n",
    "    u_IC = g(x_IC)\n",
    "    \n",
    "    x_BC1 = x_ll*np.ones((N_D,1))\n",
    "    t_BC1 = np.random.uniform(0,0.2,(N_D,1))\n",
    "    xt_BC1 = np.hstack((x_BC1,t_BC1))\n",
    "    \n",
    "    x_BC2 = x_ul*np.ones((N_D,1))\n",
    "    t_BC2 = np.random.uniform(0,0.2,(N_D,1))\n",
    "    xt_BC2 = np.hstack((x_BC2,t_BC2))\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2))\n",
    "    u_BC = true_adv(xt_BC)\n",
    "    \n",
    "    xt_train = np.vstack((xt_IC,xt_BC))\n",
    "    u_train = np.vstack((u_IC,u_BC))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll,xt_train)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        self.m_lambda = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lambdas_BC = Parameter(torch.ones(2*N_D+N_I,1))\n",
    "        self.lambdas_BC.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+2*N_D+N_I,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "             \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "    def loss_BC(self,xt,u,lambda_ind):\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_BC)\n",
    "        u_pred = self.forward(xt)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - u))/2.0\n",
    "        \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat,lambda_ind):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt + ubar*du_dx\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        # loss_f = self.loss_function(f,f_hat)\n",
    "        loss_f = (N_f+2*N_D+N_I)*self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        lambda_ind = False\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_train = self.loss_BC(xt_train,u_train,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "    \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        lambda_ind = True\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_train = self.loss_BC(xt_train,u_train,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "    \n",
    "        return -1.0*loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(30):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    # lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_I,N_D,N_f,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_tanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1569.1259 Test MSE 3.315175394759438 Test RE 0.6517411583137798\n",
      "1 Train Loss 1552.8293 Test MSE 3.2534799633113023 Test RE 0.6456482246201822\n",
      "2 Train Loss 1541.7631 Test MSE 3.2416096915579926 Test RE 0.6444693295192309\n",
      "3 Train Loss 1528.0326 Test MSE 3.198264648877749 Test RE 0.6401460820809141\n",
      "4 Train Loss 1487.1212 Test MSE 3.1014403452764165 Test RE 0.630381717291278\n",
      "5 Train Loss 1365.6827 Test MSE 2.808643368441322 Test RE 0.5998880237314947\n",
      "6 Train Loss 1298.655 Test MSE 2.658650918178455 Test RE 0.583650083449732\n",
      "7 Train Loss 1185.6132 Test MSE 2.3997893283853453 Test RE 0.5545088115507041\n",
      "8 Train Loss 1106.4999 Test MSE 2.2218028759907122 Test RE 0.5335494237973307\n",
      "9 Train Loss 993.6069 Test MSE 1.948851484971669 Test RE 0.4997022041631885\n",
      "10 Train Loss 912.0659 Test MSE 1.8187022988560269 Test RE 0.4827282340603792\n",
      "11 Train Loss 753.94525 Test MSE 1.4866428495615909 Test RE 0.43644067229760025\n",
      "12 Train Loss 621.2082 Test MSE 1.218522650628631 Test RE 0.39512880969865444\n",
      "13 Train Loss 494.60187 Test MSE 0.9644120558055134 Test RE 0.351522564777385\n",
      "14 Train Loss 375.8872 Test MSE 0.7533635190561898 Test RE 0.3106877900707492\n",
      "15 Train Loss 286.7187 Test MSE 0.5721270678778992 Test RE 0.27074979751146383\n",
      "16 Train Loss 226.54013 Test MSE 0.4646473601808797 Test RE 0.24399651872397646\n",
      "17 Train Loss 140.24365 Test MSE 0.29660466774379146 Test RE 0.1949444536422281\n",
      "18 Train Loss 106.60936 Test MSE 0.2278381362694594 Test RE 0.17085791512765536\n",
      "19 Train Loss 83.40413 Test MSE 0.1915498363429866 Test RE 0.15666168218794266\n",
      "20 Train Loss 70.16455 Test MSE 0.1682929924907659 Test RE 0.1468435615528888\n",
      "21 Train Loss 59.254948 Test MSE 0.1460837285742363 Test RE 0.13681156644951134\n",
      "22 Train Loss 49.112556 Test MSE 0.12228605457439565 Test RE 0.12517291310217252\n",
      "23 Train Loss 43.23355 Test MSE 0.1045090997588938 Test RE 0.11571747438551516\n",
      "24 Train Loss 35.977028 Test MSE 0.09447550519431358 Test RE 0.11002249882273597\n",
      "25 Train Loss 31.214151 Test MSE 0.08206741920355133 Test RE 0.10254329798583443\n",
      "26 Train Loss 28.701092 Test MSE 0.07706820079022442 Test RE 0.09937096402016755\n",
      "27 Train Loss 25.884565 Test MSE 0.07248316493978398 Test RE 0.096369691139252\n",
      "28 Train Loss 22.68694 Test MSE 0.07171989007478939 Test RE 0.09586094380511476\n",
      "29 Train Loss 19.836357 Test MSE 0.06072247941756514 Test RE 0.08820569801446101\n",
      "30 Train Loss 17.29508 Test MSE 0.04977697554368481 Test RE 0.07986125772591138\n",
      "31 Train Loss 15.192378 Test MSE 0.06459896300198112 Test RE 0.09097763984692157\n",
      "32 Train Loss 13.933234 Test MSE 0.04048459501396136 Test RE 0.07202226787129842\n",
      "33 Train Loss 12.598373 Test MSE 0.03684602917539981 Test RE 0.0687095712839842\n",
      "34 Train Loss 10.855294 Test MSE 0.03406638670608159 Test RE 0.06606705207574975\n",
      "35 Train Loss 10.09929 Test MSE 0.03473956966009986 Test RE 0.06671663138694997\n",
      "36 Train Loss 9.306456 Test MSE 0.04038236093902807 Test RE 0.07193127296004427\n",
      "37 Train Loss 8.463738 Test MSE 0.03801829471101431 Test RE 0.06979401932985253\n",
      "38 Train Loss 7.71073 Test MSE 0.03873238610717291 Test RE 0.0704464348393829\n",
      "39 Train Loss 7.1876354 Test MSE 0.03786263806850707 Test RE 0.06965099548407726\n",
      "40 Train Loss 6.6679163 Test MSE 0.03532679941827463 Test RE 0.06727814974278701\n",
      "41 Train Loss 6.1256375 Test MSE 0.034841403182752514 Test RE 0.06681434442814722\n",
      "42 Train Loss 5.690853 Test MSE 0.03563816351914503 Test RE 0.0675739881043638\n",
      "43 Train Loss 5.217757 Test MSE 0.038815186367990175 Test RE 0.07052169316611041\n",
      "44 Train Loss 4.833004 Test MSE 0.04387051522324588 Test RE 0.074973581966634\n",
      "45 Train Loss 4.5537686 Test MSE 0.045550736891269566 Test RE 0.07639581994634057\n",
      "46 Train Loss 4.2735124 Test MSE 0.04383891251746664 Test RE 0.07494657299608501\n",
      "47 Train Loss 3.914838 Test MSE 0.04765639174429736 Test RE 0.07814163095287208\n",
      "48 Train Loss 3.598276 Test MSE 0.05447456402188566 Test RE 0.08354467511458155\n",
      "49 Train Loss 3.4129813 Test MSE 0.05814337747524455 Test RE 0.08631216708758015\n",
      "50 Train Loss 3.2516854 Test MSE 0.055186383749407895 Test RE 0.08408874314187738\n",
      "51 Train Loss 3.0464597 Test MSE 0.05940553466960545 Test RE 0.08724395547580434\n",
      "52 Train Loss 2.908778 Test MSE 0.05885773645478846 Test RE 0.08684077106763452\n",
      "53 Train Loss 2.7878637 Test MSE 0.059948127003315546 Test RE 0.08764147988123983\n",
      "54 Train Loss 2.6681912 Test MSE 0.062438688306009626 Test RE 0.08944349860324399\n",
      "55 Train Loss 2.4676251 Test MSE 0.06457473615968623 Test RE 0.09096057836851117\n",
      "56 Train Loss 2.3394902 Test MSE 0.06797769893103839 Test RE 0.09332653089446492\n",
      "57 Train Loss 2.110266 Test MSE 0.08305768186461678 Test RE 0.10316010982796327\n",
      "58 Train Loss 1.9043846 Test MSE 0.09171133200479417 Test RE 0.10840102618443943\n",
      "59 Train Loss 1.7962868 Test MSE 0.09379809713879858 Test RE 0.10962734768331171\n",
      "60 Train Loss 1.6743021 Test MSE 0.10047216082002988 Test RE 0.11346051858235953\n",
      "61 Train Loss 1.5046245 Test MSE 0.09965118882892392 Test RE 0.11299601692530357\n",
      "62 Train Loss 1.3910524 Test MSE 0.1078133781130857 Test RE 0.11753256655254607\n",
      "63 Train Loss 1.3097495 Test MSE 0.10573642020276677 Test RE 0.11639496508608478\n",
      "64 Train Loss 1.2395145 Test MSE 0.10039910780262817 Test RE 0.11341926267435953\n",
      "65 Train Loss 1.2009298 Test MSE 0.09860924733174044 Test RE 0.11240372787922584\n",
      "66 Train Loss 1.076438 Test MSE 0.09811099400889987 Test RE 0.11211939117656845\n",
      "67 Train Loss 0.83841383 Test MSE 0.10342343230659458 Test RE 0.11511485378750787\n",
      "68 Train Loss 0.7749922 Test MSE 0.10745874000659768 Test RE 0.11733910327106092\n",
      "69 Train Loss 0.7246528 Test MSE 0.11362204184535644 Test RE 0.12065718469852943\n",
      "70 Train Loss 0.6907686 Test MSE 0.11999333554439177 Test RE 0.12399393875240243\n",
      "71 Train Loss 0.62815475 Test MSE 0.1260369881955991 Test RE 0.1270781551187026\n",
      "72 Train Loss 0.56961924 Test MSE 0.13905693725485188 Test RE 0.13348062233087024\n",
      "73 Train Loss 0.5403172 Test MSE 0.15690310788183157 Test RE 0.14178740886381874\n",
      "74 Train Loss 0.5059472 Test MSE 0.18790563016634998 Test RE 0.15516429388830943\n",
      "75 Train Loss 0.4853664 Test MSE 0.18171214321628185 Test RE 0.15258571191079448\n",
      "76 Train Loss 0.447111 Test MSE 0.1822697442238177 Test RE 0.15281964447752344\n",
      "77 Train Loss 0.4180571 Test MSE 0.18640165792484634 Test RE 0.15454208895452035\n",
      "78 Train Loss 0.38884246 Test MSE 0.1865042346017165 Test RE 0.1545846052974692\n",
      "79 Train Loss 0.37116712 Test MSE 0.1779166265107899 Test RE 0.150983733585602\n",
      "80 Train Loss 0.34531015 Test MSE 0.1483550962425216 Test RE 0.13787106417189252\n",
      "81 Train Loss 0.3073234 Test MSE 0.17310011998628314 Test RE 0.14892601797785632\n",
      "82 Train Loss 0.29639 Test MSE 0.17603132110554207 Test RE 0.15018164848544924\n",
      "83 Train Loss 0.26950094 Test MSE 0.1834952155268088 Test RE 0.1533325171999892\n",
      "84 Train Loss 0.254065 Test MSE 0.18035626468674879 Test RE 0.1520153728397418\n",
      "85 Train Loss 0.24573287 Test MSE 0.17879136929272324 Test RE 0.15135444091002048\n",
      "86 Train Loss 0.2361127 Test MSE 0.17258152162736445 Test RE 0.1487027635929601\n",
      "87 Train Loss 0.22175562 Test MSE 0.16692441497088514 Test RE 0.14624526855444633\n",
      "88 Train Loss 0.20019935 Test MSE 0.17138983796726048 Test RE 0.14818847441025584\n",
      "89 Train Loss 0.1913533 Test MSE 0.17367057278777975 Test RE 0.1491712095657855\n",
      "90 Train Loss 0.18236744 Test MSE 0.17522458191787152 Test RE 0.14983711731528387\n",
      "91 Train Loss 0.17520685 Test MSE 0.1785328275094148 Test RE 0.1512449680534926\n",
      "92 Train Loss 0.120772734 Test MSE 0.18058408921728 Test RE 0.15211135481446647\n",
      "93 Train Loss 0.10583915 Test MSE 0.18073329113735723 Test RE 0.1521741804499285\n",
      "94 Train Loss 0.09436596 Test MSE 0.19590822314054412 Test RE 0.15843394116666187\n",
      "95 Train Loss 0.08527732 Test MSE 0.21350556441486185 Test RE 0.16539656737660235\n",
      "96 Train Loss 0.07005165 Test MSE 0.24145482058771628 Test RE 0.1758894671107412\n",
      "97 Train Loss 0.061278753 Test MSE 0.24222311728283535 Test RE 0.17616908043192844\n",
      "98 Train Loss 0.05325183 Test MSE 0.24861194463919326 Test RE 0.1784772592777149\n",
      "99 Train Loss 0.0515068 Test MSE 0.24073257870418358 Test RE 0.17562620900116\n",
      "100 Train Loss 0.04837153 Test MSE 0.22900444793781893 Test RE 0.17129467082577704\n",
      "101 Train Loss 0.045742366 Test MSE 0.23354693558755862 Test RE 0.17298521254938015\n",
      "102 Train Loss 0.042909868 Test MSE 0.22055941196703138 Test RE 0.16810657136294369\n",
      "103 Train Loss 0.04046112 Test MSE 0.22758569101614476 Test RE 0.17076323338415483\n",
      "104 Train Loss 0.031406417 Test MSE 0.24578180668226704 Test RE 0.17745848072518716\n",
      "105 Train Loss 0.028216569 Test MSE 0.24796985793032403 Test RE 0.17824663487130368\n",
      "106 Train Loss 0.026786085 Test MSE 0.25873196260351505 Test RE 0.18207358117241607\n",
      "107 Train Loss 0.024439812 Test MSE 0.25477203645278806 Test RE 0.1806748788524462\n",
      "108 Train Loss 0.022871373 Test MSE 0.2527013400432018 Test RE 0.17993915032141156\n",
      "109 Train Loss 0.021514561 Test MSE 0.24017075409535743 Test RE 0.17542115000177658\n",
      "110 Train Loss 0.019960033 Test MSE 0.23602942476750383 Test RE 0.1739021562346124\n",
      "111 Train Loss 0.019251617 Test MSE 0.2349360662270175 Test RE 0.17349890542666307\n",
      "112 Train Loss 0.017907606 Test MSE 0.23589711656064005 Test RE 0.1738534082706377\n",
      "113 Train Loss 0.016931057 Test MSE 0.23755567748996753 Test RE 0.17446350774366862\n",
      "114 Train Loss 0.015081427 Test MSE 0.23592534926550368 Test RE 0.17386381154555886\n",
      "115 Train Loss 0.014147371 Test MSE 0.23257034167156979 Test RE 0.17262315838485864\n",
      "116 Train Loss 0.014002163 Test MSE 0.23030892668178787 Test RE 0.17178185121038553\n",
      "117 Train Loss 0.01380107 Test MSE 0.23073494797242053 Test RE 0.17194065728720115\n",
      "118 Train Loss 0.013313302 Test MSE 0.2330817555452683 Test RE 0.17281285024023108\n",
      "119 Train Loss 0.012962509 Test MSE 0.23762244928974782 Test RE 0.1744880249938102\n",
      "120 Train Loss 0.012618602 Test MSE 0.24471341268045005 Test RE 0.17707236169707702\n",
      "121 Train Loss 0.012399175 Test MSE 0.24695968286706874 Test RE 0.17788319541290315\n",
      "122 Train Loss 0.011815979 Test MSE 0.25074930081621255 Test RE 0.17924281599769226\n",
      "123 Train Loss 0.0116037335 Test MSE 0.2473351415984939 Test RE 0.17801836410242672\n",
      "124 Train Loss 0.010980661 Test MSE 0.24716961444925487 Test RE 0.1779587854192244\n",
      "125 Train Loss 0.010311859 Test MSE 0.24416518460020878 Test RE 0.176873904100559\n",
      "126 Train Loss 0.00944786 Test MSE 0.2387280226063646 Test RE 0.1748934695148235\n",
      "127 Train Loss 0.009262379 Test MSE 0.2408147643801435 Test RE 0.1756561856812151\n",
      "128 Train Loss 0.009099109 Test MSE 0.24351980323858782 Test RE 0.1766399914470648\n",
      "129 Train Loss 0.008803188 Test MSE 0.2517127418351559 Test RE 0.17958683354687802\n",
      "130 Train Loss 0.008687178 Test MSE 0.25117289794153586 Test RE 0.17939415181829207\n",
      "131 Train Loss 0.008528318 Test MSE 0.2535102631122275 Test RE 0.18022692210384825\n",
      "132 Train Loss 0.008520868 Test MSE 0.25335111368144286 Test RE 0.1801703415244211\n",
      "133 Train Loss 0.008520869 Test MSE 0.25335111368144286 Test RE 0.1801703415244211\n",
      "134 Train Loss 0.008520869 Test MSE 0.25335111368144286 Test RE 0.1801703415244211\n",
      "135 Train Loss 0.0085208705 Test MSE 0.25335111368144286 Test RE 0.1801703415244211\n",
      "136 Train Loss 0.008520871 Test MSE 0.25335111368144286 Test RE 0.1801703415244211\n",
      "137 Train Loss 0.008517992 Test MSE 0.25311105739112605 Test RE 0.1800849634232819\n",
      "138 Train Loss 0.008514526 Test MSE 0.2529107808251052 Test RE 0.18001370234165887\n",
      "139 Train Loss 0.008506694 Test MSE 0.25308693950843214 Test RE 0.18007638345143995\n",
      "140 Train Loss 0.008506043 Test MSE 0.253108609316205 Test RE 0.1800840925357096\n",
      "141 Train Loss 0.008333847 Test MSE 0.2525551874516491 Test RE 0.1798871079049495\n",
      "142 Train Loss 0.007765449 Test MSE 0.2525956955817834 Test RE 0.1799015336593392\n",
      "143 Train Loss 0.0074201804 Test MSE 0.2508988150008627 Test RE 0.1792962465549925\n",
      "144 Train Loss 0.0072976253 Test MSE 0.2513198836175482 Test RE 0.1794466346186816\n",
      "145 Train Loss 0.0070257303 Test MSE 0.2579543630847586 Test RE 0.18179977103876682\n",
      "146 Train Loss 0.0064218147 Test MSE 0.27312734785079207 Test RE 0.18707014666239516\n",
      "147 Train Loss 0.0062035746 Test MSE 0.27890636023231496 Test RE 0.18903886538289738\n",
      "148 Train Loss 0.0061184093 Test MSE 0.28149404439002357 Test RE 0.18991378891894345\n",
      "149 Train Loss 0.005825077 Test MSE 0.2872500366223574 Test RE 0.1918456425113985\n",
      "150 Train Loss 0.005407635 Test MSE 0.29452206779579554 Test RE 0.19425884999211956\n",
      "151 Train Loss 0.00531109 Test MSE 0.29180068730969516 Test RE 0.1933592924353689\n",
      "152 Train Loss 0.0053110905 Test MSE 0.29180068730969516 Test RE 0.1933592924353689\n",
      "153 Train Loss 0.005000044 Test MSE 0.2911886093682499 Test RE 0.19315639180675603\n",
      "154 Train Loss 0.00462447 Test MSE 0.2834017187477739 Test RE 0.1905562215583945\n",
      "155 Train Loss 0.004445826 Test MSE 0.2803603489951486 Test RE 0.1895309715729376\n",
      "156 Train Loss 0.004354128 Test MSE 0.28291412292148865 Test RE 0.19039222393303357\n",
      "157 Train Loss 0.0043312125 Test MSE 0.2829474224927925 Test RE 0.1904034283805613\n",
      "158 Train Loss 0.004330479 Test MSE 0.28289746257245946 Test RE 0.19038661790748365\n",
      "159 Train Loss 0.004327664 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "160 Train Loss 0.004327664 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "161 Train Loss 0.004327664 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "162 Train Loss 0.0043276646 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "163 Train Loss 0.004327665 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "164 Train Loss 0.004327665 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "165 Train Loss 0.004327665 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "166 Train Loss 0.004327665 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "167 Train Loss 0.004327666 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "168 Train Loss 0.0043276665 Test MSE 0.2827833879172828 Test RE 0.19034822859597308\n",
      "169 Train Loss 0.0043268823 Test MSE 0.28280327532879845 Test RE 0.19035492182329944\n",
      "170 Train Loss 0.004326883 Test MSE 0.28280327532879845 Test RE 0.19035492182329944\n",
      "171 Train Loss 0.004326883 Test MSE 0.28280327532879845 Test RE 0.19035492182329944\n",
      "172 Train Loss 0.004326883 Test MSE 0.28280327532879845 Test RE 0.19035492182329944\n",
      "173 Train Loss 0.004326799 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "174 Train Loss 0.0043267994 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "175 Train Loss 0.0043267994 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "176 Train Loss 0.0043268 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "177 Train Loss 0.0043268004 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "178 Train Loss 0.004326801 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "179 Train Loss 0.004326801 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "180 Train Loss 0.0043268017 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "181 Train Loss 0.0043268017 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "182 Train Loss 0.0043268017 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "183 Train Loss 0.004326802 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "184 Train Loss 0.004326802 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "185 Train Loss 0.0043268027 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "186 Train Loss 0.004326803 Test MSE 0.28280328241202146 Test RE 0.19035492420715855\n",
      "187 Train Loss 0.0043267556 Test MSE 0.28280171952202054 Test RE 0.19035439821570582\n",
      "188 Train Loss 0.004326756 Test MSE 0.28280171952202054 Test RE 0.19035439821570582\n",
      "189 Train Loss 0.0043267566 Test MSE 0.28280171952202054 Test RE 0.19035439821570582\n",
      "190 Train Loss 0.004326757 Test MSE 0.28280171952202054 Test RE 0.19035439821570582\n",
      "191 Train Loss 0.0043263966 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "192 Train Loss 0.004326397 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "193 Train Loss 0.0043263976 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "194 Train Loss 0.0043263985 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "195 Train Loss 0.0043263985 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "196 Train Loss 0.004326399 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "197 Train Loss 0.0043263994 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "198 Train Loss 0.0043263994 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "199 Train Loss 0.0043264003 Test MSE 0.2828017217440687 Test RE 0.1903543989635383\n",
      "200 Train Loss 0.0043262662 Test MSE 0.2828567635428223 Test RE 0.1903729224341474\n",
      "201 Train Loss 0.0043262667 Test MSE 0.2828567635428223 Test RE 0.1903729224341474\n",
      "202 Train Loss 0.004326267 Test MSE 0.2828567635428223 Test RE 0.1903729224341474\n",
      "203 Train Loss 0.004326267 Test MSE 0.2828567635428223 Test RE 0.1903729224341474\n",
      "204 Train Loss 0.00430505 Test MSE 0.2832236708875455 Test RE 0.19049635343517926\n",
      "205 Train Loss 0.00430505 Test MSE 0.2832236708875455 Test RE 0.19049635343517926\n",
      "206 Train Loss 0.0043036323 Test MSE 0.28322538621320537 Test RE 0.19049693029866557\n",
      "207 Train Loss 0.0042978744 Test MSE 0.28302218028200193 Test RE 0.190428580048597\n",
      "208 Train Loss 0.0042978744 Test MSE 0.28302218028200193 Test RE 0.190428580048597\n",
      "209 Train Loss 0.0042900043 Test MSE 0.28320131844904717 Test RE 0.1904888361573554\n",
      "210 Train Loss 0.0042887125 Test MSE 0.28311496878537956 Test RE 0.19045979338835106\n",
      "211 Train Loss 0.0042837304 Test MSE 0.28286330074710037 Test RE 0.190375122309938\n",
      "212 Train Loss 0.004283731 Test MSE 0.28286330074710037 Test RE 0.190375122309938\n",
      "213 Train Loss 0.0042837313 Test MSE 0.28286330074710037 Test RE 0.190375122309938\n",
      "214 Train Loss 0.004283732 Test MSE 0.28286330074710037 Test RE 0.190375122309938\n",
      "215 Train Loss 0.0042837323 Test MSE 0.28286330074710037 Test RE 0.190375122309938\n",
      "216 Train Loss 0.0042835525 Test MSE 0.28286330618605543 Test RE 0.19037512414022442\n",
      "217 Train Loss 0.0042823423 Test MSE 0.2828869934376649 Test RE 0.19038309507347076\n",
      "218 Train Loss 0.0042823427 Test MSE 0.2828869934376649 Test RE 0.19038309507347076\n",
      "219 Train Loss 0.004282343 Test MSE 0.2828869934376649 Test RE 0.19038309507347076\n",
      "220 Train Loss 0.004282344 Test MSE 0.2828869934376649 Test RE 0.19038309507347076\n",
      "221 Train Loss 0.0042823446 Test MSE 0.2828869934376649 Test RE 0.19038309507347076\n",
      "222 Train Loss 0.004282345 Test MSE 0.2828869934376649 Test RE 0.19038309507347076\n",
      "223 Train Loss 0.0042823213 Test MSE 0.2828869914268265 Test RE 0.19038309439682313\n",
      "224 Train Loss 0.004282322 Test MSE 0.2828869914268265 Test RE 0.19038309439682313\n",
      "225 Train Loss 0.0042823222 Test MSE 0.2828869914268265 Test RE 0.19038309439682313\n",
      "226 Train Loss 0.0042823227 Test MSE 0.2828869914268265 Test RE 0.19038309439682313\n",
      "227 Train Loss 0.0042823236 Test MSE 0.2828869914268265 Test RE 0.19038309439682313\n",
      "228 Train Loss 0.004282324 Test MSE 0.2828869914268265 Test RE 0.19038309439682313\n",
      "229 Train Loss 0.004282095 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "230 Train Loss 0.0042820955 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "231 Train Loss 0.004282096 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "232 Train Loss 0.0042820964 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "233 Train Loss 0.0042820973 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "234 Train Loss 0.004282098 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "235 Train Loss 0.0042820983 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "236 Train Loss 0.004282099 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "237 Train Loss 0.0042820997 Test MSE 0.28293722165967533 Test RE 0.19039999613317296\n",
      "238 Train Loss 0.004282008 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "239 Train Loss 0.004282009 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "240 Train Loss 0.00428201 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "241 Train Loss 0.0042820107 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "242 Train Loss 0.004282011 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "243 Train Loss 0.0042820117 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "244 Train Loss 0.0042820126 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "245 Train Loss 0.004282013 Test MSE 0.282937221700424 Test RE 0.19039999614688372\n",
      "246 Train Loss 0.0042793215 Test MSE 0.28293693533420383 Test RE 0.19039989979310462\n",
      "247 Train Loss 0.0042793225 Test MSE 0.28293693533420383 Test RE 0.19039989979310462\n",
      "248 Train Loss 0.004279323 Test MSE 0.28293693533420383 Test RE 0.19039989979310462\n",
      "249 Train Loss 0.004279324 Test MSE 0.28293693533420383 Test RE 0.19039989979310462\n",
      "250 Train Loss 0.004279323 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "251 Train Loss 0.004279324 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "252 Train Loss 0.0042793243 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "253 Train Loss 0.0042793253 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "254 Train Loss 0.004279326 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "255 Train Loss 0.0042793266 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "256 Train Loss 0.0042793276 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "257 Train Loss 0.0042793285 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "258 Train Loss 0.0042793294 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "259 Train Loss 0.00427933 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "260 Train Loss 0.0042793313 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "261 Train Loss 0.004279332 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "262 Train Loss 0.0042793327 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "263 Train Loss 0.004279334 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "264 Train Loss 0.0042793346 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "265 Train Loss 0.0042793355 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "266 Train Loss 0.004279337 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "267 Train Loss 0.0042793374 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "268 Train Loss 0.0042793383 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "269 Train Loss 0.0042793397 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "270 Train Loss 0.0042793406 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "271 Train Loss 0.0042793415 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "272 Train Loss 0.0042793425 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "273 Train Loss 0.0042793434 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "274 Train Loss 0.0042793443 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "275 Train Loss 0.0042793457 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "276 Train Loss 0.0042793467 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "277 Train Loss 0.0042793476 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "278 Train Loss 0.0042793485 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "279 Train Loss 0.0042793495 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "280 Train Loss 0.004279351 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "281 Train Loss 0.0042793523 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "282 Train Loss 0.0042793527 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "283 Train Loss 0.0042793546 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "284 Train Loss 0.0042793555 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "285 Train Loss 0.0042793564 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "286 Train Loss 0.004279358 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "287 Train Loss 0.004279359 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "288 Train Loss 0.00427936 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "289 Train Loss 0.0042793616 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "290 Train Loss 0.0042793625 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "291 Train Loss 0.004279364 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "292 Train Loss 0.0042793653 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "293 Train Loss 0.0042793667 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "294 Train Loss 0.004279368 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "295 Train Loss 0.0042793695 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "296 Train Loss 0.0042793704 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "297 Train Loss 0.0042793723 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "298 Train Loss 0.004279373 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "299 Train Loss 0.004279375 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "300 Train Loss 0.004279376 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "301 Train Loss 0.004279378 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "302 Train Loss 0.0042793793 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "303 Train Loss 0.0042793807 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "304 Train Loss 0.004279382 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "305 Train Loss 0.0042793835 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "306 Train Loss 0.0042793853 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "307 Train Loss 0.0042793867 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "308 Train Loss 0.004279388 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "309 Train Loss 0.00427939 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "310 Train Loss 0.0042793914 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "311 Train Loss 0.0042793932 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "312 Train Loss 0.0042793946 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "313 Train Loss 0.0042793965 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "314 Train Loss 0.0042793984 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "315 Train Loss 0.0042793993 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "316 Train Loss 0.0042794016 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "317 Train Loss 0.004279403 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "318 Train Loss 0.004279405 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "319 Train Loss 0.0042794067 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "320 Train Loss 0.0042794086 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "321 Train Loss 0.0042794105 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "322 Train Loss 0.004279412 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "323 Train Loss 0.004279414 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "324 Train Loss 0.0042794156 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "325 Train Loss 0.0042794175 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "326 Train Loss 0.00427942 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "327 Train Loss 0.0042794216 Test MSE 0.2829369443844019 Test RE 0.1903999028382299\n",
      "328 Train Loss 0.0042765373 Test MSE 0.2828143690525319 Test RE 0.19035865538013738\n",
      "329 Train Loss 0.004276539 Test MSE 0.2828143690525319 Test RE 0.19035865538013738\n",
      "330 Train Loss 0.004276541 Test MSE 0.2828143690525319 Test RE 0.19035865538013738\n",
      "331 Train Loss 0.0042765434 Test MSE 0.2828143690525319 Test RE 0.19035865538013738\n",
      "332 Train Loss 0.0042743315 Test MSE 0.2828137954255949 Test RE 0.19035846232964243\n",
      "333 Train Loss 0.004273463 Test MSE 0.28282873613339615 Test RE 0.19036349046590698\n",
      "334 Train Loss 0.004273465 Test MSE 0.28282873613339615 Test RE 0.19036349046590698\n",
      "335 Train Loss 0.0042734668 Test MSE 0.28282873613339615 Test RE 0.19036349046590698\n",
      "336 Train Loss 0.004273469 Test MSE 0.28282873613339615 Test RE 0.19036349046590698\n",
      "337 Train Loss 0.004273454 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "338 Train Loss 0.0042734565 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "339 Train Loss 0.004273459 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "340 Train Loss 0.0042734607 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "341 Train Loss 0.004273463 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "342 Train Loss 0.0042734654 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "343 Train Loss 0.0042734677 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "344 Train Loss 0.00427347 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "345 Train Loss 0.0042734724 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "346 Train Loss 0.0042734747 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "347 Train Loss 0.004273477 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "348 Train Loss 0.0042734793 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "349 Train Loss 0.004273482 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "350 Train Loss 0.0042734845 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "351 Train Loss 0.0042734873 Test MSE 0.28282873619534354 Test RE 0.19036349048675444\n",
      "352 Train Loss 0.0042733885 Test MSE 0.2828276440498509 Test RE 0.19036312294130678\n",
      "353 Train Loss 0.0042733913 Test MSE 0.2828276440498509 Test RE 0.19036312294130678\n",
      "354 Train Loss 0.004273394 Test MSE 0.2828276440498509 Test RE 0.19036312294130678\n",
      "355 Train Loss 0.004273387 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "356 Train Loss 0.00427339 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "357 Train Loss 0.0042733923 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "358 Train Loss 0.004273395 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "359 Train Loss 0.004273398 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "360 Train Loss 0.0042734006 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "361 Train Loss 0.0042734034 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "362 Train Loss 0.004273406 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "363 Train Loss 0.004273409 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "364 Train Loss 0.004273412 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "365 Train Loss 0.004273415 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "366 Train Loss 0.004273418 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "367 Train Loss 0.004273421 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "368 Train Loss 0.004273424 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "369 Train Loss 0.004273427 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "370 Train Loss 0.00427343 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "371 Train Loss 0.0042734332 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "372 Train Loss 0.0042734365 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "373 Train Loss 0.0042734398 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "374 Train Loss 0.0042734426 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "375 Train Loss 0.0042734463 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "376 Train Loss 0.004273449 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "377 Train Loss 0.0042734523 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "378 Train Loss 0.0042734556 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "379 Train Loss 0.004273459 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "380 Train Loss 0.0042734626 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "381 Train Loss 0.004273466 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "382 Train Loss 0.0042734696 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "383 Train Loss 0.004273473 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "384 Train Loss 0.004273476 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "385 Train Loss 0.00427348 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "386 Train Loss 0.0042734835 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "387 Train Loss 0.0042734873 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "388 Train Loss 0.004273491 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "389 Train Loss 0.0042734947 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "390 Train Loss 0.0042734984 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "391 Train Loss 0.004273502 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "392 Train Loss 0.004273506 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "393 Train Loss 0.00427351 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "394 Train Loss 0.004273514 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "395 Train Loss 0.0042735175 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "396 Train Loss 0.0042735217 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "397 Train Loss 0.0042735254 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "398 Train Loss 0.004273529 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "399 Train Loss 0.0042735334 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "400 Train Loss 0.0042735375 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "401 Train Loss 0.0042735417 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "402 Train Loss 0.004273546 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "403 Train Loss 0.00427355 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "404 Train Loss 0.0042735543 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "405 Train Loss 0.004273559 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "406 Train Loss 0.004273563 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "407 Train Loss 0.004273567 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "408 Train Loss 0.004273572 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "409 Train Loss 0.004273576 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "410 Train Loss 0.0042735804 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "411 Train Loss 0.004273585 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "412 Train Loss 0.0042735892 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "413 Train Loss 0.004273594 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "414 Train Loss 0.0042735985 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "415 Train Loss 0.004273603 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "416 Train Loss 0.004273608 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "417 Train Loss 0.004273613 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "418 Train Loss 0.004273617 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "419 Train Loss 0.004273622 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "420 Train Loss 0.004273627 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "421 Train Loss 0.0042736316 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "422 Train Loss 0.0042736367 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "423 Train Loss 0.0042736414 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "424 Train Loss 0.0042736465 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "425 Train Loss 0.0042736516 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "426 Train Loss 0.0042736563 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "427 Train Loss 0.0042736614 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "428 Train Loss 0.0042736665 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "429 Train Loss 0.004273671 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "430 Train Loss 0.004273677 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "431 Train Loss 0.004273682 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "432 Train Loss 0.0042736875 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "433 Train Loss 0.0042736926 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "434 Train Loss 0.0042736973 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "435 Train Loss 0.0042737033 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "436 Train Loss 0.0042737084 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "437 Train Loss 0.0042737136 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "438 Train Loss 0.004273719 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "439 Train Loss 0.0042737247 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "440 Train Loss 0.00427373 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "441 Train Loss 0.004273736 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "442 Train Loss 0.0042737415 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "443 Train Loss 0.004273747 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "444 Train Loss 0.0042737527 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "445 Train Loss 0.004273758 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "446 Train Loss 0.004273764 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "447 Train Loss 0.0042737694 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "448 Train Loss 0.004273775 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "449 Train Loss 0.004273781 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "450 Train Loss 0.0042737867 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "451 Train Loss 0.0042737923 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "452 Train Loss 0.0042737983 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "453 Train Loss 0.0042738044 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "454 Train Loss 0.0042738104 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "455 Train Loss 0.0042738165 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "456 Train Loss 0.0042738225 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "457 Train Loss 0.0042738286 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "458 Train Loss 0.0042738346 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "459 Train Loss 0.0042738407 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "460 Train Loss 0.0042738467 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "461 Train Loss 0.004273853 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "462 Train Loss 0.0042738593 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "463 Train Loss 0.0042738654 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "464 Train Loss 0.0042738714 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "465 Train Loss 0.0042738775 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "466 Train Loss 0.0042738835 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "467 Train Loss 0.00427389 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "468 Train Loss 0.0042738966 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "469 Train Loss 0.004273903 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "470 Train Loss 0.004273909 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "471 Train Loss 0.0042739157 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "472 Train Loss 0.004273922 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "473 Train Loss 0.0042739287 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "474 Train Loss 0.004273935 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "475 Train Loss 0.0042739417 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "476 Train Loss 0.0042739487 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "477 Train Loss 0.004273955 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "478 Train Loss 0.0042739613 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "479 Train Loss 0.004273968 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "480 Train Loss 0.004273975 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "481 Train Loss 0.0042739813 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "482 Train Loss 0.004273988 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "483 Train Loss 0.0042739944 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "484 Train Loss 0.0042740013 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "485 Train Loss 0.004274008 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "486 Train Loss 0.004274015 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "487 Train Loss 0.0042740214 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "488 Train Loss 0.0042740284 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "489 Train Loss 0.0042740353 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "490 Train Loss 0.004274042 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "491 Train Loss 0.004274049 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "492 Train Loss 0.0042740554 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "493 Train Loss 0.0042740623 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "494 Train Loss 0.00427407 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "495 Train Loss 0.0042740763 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "496 Train Loss 0.0042740833 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "497 Train Loss 0.00427409 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "498 Train Loss 0.004274097 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "499 Train Loss 0.0042741043 Test MSE 0.28282764499641017 Test RE 0.19036312325985766\n",
      "Training time: 138.89\n",
      "Training time: 138.89\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1 #10\n",
    "max_iter = 500 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "N_I = 1000\n",
    "N_D = 1000\n",
    "N_f = 5000\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr= 1.0, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4900066b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred= PINN.test()\n",
    "plt.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = 'jet',vmax = 4,vmin = 0.2)\n",
    "plt.figure()\n",
    "plt.imshow(np.flip(u_true.reshape(500,500),axis = 0),cmap = 'jet',vmax = 4,vmin = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
