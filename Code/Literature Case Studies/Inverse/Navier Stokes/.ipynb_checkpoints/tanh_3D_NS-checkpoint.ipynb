{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "data = scipy.io.loadmat('cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_tanh\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.random.choice(N_train, 5000, replace=False)\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# x = np.linspace(1,8,100).reshape(-1,1)\n",
    "# y = np.linspace(-2,2,100).reshape(-1,1)\n",
    "# t = np.linspace(0,20,100).reshape(-1,1)\n",
    "\n",
    "# X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "# X = X.flatten('F').reshape(-1,1)\n",
    "# Y = Y.flatten('F').reshape(-1,1)\n",
    "# T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "# xyt = np.hstack((X,Y,T))\n",
    "\n",
    "# initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "# DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "# NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "# NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "# NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "# NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "# xyt_initial = xyt[initial_pts,:]\n",
    "# xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "# xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "# xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "# #xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "# xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "# u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "# u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "# xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "# #xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "# xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "# #xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "# xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "# u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "# lb_xyt = xyt[0]\n",
    "# ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "# xy = fea_data['xy']\n",
    "# t = fea_data['t']/3000\n",
    "# xyt = np.zeros((497*101,3))\n",
    "# u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "# for i in range(101):\n",
    "#     t_temp = t[0,i]*np.ones((497,1))\n",
    "#     xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "#     u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "#     #print(i)\n",
    "# #print(xyt)\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "# u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_T,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    idx = np.random.choice(N_train, N_T, replace=False)\n",
    "    x_train = x[idx,:]\n",
    "    y_train = y[idx,:]\n",
    "    t_train = t[idx,:]\n",
    "    u_train = u_true[idx,:]\n",
    "    v_train = v_true[idx,:]\n",
    "    \n",
    "    return x_train,y_train,t_train,u_train,v_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "\n",
    "        \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requires_grad = True\n",
    "        \n",
    "        self.lambda2 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda2.requires_grad = True\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = 2.0*(xyt- lbxyt)/(ubxyt - lbxyt)-1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_uv(self,x_train,y_train,t_train,u_train,v_train):\n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "#         print(torch.cat((x1,y1,t1),dim=1).shape)\n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        \n",
    "        psi = psi_p[:,0:1]\n",
    "        \n",
    "#         print(psi.shape)\n",
    "        psi_x = autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        psi_y = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        loss_u = self.loss_function(psi_y,u_train)\n",
    "        loss_v = self.loss_function(-1*psi_x,v_train)\n",
    "                \n",
    "        return loss_u + loss_v\n",
    "    \n",
    "    def loss_PDE(self, x_train,y_train,t_train,fg_hat):\n",
    "        \n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p[:,0:1]\n",
    "        p = psi_p[:,1:2]\n",
    "        \n",
    "        \n",
    "        u = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        u_t = autograd.grad(u,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_x = autograd.grad(u,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_xx = autograd.grad(u_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        u_y = autograd.grad(u,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_yy = autograd.grad(u_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------\n",
    "        \n",
    "        v_t = autograd.grad(v,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        v_x = autograd.grad(v,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_xx = autograd.grad(v_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        v_y = autograd.grad(v,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_yy = autograd.grad(v_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #------------------------------------------------------------------------------------\n",
    "        p_x = autograd.grad(p,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        p_y = autograd.grad(p,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "            \n",
    "            \n",
    "\n",
    "        f = u_t + self.lambda1*(u*u_x + v*u_y) + p_x - self.lambda2*(u_xx + u_yy)\n",
    "        g = v_t + self.lambda1*(u*v_x + v*v_y) + p_y - self.lambda2*(v_xx + v_yy)\n",
    "        \n",
    "        loss_f = self.loss_function(f,fg_hat)\n",
    "        loss_g = self.loss_function(g,fg_hat)\n",
    "                \n",
    "        return loss_f + loss_g\n",
    "    \n",
    "    def loss(self,x_train,y_train,t_train,u_train,v_train,fg_hat):\n",
    "\n",
    "        loss_uv = self.loss_uv(x_train,y_train,t_train,u_train,v_train)\n",
    "        loss_fg = self.loss_PDE(x_train,y_train,t_train,fg_hat)\n",
    "        loss_val = loss_uv + loss_fg\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        x1 = x_tensor.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_tensor.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_tensor.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p_pred = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p_pred[:,0:1]\n",
    "        p_pred = psi_p_pred[:,1:2]\n",
    "        \n",
    "        u_pred = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_pred = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "   \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        v_pred = v_pred.cpu().detach().numpy()\n",
    "        p_pred = p_pred.cpu().detach().numpy()\n",
    "    \n",
    "        return u_pred,v_pred,p_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred,v_pred,p_pred = self.test()\n",
    "        \n",
    "        test_mse_u = np.mean(np.square(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1)))\n",
    "        test_re_u = np.linalg.norm(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        test_mse_v = np.mean(np.square(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1)))\n",
    "        test_re_v = np.linalg.norm(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1),2)/v_true_norm\n",
    "        \n",
    "        test_mse_p = np.mean(np.square(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1)))\n",
    "        test_re_p = np.linalg.norm(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1),2)/p_true_norm\n",
    "        \n",
    "        return test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np,x_train,y_train,t_train):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    lambda2_val.append(PINN.lambda2.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p = PINN.test_loss()\n",
    "    \n",
    "    test_mse_u_loss.append(test_mse_u)\n",
    "    test_re_u_loss.append(test_re_u)\n",
    "    \n",
    "    test_mse_v_loss.append(test_mse_v)\n",
    "    test_re_v_loss.append(test_re_v)\n",
    "    \n",
    "    test_mse_p_loss.append(test_mse_p)\n",
    "    test_re_p_loss.append(test_re_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_train,y_train,t_train,u_train,v_train = trainingdata(N_T,(reps)*22)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train).float().to(device)\n",
    "    t_train = torch.from_numpy(t_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    v_train = torch.from_numpy(v_train).float().to(device)\n",
    "        \n",
    "    fg_hat = torch.zeros(x_train.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np,x_train,y_train,t_train)\n",
    "        print(i,\"Loss\",train_loss[-1], \"L1\",lambda1_val[-1],\"L2\",lambda2_val[-1])\n",
    "        # print(i,\"Loss\",train_loss[-1],\"RE\",test_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_NS_tanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Loss 0.1477441 L1 -0.004175452 L2 0.01964014\n",
      "1 Loss 0.116324306 L1 -0.016262304 L2 -0.0067131794\n",
      "2 Loss 0.0998215 L1 -0.021001097 L2 0.0017010216\n",
      "3 Loss 0.096306674 L1 -0.012480528 L2 -0.00026701708\n",
      "4 Loss 0.0957253 L1 0.01167399 L2 0.0010510955\n",
      "5 Loss 0.095523015 L1 0.017231854 L2 0.0007313483\n",
      "6 Loss 0.09530563 L1 -0.008415423 L2 -0.0006976991\n",
      "7 Loss 0.09510628 L1 -0.033067595 L2 0.00016635968\n",
      "8 Loss 0.09477197 L1 -0.02422425 L2 -0.0004923684\n",
      "9 Loss 0.094499834 L1 -0.016607204 L2 5.606632e-05\n",
      "10 Loss 0.09427968 L1 -0.013182117 L2 0.0001473676\n",
      "11 Loss 0.09408815 L1 0.005684763 L2 0.00022580754\n",
      "12 Loss 0.09391228 L1 0.005300239 L2 0.00080581603\n",
      "13 Loss 0.09375178 L1 -0.00050821295 L2 0.000712398\n",
      "14 Loss 0.09347627 L1 0.00627143 L2 0.0011890691\n",
      "15 Loss 0.09307002 L1 0.025580468 L2 0.0016800993\n",
      "16 Loss 0.09277219 L1 0.02871179 L2 0.0024359107\n",
      "17 Loss 0.09242021 L1 0.04371308 L2 0.0020130752\n",
      "18 Loss 0.092138454 L1 0.038896043 L2 0.001308211\n",
      "19 Loss 0.091867 L1 0.04816774 L2 0.0019661125\n",
      "20 Loss 0.09164045 L1 0.06354241 L2 0.0034843213\n",
      "21 Loss 0.09145633 L1 0.083397605 L2 0.003926314\n",
      "22 Loss 0.09129284 L1 0.07610275 L2 0.0037341875\n",
      "23 Loss 0.091034636 L1 0.079779394 L2 0.0041303146\n",
      "24 Loss 0.090561375 L1 0.10447872 L2 0.0045664627\n",
      "25 Loss 0.089725055 L1 0.11396891 L2 0.0053732945\n",
      "26 Loss 0.08867918 L1 0.14711025 L2 0.0045716157\n",
      "27 Loss 0.08746552 L1 0.2361866 L2 0.009788388\n",
      "28 Loss 0.08609187 L1 0.36097473 L2 0.013161569\n",
      "29 Loss 0.0816871 L1 0.58326066 L2 0.015855521\n",
      "30 Loss 0.07283342 L1 0.8215617 L2 0.019523442\n",
      "31 Loss 0.06426649 L1 0.8335077 L2 0.016069286\n",
      "32 Loss 0.053545345 L1 0.873465 L2 0.019065687\n",
      "33 Loss 0.03662774 L1 0.8592644 L2 0.016489778\n",
      "34 Loss 0.026605554 L1 0.84664696 L2 0.013704835\n",
      "35 Loss 0.022395287 L1 0.86682874 L2 0.0147761395\n",
      "36 Loss 0.020082224 L1 0.88168025 L2 0.014942662\n",
      "37 Loss 0.017963463 L1 0.8908122 L2 0.015659956\n",
      "38 Loss 0.0160699 L1 0.92007864 L2 0.015953539\n",
      "39 Loss 0.013980025 L1 0.9201454 L2 0.015478305\n",
      "40 Loss 0.012605455 L1 0.91474247 L2 0.014909171\n",
      "41 Loss 0.011493564 L1 0.9265002 L2 0.016419455\n",
      "42 Loss 0.010342868 L1 0.9322018 L2 0.015453331\n",
      "43 Loss 0.009336928 L1 0.9407078 L2 0.014026246\n",
      "44 Loss 0.0085082585 L1 0.944023 L2 0.015310124\n",
      "45 Loss 0.007727637 L1 0.9469888 L2 0.015812049\n",
      "46 Loss 0.006921197 L1 0.9483509 L2 0.014420114\n",
      "47 Loss 0.006289521 L1 0.9591029 L2 0.014539057\n",
      "48 Loss 0.0058206157 L1 0.96180207 L2 0.014063327\n",
      "49 Loss 0.0053522876 L1 0.96196336 L2 0.013885848\n",
      "50 Loss 0.0049345894 L1 0.9708726 L2 0.014170571\n",
      "51 Loss 0.0045966054 L1 0.9715552 L2 0.014276663\n",
      "52 Loss 0.004243425 L1 0.97407985 L2 0.014079753\n",
      "53 Loss 0.003937768 L1 0.9732995 L2 0.013750771\n",
      "54 Loss 0.003646337 L1 0.9761046 L2 0.01417245\n",
      "55 Loss 0.0034171806 L1 0.97566926 L2 0.013360405\n",
      "56 Loss 0.0032036235 L1 0.9810251 L2 0.013321689\n",
      "57 Loss 0.0030007535 L1 0.98003775 L2 0.013506161\n",
      "58 Loss 0.0028263489 L1 0.9800989 L2 0.013183688\n",
      "59 Loss 0.002675681 L1 0.98017293 L2 0.013595369\n",
      "60 Loss 0.002517061 L1 0.98176754 L2 0.013726686\n",
      "61 Loss 0.0023858142 L1 0.9840335 L2 0.013085964\n",
      "62 Loss 0.0022621518 L1 0.9818618 L2 0.01265913\n",
      "63 Loss 0.0021485477 L1 0.9829813 L2 0.012369762\n",
      "64 Loss 0.0020456824 L1 0.9830498 L2 0.0118034305\n",
      "65 Loss 0.0019514973 L1 0.9831737 L2 0.011981758\n",
      "66 Loss 0.0018602501 L1 0.9853664 L2 0.011787962\n",
      "67 Loss 0.0017670057 L1 0.9856044 L2 0.011961079\n",
      "68 Loss 0.0016941177 L1 0.98511004 L2 0.011873401\n",
      "69 Loss 0.0016338488 L1 0.9865556 L2 0.011929765\n",
      "70 Loss 0.0015751517 L1 0.98632735 L2 0.011890084\n",
      "71 Loss 0.0015112197 L1 0.98594123 L2 0.011992592\n",
      "72 Loss 0.0014484113 L1 0.98448664 L2 0.0117674535\n",
      "73 Loss 0.0013899148 L1 0.9859996 L2 0.011568768\n",
      "74 Loss 0.0013378836 L1 0.987568 L2 0.011710502\n",
      "75 Loss 0.0012883376 L1 0.9872658 L2 0.011605477\n",
      "76 Loss 0.0012408646 L1 0.9877875 L2 0.011593006\n",
      "77 Loss 0.0011936073 L1 0.98872584 L2 0.011612796\n",
      "78 Loss 0.0011546453 L1 0.988794 L2 0.011630634\n",
      "79 Loss 0.0011206523 L1 0.9892409 L2 0.011518529\n",
      "80 Loss 0.0010784849 L1 0.9903087 L2 0.01164478\n",
      "81 Loss 0.0010418147 L1 0.99078673 L2 0.011797121\n",
      "82 Loss 0.0010100356 L1 0.9892702 L2 0.011632094\n",
      "83 Loss 0.0009779611 L1 0.9904878 L2 0.011652067\n",
      "84 Loss 0.0009498738 L1 0.9903219 L2 0.011707361\n",
      "85 Loss 0.0009287052 L1 0.9913146 L2 0.011757026\n",
      "86 Loss 0.00090382434 L1 0.99133396 L2 0.011743597\n",
      "87 Loss 0.00088173634 L1 0.9917994 L2 0.011978115\n",
      "88 Loss 0.00085700164 L1 0.9922377 L2 0.011662544\n",
      "89 Loss 0.00083410717 L1 0.99255383 L2 0.0116005875\n",
      "90 Loss 0.0008138153 L1 0.9920888 L2 0.011832033\n",
      "91 Loss 0.0007915869 L1 0.9923853 L2 0.011492859\n",
      "92 Loss 0.00077046873 L1 0.9919348 L2 0.011566242\n",
      "93 Loss 0.00074815465 L1 0.99335086 L2 0.011483328\n",
      "94 Loss 0.0007279207 L1 0.9927376 L2 0.0116243\n",
      "95 Loss 0.0007078537 L1 0.9924938 L2 0.011680436\n",
      "96 Loss 0.00069104007 L1 0.9926723 L2 0.011583708\n",
      "97 Loss 0.00067357207 L1 0.99322444 L2 0.011607032\n",
      "98 Loss 0.00065494364 L1 0.9934081 L2 0.011542382\n",
      "99 Loss 0.0006404872 L1 0.99315286 L2 0.011539095\n",
      "100 Loss 0.00062562316 L1 0.9938757 L2 0.011463376\n",
      "101 Loss 0.0006101368 L1 0.99337006 L2 0.011434907\n",
      "102 Loss 0.0005968934 L1 0.9932413 L2 0.01145915\n",
      "103 Loss 0.00058321777 L1 0.9936676 L2 0.011385593\n",
      "104 Loss 0.0005694493 L1 0.99354774 L2 0.01148043\n",
      "105 Loss 0.0005577888 L1 0.9941432 L2 0.011380714\n",
      "106 Loss 0.0005446166 L1 0.9940791 L2 0.011441442\n",
      "107 Loss 0.00053310953 L1 0.9946094 L2 0.011461596\n",
      "108 Loss 0.00052181253 L1 0.9943533 L2 0.011399068\n",
      "109 Loss 0.00051189517 L1 0.99518585 L2 0.0114976\n",
      "110 Loss 0.00050308654 L1 0.9948401 L2 0.011488448\n",
      "111 Loss 0.0004940055 L1 0.99487215 L2 0.011428324\n",
      "112 Loss 0.00048506635 L1 0.9949265 L2 0.011337797\n",
      "113 Loss 0.00047582644 L1 0.9944075 L2 0.01131605\n",
      "114 Loss 0.00046712745 L1 0.9947933 L2 0.011397634\n",
      "115 Loss 0.00045829156 L1 0.9954873 L2 0.011372556\n",
      "116 Loss 0.00044943945 L1 0.995376 L2 0.011398492\n",
      "117 Loss 0.00044126893 L1 0.9951756 L2 0.011381931\n",
      "118 Loss 0.00043460363 L1 0.9956013 L2 0.011384586\n",
      "119 Loss 0.00042723067 L1 0.99579734 L2 0.011307091\n",
      "120 Loss 0.00042107582 L1 0.99551296 L2 0.011340886\n",
      "121 Loss 0.00041466014 L1 0.995321 L2 0.011299711\n",
      "122 Loss 0.0004076584 L1 0.99603784 L2 0.01132265\n",
      "123 Loss 0.0004012514 L1 0.99582183 L2 0.011289134\n",
      "124 Loss 0.000395145 L1 0.9957518 L2 0.011265413\n",
      "125 Loss 0.00038850383 L1 0.9956904 L2 0.011348435\n",
      "126 Loss 0.00038237867 L1 0.9957298 L2 0.011291664\n",
      "127 Loss 0.00037699618 L1 0.9955586 L2 0.011226692\n",
      "128 Loss 0.00037054124 L1 0.9961769 L2 0.011235491\n",
      "129 Loss 0.00036455685 L1 0.99652076 L2 0.011224578\n",
      "130 Loss 0.0003593316 L1 0.996476 L2 0.011283942\n",
      "131 Loss 0.0003539913 L1 0.9964527 L2 0.0112319235\n",
      "132 Loss 0.00034865816 L1 0.9962198 L2 0.011255152\n",
      "133 Loss 0.0003432635 L1 0.9963807 L2 0.011297214\n",
      "134 Loss 0.00033798543 L1 0.99654347 L2 0.011315042\n",
      "135 Loss 0.00033244363 L1 0.9968782 L2 0.011323965\n",
      "136 Loss 0.00032690616 L1 0.9961386 L2 0.011294423\n",
      "137 Loss 0.0003223392 L1 0.9962826 L2 0.011303484\n",
      "138 Loss 0.0003177715 L1 0.99651307 L2 0.011214585\n",
      "139 Loss 0.00031315285 L1 0.9965379 L2 0.011230788\n",
      "140 Loss 0.00030847272 L1 0.9965438 L2 0.011299409\n",
      "141 Loss 0.0003035771 L1 0.99649584 L2 0.011331948\n",
      "142 Loss 0.00029952574 L1 0.9969959 L2 0.011327039\n",
      "143 Loss 0.0002955904 L1 0.9967223 L2 0.011304522\n",
      "144 Loss 0.0002918494 L1 0.99688566 L2 0.0113027645\n",
      "145 Loss 0.00028806168 L1 0.99715656 L2 0.011267946\n",
      "146 Loss 0.00028401296 L1 0.99681735 L2 0.011252444\n",
      "147 Loss 0.00028029422 L1 0.99712497 L2 0.01128831\n",
      "148 Loss 0.0002772052 L1 0.99718595 L2 0.011240768\n",
      "149 Loss 0.00027355208 L1 0.9973288 L2 0.011294381\n",
      "150 Loss 0.00027022476 L1 0.9973382 L2 0.01125075\n",
      "151 Loss 0.00026716065 L1 0.99748135 L2 0.01123852\n",
      "152 Loss 0.0002639812 L1 0.997487 L2 0.0112267\n",
      "153 Loss 0.0002609765 L1 0.99768 L2 0.011221434\n",
      "154 Loss 0.00025787402 L1 0.9976301 L2 0.011213824\n",
      "155 Loss 0.00025491495 L1 0.9975242 L2 0.011204408\n",
      "156 Loss 0.00025178754 L1 0.99757993 L2 0.0111901695\n",
      "157 Loss 0.0002485108 L1 0.9978943 L2 0.011212496\n",
      "158 Loss 0.00024521298 L1 0.9975347 L2 0.011200634\n",
      "159 Loss 0.00024207399 L1 0.9978998 L2 0.011212675\n",
      "160 Loss 0.00023901837 L1 0.99790335 L2 0.011227324\n",
      "161 Loss 0.00023623282 L1 0.99784124 L2 0.011213724\n",
      "162 Loss 0.00023329015 L1 0.9978963 L2 0.011215067\n",
      "163 Loss 0.00023014095 L1 0.9981127 L2 0.011209884\n",
      "164 Loss 0.00022738708 L1 0.9977921 L2 0.011218863\n",
      "165 Loss 0.00022468818 L1 0.99838305 L2 0.011207926\n",
      "166 Loss 0.00022181995 L1 0.99810773 L2 0.011201607\n",
      "167 Loss 0.00021935461 L1 0.9983153 L2 0.011183959\n",
      "168 Loss 0.000216832 L1 0.99825984 L2 0.01121632\n",
      "169 Loss 0.00021440753 L1 0.9983775 L2 0.011221942\n",
      "170 Loss 0.00021184518 L1 0.9983131 L2 0.011271657\n",
      "171 Loss 0.00020917968 L1 0.99815094 L2 0.011223897\n",
      "172 Loss 0.00020688487 L1 0.99850476 L2 0.011206101\n",
      "173 Loss 0.00020446836 L1 0.99865264 L2 0.011223475\n",
      "174 Loss 0.00020236213 L1 0.9985626 L2 0.011230391\n",
      "175 Loss 0.00020015339 L1 0.9986156 L2 0.011229639\n",
      "176 Loss 0.00019789851 L1 0.9988083 L2 0.01122965\n",
      "177 Loss 0.00019566559 L1 0.9986591 L2 0.011206373\n",
      "178 Loss 0.0001936504 L1 0.9986034 L2 0.011215634\n",
      "179 Loss 0.00019130773 L1 0.99846804 L2 0.011187202\n",
      "180 Loss 0.0001891248 L1 0.99861914 L2 0.011198984\n",
      "181 Loss 0.00018720396 L1 0.99865985 L2 0.01119874\n",
      "182 Loss 0.00018516337 L1 0.9987494 L2 0.011205252\n",
      "183 Loss 0.0001828133 L1 0.99876267 L2 0.011198724\n",
      "184 Loss 0.0001809358 L1 0.99846464 L2 0.011179011\n",
      "185 Loss 0.00017908585 L1 0.998685 L2 0.01115839\n",
      "186 Loss 0.0001774024 L1 0.99874675 L2 0.011171983\n",
      "187 Loss 0.00017553548 L1 0.9988016 L2 0.011160739\n",
      "188 Loss 0.00017397187 L1 0.9988735 L2 0.011143974\n",
      "189 Loss 0.00017250706 L1 0.99879116 L2 0.011141786\n",
      "190 Loss 0.00017086115 L1 0.9990309 L2 0.011152939\n",
      "191 Loss 0.00016946897 L1 0.9992438 L2 0.011144169\n",
      "192 Loss 0.00016799397 L1 0.99879324 L2 0.011155748\n",
      "193 Loss 0.00016657985 L1 0.9989445 L2 0.01114443\n",
      "194 Loss 0.00016528589 L1 0.99879533 L2 0.011145458\n",
      "195 Loss 0.0001638796 L1 0.99861276 L2 0.011150692\n",
      "196 Loss 0.00016254696 L1 0.99893886 L2 0.011163128\n",
      "197 Loss 0.00016088106 L1 0.99876076 L2 0.0111487\n",
      "198 Loss 0.00015947965 L1 0.9986644 L2 0.011143855\n",
      "199 Loss 0.00015804268 L1 0.99884176 L2 0.011147437\n",
      "200 Loss 0.00015662442 L1 0.998729 L2 0.0111471135\n",
      "201 Loss 0.00015536894 L1 0.9987725 L2 0.011114708\n",
      "202 Loss 0.00015383665 L1 0.9987268 L2 0.011109561\n",
      "203 Loss 0.00015264076 L1 0.99874043 L2 0.011089602\n",
      "204 Loss 0.00015138807 L1 0.99895775 L2 0.011119794\n",
      "205 Loss 0.00015017303 L1 0.9989262 L2 0.011111057\n",
      "206 Loss 0.00014890546 L1 0.9988257 L2 0.0110737635\n",
      "207 Loss 0.0001474274 L1 0.99877447 L2 0.01108424\n",
      "208 Loss 0.00014606153 L1 0.9987904 L2 0.0110780755\n",
      "209 Loss 0.00014517407 L1 0.99877596 L2 0.011102012\n",
      "210 Loss 0.000143896 L1 0.9988023 L2 0.0111154765\n",
      "211 Loss 0.00014256296 L1 0.9987835 L2 0.0111038685\n",
      "212 Loss 0.00014135482 L1 0.99857324 L2 0.011054802\n",
      "213 Loss 0.00014047947 L1 0.9987228 L2 0.011051318\n",
      "214 Loss 0.00013932494 L1 0.99864393 L2 0.011085915\n",
      "215 Loss 0.0001392668 L1 0.9986899 L2 0.01111936\n",
      "216 Loss 0.00013806805 L1 0.9986312 L2 0.011089067\n",
      "217 Loss 0.00013703859 L1 0.9986669 L2 0.011091329\n",
      "218 Loss 0.00013610671 L1 0.99867773 L2 0.011056082\n",
      "219 Loss 0.00013508856 L1 0.9985522 L2 0.011049885\n",
      "220 Loss 0.00013393653 L1 0.99871916 L2 0.01102607\n",
      "221 Loss 0.00013288957 L1 0.9987911 L2 0.011016452\n",
      "222 Loss 0.00013174523 L1 0.9987775 L2 0.011027642\n",
      "223 Loss 0.00013092728 L1 0.99878466 L2 0.010995331\n",
      "224 Loss 0.00012989013 L1 0.9988414 L2 0.011005396\n",
      "225 Loss 0.00012892982 L1 0.9988705 L2 0.01099584\n",
      "226 Loss 0.00012795757 L1 0.9987117 L2 0.010999939\n",
      "227 Loss 0.00012694244 L1 0.9988293 L2 0.011004736\n",
      "228 Loss 0.00012598014 L1 0.9988175 L2 0.010974275\n",
      "229 Loss 0.00012509043 L1 0.99886674 L2 0.010990043\n",
      "230 Loss 0.0001241513 L1 0.9987931 L2 0.010992146\n",
      "231 Loss 0.00012327946 L1 0.99888664 L2 0.010972308\n",
      "232 Loss 0.00012234585 L1 0.9987692 L2 0.010970014\n",
      "233 Loss 0.000121336445 L1 0.99863297 L2 0.010978659\n",
      "234 Loss 0.00012049572 L1 0.9986681 L2 0.010953689\n",
      "235 Loss 0.000119611126 L1 0.99855644 L2 0.010971524\n",
      "236 Loss 0.00011867272 L1 0.9985195 L2 0.010947386\n",
      "237 Loss 0.000118519245 L1 0.99855894 L2 0.01095885\n",
      "238 Loss 0.000117714284 L1 0.99850595 L2 0.010940527\n",
      "239 Loss 0.00011688325 L1 0.99855065 L2 0.010940023\n",
      "240 Loss 0.000116002266 L1 0.99893093 L2 0.010943517\n",
      "241 Loss 0.00011517024 L1 0.9986012 L2 0.010925562\n",
      "242 Loss 0.00011437019 L1 0.9987892 L2 0.010941933\n",
      "243 Loss 0.00011401564 L1 0.99864405 L2 0.010934385\n",
      "244 Loss 0.00011331794 L1 0.99877983 L2 0.010945862\n",
      "245 Loss 0.00011322582 L1 0.9987841 L2 0.010935941\n",
      "246 Loss 0.00011255723 L1 0.9987326 L2 0.01093685\n",
      "247 Loss 0.000111783156 L1 0.99887383 L2 0.0109500885\n",
      "248 Loss 0.00011098211 L1 0.99893194 L2 0.010967057\n",
      "249 Loss 0.0001101619 L1 0.99894524 L2 0.010972698\n",
      "250 Loss 0.000109287794 L1 0.9990736 L2 0.010947498\n",
      "251 Loss 0.00010850622 L1 0.99889046 L2 0.010951396\n",
      "252 Loss 0.00010763042 L1 0.99896467 L2 0.010946995\n",
      "253 Loss 0.000106858555 L1 0.9989653 L2 0.0109430095\n",
      "254 Loss 0.00010609103 L1 0.99890804 L2 0.010957252\n",
      "255 Loss 0.000106020816 L1 0.9989669 L2 0.010963929\n",
      "256 Loss 0.00010518314 L1 0.9989777 L2 0.010936468\n",
      "257 Loss 0.000104411316 L1 0.99896026 L2 0.010935312\n",
      "258 Loss 0.000103690414 L1 0.9988 L2 0.010918125\n",
      "259 Loss 0.00010297361 L1 0.99895847 L2 0.010922121\n",
      "260 Loss 0.00010215878 L1 0.9990507 L2 0.010918525\n",
      "261 Loss 0.00010155878 L1 0.9989906 L2 0.010913931\n",
      "262 Loss 0.000100910314 L1 0.9990328 L2 0.0109231295\n",
      "263 Loss 0.00010026085 L1 0.9990828 L2 0.010909605\n",
      "264 Loss 9.963145e-05 L1 0.99900687 L2 0.010919302\n",
      "265 Loss 9.892826e-05 L1 0.9989352 L2 0.010897367\n",
      "266 Loss 9.834921e-05 L1 0.9988817 L2 0.01089878\n",
      "267 Loss 9.763899e-05 L1 0.9989555 L2 0.010899936\n",
      "268 Loss 9.695247e-05 L1 0.99886554 L2 0.010918262\n",
      "269 Loss 9.632434e-05 L1 0.9989337 L2 0.010906991\n",
      "270 Loss 9.56879e-05 L1 0.9991335 L2 0.010908254\n",
      "271 Loss 9.515103e-05 L1 0.9990173 L2 0.01090855\n",
      "272 Loss 9.457247e-05 L1 0.9990328 L2 0.010925519\n",
      "273 Loss 9.396324e-05 L1 0.9990866 L2 0.010900778\n",
      "274 Loss 9.337105e-05 L1 0.99916065 L2 0.010891807\n",
      "275 Loss 9.280879e-05 L1 0.9990927 L2 0.0108918855\n",
      "276 Loss 9.2184266e-05 L1 0.9991505 L2 0.010880154\n",
      "277 Loss 9.1652284e-05 L1 0.99904686 L2 0.010878926\n",
      "278 Loss 9.1306036e-05 L1 0.999012 L2 0.01088644\n",
      "279 Loss 9.073756e-05 L1 0.9990651 L2 0.01087864\n",
      "280 Loss 9.017454e-05 L1 0.9989772 L2 0.010890642\n",
      "281 Loss 8.963313e-05 L1 0.99891 L2 0.010891804\n",
      "282 Loss 8.912692e-05 L1 0.99904937 L2 0.010876946\n",
      "283 Loss 8.879668e-05 L1 0.99907434 L2 0.010896648\n",
      "284 Loss 8.832282e-05 L1 0.99904835 L2 0.010858658\n",
      "285 Loss 8.775655e-05 L1 0.9990375 L2 0.010893358\n",
      "286 Loss 8.717743e-05 L1 0.99904823 L2 0.010890789\n",
      "287 Loss 8.689717e-05 L1 0.9990627 L2 0.010896677\n",
      "288 Loss 8.677425e-05 L1 0.9990806 L2 0.010876653\n",
      "289 Loss 8.6571425e-05 L1 0.99912965 L2 0.010903705\n",
      "290 Loss 8.606947e-05 L1 0.99898535 L2 0.010887515\n",
      "291 Loss 8.5521475e-05 L1 0.9990169 L2 0.010885174\n",
      "292 Loss 8.496907e-05 L1 0.99919 L2 0.010888953\n",
      "293 Loss 8.434472e-05 L1 0.99911875 L2 0.010902748\n",
      "294 Loss 8.382015e-05 L1 0.9991094 L2 0.010902556\n",
      "295 Loss 8.324084e-05 L1 0.9990979 L2 0.010907038\n",
      "296 Loss 8.2764556e-05 L1 0.99914414 L2 0.010903315\n",
      "297 Loss 8.221643e-05 L1 0.99930054 L2 0.010905309\n",
      "298 Loss 8.209598e-05 L1 0.9992594 L2 0.010908505\n",
      "299 Loss 8.150933e-05 L1 0.9992314 L2 0.010901216\n",
      "300 Loss 8.138184e-05 L1 0.9992788 L2 0.010874694\n",
      "301 Loss 8.0979466e-05 L1 0.99913913 L2 0.010898504\n",
      "302 Loss 8.0815575e-05 L1 0.999186 L2 0.010902193\n",
      "303 Loss 8.037675e-05 L1 0.9991964 L2 0.010914301\n",
      "304 Loss 8.0287806e-05 L1 0.9991479 L2 0.010896534\n",
      "305 Loss 7.981107e-05 L1 0.99916166 L2 0.010896622\n",
      "306 Loss 7.93619e-05 L1 0.9992144 L2 0.010896828\n",
      "307 Loss 7.88982e-05 L1 0.99917614 L2 0.010885685\n",
      "308 Loss 7.855352e-05 L1 0.99922717 L2 0.010876412\n",
      "309 Loss 7.846179e-05 L1 0.99926174 L2 0.010881156\n",
      "310 Loss 7.824851e-05 L1 0.9993086 L2 0.010891784\n",
      "311 Loss 7.778325e-05 L1 0.9992709 L2 0.01088278\n",
      "312 Loss 7.7345336e-05 L1 0.9991677 L2 0.010879055\n",
      "313 Loss 7.6888966e-05 L1 0.99921477 L2 0.010889434\n",
      "314 Loss 7.6426804e-05 L1 0.99916524 L2 0.01088811\n",
      "315 Loss 7.624325e-05 L1 0.999181 L2 0.010880382\n",
      "316 Loss 7.581019e-05 L1 0.99920404 L2 0.010892076\n",
      "317 Loss 7.5403346e-05 L1 0.9992451 L2 0.010904182\n",
      "318 Loss 7.492421e-05 L1 0.99922585 L2 0.010896268\n",
      "319 Loss 7.491684e-05 L1 0.99920183 L2 0.010879999\n",
      "320 Loss 7.482935e-05 L1 0.9991756 L2 0.010900705\n",
      "321 Loss 7.4754236e-05 L1 0.99920416 L2 0.010874152\n",
      "322 Loss 7.430639e-05 L1 0.9992462 L2 0.010876403\n",
      "323 Loss 7.393091e-05 L1 0.9992288 L2 0.010872496\n",
      "324 Loss 7.348754e-05 L1 0.9993168 L2 0.010882245\n",
      "325 Loss 7.33834e-05 L1 0.9993234 L2 0.010889388\n",
      "326 Loss 7.292775e-05 L1 0.99922866 L2 0.010877941\n",
      "327 Loss 7.251109e-05 L1 0.99928415 L2 0.010869718\n",
      "328 Loss 7.22188e-05 L1 0.99931645 L2 0.010871006\n",
      "329 Loss 7.210386e-05 L1 0.99930584 L2 0.010874934\n",
      "330 Loss 7.195756e-05 L1 0.9993153 L2 0.010877702\n",
      "331 Loss 7.160367e-05 L1 0.9993541 L2 0.01087259\n",
      "332 Loss 7.1511524e-05 L1 0.9993254 L2 0.0108659705\n",
      "333 Loss 7.147738e-05 L1 0.9993292 L2 0.010861589\n",
      "334 Loss 7.1175265e-05 L1 0.99932635 L2 0.010875544\n",
      "335 Loss 7.111613e-05 L1 0.99937016 L2 0.010888137\n",
      "336 Loss 7.108165e-05 L1 0.9993666 L2 0.010889163\n",
      "337 Loss 7.091435e-05 L1 0.99936736 L2 0.010888445\n",
      "338 Loss 7.0566675e-05 L1 0.99937934 L2 0.010870309\n",
      "339 Loss 7.0406495e-05 L1 0.9994873 L2 0.010876249\n",
      "340 Loss 7.035672e-05 L1 0.9994406 L2 0.010878105\n",
      "341 Loss 7.003677e-05 L1 0.9993309 L2 0.010881059\n",
      "342 Loss 6.9889174e-05 L1 0.9993364 L2 0.010870076\n",
      "343 Loss 6.977253e-05 L1 0.9993369 L2 0.010879886\n",
      "344 Loss 6.94452e-05 L1 0.99935186 L2 0.010870935\n",
      "345 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "346 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "347 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "348 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "349 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "350 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "351 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "352 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "353 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "354 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "355 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "356 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "357 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "358 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "359 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "360 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "361 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "362 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "363 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "364 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "365 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "366 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "367 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "368 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "369 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "370 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "371 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "372 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "373 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "374 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "375 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "376 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "377 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "378 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "379 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "380 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "381 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "382 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "383 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "384 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "385 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "386 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "387 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "388 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "389 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "390 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "391 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "392 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "393 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "394 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "395 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "396 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "397 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "398 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "399 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "400 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "401 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "402 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "403 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "404 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "405 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "406 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "407 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "408 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "409 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "410 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "411 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "412 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "413 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "414 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "415 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "416 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "417 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "418 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "419 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "420 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "421 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "422 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "423 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "424 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "425 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "426 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "427 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "428 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "429 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "430 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "431 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "432 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "433 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "434 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "435 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "436 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "437 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "438 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "439 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "440 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "441 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "442 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "443 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "444 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "445 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "446 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "447 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "448 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "449 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "450 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "451 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "452 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "453 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "454 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "455 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "456 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "457 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "458 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "459 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "460 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "461 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "462 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "463 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "464 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "465 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "466 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "467 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "468 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "469 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "470 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "471 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "472 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "473 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "474 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "475 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "476 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "477 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "478 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "479 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "480 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "481 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "482 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "483 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "484 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "485 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "486 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "487 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "488 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "489 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "490 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "491 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "492 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "493 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "494 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "495 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "496 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "497 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "498 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "499 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "500 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "501 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "502 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "503 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "504 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "505 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "506 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "507 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "508 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "509 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "510 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "511 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "512 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "513 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "514 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "515 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "516 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "517 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "518 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "519 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "520 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "521 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "522 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "523 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "524 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "525 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "526 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "527 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "528 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "529 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "530 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "531 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "532 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "533 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "534 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "535 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "536 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "537 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "538 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "539 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "540 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "541 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "542 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "543 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "544 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "545 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "546 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "547 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "548 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "549 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "550 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "551 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "552 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "553 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "554 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "555 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "556 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "557 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "558 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "559 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "560 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "561 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "562 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "563 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "564 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "565 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "566 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "567 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "568 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "569 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "570 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "571 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "572 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "573 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "574 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "575 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "576 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "577 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "578 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "579 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "580 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "581 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "582 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "583 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "584 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "585 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "586 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "587 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "588 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "589 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "590 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "591 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "592 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "593 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "594 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "595 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "596 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "597 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "598 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "599 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "600 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "601 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "602 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "603 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "604 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "605 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "606 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "607 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "608 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "609 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "610 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "611 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "612 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "613 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "614 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "615 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "616 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "617 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "618 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "619 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "620 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "621 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "622 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "623 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "624 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "625 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "626 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "627 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "628 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "629 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "630 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "631 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "632 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "633 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "634 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "635 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "636 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "637 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "638 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "639 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "640 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "641 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "642 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "643 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "644 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "645 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "646 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "647 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "648 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "649 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "650 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "651 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "652 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "653 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "654 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "655 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "656 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "657 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "658 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "659 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "660 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "661 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "662 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "663 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "664 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "665 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "666 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "667 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "668 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "669 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "670 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "671 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "672 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "673 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "674 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "675 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "676 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "677 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "678 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "679 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "680 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "681 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "682 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "683 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "684 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "685 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "686 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "687 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "688 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "689 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "690 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "691 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "692 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "693 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "694 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "695 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "696 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "697 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "698 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "699 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "700 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "701 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "702 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "703 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "704 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "705 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "706 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "707 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "708 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "709 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "710 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "711 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "712 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "713 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "714 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "715 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "716 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "717 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "718 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "719 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "720 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "721 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "722 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "723 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "724 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "725 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "726 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "727 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "728 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "729 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "730 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "731 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "732 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "733 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "734 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "735 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "736 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "737 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "738 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "739 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "740 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "741 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "742 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "743 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "744 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "745 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "746 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "747 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "748 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "749 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "750 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "751 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "752 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "753 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "754 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "755 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "756 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "757 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "758 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "759 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "760 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "761 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "762 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "763 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "764 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "765 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "766 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "767 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "768 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "769 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "770 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "771 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "772 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "773 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "774 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "775 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "776 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "777 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "778 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "779 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "780 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "781 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "782 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "783 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "784 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "785 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "786 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "787 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "788 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "789 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "790 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "791 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "792 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "793 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "794 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "795 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "796 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "797 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "798 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "799 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "800 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "801 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "802 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "803 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "804 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "805 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "806 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "807 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "808 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "809 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "810 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "811 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "812 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "813 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "814 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "815 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "816 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "817 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "818 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "819 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "820 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "821 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "822 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "823 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "824 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "825 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "826 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "827 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "828 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "829 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "830 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "831 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "832 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "833 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "834 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "835 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "836 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "837 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "838 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "839 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "840 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "841 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "842 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "843 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "844 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "845 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "846 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "847 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "848 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "849 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "850 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "851 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "852 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "853 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "854 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "855 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "856 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "857 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "858 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "859 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "860 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "861 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "862 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "863 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "864 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "865 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "866 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "867 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "868 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "869 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "870 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "871 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "872 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "873 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "874 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "875 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "876 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "877 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "878 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "879 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "880 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "881 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "882 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "883 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "884 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "885 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "886 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "887 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "888 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "889 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "890 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "891 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "892 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "893 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "894 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "895 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "896 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "897 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "898 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "899 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "900 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "901 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "902 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "903 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "904 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "905 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "906 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "907 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "908 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "909 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "910 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "911 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "912 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "913 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "914 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "915 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "916 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "917 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "918 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "919 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "920 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "921 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "922 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "923 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "924 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "925 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "926 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "927 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "928 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "929 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "930 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "931 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "932 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "933 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "934 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "935 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "936 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "937 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "938 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "939 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "940 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "941 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "942 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "943 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "944 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "945 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "946 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "947 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "948 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "949 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "950 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "951 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "952 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "953 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "954 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "955 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "956 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "957 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "958 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "959 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "960 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "961 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "962 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "963 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "964 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "965 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "966 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "967 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "968 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "969 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "970 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "971 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "972 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "973 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "974 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "975 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "976 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "977 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "978 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "979 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "980 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "981 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "982 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "983 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "984 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "985 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "986 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "987 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "988 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "989 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "990 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "991 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "992 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "993 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "994 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "995 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "996 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "997 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "998 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "999 Loss 6.931019e-05 L1 0.9992588 L2 0.0108652795\n",
      "Training time: 485.13\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "\n",
    "test_mse_u_full = []\n",
    "test_re_u_full = []\n",
    "test_mse_v_full = []\n",
    "test_re_v_full = []\n",
    "test_mse_p_full = []\n",
    "test_re_p_full = []\n",
    "\n",
    "\n",
    "beta_full = []\n",
    "lambda1_full = []\n",
    "lambda2_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    \n",
    "    test_mse_u_loss = []\n",
    "    test_re_u_loss = []\n",
    "    \n",
    "    test_mse_v_loss = []\n",
    "    test_re_v_loss = []\n",
    "    \n",
    "    test_mse_p_loss = []\n",
    "    test_re_p_loss = []\n",
    "    \n",
    "    lambda1_val = []\n",
    "    lambda2_val = []\n",
    "    beta_val = []\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_T = 5000\n",
    "\n",
    "    layers = np.array([3,50,50,50,50,2]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    \n",
    "    test_mse_u_full.append(test_mse_u_loss)\n",
    "    test_re_u_full.append(test_re_u_loss)\n",
    "    test_mse_v_full.append(test_mse_v_loss)\n",
    "    test_re_v_full.append(test_re_v_loss)\n",
    "    test_mse_p_full.append(test_mse_p_loss)\n",
    "    test_re_p_full.append(test_re_p_loss)\n",
    "\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "    lambda1_full.append(lambda1_val)\n",
    "    lambda2_full.append(lambda2_val)\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_u\": test_mse_u_full,\"test_re_u_loss\": test_re_u_full,\"test_mse_v\": test_mse_v_full,\"test_re_v_loss\": test_re_v_full,\"test_mse_p\": test_mse_p_full,\"test_re_p_loss\": test_re_p_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold, \"lambda1\":lambda1_full,\"lambda2\":lambda2_full}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_stan\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.where(t==5)\n",
    "idx = idx[0]\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u,v,p = PINN.test()\n",
    "fig,ax = plt.subplots(1,1)\n",
    "img = ax.imshow(p.reshape(50,100),cmap = 'jet')\n",
    "cbar = fig.colorbar(img, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
