{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/atanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    y = np.zeros((x.shape[0],1))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        k = x[i]\n",
    "        if(k<0.25):\n",
    "            y[i] = 4.0\n",
    "        elif(k<=0.5):\n",
    "            y[i] = 0.0\n",
    "        elif(k<=0.75):\n",
    "            y[i] = 4.0\n",
    "        elif(k<=1.0):\n",
    "            y[i] = 1.5\n",
    "        elif(k<=1.25):\n",
    "            y[i] = 3.0\n",
    "        elif(k<=1.50):\n",
    "            y[i] = 0.0\n",
    "        elif(k<=1.75):\n",
    "            y[i] = 4.0\n",
    "        else:\n",
    "            y[i] = 0.0\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_adv(xt): #True function for 2D_4 Heat Transfer in a rod x \\in [0,1] t \\in [0,0.1]\n",
    "    q = g(xt[:,0].reshape(-1,1) - ubar*xt[:,1].reshape(-1,1))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubar = 0.5\n",
    "loss_thresh = 0.1\n",
    "label = \"Adv_tanh\"\n",
    "\n",
    "x_ll = np.array(0.0)\n",
    "x_ul = np.array(2.0)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,500).reshape(-1,1)\n",
    "t = np.linspace(0,0.2,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = true_adv(xt)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_D,N_f,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #X_Train\n",
    "    np.random.seed(seed)\n",
    "    x_IC = np.random.uniform(x_ll,x_ul,(N_I,1))\n",
    "    t_IC = np.zeros((N_I,1))\n",
    "    xt_IC = np.hstack((x_IC,t_IC))\n",
    "    u_IC = g(x_IC)\n",
    "    \n",
    "    x_BC1 = x_ll*np.ones((N_D,1))\n",
    "    t_BC1 = np.random.uniform(0,0.2,(N_D,1))\n",
    "    xt_BC1 = np.hstack((x_BC1,t_BC1))\n",
    "    \n",
    "    x_BC2 = x_ul*np.ones((N_D,1))\n",
    "    t_BC2 = np.random.uniform(0,0.2,(N_D,1))\n",
    "    xt_BC2 = np.hstack((x_BC2,t_BC2))\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2))\n",
    "    u_BC = true_adv(xt_BC)\n",
    "    \n",
    "    xt_train = np.vstack((xt_IC,xt_BC))\n",
    "    u_train = np.vstack((u_IC,u_BC))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll,xt_train)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        self.m_lambda = nn.Sigmoid()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lambdas_BC = Parameter(torch.ones(2*N_D+N_I,1))\n",
    "        self.lambdas_BC.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+2*N_D+N_I,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "             \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "    def loss_BC(self,xt,u,lambda_ind):\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_BC)\n",
    "        u_pred = self.forward(xt)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - u))/2.0\n",
    "        \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "                        \n",
    "    \n",
    "    def loss_PDE(self, xt_coll,f_hat,lambda_ind):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_t[:,[0]]\n",
    "                \n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        f = du_dt + ubar*du_dx\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        # loss_f = self.loss_function(f,f_hat)\n",
    "        loss_f = (N_f+2*N_D+N_I)*self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        lambda_ind = False\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_train = self.loss_BC(xt_train,u_train,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "    \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,xt_coll,f_hat, xt_train, u_train):\n",
    "\n",
    "        lambda_ind = True\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_train = self.loss_BC(xt_train,u_train,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_f + loss_train\n",
    "    \n",
    "        return -1.0*loss_val\n",
    "     \n",
    "    'callable for optimizer'                                    \n",
    "    \n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "               \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_coll,f_hat, xt_train, u_train,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(30):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(xt_coll,f_hat, xt_train, u_train)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    # lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    xt_coll, xt_train, u_train = trainingdata(N_I,N_D,N_f,123)\n",
    "    \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_train = torch.from_numpy(xt_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    \n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_coll,f_hat, xt_train, u_train,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_coll,f_hat, xt_train, u_train).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv_tanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 5353.9824 Test MSE 4.215308632598564 Test RE 0.734914103521076\n",
      "1 Train Loss 1567.944 Test MSE 3.283475864993222 Test RE 0.6486177167899114\n",
      "2 Train Loss 1548.3018 Test MSE 3.25200054991107 Test RE 0.6455014142425124\n",
      "3 Train Loss 1533.0786 Test MSE 3.2057847020651433 Test RE 0.6408982253046731\n",
      "4 Train Loss 1476.9711 Test MSE 3.078230988867425 Test RE 0.6280185848095157\n",
      "5 Train Loss 1304.205 Test MSE 2.648876553898009 Test RE 0.5825762189772078\n",
      "6 Train Loss 1178.5786 Test MSE 2.3609574916952734 Test RE 0.5500041631744028\n",
      "7 Train Loss 1088.1664 Test MSE 2.2086065708256184 Test RE 0.5319625667878001\n",
      "8 Train Loss 982.60596 Test MSE 1.9693035510647097 Test RE 0.5023174032356799\n",
      "9 Train Loss 841.66223 Test MSE 1.6835737362386387 Test RE 0.4644489271211194\n",
      "10 Train Loss 751.5851 Test MSE 1.5011334501698872 Test RE 0.4385625508332421\n",
      "11 Train Loss 591.35425 Test MSE 1.1583299995807257 Test RE 0.3852459174347796\n",
      "12 Train Loss 455.14664 Test MSE 0.9017778849688057 Test RE 0.3399160592479516\n",
      "13 Train Loss 390.5541 Test MSE 0.7820814289269853 Test RE 0.3165540539012949\n",
      "14 Train Loss 295.88013 Test MSE 0.5821043832838131 Test RE 0.2731003946385982\n",
      "15 Train Loss 225.66245 Test MSE 0.4405791954379652 Test RE 0.23759313372329607\n",
      "16 Train Loss 166.29443 Test MSE 0.32789501276841493 Test RE 0.20496952681134614\n",
      "17 Train Loss 103.57961 Test MSE 0.21236310952706078 Test RE 0.1649534604916225\n",
      "18 Train Loss 77.83928 Test MSE 0.16493123706922416 Test RE 0.1453695182286017\n",
      "19 Train Loss 61.98008 Test MSE 0.14107767416197256 Test RE 0.13444697601846614\n",
      "20 Train Loss 53.61302 Test MSE 0.12726319228477145 Test RE 0.1276948256249741\n",
      "21 Train Loss 48.13113 Test MSE 0.11003862517333278 Test RE 0.11873929629334194\n",
      "22 Train Loss 42.947533 Test MSE 0.10169819638431586 Test RE 0.11415068404198815\n",
      "23 Train Loss 39.57582 Test MSE 0.09429160465802222 Test RE 0.1099153649562261\n",
      "24 Train Loss 36.840343 Test MSE 0.08944636166503492 Test RE 0.10705408227963446\n",
      "25 Train Loss 34.072754 Test MSE 0.08619396329048679 Test RE 0.10508974014860455\n",
      "26 Train Loss 31.56337 Test MSE 0.07862968790094858 Test RE 0.1003725988029508\n",
      "27 Train Loss 29.151266 Test MSE 0.07574779265731492 Test RE 0.09851602579694017\n",
      "28 Train Loss 26.673706 Test MSE 0.07244854046130635 Test RE 0.09634667097396442\n",
      "29 Train Loss 24.8257 Test MSE 0.06992671086450654 Test RE 0.09465497461609533\n",
      "30 Train Loss 23.587765 Test MSE 0.07093739806686894 Test RE 0.0953365694344641\n",
      "31 Train Loss 21.994673 Test MSE 0.07008787878612019 Test RE 0.09476399279584552\n",
      "32 Train Loss 20.236534 Test MSE 0.06401670527067256 Test RE 0.09056670181915934\n",
      "33 Train Loss 18.792044 Test MSE 0.055916317564469006 Test RE 0.08464302467877786\n",
      "34 Train Loss 17.338764 Test MSE 0.06614452973811819 Test RE 0.09205955250678195\n",
      "35 Train Loss 16.428013 Test MSE 0.06964269307516543 Test RE 0.09446255136377353\n",
      "36 Train Loss 15.646859 Test MSE 0.07047161499026487 Test RE 0.0950230585352663\n",
      "37 Train Loss 14.9243765 Test MSE 0.07767159299974082 Test RE 0.0997592095037236\n",
      "38 Train Loss 14.219345 Test MSE 0.08093700705685306 Test RE 0.10183462378490746\n",
      "39 Train Loss 13.616668 Test MSE 0.07253062797258143 Test RE 0.09640123811488403\n",
      "40 Train Loss 12.941054 Test MSE 0.06789208298412228 Test RE 0.09326774134366328\n",
      "41 Train Loss 12.5125065 Test MSE 0.07487983428069916 Test RE 0.09794997516512254\n",
      "42 Train Loss 12.064648 Test MSE 0.06716007819082491 Test RE 0.09276357755939064\n",
      "43 Train Loss 11.487912 Test MSE 0.0744531266530421 Test RE 0.09767048926274022\n",
      "44 Train Loss 11.126915 Test MSE 0.08080441834507363 Test RE 0.10175117829868574\n",
      "45 Train Loss 10.795452 Test MSE 0.0768887008431472 Test RE 0.09925517384456335\n",
      "46 Train Loss 10.462504 Test MSE 0.09665285459124753 Test RE 0.11128310519973532\n",
      "47 Train Loss 10.164236 Test MSE 0.09234784770144204 Test RE 0.10877655037716476\n",
      "48 Train Loss 9.722751 Test MSE 0.054743200015813474 Test RE 0.08375041798059293\n",
      "49 Train Loss 9.454181 Test MSE 0.06356157843552955 Test RE 0.09024418553208453\n",
      "50 Train Loss 9.231103 Test MSE 0.07053254803641411 Test RE 0.09506413034387277\n",
      "51 Train Loss 8.601759 Test MSE 0.07935739861953965 Test RE 0.10083599879664275\n",
      "52 Train Loss 8.088714 Test MSE 0.0803108971898069 Test RE 0.10143997458906616\n",
      "53 Train Loss 7.769392 Test MSE 0.08243528823924152 Test RE 0.10277286732041953\n",
      "54 Train Loss 7.477397 Test MSE 0.09852581094419817 Test RE 0.11235616364986596\n",
      "55 Train Loss 6.7564063 Test MSE 0.08912466707718247 Test RE 0.10686139842515242\n",
      "56 Train Loss 6.540524 Test MSE 0.09431851686024224 Test RE 0.10993104956304192\n",
      "57 Train Loss 6.207407 Test MSE 0.09176689250067809 Test RE 0.10843385693026616\n",
      "58 Train Loss 5.967362 Test MSE 0.09354012805663774 Test RE 0.10947649207662494\n",
      "59 Train Loss 5.5314236 Test MSE 0.10538209036164553 Test RE 0.11619977775718884\n",
      "60 Train Loss 5.288216 Test MSE 0.09611339357396582 Test RE 0.11097211128592492\n",
      "61 Train Loss 5.028665 Test MSE 0.0972518857145361 Test RE 0.11162742546742585\n",
      "62 Train Loss 4.712657 Test MSE 0.10189098984512822 Test RE 0.11425883288805255\n",
      "63 Train Loss 4.320592 Test MSE 0.10859278362628265 Test RE 0.11795663529427083\n",
      "64 Train Loss 4.113088 Test MSE 0.1176043000459664 Test RE 0.12275338973382088\n",
      "65 Train Loss 3.9131832 Test MSE 0.11098854359896089 Test RE 0.11925070884089348\n",
      "66 Train Loss 3.7121315 Test MSE 0.09674847264724469 Test RE 0.11133813742740518\n",
      "67 Train Loss 3.5334172 Test MSE 0.0864467454749213 Test RE 0.105243726372376\n",
      "68 Train Loss 3.406726 Test MSE 0.08345766798394506 Test RE 0.10340820881527302\n",
      "69 Train Loss 3.191383 Test MSE 0.09210124438468041 Test RE 0.10863121623729734\n",
      "70 Train Loss 3.0730593 Test MSE 0.08848976019161676 Test RE 0.10648008815212892\n",
      "71 Train Loss 2.7380285 Test MSE 0.08032111412087835 Test RE 0.10144642684090545\n",
      "72 Train Loss 2.4587142 Test MSE 0.09399327413487017 Test RE 0.10974134582364929\n",
      "73 Train Loss 2.3401484 Test MSE 0.11409781438407424 Test RE 0.12090953629281063\n",
      "74 Train Loss 2.2478902 Test MSE 0.15825365758853163 Test RE 0.14239632172578465\n",
      "75 Train Loss 2.1097577 Test MSE 0.17971726916604436 Test RE 0.15174584149420303\n",
      "76 Train Loss 1.9458477 Test MSE 0.16322501024949998 Test RE 0.14461563377303682\n",
      "77 Train Loss 1.8437309 Test MSE 0.17153172678397108 Test RE 0.14824980225082765\n",
      "78 Train Loss 1.7797294 Test MSE 0.198774081250886 Test RE 0.15958856525659285\n",
      "79 Train Loss 1.6871077 Test MSE 0.24083159424146675 Test RE 0.1756623236304469\n",
      "80 Train Loss 1.5701469 Test MSE 0.274937840374153 Test RE 0.1876891430497973\n",
      "81 Train Loss 1.5063729 Test MSE 0.2928183648212595 Test RE 0.1936961767223707\n",
      "82 Train Loss 1.4540774 Test MSE 0.30983408902546533 Test RE 0.19924456900025755\n",
      "83 Train Loss 1.3769224 Test MSE 0.35624680953943505 Test RE 0.2136472881937381\n",
      "84 Train Loss 1.2923939 Test MSE 0.4403755108293993 Test RE 0.23753820641452714\n",
      "85 Train Loss 1.2562252 Test MSE 0.45600343497370216 Test RE 0.24171630678703468\n",
      "86 Train Loss 1.2275602 Test MSE 0.4439124835408858 Test RE 0.2384902188215939\n",
      "87 Train Loss 1.1800487 Test MSE 0.4275439439309609 Test RE 0.2340519546855935\n",
      "88 Train Loss 1.1537424 Test MSE 0.4388860033853645 Test RE 0.23713614657334514\n",
      "89 Train Loss 1.1206015 Test MSE 0.4616726167960124 Test RE 0.24321421310914854\n",
      "90 Train Loss 1.0582901 Test MSE 0.5260507531319285 Test RE 0.2596185477741726\n",
      "91 Train Loss 1.0349644 Test MSE 0.5623092606428398 Test RE 0.2684166865227621\n",
      "92 Train Loss 1.0079778 Test MSE 0.5636618779055438 Test RE 0.268739326568019\n",
      "93 Train Loss 0.94935894 Test MSE 0.6120524410940004 Test RE 0.28003751176300096\n",
      "94 Train Loss 0.90351224 Test MSE 0.631889162992743 Test RE 0.2845393575040302\n",
      "95 Train Loss 0.7956338 Test MSE 0.6784896921500034 Test RE 0.294844832033083\n",
      "96 Train Loss 0.74262494 Test MSE 0.6640410095414451 Test RE 0.2916885244315887\n",
      "97 Train Loss 0.6982054 Test MSE 0.6304196793877642 Test RE 0.28420831107963374\n",
      "98 Train Loss 0.6655122 Test MSE 0.6100843967110443 Test RE 0.2795869212882008\n",
      "99 Train Loss 0.6442928 Test MSE 0.6099712321278417 Test RE 0.27956098979010063\n",
      "100 Train Loss 0.63129663 Test MSE 0.5705855645025305 Test RE 0.2703848058357596\n",
      "101 Train Loss 0.6130089 Test MSE 0.5585434355800036 Test RE 0.2675163738175375\n",
      "102 Train Loss 0.5968268 Test MSE 0.5851846413049735 Test RE 0.2738220090023124\n",
      "103 Train Loss 0.5766254 Test MSE 0.5765273287895534 Test RE 0.2717889790690704\n",
      "104 Train Loss 0.55159944 Test MSE 0.5780364453915146 Test RE 0.2721444636935653\n",
      "105 Train Loss 0.53757644 Test MSE 0.587289031635563 Test RE 0.27431391466006627\n",
      "106 Train Loss 0.52150285 Test MSE 0.5667569087972404 Test RE 0.2694761316401764\n",
      "107 Train Loss 0.50713146 Test MSE 0.5348251156603253 Test RE 0.2617747718823679\n",
      "108 Train Loss 0.493559 Test MSE 0.5312454907367908 Test RE 0.260897261939996\n",
      "109 Train Loss 0.48354664 Test MSE 0.5360335686220106 Test RE 0.2620703488798206\n",
      "110 Train Loss 0.4731387 Test MSE 0.546867701161149 Test RE 0.264705539535635\n",
      "111 Train Loss 0.45372263 Test MSE 0.5650225352736133 Test RE 0.2690634940801273\n",
      "112 Train Loss 0.41294572 Test MSE 0.5646482132426754 Test RE 0.2689743533169665\n",
      "113 Train Loss 0.39136797 Test MSE 0.5754576491034253 Test RE 0.27153672552167996\n",
      "114 Train Loss 0.3798516 Test MSE 0.5652535347748131 Test RE 0.2691184893924804\n",
      "115 Train Loss 0.37358198 Test MSE 0.5583638307755272 Test RE 0.2674733591779577\n",
      "116 Train Loss 0.3695033 Test MSE 0.5685589473817365 Test RE 0.26990419964239837\n",
      "117 Train Loss 0.36654097 Test MSE 0.5633328470191794 Test RE 0.2686608784326195\n",
      "118 Train Loss 0.35959283 Test MSE 0.5712124126409183 Test RE 0.27053328811219945\n",
      "119 Train Loss 0.35239947 Test MSE 0.5636859918365028 Test RE 0.26874507495416533\n",
      "120 Train Loss 0.34706968 Test MSE 0.5580047916961576 Test RE 0.2673873499970682\n",
      "121 Train Loss 0.3452679 Test MSE 0.5611567946087506 Test RE 0.2681414822899983\n",
      "122 Train Loss 0.34336686 Test MSE 0.5633244523688788 Test RE 0.26865887666533567\n",
      "123 Train Loss 0.3420276 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "124 Train Loss 0.34202826 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "125 Train Loss 0.34202892 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "126 Train Loss 0.34202954 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "127 Train Loss 0.34203023 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "128 Train Loss 0.34203082 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "129 Train Loss 0.34203145 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "130 Train Loss 0.34203216 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "131 Train Loss 0.3420328 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "132 Train Loss 0.34203345 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "133 Train Loss 0.3420341 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "134 Train Loss 0.3420348 Test MSE 0.5681351500234634 Test RE 0.26980358913355773\n",
      "135 Train Loss 0.33884573 Test MSE 0.5782868099891553 Test RE 0.2722033942016998\n",
      "136 Train Loss 0.3364771 Test MSE 0.5830868784173451 Test RE 0.27333077143361556\n",
      "137 Train Loss 0.33555457 Test MSE 0.5897855048174832 Test RE 0.27489632900340877\n",
      "138 Train Loss 0.33272448 Test MSE 0.5935294341552924 Test RE 0.27576746289066956\n",
      "139 Train Loss 0.33161303 Test MSE 0.5945073710854014 Test RE 0.27599455541143686\n",
      "140 Train Loss 0.33142927 Test MSE 0.5951604757472022 Test RE 0.27614611270217304\n",
      "141 Train Loss 0.32857412 Test MSE 0.6032927607651652 Test RE 0.2780263447021738\n",
      "142 Train Loss 0.3267083 Test MSE 0.6090394179555232 Test RE 0.27934737442470425\n",
      "143 Train Loss 0.32135415 Test MSE 0.6183458803524499 Test RE 0.28147357477143503\n",
      "144 Train Loss 0.3187371 Test MSE 0.6173123117357969 Test RE 0.2812382340553386\n",
      "145 Train Loss 0.3187377 Test MSE 0.6173123117357969 Test RE 0.2812382340553386\n",
      "146 Train Loss 0.31873825 Test MSE 0.6173123117357969 Test RE 0.2812382340553386\n",
      "147 Train Loss 0.31873766 Test MSE 0.6173123190543508 Test RE 0.2812382357224502\n",
      "148 Train Loss 0.31873822 Test MSE 0.6173123190543508 Test RE 0.2812382357224502\n",
      "149 Train Loss 0.31873837 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "150 Train Loss 0.31873897 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "151 Train Loss 0.31873956 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "152 Train Loss 0.31874013 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "153 Train Loss 0.31874073 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "154 Train Loss 0.31874132 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "155 Train Loss 0.31874192 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "156 Train Loss 0.31874248 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "157 Train Loss 0.3187431 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "158 Train Loss 0.3187437 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "159 Train Loss 0.3187443 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "160 Train Loss 0.31874487 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "161 Train Loss 0.31874546 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "162 Train Loss 0.3187461 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "163 Train Loss 0.3187467 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "164 Train Loss 0.31874725 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "165 Train Loss 0.31874785 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "166 Train Loss 0.31874847 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "167 Train Loss 0.31874907 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "168 Train Loss 0.31874967 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "169 Train Loss 0.31875026 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "170 Train Loss 0.31875083 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "171 Train Loss 0.31875145 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "172 Train Loss 0.31875205 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "173 Train Loss 0.31875265 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "174 Train Loss 0.31875324 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "175 Train Loss 0.31875384 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "176 Train Loss 0.31875443 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "177 Train Loss 0.31875503 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "178 Train Loss 0.31875563 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "179 Train Loss 0.31875622 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "180 Train Loss 0.31875685 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "181 Train Loss 0.3187574 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "182 Train Loss 0.31875804 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "183 Train Loss 0.31875864 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "184 Train Loss 0.31875926 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "185 Train Loss 0.31875983 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "186 Train Loss 0.31876042 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "187 Train Loss 0.31876105 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "188 Train Loss 0.31876168 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "189 Train Loss 0.31876224 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "190 Train Loss 0.31876284 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "191 Train Loss 0.31876346 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "192 Train Loss 0.31876406 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "193 Train Loss 0.31876463 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "194 Train Loss 0.31876525 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "195 Train Loss 0.31876585 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "196 Train Loss 0.31876644 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "197 Train Loss 0.318767 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "198 Train Loss 0.31876764 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "199 Train Loss 0.31876826 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "200 Train Loss 0.31876886 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "201 Train Loss 0.31876943 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "202 Train Loss 0.31877005 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "203 Train Loss 0.3187706 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "204 Train Loss 0.3187712 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "205 Train Loss 0.31877187 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "206 Train Loss 0.3187724 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "207 Train Loss 0.318773 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "208 Train Loss 0.31877363 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "209 Train Loss 0.3187742 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "210 Train Loss 0.3187748 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "211 Train Loss 0.31877542 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "212 Train Loss 0.318776 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "213 Train Loss 0.31877658 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "214 Train Loss 0.31877717 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "215 Train Loss 0.31877777 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "216 Train Loss 0.3187784 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "217 Train Loss 0.318779 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "218 Train Loss 0.31877953 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "219 Train Loss 0.31878015 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "220 Train Loss 0.31878072 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "221 Train Loss 0.31878132 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "222 Train Loss 0.3187819 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "223 Train Loss 0.31878248 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "224 Train Loss 0.31878304 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "225 Train Loss 0.31878367 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "226 Train Loss 0.31878424 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "227 Train Loss 0.31878477 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "228 Train Loss 0.31878543 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "229 Train Loss 0.318786 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "230 Train Loss 0.31878656 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "231 Train Loss 0.31878713 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "232 Train Loss 0.31878772 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "233 Train Loss 0.31878826 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "234 Train Loss 0.31878886 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "235 Train Loss 0.31878942 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "236 Train Loss 0.31879002 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "237 Train Loss 0.31879058 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "238 Train Loss 0.31879115 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "239 Train Loss 0.31879172 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "240 Train Loss 0.3187923 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "241 Train Loss 0.31879285 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "242 Train Loss 0.31879342 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "243 Train Loss 0.318794 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "244 Train Loss 0.31879455 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "245 Train Loss 0.3187951 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "246 Train Loss 0.31879568 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "247 Train Loss 0.31879625 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "248 Train Loss 0.3187968 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "249 Train Loss 0.31879732 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "250 Train Loss 0.31879795 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "251 Train Loss 0.31879848 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "252 Train Loss 0.31879902 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "253 Train Loss 0.3187996 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "254 Train Loss 0.31880012 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "255 Train Loss 0.31880066 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "256 Train Loss 0.3188012 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "257 Train Loss 0.31880173 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "258 Train Loss 0.3188023 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "259 Train Loss 0.31880283 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "260 Train Loss 0.31880337 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "261 Train Loss 0.3188039 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "262 Train Loss 0.31880444 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "263 Train Loss 0.31880498 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "264 Train Loss 0.31880552 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "265 Train Loss 0.318806 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "266 Train Loss 0.31880653 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "267 Train Loss 0.3188071 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "268 Train Loss 0.3188076 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "269 Train Loss 0.31880814 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "270 Train Loss 0.31880865 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "271 Train Loss 0.31880915 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "272 Train Loss 0.3188097 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "273 Train Loss 0.31881014 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "274 Train Loss 0.3188107 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "275 Train Loss 0.3188112 Test MSE 0.6173123248890056 Test RE 0.2812382370515408\n",
      "276 Train Loss 0.31879348 Test MSE 0.6177378709223899 Test RE 0.28133515654681746\n",
      "277 Train Loss 0.31876796 Test MSE 0.6179943829257053 Test RE 0.2813935618656031\n",
      "278 Train Loss 0.3187684 Test MSE 0.6179943829257053 Test RE 0.2813935618656031\n",
      "279 Train Loss 0.31876892 Test MSE 0.6179943829257053 Test RE 0.2813935618656031\n",
      "280 Train Loss 0.3187694 Test MSE 0.6179943829257053 Test RE 0.2813935618656031\n",
      "281 Train Loss 0.31876227 Test MSE 0.6179977987647075 Test RE 0.28139433953753823\n",
      "282 Train Loss 0.31876275 Test MSE 0.6179977987647075 Test RE 0.28139433953753823\n",
      "283 Train Loss 0.31874803 Test MSE 0.6181639734127945 Test RE 0.2814321693344036\n",
      "284 Train Loss 0.31869656 Test MSE 0.6181572302212853 Test RE 0.2814306343402341\n",
      "285 Train Loss 0.31863767 Test MSE 0.6181458213977831 Test RE 0.28142803726048377\n",
      "286 Train Loss 0.31829697 Test MSE 0.6220108786419866 Test RE 0.28230650349035197\n",
      "287 Train Loss 0.3182391 Test MSE 0.6226308890152178 Test RE 0.28244716772213835\n",
      "288 Train Loss 0.31823957 Test MSE 0.6226308890152178 Test RE 0.28244716772213835\n",
      "289 Train Loss 0.31776726 Test MSE 0.623764118488811 Test RE 0.2827040871528018\n",
      "290 Train Loss 0.31046107 Test MSE 0.6260774453892701 Test RE 0.28322782819434544\n",
      "291 Train Loss 0.29982844 Test MSE 0.6272894738095336 Test RE 0.2835018471680426\n",
      "292 Train Loss 0.2910788 Test MSE 0.6544925166125601 Test RE 0.28958378186476535\n",
      "293 Train Loss 0.28750688 Test MSE 0.6538661578808029 Test RE 0.28944518077832676\n",
      "294 Train Loss 0.28303745 Test MSE 0.6681182964884445 Test RE 0.2925826542923182\n",
      "295 Train Loss 0.27648783 Test MSE 0.6620257656769658 Test RE 0.2912455772617811\n",
      "296 Train Loss 0.26915035 Test MSE 0.6546222875520341 Test RE 0.2896124893716251\n",
      "297 Train Loss 0.25774175 Test MSE 0.651812440508751 Test RE 0.2889902665050431\n",
      "298 Train Loss 0.24932966 Test MSE 0.6533921714907766 Test RE 0.2893402526164527\n",
      "299 Train Loss 0.24658799 Test MSE 0.649944138611225 Test RE 0.2885758002555688\n",
      "300 Train Loss 0.24259831 Test MSE 0.6633760694533977 Test RE 0.2915424461253129\n",
      "301 Train Loss 0.23116915 Test MSE 0.6812395522539771 Test RE 0.2954417181781458\n",
      "302 Train Loss 0.22470012 Test MSE 0.6775089869023344 Test RE 0.2946316670866154\n",
      "303 Train Loss 0.22204922 Test MSE 0.6796354428162364 Test RE 0.2950936760245188\n",
      "304 Train Loss 0.2203395 Test MSE 0.6810607865316233 Test RE 0.2954029518460518\n",
      "305 Train Loss 0.21761733 Test MSE 0.6900006852424543 Test RE 0.2973354244153881\n",
      "306 Train Loss 0.21542849 Test MSE 0.6998147159477291 Test RE 0.29944249194461653\n",
      "307 Train Loss 0.21510991 Test MSE 0.7013400570914483 Test RE 0.29976865208771947\n",
      "308 Train Loss 0.20996144 Test MSE 0.6935730453631638 Test RE 0.29810413230021066\n",
      "309 Train Loss 0.20525497 Test MSE 0.6862868397659023 Test RE 0.2965341587495935\n",
      "310 Train Loss 0.2011017 Test MSE 0.7198392508332272 Test RE 0.30369640769078\n",
      "311 Train Loss 0.19852237 Test MSE 0.7253072569302355 Test RE 0.30484768706355375\n",
      "312 Train Loss 0.19564465 Test MSE 0.7341693693576575 Test RE 0.30670441204193966\n",
      "313 Train Loss 0.19326845 Test MSE 0.7365372031956036 Test RE 0.3071986035083224\n",
      "314 Train Loss 0.19126777 Test MSE 0.7375486319961385 Test RE 0.3074094570257325\n",
      "315 Train Loss 0.18840207 Test MSE 0.75902116037417 Test RE 0.31185221616935144\n",
      "316 Train Loss 0.18676448 Test MSE 0.764781294552295 Test RE 0.31303328698032973\n",
      "317 Train Loss 0.18676592 Test MSE 0.7647812949527542 Test RE 0.31303328706228584\n",
      "318 Train Loss 0.1867674 Test MSE 0.7647812949527542 Test RE 0.31303328706228584\n",
      "319 Train Loss 0.1867688 Test MSE 0.7647812949527542 Test RE 0.31303328706228584\n",
      "320 Train Loss 0.18677013 Test MSE 0.7647812949527542 Test RE 0.31303328706228584\n",
      "321 Train Loss 0.18677133 Test MSE 0.7647812949527542 Test RE 0.31303328706228584\n",
      "322 Train Loss 0.18673882 Test MSE 0.7616819312462281 Test RE 0.312398341504775\n",
      "323 Train Loss 0.18673947 Test MSE 0.7616819267986887 Test RE 0.31239834059271193\n",
      "324 Train Loss 0.18674049 Test MSE 0.7616819267986887 Test RE 0.31239834059271193\n",
      "325 Train Loss 0.18674147 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "326 Train Loss 0.1867424 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "327 Train Loss 0.18674332 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "328 Train Loss 0.18674418 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "329 Train Loss 0.18674505 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "330 Train Loss 0.18674587 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "331 Train Loss 0.18674666 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "332 Train Loss 0.18674746 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "333 Train Loss 0.1867482 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "334 Train Loss 0.18674895 Test MSE 0.761681926808422 Test RE 0.31239834059470795\n",
      "335 Train Loss 0.18673334 Test MSE 0.762148874561347 Test RE 0.3124940835472262\n",
      "336 Train Loss 0.1859453 Test MSE 0.7575436578754379 Test RE 0.31154854421657435\n",
      "337 Train Loss 0.18126284 Test MSE 0.7666201817883758 Test RE 0.3134093992843056\n",
      "338 Train Loss 0.17826158 Test MSE 0.7894605217631275 Test RE 0.3180439228777383\n",
      "339 Train Loss 0.17444883 Test MSE 0.7985334349516521 Test RE 0.3198662695582701\n",
      "340 Train Loss 0.16520189 Test MSE 0.8451095252366543 Test RE 0.3290624987855555\n",
      "341 Train Loss 0.15952988 Test MSE 0.887572241978033 Test RE 0.33722809462518427\n",
      "342 Train Loss 0.1569403 Test MSE 0.9149265422656997 Test RE 0.3423852179684059\n",
      "343 Train Loss 0.15457194 Test MSE 0.9139721974629869 Test RE 0.3422066031887073\n",
      "344 Train Loss 0.15457276 Test MSE 0.9139721974629869 Test RE 0.3422066031887073\n",
      "345 Train Loss 0.15194349 Test MSE 0.9174894704867839 Test RE 0.3428644340739192\n",
      "346 Train Loss 0.151583 Test MSE 0.918587318984662 Test RE 0.3430695049119651\n",
      "347 Train Loss 0.15143499 Test MSE 0.9168531563858082 Test RE 0.3427455186413781\n",
      "348 Train Loss 0.15142253 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "349 Train Loss 0.1514232 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "350 Train Loss 0.15142387 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "351 Train Loss 0.15142451 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "352 Train Loss 0.15142515 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "353 Train Loss 0.15142576 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "354 Train Loss 0.15142635 Test MSE 0.9168494529055258 Test RE 0.3427448264080891\n",
      "355 Train Loss 0.1514255 Test MSE 0.9168356576817315 Test RE 0.3427422478716917\n",
      "356 Train Loss 0.15142259 Test MSE 0.9152716477922448 Test RE 0.342449784840897\n",
      "357 Train Loss 0.15134592 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "358 Train Loss 0.15134649 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "359 Train Loss 0.15134701 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "360 Train Loss 0.15134755 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "361 Train Loss 0.15134805 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "362 Train Loss 0.15134858 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "363 Train Loss 0.15134907 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "364 Train Loss 0.15134957 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "365 Train Loss 0.15135004 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "366 Train Loss 0.15135051 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "367 Train Loss 0.15135096 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "368 Train Loss 0.1513514 Test MSE 0.915434954057414 Test RE 0.34248033407793255\n",
      "369 Train Loss 0.15135054 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "370 Train Loss 0.15135098 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "371 Train Loss 0.1513514 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "372 Train Loss 0.15135184 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "373 Train Loss 0.15135226 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "374 Train Loss 0.15135266 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "375 Train Loss 0.15135306 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "376 Train Loss 0.15135345 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "377 Train Loss 0.15135384 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "378 Train Loss 0.15135425 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "379 Train Loss 0.15135463 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "380 Train Loss 0.151355 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "381 Train Loss 0.15135537 Test MSE 0.915434939559242 Test RE 0.3424803313659218\n",
      "382 Train Loss 0.14978762 Test MSE 0.9327015854663937 Test RE 0.3456951212388739\n",
      "383 Train Loss 0.14937161 Test MSE 0.9318229877751677 Test RE 0.3455322618093746\n",
      "384 Train Loss 0.14937197 Test MSE 0.9318229877751677 Test RE 0.3455322618093746\n",
      "385 Train Loss 0.14937234 Test MSE 0.9318229877751677 Test RE 0.3455322618093746\n",
      "386 Train Loss 0.14937271 Test MSE 0.9318229877751677 Test RE 0.3455322618093746\n",
      "387 Train Loss 0.14937307 Test MSE 0.9318229877751677 Test RE 0.3455322618093746\n",
      "388 Train Loss 0.14937344 Test MSE 0.9318229877751677 Test RE 0.3455322618093746\n",
      "389 Train Loss 0.14936446 Test MSE 0.9318074841533455 Test RE 0.34552938732362604\n",
      "390 Train Loss 0.14901116 Test MSE 0.9215712019809883 Test RE 0.34362625617244563\n",
      "391 Train Loss 0.14858474 Test MSE 0.9202862912889362 Test RE 0.34338662028824474\n",
      "392 Train Loss 0.14851686 Test MSE 0.9206929722144699 Test RE 0.34346248437822474\n",
      "393 Train Loss 0.14557955 Test MSE 0.9250710067399275 Test RE 0.3442781239717004\n",
      "394 Train Loss 0.13774168 Test MSE 0.9271090260275898 Test RE 0.3446571540294339\n",
      "395 Train Loss 0.13115893 Test MSE 0.9300852054888811 Test RE 0.34520991514769117\n",
      "396 Train Loss 0.12848735 Test MSE 0.9423882511692602 Test RE 0.34748561006877876\n",
      "397 Train Loss 0.12848765 Test MSE 0.9423882511692602 Test RE 0.34748561006877876\n",
      "398 Train Loss 0.12848793 Test MSE 0.9423882511692602 Test RE 0.34748561006877876\n",
      "399 Train Loss 0.1284882 Test MSE 0.9423882511692602 Test RE 0.34748561006877876\n",
      "400 Train Loss 0.12223279 Test MSE 0.9867660848854295 Test RE 0.35557318355373096\n",
      "401 Train Loss 0.12024084 Test MSE 1.0155064207130358 Test RE 0.36071419211905037\n",
      "402 Train Loss 0.11989328 Test MSE 1.017825575444616 Test RE 0.3611258463123106\n",
      "403 Train Loss 0.11985313 Test MSE 1.019346409933979 Test RE 0.3613955426385431\n",
      "404 Train Loss 0.119853556 Test MSE 1.019346409933979 Test RE 0.3613955426385431\n",
      "405 Train Loss 0.119853966 Test MSE 1.019346409933979 Test RE 0.3613955426385431\n",
      "406 Train Loss 0.11985437 Test MSE 1.019346409933979 Test RE 0.3613955426385431\n",
      "407 Train Loss 0.11984272 Test MSE 1.0195829473499376 Test RE 0.3614374707840988\n",
      "408 Train Loss 0.11984309 Test MSE 1.0195829473499376 Test RE 0.3614374707840988\n",
      "409 Train Loss 0.11984345 Test MSE 1.0195829473499376 Test RE 0.3614374707840988\n",
      "410 Train Loss 0.119835 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "411 Train Loss 0.11983535 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "412 Train Loss 0.11983569 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "413 Train Loss 0.11983603 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "414 Train Loss 0.11983635 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "415 Train Loss 0.11983666 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "416 Train Loss 0.119836986 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "417 Train Loss 0.1198373 Test MSE 1.0195869147158498 Test RE 0.36143817398990946\n",
      "418 Train Loss 0.11983755 Test MSE 1.0195869158964763 Test RE 0.3614381741991724\n",
      "419 Train Loss 0.119837835 Test MSE 1.0195869158964763 Test RE 0.3614381741991724\n",
      "420 Train Loss 0.11983813 Test MSE 1.0195869158964763 Test RE 0.3614381741991724\n",
      "421 Train Loss 0.119838424 Test MSE 1.0195869158964763 Test RE 0.3614381741991724\n",
      "422 Train Loss 0.11983871 Test MSE 1.0195869158964763 Test RE 0.3614381741991724\n",
      "423 Train Loss 0.11983898 Test MSE 1.0195869158964763 Test RE 0.3614381741991724\n",
      "424 Train Loss 0.11967225 Test MSE 1.0213308096828324 Test RE 0.361747142707537\n",
      "425 Train Loss 0.1196725 Test MSE 1.0213308096828324 Test RE 0.361747142707537\n",
      "426 Train Loss 0.11961734 Test MSE 1.02343571988074 Test RE 0.36211972195484526\n",
      "427 Train Loss 0.119569905 Test MSE 1.0274158613610194 Test RE 0.3628231804742406\n",
      "428 Train Loss 0.11951563 Test MSE 1.0277161845767064 Test RE 0.3628762048953796\n",
      "429 Train Loss 0.11946561 Test MSE 1.0282494163328961 Test RE 0.36297033206121426\n",
      "430 Train Loss 0.11946586 Test MSE 1.0282494163328961 Test RE 0.36297033206121426\n",
      "431 Train Loss 0.11930138 Test MSE 1.0294129081931926 Test RE 0.36317562934759295\n",
      "432 Train Loss 0.11930161 Test MSE 1.0294129081931926 Test RE 0.36317562934759295\n",
      "433 Train Loss 0.11794882 Test MSE 1.0396607633267714 Test RE 0.36497886807958163\n",
      "434 Train Loss 0.11775293 Test MSE 1.0424754336965534 Test RE 0.36547258722485926\n",
      "435 Train Loss 0.11605662 Test MSE 1.0659417694563487 Test RE 0.36956312704629635\n",
      "436 Train Loss 0.115454614 Test MSE 1.0685734417474892 Test RE 0.3700190475863402\n",
      "437 Train Loss 0.115454905 Test MSE 1.0685734417474892 Test RE 0.3700190475863402\n",
      "438 Train Loss 0.11545519 Test MSE 1.0685734417474892 Test RE 0.3700190475863402\n",
      "439 Train Loss 0.115455456 Test MSE 1.0685734417474892 Test RE 0.3700190475863402\n",
      "440 Train Loss 0.115451746 Test MSE 1.0686416355038886 Test RE 0.37003085425551435\n",
      "441 Train Loss 0.115340084 Test MSE 1.0712917925465297 Test RE 0.3704893955545106\n",
      "442 Train Loss 0.11529174 Test MSE 1.0724299554120913 Test RE 0.3706861511605924\n",
      "443 Train Loss 0.115158476 Test MSE 1.0738191561521415 Test RE 0.37092616255474986\n",
      "444 Train Loss 0.11515871 Test MSE 1.0738191561521415 Test RE 0.37092616255474986\n",
      "445 Train Loss 0.115158945 Test MSE 1.0738191561521415 Test RE 0.37092616255474986\n",
      "446 Train Loss 0.115044974 Test MSE 1.0755606162460767 Test RE 0.37122681440363875\n",
      "447 Train Loss 0.114714056 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "448 Train Loss 0.114714265 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "449 Train Loss 0.11471447 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "450 Train Loss 0.11471468 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "451 Train Loss 0.11471489 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "452 Train Loss 0.114715084 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "453 Train Loss 0.11471529 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "454 Train Loss 0.11471549 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "455 Train Loss 0.11471569 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "456 Train Loss 0.114715874 Test MSE 1.0709656386109838 Test RE 0.3704329936601672\n",
      "457 Train Loss 0.114627145 Test MSE 1.0696356977432149 Test RE 0.3702029176425304\n",
      "458 Train Loss 0.114547364 Test MSE 1.0694659779877482 Test RE 0.37017354631482785\n",
      "459 Train Loss 0.114172176 Test MSE 1.0731680937497756 Test RE 0.37081369822902627\n",
      "460 Train Loss 0.11415359 Test MSE 1.0731672416352667 Test RE 0.3708135510126691\n",
      "461 Train Loss 0.11415355 Test MSE 1.0731672426287537 Test RE 0.3708135511843098\n",
      "462 Train Loss 0.11403411 Test MSE 1.0728052906455994 Test RE 0.37075101292662843\n",
      "463 Train Loss 0.11386355 Test MSE 1.0726577552267946 Test RE 0.3707255186514044\n",
      "464 Train Loss 0.1138159 Test MSE 1.0729337682714466 Test RE 0.37077321256728124\n",
      "465 Train Loss 0.11264762 Test MSE 1.084507899548091 Test RE 0.37276768175033875\n",
      "466 Train Loss 0.11264782 Test MSE 1.084507899548091 Test RE 0.37276768175033875\n",
      "467 Train Loss 0.11230586 Test MSE 1.0880953251586314 Test RE 0.37338370873623294\n",
      "468 Train Loss 0.11166913 Test MSE 1.0975714042658302 Test RE 0.37500605899510264\n",
      "469 Train Loss 0.111295946 Test MSE 1.101503821496373 Test RE 0.37567725075500064\n",
      "470 Train Loss 0.11129614 Test MSE 1.101503821496373 Test RE 0.37567725075500064\n",
      "471 Train Loss 0.11129634 Test MSE 1.101503821496373 Test RE 0.37567725075500064\n",
      "472 Train Loss 0.11129653 Test MSE 1.101503821496373 Test RE 0.37567725075500064\n",
      "473 Train Loss 0.11128569 Test MSE 1.1014672096846823 Test RE 0.3756710073181713\n",
      "474 Train Loss 0.11128587 Test MSE 1.1014672096846823 Test RE 0.3756710073181713\n",
      "475 Train Loss 0.11128606 Test MSE 1.1014672096846823 Test RE 0.3756710073181713\n",
      "476 Train Loss 0.111221366 Test MSE 1.1015919255954238 Test RE 0.37569227478090067\n",
      "477 Train Loss 0.11122154 Test MSE 1.1015919255954238 Test RE 0.37569227478090067\n",
      "478 Train Loss 0.11114072 Test MSE 1.1024695248741847 Test RE 0.37584189535333856\n",
      "479 Train Loss 0.11109211 Test MSE 1.1023921856200996 Test RE 0.3758287122943162\n",
      "480 Train Loss 0.11109228 Test MSE 1.1023921856200996 Test RE 0.3758287122943162\n",
      "481 Train Loss 0.1110881 Test MSE 1.102557452324452 Test RE 0.3758568826885576\n",
      "482 Train Loss 0.11108827 Test MSE 1.102557452324452 Test RE 0.3758568826885576\n",
      "483 Train Loss 0.11108843 Test MSE 1.102557452324452 Test RE 0.3758568826885576\n",
      "484 Train Loss 0.111088604 Test MSE 1.102557452324452 Test RE 0.3758568826885576\n",
      "485 Train Loss 0.11108876 Test MSE 1.102557452324452 Test RE 0.3758568826885576\n",
      "486 Train Loss 0.111036465 Test MSE 1.1025757665739484 Test RE 0.37586000429828637\n",
      "487 Train Loss 0.111036606 Test MSE 1.1025757665739484 Test RE 0.37586000429828637\n",
      "488 Train Loss 0.11103676 Test MSE 1.1025757665739484 Test RE 0.37586000429828637\n",
      "489 Train Loss 0.111036904 Test MSE 1.1025757665739484 Test RE 0.37586000429828637\n",
      "490 Train Loss 0.11103706 Test MSE 1.1025757665739484 Test RE 0.37586000429828637\n",
      "491 Train Loss 0.11103502 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "492 Train Loss 0.11103517 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "493 Train Loss 0.1110353 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "494 Train Loss 0.11103546 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "495 Train Loss 0.11103559 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "496 Train Loss 0.111035734 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "497 Train Loss 0.111035876 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "498 Train Loss 0.111036 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "499 Train Loss 0.111036144 Test MSE 1.1025757555514373 Test RE 0.37586000241953976\n",
      "Training time: 145.72\n",
      "Training time: 145.72\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1 #10\n",
    "max_iter = 500 #75\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "\n",
    "lambda1_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "N_I = 1000\n",
    "N_D = 1000\n",
    "N_f = 5000\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    'Generate Training data'\n",
    "    print(reps)\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "\n",
    "    lambda1_val = []\n",
    "\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr= 1.0, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "    lambda1_full.append(lambda1_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"lambda1\": lambda1_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9dec31bc90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred= PINN.test()\n",
    "plt.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = 'jet',vmax = 4,vmin = 0.2)\n",
    "plt.figure()\n",
    "plt.imshow(np.flip(u_true.reshape(500,500),axis = 0),cmap = 'jet',vmax = 4,vmin = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
