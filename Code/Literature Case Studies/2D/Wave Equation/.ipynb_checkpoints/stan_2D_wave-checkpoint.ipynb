{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_low\"\n",
    "label = \"Burgers_stan\" + level\n",
    "\n",
    "x = np.linspace(-1,1,256).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../burgers_shock_high.mat') \n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "x = np.array(data['x'])\n",
    "t = np.array(data['t'])\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = np.array(data['usol'][:])\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_l = x[0]*np.ones((N_T,1))\n",
    "    t_l = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_l = np.hstack((x_l,t_l))\n",
    "    u_l = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_r = x[-1]*np.ones((N_T,1))\n",
    "    t_r = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_r = np.hstack((x_r,t_r))\n",
    "    u_r = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_0 = np.random.uniform(x[0],x[-1],(N_T,1))\n",
    "    t_0 = t[0]*np.ones((N_T,1))\n",
    "    xt_0 = np.hstack((x_0,t_0))\n",
    "    u_0 = -10.0*np.sin(np.pi*x_0)\n",
    "    \n",
    "    xt_BC = np.vstack((xt_l,xt_r,xt_0)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_l,u_r,u_0))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        beta_mean = 1.0*torch.ones((50,len(layers)-2))\n",
    "        beta_std = 0.1*torch.ones((50,len(layers)-2))\n",
    "        \n",
    "        self.beta = Parameter(torch.normal(beta_mean,beta_std))\n",
    "        self.beta.requiresGrad = True\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 =self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "       \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_y[:,[0]]\n",
    "        du_dt = u_x_y[:,[1]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        \n",
    "\n",
    "        f = du_dt + u*du_dx - 0.01*d2u_dx2/pi\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers_stan_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 12.597392 Test MSE 4.1508001426414936 Test RE 0.8418045975263486\n",
      "1 Train Loss 12.294736 Test MSE 3.9214641926862295 Test RE 0.8182189031860297\n",
      "2 Train Loss 12.085562 Test MSE 3.8363096472726874 Test RE 0.8092863378240855\n",
      "3 Train Loss 11.732805 Test MSE 3.774048995042826 Test RE 0.8026923958419205\n",
      "4 Train Loss 11.092997 Test MSE 3.6371871956148207 Test RE 0.7880036127382631\n",
      "5 Train Loss 10.289201 Test MSE 3.4837906074437774 Test RE 0.77120778299449\n",
      "6 Train Loss 9.441405 Test MSE 3.3569871031697573 Test RE 0.7570424249665542\n",
      "7 Train Loss 8.555323 Test MSE 3.108403258554993 Test RE 0.7284740061275189\n",
      "8 Train Loss 7.839533 Test MSE 2.945660522479863 Test RE 0.709147749572501\n",
      "9 Train Loss 7.2274513 Test MSE 2.835406047049835 Test RE 0.6957496755017157\n",
      "10 Train Loss 6.710533 Test MSE 2.736149653703482 Test RE 0.6834634674028983\n",
      "11 Train Loss 6.289247 Test MSE 2.6958837173586687 Test RE 0.6784158090190988\n",
      "12 Train Loss 6.0058503 Test MSE 2.684647915690422 Test RE 0.6770005951019811\n",
      "13 Train Loss 5.8398976 Test MSE 2.6979618515656756 Test RE 0.6786772386014788\n",
      "14 Train Loss 5.6247635 Test MSE 2.682836111515382 Test RE 0.6767721108532955\n",
      "15 Train Loss 5.4565406 Test MSE 2.7008627253109228 Test RE 0.6790420006272389\n",
      "16 Train Loss 5.3075676 Test MSE 2.6925879789593252 Test RE 0.678000997953339\n",
      "17 Train Loss 5.1877394 Test MSE 2.6861103349991144 Test RE 0.6771849626578726\n",
      "18 Train Loss 5.1080637 Test MSE 2.6752473493901774 Test RE 0.6758142626353061\n",
      "19 Train Loss 5.015666 Test MSE 2.6969969362131776 Test RE 0.6785558646373557\n",
      "20 Train Loss 4.961524 Test MSE 2.688067534850359 Test RE 0.677431628803227\n",
      "21 Train Loss 4.9025464 Test MSE 2.6929917466635827 Test RE 0.6780518309524073\n",
      "22 Train Loss 4.8622236 Test MSE 2.700115771758234 Test RE 0.6789480958345732\n",
      "23 Train Loss 4.7880735 Test MSE 2.7086524101027076 Test RE 0.6800205240163415\n",
      "24 Train Loss 4.754991 Test MSE 2.703210376412751 Test RE 0.6793370558734511\n",
      "25 Train Loss 4.6750193 Test MSE 2.698507631296148 Test RE 0.6787458810749182\n",
      "26 Train Loss 4.6479683 Test MSE 2.6933044386490876 Test RE 0.6780911952058056\n",
      "27 Train Loss 4.623713 Test MSE 2.693212558692444 Test RE 0.6780796288340125\n",
      "28 Train Loss 4.5847874 Test MSE 2.6907032974186875 Test RE 0.6777636724404831\n",
      "29 Train Loss 4.5593963 Test MSE 2.6921635490647513 Test RE 0.6779475595447879\n",
      "30 Train Loss 4.543603 Test MSE 2.6897625403507157 Test RE 0.677645178010424\n",
      "31 Train Loss 4.5326724 Test MSE 2.6901506297610815 Test RE 0.6776940628893151\n",
      "32 Train Loss 4.498127 Test MSE 2.6912054733009296 Test RE 0.6778269162569729\n",
      "33 Train Loss 4.4767523 Test MSE 2.690971640770099 Test RE 0.677797468221902\n",
      "34 Train Loss 4.44524 Test MSE 2.6879831118475606 Test RE 0.6774209908150439\n",
      "35 Train Loss 4.4324417 Test MSE 2.690363697283072 Test RE 0.6777208999991127\n",
      "36 Train Loss 4.3872957 Test MSE 2.67768800542031 Test RE 0.6761224685530048\n",
      "37 Train Loss 4.373975 Test MSE 2.6737904321440693 Test RE 0.6756302161875818\n",
      "38 Train Loss 4.3534465 Test MSE 2.6766175725256245 Test RE 0.675987311642427\n",
      "39 Train Loss 4.2937346 Test MSE 2.679479996567439 Test RE 0.6763486716865404\n",
      "40 Train Loss 4.265321 Test MSE 2.679016355477424 Test RE 0.6762901535035204\n",
      "41 Train Loss 4.243132 Test MSE 2.6816517072870796 Test RE 0.6766227054844411\n",
      "42 Train Loss 4.209914 Test MSE 2.678195932279638 Test RE 0.6761865918707834\n",
      "43 Train Loss 4.1801887 Test MSE 2.673715666256504 Test RE 0.67562076996373\n",
      "44 Train Loss 4.1525784 Test MSE 2.6741978040149483 Test RE 0.6756816828730858\n",
      "45 Train Loss 4.127716 Test MSE 2.6749970854608067 Test RE 0.6757826513735283\n",
      "46 Train Loss 4.120154 Test MSE 2.6761994975096166 Test RE 0.6759345165689694\n",
      "47 Train Loss 4.1158886 Test MSE 2.6756885956476366 Test RE 0.6758699935977462\n",
      "48 Train Loss 4.0796075 Test MSE 2.675295875414983 Test RE 0.6758203918687333\n",
      "49 Train Loss 4.0530005 Test MSE 2.672128391422405 Test RE 0.6754201960834063\n",
      "50 Train Loss 4.04291 Test MSE 2.678517146270185 Test RE 0.6762271404502807\n",
      "51 Train Loss 4.0373363 Test MSE 2.677587383754527 Test RE 0.6761097648309761\n",
      "52 Train Loss 4.019516 Test MSE 2.6757922142373136 Test RE 0.6758830803257102\n",
      "53 Train Loss 4.0008965 Test MSE 2.6758485415292226 Test RE 0.6758901941933355\n",
      "54 Train Loss 3.9857833 Test MSE 2.6689952364578873 Test RE 0.6750241041819204\n",
      "55 Train Loss 3.9779217 Test MSE 2.6694099998092162 Test RE 0.6750765517059215\n",
      "56 Train Loss 3.9687593 Test MSE 2.6621953201311674 Test RE 0.674163661514395\n",
      "57 Train Loss 3.9580927 Test MSE 2.656778923438907 Test RE 0.6734774990437338\n",
      "58 Train Loss 3.9461386 Test MSE 2.655076144661688 Test RE 0.6732616423554453\n",
      "59 Train Loss 3.9280076 Test MSE 2.6562423706818157 Test RE 0.6734094891592587\n",
      "60 Train Loss 3.9109359 Test MSE 2.6573307842294644 Test RE 0.6735474421096304\n",
      "61 Train Loss 3.904665 Test MSE 2.657062082452212 Test RE 0.6735133876420427\n",
      "62 Train Loss 3.8915591 Test MSE 2.653246967226421 Test RE 0.6730296853193838\n",
      "63 Train Loss 3.8801923 Test MSE 2.6541635090011493 Test RE 0.6731459214946797\n",
      "64 Train Loss 3.865107 Test MSE 2.652901574047389 Test RE 0.6729858772172314\n",
      "65 Train Loss 3.8470528 Test MSE 2.6509376783712275 Test RE 0.6727367313836989\n",
      "66 Train Loss 3.8358428 Test MSE 2.6512978228479613 Test RE 0.6727824273258913\n",
      "67 Train Loss 3.8091886 Test MSE 2.654572193455376 Test RE 0.6731977445423891\n",
      "68 Train Loss 3.794936 Test MSE 2.6534543992275066 Test RE 0.6730559936818533\n",
      "69 Train Loss 3.7903342 Test MSE 2.6549687394568844 Test RE 0.6732480245659804\n",
      "70 Train Loss 3.7786555 Test MSE 2.6517368033861595 Test RE 0.6728381219682598\n",
      "71 Train Loss 3.7690847 Test MSE 2.656373945275208 Test RE 0.6734261673215195\n",
      "72 Train Loss 3.7539332 Test MSE 2.661046938880207 Test RE 0.6740182401000218\n",
      "73 Train Loss 3.7433825 Test MSE 2.663047223249954 Test RE 0.6742715191503504\n",
      "74 Train Loss 3.7356806 Test MSE 2.6646963558576253 Test RE 0.6744802632991956\n",
      "75 Train Loss 3.726694 Test MSE 2.661249525529691 Test RE 0.6740438962611758\n",
      "76 Train Loss 3.7210634 Test MSE 2.661100052327579 Test RE 0.6740249666355758\n",
      "77 Train Loss 3.7146847 Test MSE 2.6601518428667243 Test RE 0.6739048708517458\n",
      "78 Train Loss 3.6932607 Test MSE 2.6585529594497324 Test RE 0.6737023152567982\n",
      "79 Train Loss 3.6734064 Test MSE 2.662237848802739 Test RE 0.6741690463879659\n",
      "80 Train Loss 3.655695 Test MSE 2.665825925040423 Test RE 0.6746232048008698\n",
      "81 Train Loss 3.64324 Test MSE 2.667296164403615 Test RE 0.6748092111082689\n",
      "82 Train Loss 3.633872 Test MSE 2.6668210179611207 Test RE 0.6747491038954294\n",
      "83 Train Loss 3.627189 Test MSE 2.6677939338308803 Test RE 0.6748721744434112\n",
      "84 Train Loss 3.6195056 Test MSE 2.6689004266611294 Test RE 0.6750121147512885\n",
      "85 Train Loss 3.6072419 Test MSE 2.673560487484809 Test RE 0.675601163630154\n",
      "86 Train Loss 3.6006565 Test MSE 2.6743510620688395 Test RE 0.6757010442262348\n",
      "87 Train Loss 3.5921555 Test MSE 2.674259141730842 Test RE 0.6756894318386609\n",
      "88 Train Loss 3.5832438 Test MSE 2.6751536706405887 Test RE 0.6758024300865652\n",
      "89 Train Loss 3.5786066 Test MSE 2.6761379637015055 Test RE 0.6759267456490575\n",
      "90 Train Loss 3.5737257 Test MSE 2.67643664890863 Test RE 0.6759644648716527\n",
      "91 Train Loss 3.5677092 Test MSE 2.6791330220652525 Test RE 0.6763048789839106\n",
      "92 Train Loss 3.5574925 Test MSE 2.6792721732934086 Test RE 0.6763224420223637\n",
      "93 Train Loss 3.5476673 Test MSE 2.684347959908355 Test RE 0.6769627733983685\n",
      "94 Train Loss 3.5398319 Test MSE 2.682375619175314 Test RE 0.6767140264727474\n",
      "95 Train Loss 3.5317845 Test MSE 2.679463081443443 Test RE 0.6763465368432654\n",
      "96 Train Loss 3.5259733 Test MSE 2.679645037869414 Test RE 0.6763695010539326\n",
      "97 Train Loss 3.5228252 Test MSE 2.6799545965669886 Test RE 0.6764085677982803\n",
      "98 Train Loss 3.517295 Test MSE 2.6794987248052133 Test RE 0.676351035353488\n",
      "99 Train Loss 3.5110106 Test MSE 2.682037156489734 Test RE 0.6766713311711007\n",
      "100 Train Loss 3.5023108 Test MSE 2.6812094269745437 Test RE 0.67656690606037\n",
      "101 Train Loss 3.4864419 Test MSE 2.6855753206766058 Test RE 0.677117519096113\n",
      "102 Train Loss 3.4799142 Test MSE 2.682166879760652 Test RE 0.6766876954031932\n",
      "103 Train Loss 3.4711847 Test MSE 2.6824927194801313 Test RE 0.6767287974379649\n",
      "104 Train Loss 3.4634447 Test MSE 2.6874017672416612 Test RE 0.6773477321080552\n",
      "105 Train Loss 3.4587164 Test MSE 2.688518269478747 Test RE 0.6774884222227867\n",
      "106 Train Loss 3.4518943 Test MSE 2.6936513974617657 Test RE 0.6781348705764891\n",
      "107 Train Loss 3.4451294 Test MSE 2.692352351395287 Test RE 0.6779713314726131\n",
      "108 Train Loss 3.4366837 Test MSE 2.6902624782348488 Test RE 0.6777081509961144\n",
      "109 Train Loss 3.4260635 Test MSE 2.6933162299898097 Test RE 0.678092679552591\n",
      "110 Train Loss 3.4217772 Test MSE 2.6932884236911043 Test RE 0.6780891791659572\n",
      "111 Train Loss 3.4155822 Test MSE 2.6943460510787896 Test RE 0.6782223055123126\n",
      "112 Train Loss 3.4070227 Test MSE 2.69826534027434 Test RE 0.6787154090979646\n",
      "113 Train Loss 3.3957915 Test MSE 2.6982764748604975 Test RE 0.6787168094805159\n",
      "114 Train Loss 3.389539 Test MSE 2.7005546173405315 Test RE 0.6790032677775864\n",
      "115 Train Loss 3.3838472 Test MSE 2.7012612859033904 Test RE 0.6790921011739754\n",
      "116 Train Loss 3.3776922 Test MSE 2.699014755297226 Test RE 0.6788096556132768\n",
      "117 Train Loss 3.373107 Test MSE 2.700371971207918 Test RE 0.6789803059354008\n",
      "118 Train Loss 3.3636374 Test MSE 2.7003269808771764 Test RE 0.6789746497376447\n",
      "119 Train Loss 3.3541942 Test MSE 2.700045711694068 Test RE 0.6789392874241675\n",
      "120 Train Loss 3.347402 Test MSE 2.699107055289323 Test RE 0.6788212623653479\n",
      "121 Train Loss 3.340638 Test MSE 2.6979988920244073 Test RE 0.6786818973830608\n",
      "122 Train Loss 3.3294759 Test MSE 2.6975214615908123 Test RE 0.678621845889093\n",
      "123 Train Loss 3.3231673 Test MSE 2.6935808762814926 Test RE 0.6781259935583801\n",
      "124 Train Loss 3.317684 Test MSE 2.695820466998165 Test RE 0.6784078505348541\n",
      "125 Train Loss 3.311705 Test MSE 2.694145056808368 Test RE 0.6781970078450333\n",
      "126 Train Loss 3.305447 Test MSE 2.6913746916143144 Test RE 0.6778482262100191\n",
      "127 Train Loss 3.297785 Test MSE 2.6868123022247503 Test RE 0.677273442008574\n",
      "128 Train Loss 3.2912686 Test MSE 2.686874406582923 Test RE 0.6772812693862684\n",
      "129 Train Loss 3.2788157 Test MSE 2.686838146316587 Test RE 0.6772766993025293\n",
      "130 Train Loss 3.2757833 Test MSE 2.688533522000621 Test RE 0.6774903439862432\n",
      "131 Train Loss 3.2707272 Test MSE 2.691815090363946 Test RE 0.6779036832411579\n",
      "132 Train Loss 3.266841 Test MSE 2.6878543314394214 Test RE 0.6774047631095549\n",
      "133 Train Loss 3.2624435 Test MSE 2.6884137870557963 Test RE 0.6774752576628845\n",
      "134 Train Loss 3.2567182 Test MSE 2.689227807463621 Test RE 0.6775778157120166\n",
      "135 Train Loss 3.2551267 Test MSE 2.6901273498768914 Test RE 0.6776911305863321\n",
      "136 Train Loss 3.252102 Test MSE 2.6921949647269536 Test RE 0.6779515151197372\n",
      "137 Train Loss 3.2473004 Test MSE 2.693775390090805 Test RE 0.6781504781562085\n",
      "138 Train Loss 3.2379546 Test MSE 2.697727500464856 Test RE 0.6786477622373376\n",
      "139 Train Loss 3.2273705 Test MSE 2.6949070719414454 Test RE 0.6782929120808175\n",
      "140 Train Loss 3.2206316 Test MSE 2.69554502990062 Test RE 0.6783731925402355\n",
      "141 Train Loss 3.2168517 Test MSE 2.6981674500921367 Test RE 0.6787030974515396\n",
      "142 Train Loss 3.2126424 Test MSE 2.697988065686852 Test RE 0.6786805356985351\n",
      "143 Train Loss 3.2101207 Test MSE 2.6979927658358975 Test RE 0.6786811268609503\n",
      "144 Train Loss 3.2077465 Test MSE 2.6990889323048224 Test RE 0.6788189834101944\n",
      "145 Train Loss 3.2046933 Test MSE 2.7021244564932583 Test RE 0.6792005922542698\n",
      "146 Train Loss 3.2012053 Test MSE 2.704426951148437 Test RE 0.6794899058588137\n",
      "147 Train Loss 3.198219 Test MSE 2.7037312516975747 Test RE 0.6794025026550949\n",
      "148 Train Loss 3.1946075 Test MSE 2.7032916113800245 Test RE 0.6793472632752475\n",
      "149 Train Loss 3.186443 Test MSE 2.7034821425170317 Test RE 0.6793712034460593\n",
      "150 Train Loss 3.1818438 Test MSE 2.699111349789805 Test RE 0.6788218023952577\n",
      "151 Train Loss 3.1749437 Test MSE 2.7015754508006413 Test RE 0.6791315902634686\n",
      "152 Train Loss 3.165524 Test MSE 2.7007822379127577 Test RE 0.6790318826137332\n",
      "153 Train Loss 3.1562681 Test MSE 2.7038018361407063 Test RE 0.6794113709430026\n",
      "154 Train Loss 3.1499584 Test MSE 2.7032326777134172 Test RE 0.6793398581097885\n",
      "155 Train Loss 3.1463845 Test MSE 2.7025407808877713 Test RE 0.6792529134720737\n",
      "156 Train Loss 3.1433418 Test MSE 2.7034431673058 Test RE 0.679366306293018\n",
      "157 Train Loss 3.1406443 Test MSE 2.703077843639219 Test RE 0.6793204024289871\n",
      "158 Train Loss 3.1358647 Test MSE 2.703763313682101 Test RE 0.6794065309637396\n",
      "159 Train Loss 3.130937 Test MSE 2.708274785480608 Test RE 0.6799731201037541\n",
      "160 Train Loss 3.124763 Test MSE 2.7112501025136657 Test RE 0.6803465276078118\n",
      "161 Train Loss 3.1214483 Test MSE 2.7123184292180484 Test RE 0.6804805544867428\n",
      "162 Train Loss 3.1156635 Test MSE 2.7117589256065484 Test RE 0.68041036527882\n",
      "163 Train Loss 3.1089044 Test MSE 2.71053221834567 Test RE 0.6802564506475527\n",
      "164 Train Loss 3.1052496 Test MSE 2.709586017482821 Test RE 0.6801377072844146\n",
      "165 Train Loss 3.101252 Test MSE 2.7107728684962153 Test RE 0.6802866477030918\n",
      "166 Train Loss 3.0972311 Test MSE 2.7099145376297002 Test RE 0.6801789372298428\n",
      "167 Train Loss 3.094338 Test MSE 2.709453360071905 Test RE 0.6801210578015997\n",
      "168 Train Loss 3.0922353 Test MSE 2.7074769213116325 Test RE 0.6798729518795829\n",
      "169 Train Loss 3.0874355 Test MSE 2.705864042901167 Test RE 0.6796704172331144\n",
      "170 Train Loss 3.0846334 Test MSE 2.7042565250268127 Test RE 0.6794684956569691\n",
      "171 Train Loss 3.0818324 Test MSE 2.7056478268555058 Test RE 0.6796432616585121\n",
      "172 Train Loss 3.0783515 Test MSE 2.7069937043606234 Test RE 0.6798122790088319\n",
      "173 Train Loss 3.074858 Test MSE 2.707860479694588 Test RE 0.679921107735136\n",
      "174 Train Loss 3.067943 Test MSE 2.7063426355243725 Test RE 0.6797305220389215\n",
      "175 Train Loss 3.0619416 Test MSE 2.7061247518127964 Test RE 0.6797031594300516\n",
      "176 Train Loss 3.059379 Test MSE 2.7051555855792286 Test RE 0.6795814347494847\n",
      "177 Train Loss 3.0566337 Test MSE 2.7055598653028476 Test RE 0.6796322138492128\n",
      "178 Train Loss 3.0545487 Test MSE 2.7066397029341993 Test RE 0.6797678270427322\n",
      "179 Train Loss 3.0524063 Test MSE 2.7051331873958486 Test RE 0.6795786213399371\n",
      "180 Train Loss 3.0493724 Test MSE 2.7058212016365877 Test RE 0.6796650366864636\n",
      "181 Train Loss 3.0430107 Test MSE 2.7032846337685 Test RE 0.6793463865247829\n",
      "182 Train Loss 3.0380666 Test MSE 2.705565043415457 Test RE 0.6796328642156115\n",
      "183 Train Loss 3.0318198 Test MSE 2.7025326157790146 Test RE 0.6792518873676501\n",
      "184 Train Loss 3.0283723 Test MSE 2.7030516302561876 Test RE 0.679317108530307\n",
      "185 Train Loss 3.0265937 Test MSE 2.7023721287186673 Test RE 0.6792317187383375\n",
      "186 Train Loss 3.0246713 Test MSE 2.700444176093829 Test RE 0.6789893834566332\n",
      "187 Train Loss 3.0186555 Test MSE 2.701360069903629 Test RE 0.679104518118116\n",
      "188 Train Loss 3.0169978 Test MSE 2.6991902806695838 Test RE 0.6788317278120752\n",
      "189 Train Loss 3.0143623 Test MSE 2.6989712359896822 Test RE 0.6788041829782734\n",
      "190 Train Loss 3.012281 Test MSE 2.7001342077258035 Test RE 0.6789504137062438\n",
      "191 Train Loss 3.0083826 Test MSE 2.696604296465529 Test RE 0.6785064693826739\n",
      "192 Train Loss 3.0046086 Test MSE 2.695998641924321 Test RE 0.6784302691775204\n",
      "193 Train Loss 3.0008376 Test MSE 2.6968101743547748 Test RE 0.6785323698853579\n",
      "194 Train Loss 2.9979758 Test MSE 2.6971131345991615 Test RE 0.6785704820523702\n",
      "195 Train Loss 2.9916053 Test MSE 2.700838759457307 Test RE 0.6790389879126096\n",
      "196 Train Loss 2.9850755 Test MSE 2.700092734555245 Test RE 0.6789451994590495\n",
      "197 Train Loss 2.9787803 Test MSE 2.698383046222433 Test RE 0.6787302126771931\n",
      "198 Train Loss 2.9730196 Test MSE 2.7018494395823347 Test RE 0.679166027524767\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20858/3028702344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20858/1919774869.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20858/896708044.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xy_BC, u_BC, xy_coll, f_hat, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 500 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "\n",
    "N_T = 500 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    " \n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "u_pred = PINN.forward(xy_test_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
