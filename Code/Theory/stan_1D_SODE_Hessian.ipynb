{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "\n",
    "    \n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def forward_hess(self,x,Ws,bs,hess_betas):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        # print(a.shape)\n",
    "        # print(Ws[0].shape)\n",
    "       \n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = torch.matmul(a,torch.transpose(Ws[i],0,1)) + bs[i].reshape(1,-1)\n",
    "            a = self.activation(z) + hess_betas[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = torch.matmul(a,torch.transpose(Ws[-1],0,1)) + bs[-1].reshape(1,-1) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def loss_hess(self,a,b,c,d,e,f,g):\n",
    "        \n",
    "        hess_betas = a\n",
    "        Ws = [b,d,f]\n",
    "        bs = [c,e,g]\n",
    "#         hess_linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "#         hess_linears[0].weight.data = b\n",
    "#         hess_linears[0].bias.data = c\n",
    "#         hess_linears[1].weight.data = d\n",
    "#         hess_linears[1].bias.data = e\n",
    "#         hess_linears[2].weight.data = f\n",
    "#         hess_linears[2].bias.data = g\n",
    "        \n",
    "        loss_val = self.loss_function(self.forward_hess(x_test_tensor,Ws,bs,hess_betas),y_true_tensor)        \n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def Hess(self):\n",
    "        H = torch.autograd.functional.hessian(self.loss_hess, tuple([_ for _ in self.parameters()]))\n",
    "        \n",
    "        shapes = [100,50,50,2500,50,50,1]\n",
    "        total_hess = np.zeros((2850,2850))\n",
    "\n",
    "        col_ind = 0\n",
    "        for i in range(7):\n",
    "            shape1 = shapes[i]\n",
    "            col_ind = col_ind + shapes[i] \n",
    "            row_ind = 0\n",
    "            for j in range(7):\n",
    "                shape2 = shapes[j]\n",
    "                hess = H[i][j].reshape(shape2,shape1).cpu().detach().numpy()\n",
    "\n",
    "                row_ind = row_ind + shapes[j]\n",
    "\n",
    "                total_hess[row_ind-shapes[j]:row_ind,col_ind-shapes[i]:col_ind] = hess\n",
    "\n",
    "        w,_ = np.linalg.eig(total_hess)\n",
    "        k = np.max(w)\n",
    "        return k\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    return PINN.Hess()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    k = np.zeros((max_iter,1))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        k[i] = np.abs(train_step(x_coll,f_hat))\n",
    "        \n",
    "        print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "k = [109.76254886]\n",
      "0 Train Loss 3.0282452 Test MSE 385.26688197748473 Test RE 1.000560656033442\n",
      "k = [124.7931408]\n",
      "1 Train Loss 2.547062 Test MSE 384.4540138067491 Test RE 0.9995045655594635\n",
      "k = [111.15253218]\n",
      "2 Train Loss 2.4269955 Test MSE 379.5004495459254 Test RE 0.9930445445239243\n",
      "k = [110.51513115]\n",
      "3 Train Loss 2.4199114 Test MSE 378.89497497415897 Test RE 0.9922520510840037\n",
      "k = [101.18583225]\n",
      "4 Train Loss 2.3670626 Test MSE 375.14794482622744 Test RE 0.9873334898857038\n",
      "k = [117.27472661]\n",
      "5 Train Loss 2.3532684 Test MSE 375.2984611624145 Test RE 0.9875315383067\n",
      "k = [136.93919743]\n",
      "6 Train Loss 2.3205035 Test MSE 368.12000393235735 Test RE 0.9780415190935181\n",
      "k = [159.71156505]\n",
      "7 Train Loss 2.3054562 Test MSE 362.83800125453786 Test RE 0.9709994087928703\n",
      "k = [186.80568594]\n",
      "8 Train Loss 2.2743468 Test MSE 357.0383765315483 Test RE 0.9632078914897463\n",
      "k = [211.4318825]\n",
      "9 Train Loss 2.236515 Test MSE 342.43754082176025 Test RE 0.943307456312529\n",
      "k = [216.15767909]\n",
      "10 Train Loss 2.0674584 Test MSE 307.2932154272583 Test RE 0.8935915810355456\n",
      "k = [235.00269031]\n",
      "11 Train Loss 1.8772461 Test MSE 292.1661344696417 Test RE 0.8713196730435778\n",
      "k = [246.63396919]\n",
      "12 Train Loss 1.8608075 Test MSE 290.0183455712074 Test RE 0.8681111171814583\n",
      "k = [355.77654088]\n",
      "13 Train Loss 1.7845376 Test MSE 263.5272312721171 Test RE 0.8275139659850193\n",
      "k = [375.90853029]\n",
      "14 Train Loss 1.7382474 Test MSE 260.1993474688282 Test RE 0.8222723448926565\n",
      "k = [450.85148038]\n",
      "15 Train Loss 1.6515754 Test MSE 244.83545085426144 Test RE 0.7976267919039556\n",
      "k = [586.55679114]\n",
      "16 Train Loss 1.6010696 Test MSE 219.11209623315204 Test RE 0.7545634406986924\n",
      "k = [700.95583878]\n",
      "17 Train Loss 1.4290675 Test MSE 205.00033419961358 Test RE 0.7298605113615736\n",
      "k = [903.06662141]\n",
      "18 Train Loss 1.314535 Test MSE 190.7559323042238 Test RE 0.7040469298493909\n",
      "k = [1308.63326214]\n",
      "19 Train Loss 1.1734568 Test MSE 175.44585301581796 Test RE 0.6752026470083312\n",
      "k = [1877.4359493]\n",
      "20 Train Loss 0.93240726 Test MSE 127.89779232019211 Test RE 0.576493090622655\n",
      "k = [2895.31091314]\n",
      "21 Train Loss 0.6918166 Test MSE 69.66353774447117 Test RE 0.425466475162793\n",
      "k = [3401.75806925]\n",
      "22 Train Loss 0.4717002 Test MSE 51.398601568212754 Test RE 0.36545864027757397\n",
      "k = [3761.45666802]\n",
      "23 Train Loss 0.39260316 Test MSE 41.13954939080635 Test RE 0.32695828580578445\n",
      "k = [4026.07629605]\n",
      "24 Train Loss 0.35423365 Test MSE 33.59490716469263 Test RE 0.29546041614843327\n",
      "k = [4110.73024826]\n",
      "25 Train Loss 0.32756168 Test MSE 35.17766520487412 Test RE 0.30234033376639124\n",
      "k = [4826.67942925]\n",
      "26 Train Loss 0.24597514 Test MSE 29.20683821824558 Test RE 0.27548937581785754\n",
      "k = [5030.12277391]\n",
      "27 Train Loss 0.19301656 Test MSE 19.214745531230495 Test RE 0.22344976444705963\n",
      "k = [5408.00988758]\n",
      "28 Train Loss 0.18032444 Test MSE 12.662777625001084 Test RE 0.18139573539816847\n",
      "k = [6056.39120957]\n",
      "29 Train Loss 0.1583114 Test MSE 8.283692429704551 Test RE 0.1467150159570576\n",
      "k = [7047.09727401]\n",
      "30 Train Loss 0.09239101 Test MSE 7.931708435139185 Test RE 0.14356413323145378\n",
      "k = [8565.55610649]\n",
      "31 Train Loss 0.045811623 Test MSE 3.340591970395487 Test RE 0.09316961459102431\n",
      "k = [10258.22660983]\n",
      "32 Train Loss 0.024191264 Test MSE 0.5985248221957651 Test RE 0.03943698140621057\n",
      "k = [10815.06112421]\n",
      "33 Train Loss 0.019105192 Test MSE 0.36585959244968014 Test RE 0.030833285598198\n",
      "k = [11328.91277285]\n",
      "34 Train Loss 0.016446052 Test MSE 0.26268601847150225 Test RE 0.026126489625228705\n",
      "k = [11621.60619131]\n",
      "35 Train Loss 0.014397149 Test MSE 0.41644659232345865 Test RE 0.03289593530609679\n",
      "k = [12224.33005332]\n",
      "36 Train Loss 0.010451412 Test MSE 0.2271220572501868 Test RE 0.024293620291505655\n",
      "k = [12598.48736615]\n",
      "37 Train Loss 0.007545586 Test MSE 0.0166197417596658 Test RE 0.006571654432901023\n",
      "k = [12780.32008715]\n",
      "38 Train Loss 0.0056004794 Test MSE 0.05014864757830535 Test RE 0.011415427895319838\n",
      "k = [12957.44056787]\n",
      "39 Train Loss 0.004578511 Test MSE 0.10155400841576227 Test RE 0.016244678385451284\n",
      "k = [13254.14114567]\n",
      "40 Train Loss 0.00374996 Test MSE 0.10674464390327255 Test RE 0.01665465454644321\n",
      "k = [13834.26357241]\n",
      "41 Train Loss 0.002598505 Test MSE 0.03900753605249729 Test RE 0.010067852443395909\n",
      "k = [14367.17970796]\n",
      "42 Train Loss 0.0013258065 Test MSE 0.0009596836793088778 Test RE 0.001579161826319596\n",
      "k = [14380.7052592]\n",
      "43 Train Loss 0.0013033252 Test MSE 0.0010529685840438663 Test RE 0.00165413248040232\n",
      "k = [14384.89958772]\n",
      "44 Train Loss 0.0012946252 Test MSE 0.0011887475276510862 Test RE 0.001757548813865336\n",
      "k = [14390.33933029]\n",
      "45 Train Loss 0.0012865677 Test MSE 0.0013564262659313117 Test RE 0.001877416705590706\n",
      "k = [14393.52185812]\n",
      "46 Train Loss 0.0012785563 Test MSE 0.0016076859945674431 Test RE 0.0020439167217256083\n",
      "k = [14399.71004779]\n",
      "47 Train Loss 0.0012702515 Test MSE 0.0018855382095429773 Test RE 0.002213503695246726\n",
      "k = [14404.52404691]\n",
      "48 Train Loss 0.0012605061 Test MSE 0.002278445487596686 Test RE 0.0024332229483134313\n",
      "k = [14533.41201638]\n",
      "49 Train Loss 0.0010542088 Test MSE 0.012439631764359466 Test RE 0.005685469641006273\n",
      "k = [14563.61670021]\n",
      "50 Train Loss 0.0010143088 Test MSE 0.012388840717532516 Test RE 0.005673850875809976\n",
      "k = [14570.64573199]\n",
      "51 Train Loss 0.0010059294 Test MSE 0.01168050546135404 Test RE 0.005509261678308934\n",
      "k = [14576.61924328]\n",
      "52 Train Loss 0.0009974997 Test MSE 0.010826527001877181 Test RE 0.0053040445629530236\n",
      "k = [14587.36663796]\n",
      "53 Train Loss 0.000978978 Test MSE 0.00823128933576445 Test RE 0.004624837910825584\n",
      "k = [14589.89305787]\n",
      "54 Train Loss 0.0009702421 Test MSE 0.007049343775039119 Test RE 0.004279932578769059\n",
      "k = [14582.80237241]\n",
      "55 Train Loss 0.0009319837 Test MSE 0.0027374714427371616 Test RE 0.0026670882199058992\n",
      "k = [14577.06665474]\n",
      "56 Train Loss 0.0009253562 Test MSE 0.002229135410592944 Test RE 0.0024067490438485486\n",
      "k = [14570.70445014]\n",
      "57 Train Loss 0.00092007953 Test MSE 0.0019315013352467747 Test RE 0.0022403201697771347\n",
      "k = [14563.70464881]\n",
      "58 Train Loss 0.0009157979 Test MSE 0.00175557613738239 Test RE 0.002135858203155104\n",
      "k = [14556.69280075]\n",
      "59 Train Loss 0.0009119628 Test MSE 0.001683214886935649 Test RE 0.0020913771770279064\n",
      "k = [14549.45321752]\n",
      "60 Train Loss 0.0009087835 Test MSE 0.0016785123929451374 Test RE 0.0020884537335541875\n",
      "k = [14542.40941189]\n",
      "61 Train Loss 0.0009059423 Test MSE 0.0017421146334380593 Test RE 0.0021276537192748525\n",
      "k = [14535.71780653]\n",
      "62 Train Loss 0.0009034609 Test MSE 0.0018356251903485813 Test RE 0.002184009821094845\n",
      "k = [14529.36106802]\n",
      "63 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "64 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "65 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "66 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "67 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "68 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "69 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "70 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "71 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "72 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "73 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "74 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "75 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "76 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "77 Train Loss 0.00090137677 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "78 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "79 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "80 Train Loss 0.00090137677 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "81 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "82 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "83 Train Loss 0.00090137677 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n",
      "k = [14529.36106802]\n",
      "84 Train Loss 0.0009013767 Test MSE 0.001958611743062361 Test RE 0.002255987866599086\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9331/1063220127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9331/3145208825.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx_coll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolloc_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mf_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"k =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9331/3269130665.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_coll, f_hat)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9331/170896900.py\u001b[0m in \u001b[0;36mHess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mtotal_hess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_ind\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrow_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_ind\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcol_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_hess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meig\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0m_assert_stacked_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m     \u001b[0m_assert_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_finite\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Array must not contain infs or NaNs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_empty_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
