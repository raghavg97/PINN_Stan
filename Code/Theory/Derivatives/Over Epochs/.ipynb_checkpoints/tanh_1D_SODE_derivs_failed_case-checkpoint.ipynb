{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-4.0*x) + np.exp(3.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_tanh\" + level\n",
    "\n",
    "u_coeff = 12.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "\n",
    "    def forward_derivs(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        k_mean = np.zeros((2,3))\n",
    "        k_std = np.zeros((2,3))\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "            a_np = a.cpu().detach().numpy()\n",
    "            a1_np = 1-np.square(a_np)\n",
    "            a2_np = -2*a_np*(a1_np)\n",
    "            \n",
    "            k_mean[i,0] = np.mean(a_np)\n",
    "            k_std[i,0] = np.std(a_np)\n",
    "            k_mean[i,1] = np.mean(a1_np)\n",
    "            k_std[i,1] = np.std(a1_np)\n",
    "            k_mean[i,2] = np.mean(a2_np)\n",
    "            k_std[i,2] = np.std(a2_np)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return k_mean,k_std\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    k_mean,k_std = PINN.forward_derivs(x_coll)\n",
    "    optimizer.step(closure)\n",
    "   \n",
    "    \n",
    "    return k_mean,k_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    k_mean = np.zeros((max_iter,2,3))\n",
    "    k_std = np.zeros((max_iter,2,3))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        a,b = train_step(x_coll,f_hat)\n",
    "        k_mean[i,:,:] = a\n",
    "        k_std[i,:,:] = b\n",
    "        \n",
    "        \n",
    "        # print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return k_mean,k_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.719449 Test MSE 14262.005166550041 Test RE 1.0001286271566618\n",
      "1 Train Loss 4.1384583 Test MSE 14261.693930301552 Test RE 1.0001177143159872\n",
      "2 Train Loss 3.91612 Test MSE 14270.46069412883 Test RE 1.0004250567975752\n",
      "3 Train Loss 3.844775 Test MSE 14271.664508998338 Test RE 1.0004672523926477\n",
      "4 Train Loss 3.6838636 Test MSE 14270.118731312768 Test RE 1.0004130701411342\n",
      "5 Train Loss 3.5828848 Test MSE 14272.176174629189 Test RE 1.0004851865347872\n",
      "6 Train Loss 3.473349 Test MSE 14271.08441939956 Test RE 1.000446919566573\n",
      "7 Train Loss 3.0468073 Test MSE 14265.112128513409 Test RE 1.0002375596788204\n",
      "8 Train Loss 2.8455927 Test MSE 14257.113400616716 Test RE 0.99995709395792\n",
      "9 Train Loss 2.8363411 Test MSE 14256.010270906852 Test RE 0.999918407875884\n",
      "10 Train Loss 2.8305643 Test MSE 14258.082375688664 Test RE 0.9999910740844423\n",
      "11 Train Loss 2.829269 Test MSE 14258.77580344296 Test RE 1.0000153905779157\n",
      "12 Train Loss 2.8292596 Test MSE 14258.765226612613 Test RE 1.0000150196836946\n",
      "13 Train Loss 2.8292525 Test MSE 14258.757428354516 Test RE 1.0000147462246618\n",
      "14 Train Loss 2.829248 Test MSE 14258.744960403052 Test RE 1.0000143090148241\n",
      "15 Train Loss 2.8292406 Test MSE 14258.733537073025 Test RE 1.000013908436239\n",
      "16 Train Loss 2.8292315 Test MSE 14258.720409318777 Test RE 1.0000134480889078\n",
      "17 Train Loss 2.8292255 Test MSE 14258.705974919156 Test RE 1.0000129419215595\n",
      "18 Train Loss 2.8292172 Test MSE 14258.689031524867 Test RE 1.0000123477716194\n",
      "19 Train Loss 2.8287287 Test MSE 14257.936650702513 Test RE 0.9999859638580485\n",
      "20 Train Loss 2.827587 Test MSE 14256.461607822515 Test RE 0.9999342361654837\n",
      "21 Train Loss 2.8275232 Test MSE 14256.299735222496 Test RE 0.9999285593566593\n",
      "22 Train Loss 2.8275208 Test MSE 14256.281302469626 Test RE 0.9999279129251756\n",
      "23 Train Loss 2.8275163 Test MSE 14256.266520914358 Test RE 0.999927394539868\n",
      "24 Train Loss 2.8275163 Test MSE 14256.266520914358 Test RE 0.999927394539868\n",
      "25 Train Loss 2.8275163 Test MSE 14256.266520914358 Test RE 0.999927394539868\n",
      "26 Train Loss 2.8275156 Test MSE 14256.266520594736 Test RE 0.9999273945286589\n",
      "27 Train Loss 2.827513 Test MSE 14256.212901383427 Test RE 0.9999255141145759\n",
      "28 Train Loss 2.827513 Test MSE 14256.212901383427 Test RE 0.9999255141145759\n",
      "29 Train Loss 2.827513 Test MSE 14256.212901383427 Test RE 0.9999255141145759\n",
      "30 Train Loss 2.827513 Test MSE 14256.212901383427 Test RE 0.9999255141145759\n",
      "31 Train Loss 2.827513 Test MSE 14256.212901383427 Test RE 0.9999255141145759\n",
      "32 Train Loss 2.8275125 Test MSE 14256.212915160419 Test RE 0.9999255145977322\n",
      "33 Train Loss 2.8275113 Test MSE 14256.230647798153 Test RE 0.9999261364778737\n",
      "34 Train Loss 2.8275113 Test MSE 14256.230647798153 Test RE 0.9999261364778737\n",
      "35 Train Loss 2.8275113 Test MSE 14256.230647798153 Test RE 0.9999261364778737\n",
      "36 Train Loss 2.8275113 Test MSE 14256.230647798153 Test RE 0.9999261364778737\n",
      "37 Train Loss 2.8275113 Test MSE 14256.230647798153 Test RE 0.9999261364778737\n",
      "38 Train Loss 2.82751 Test MSE 14256.268483210311 Test RE 0.9999274633570933\n",
      "39 Train Loss 2.827509 Test MSE 14256.266748577013 Test RE 0.9999274025239402\n",
      "40 Train Loss 2.8275087 Test MSE 14256.266774163438 Test RE 0.9999274034212496\n",
      "41 Train Loss 2.827508 Test MSE 14256.286881649674 Test RE 0.9999281085855022\n",
      "42 Train Loss 2.827508 Test MSE 14256.286881649674 Test RE 0.9999281085855022\n",
      "43 Train Loss 2.8272936 Test MSE 14256.294057143074 Test RE 0.9999283602280479\n",
      "44 Train Loss 2.8272626 Test MSE 14256.22326172248 Test RE 0.9999258774496897\n",
      "45 Train Loss 2.8272529 Test MSE 14256.21237974146 Test RE 0.9999254958206881\n",
      "46 Train Loss 2.8272493 Test MSE 14256.176424886396 Test RE 0.9999242348897346\n",
      "47 Train Loss 2.8272464 Test MSE 14256.166842766308 Test RE 0.9999238988460639\n",
      "48 Train Loss 2.8272464 Test MSE 14256.166842766308 Test RE 0.9999238988460639\n",
      "49 Train Loss 2.8272464 Test MSE 14256.166842766308 Test RE 0.9999238988460639\n",
      "50 Train Loss 2.8272464 Test MSE 14256.166842766308 Test RE 0.9999238988460639\n",
      "51 Train Loss 2.8272455 Test MSE 14256.044204224632 Test RE 0.9999195979188291\n",
      "52 Train Loss 2.8272455 Test MSE 14256.044204224632 Test RE 0.9999195979188291\n",
      "53 Train Loss 2.8272455 Test MSE 14256.044204224632 Test RE 0.9999195979188291\n",
      "54 Train Loss 2.8272455 Test MSE 14256.044204224632 Test RE 0.9999195979188291\n",
      "55 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "56 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "57 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "58 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "59 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "60 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "61 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "62 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "63 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "64 Train Loss 2.8272452 Test MSE 14256.04420582858 Test RE 0.9999195979750797\n",
      "65 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "66 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "67 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "68 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "69 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "70 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "71 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "72 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "73 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "74 Train Loss 2.8272443 Test MSE 14256.037703656031 Test RE 0.9999193699437123\n",
      "Training time: 6.28\n",
      "Training time: 6.28\n"
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 75\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    k_mean,k_std = train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "# mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold,\"k\":k}\n",
    "# savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7f9d4055d2d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFKElEQVR4nO3deWBV5YE+/uecu+cmudnIJgmLoCKgQFIRFLvoRKla7bQj1hnQavmWqVaRTn+KztRl2onTxWqnBZdaO1Sq1LoMThklrYoLoIKgCMoiS1gSQra7r+e8vz/OzSU3uYEE7pJz8nzs6bn3rO+5Ae6T97zveyQhhAARERGRTsi5LgARERHRUDC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka6Yc12AdFNVFUeOHEFBQQEkScp1cYiIiGgQhBDwer2orq6GLJ+4bsVw4eXIkSOoqanJdTGIiIjoFBw8eBCjR48+4TaGCy8FBQUAtIsvLCzMcWmIiIhoMDweD2pqahLf4ydiuPDSc6uosLCQ4YWIiEhnBtPkgw12iYiISFcYXoiIiEhXGF6IiIhIVxheiIiISFcYXoiIiEhXGF6IiIhIVxheiIiISFcYXoiIiEhXGF6IiIhIVxheiIiISFcyGl7eeustXH311aiuroYkSXj55ZdPus+6detQV1cHu92O8ePH47HHHstkEYmIiEhnMhpe/H4/zj//fPz6178e1Pb79u3DV7/6VcyZMwdbtmzBPffcg9tvvx0vvPBCJotJREREOpLRBzPOnTsXc+fOHfT2jz32GGpra/HII48AACZNmoRNmzbh5z//Ob7xjW9kqJRERESkJ8PqqdIbNmxAQ0ND0rLLL78cTz31FKLRKCwWS799wuEwwuFw4r3H48l4OQcl4geC3UAsBJjtgMUBWPIAsw0YxBMziYiIKLVhFV5aW1tRUVGRtKyiogKxWAzt7e2oqqrqt09jYyMeeOCBbBURiIUBIQCI4/Oe5cEuINStzZVo6v0lWQszZjtgMgNyn8lkAUw2wNKzjZVhh4iIqJdhFV4AQOrzRS2ESLm8x9KlS7FkyZLEe4/Hg5qamswV8PM3kAgsp0KoQDSgTYMiAWarVmtjLwIcxYCjSKvBISIiGoGGVXiprKxEa2tr0rK2tjaYzWaUlpam3Mdms8FmM/IXudBqdXpqdrr2aYsteVqIcZQA+RVawCEiIhoBhlV4mTVrFl555ZWkZWvXrkV9fX3K9i5ZFYsBN9wA+I4CEgBI2lyStOmMcuAfrwZGlWSnPD21N54jQNsOIK8MKKzSgoxsyk4ZiIiIciCj4cXn82HPnj2J9/v27cPWrVtRUlKC2tpaLF26FIcPH8aKFSsAAIsWLcKvf/1rLFmyBAsXLsSGDRvw1FNP4dlnn81kMQdHUYDnnz/xNr9/GfjWlcB3vgmUFmWjVBqhAv42bZJMQH45UFQL5GUpSBEREWWRJHoalWTAm2++iS9/+cv9lt944434/e9/j5tuugn79+/Hm2++mVi3bt063Hnnndi+fTuqq6tx1113YdGiRYM+p8fjgcvlgtvtRmFhYTouQ6MowPLlQOv2Xg12ob1WVGDtu8BHn2nbOmzAP30NuPnvgWJX+sowFJIMjL0YsDpzc34iIqIhGMr3d0bDSy5kLLz02PkqUjbYFQJ4ZzPwq2eAbbu0ZXkO4GtfBgqcgEkG5PhkkgGHHRg/Gpg4Fqgsy0yPImc5MLou/cclIiJKs6F8fw+rNi+6YM0DVAVazYsar31RASjAnHrg4jrgzfeB/1oJ7NgDPLfm5MfMzwMmjAEmjgEm1gJnjQPOHnv6tTb+NsDXpt1GIiIiMgjWvKRLx+dA+67j74UA3ngf2PSJdstJVbXbSz1zrw/4/CCw7xAQU1Ifc1SxFmTOGgtMngA0XARYh9hw2eIAxl6i1fgQERENU7xtlIvwAgBHtgDe1pNv11skCuw/DOw+AOzer8137QcOpjjORTOA5fcNPcCUTgTKJgxtHyIioixieMlVeFEVoHkDEPae/rF8AWBPsxZkdu4DXlwLBMPApRcCj9wDWIZwx08yxRvv5p1+uYiIiDJgKN/fvJeQTrIJOKNOG+L/dOXnAdPOAa67Avi3fwaWxWtc/rYRWPqwditqsIQCHPv09MtEREQ0DDC8pJvFAVRNR3wku/SZNQ149B7AbAL+903g/l/Hu2wPkq8N8B1Lb5mIiIhygOElE5ylwKiz03/cL88Efv7/aY1vn38N+I/HhxZg2nZoDYaJiIh0jOElU0rGAYVnpF4nydoTpE/FFXOAnyzWXv9hNfDIisHvGw0cfzYSERGRTnGcl0yqmKKNcGuyAma79iRos117iKIQ2oMW/e2A/xgQ9gz+uF+/DAiGgAeXAY+vApwO4P9dN7h9O/ZobXKKak/tmoiIiHKM4SWTZBkoPTP1OknSnj2UVwKMOguIhoBAPMgEOgElcuJj33CVFmB+9jvg4d8DRYVa496TESpwdDsQ9gHlkzIzsi8REVEGMbwMFxY74BqtTYDW3TrQCQQ7Bw4zt3wTcPuAJ/6kNeB15QOXXzy483UfACJ+oHpaenpHERERZQnbvAxXtgKgeAxQPR2YcClQOTX1dnfeCPzDFVpD3H/5KbBh6+DPEWjXxqWJ+NNSZCIiomxgeNEL12jAnuJZR5IE3H+r9uiAaAy49d+PPxhyMCJ+4MAGwN+RvrISERFlEMOLnpQN0P3aZNK6UM+aBgSCwMIfAXsPDv64ahQ49AGw/12tPYz7MGtjiIho2OLjAfTm4Afa7Z5UfAHgpqXAJ7uBqlHAH3+uzU+VyQLYi7XHCpisWm8pky0+tyZ39x6o4e8J/3gN8Y/eoP6oDvGYkgkwsekXEVGu8dlGRg4vITdwYP3A67vcwA0/1J5W3XAR8Kt7s1c23ZK0ICabtMAmWwBbPpBXCjhKtK7tRESUUUP5/uavnHpjdwEFVYC3JfX6YhfQuAS4fgmw8SPtGUgmU3bLqDtCu3WmRoFYSFsU7AS6m7XXtgItxOSVao9/kE1a2JFM2mt2NyciyiqGFz0qmwj4jmpjtqQyZSKQ5wA8PmD3AeCc8dktn9GEvdrUfSD1eknWJu1Nr+X9XmQm6Eiydg5J1o6fVJ4cEQKJW3i9XxuVxXE84NoNWONLNMwwvOiR1an1PuqpGejLbAJmnAu8sxl4fxvDS6YJdeAgSSND2Ks9/BTQbj064gNQ2gqPtxHjeEpEacPwolelEwDPEUCNpV5fP0ULL5s+ARZck92yEY1kSlSrGfUdTV4uyVqAMcXDTE9NmWyKvzYdrzVL1KL1rknrVbuW9DodJK3hutmulU9mR1Qa3hhe9MpsA4rHas8qSuWCKdp80ydatT3bZRDlllCBWFibhjuTJf4cNnvub0HqWU/QBPqETZ3/eyybgfJzcloEhhc9Kx6ntcNQov3XTTkLsFmBTrc25suZfBAjEQ2SEtWmsDfXJaHhSLbkPLwwUuuZyazdPkrFagGmxf9wffBJ9spERESUYQwveueqBSx5qdfVx28dfbAte+UhIiLKMIYXvZNlYNQAjw34Qp92L0RERAbA8GIEBZWAvaj/8vPPASxm4GgHcKg168UiIiLKBIYXo0hV++KwawPWAdp4L0RERAbA8GIUeSVAfnn/5V+Yqs03sdEuEREZA8OLkYw6B/3GD+hp98IeR0REZBAML0ZidQJFNcnLpp+rNeo91Aq0HMtNuYiIiNKI4cVoSidoox/2yM8Dzj1Te81bR0REZAAML0ZjtgElfR7EWM9bR0REZBwML0ZUPFYLMT0uYKNdIiIyDoYXI5JNQMmZx9/PmKzN9x4EOrpzUiQiIqJ0YXgxKkfR8ddFBcBZY7XXvHVEREQ6x/BiVNb85Pcc74WIiAyC4cWoZFNyu5cv8CGNRERkDAwvRmYtOP66p8fRrv1AtzcnxSEiIkoHhhcjs+Ydf11WDIwbrT1d+sMduSsTERHRaWJ4MTKrM/l9PW8dERGR/jG8GFnfRrs94728+jbg5q0jIiLSp6yEl2XLlmHcuHGw2+2oq6vD22+/fcLtV65cifPPPx95eXmoqqrCt7/9bXR0dGSjqMbSt+blyzOBMyq0Zxzd/TCgqrkpFxER0WnIeHhZtWoVFi9ejHvvvRdbtmzBnDlzMHfuXDQ3N6fc/p133sGCBQtwyy23YPv27Xj++efxwQcf4Dvf+U6mi2o8FgcgmY6/z88DHr0HsJiBN94DfvdC7spGRER0ijIeXh5++GHccsst+M53voNJkybhkUceQU1NDZYvX55y+40bN2Ls2LG4/fbbMW7cOFx88cX47ne/i02bNmW6qMbUu9EuAEyZCPzrP2uvf/nfwPts/0JERPqS0fASiUSwefNmNDQ0JC1vaGjA+vXrU+4ze/ZsHDp0CGvWrIEQAkePHsWf//xnXHnllSm3D4fD8Hg8SRP10vfWEQBcdwVwzVcARQV+8BBwrDP75SIiIjpFGQ0v7e3tUBQFFRUVScsrKirQ2tqacp/Zs2dj5cqVmDdvHqxWKyorK1FUVIT/+q//Srl9Y2MjXC5XYqqpqUn7deha30a7ACBJwH23ARPHAMe6gB/8FIgp2S8bERHRKchKg11JkpLeCyH6LeuxY8cO3H777fjRj36EzZs349VXX8W+ffuwaNGilNsvXboUbrc7MR08eDDt5de1VDUvAJBnBx69F8hzAO9/DDy6IrvlIiIiOkXmTB68rKwMJpOpXy1LW1tbv9qYHo2Njbjooovwwx/+EABw3nnnwel0Ys6cOfjxj3+MqqqqpO1tNhtsNluqQxEwcHgBgPGjgZ/cAdz5EPDk88CEWuCqLwEm08D7EBER5VhGa16sVivq6urQ1NSUtLypqQmzZ89OuU8gEIAsJxfLFP8yFUJkpqBGZjlBeAGAuZcA87+mvb7rF8AXFwAP/AbY+BFvJRER0bCU0ZoXAFiyZAnmz5+P+vp6zJo1C0888QSam5sTt4GWLl2Kw4cPY8UK7bbF1VdfjYULF2L58uW4/PLL0dLSgsWLF+OCCy5AdXV1potrPCaz9oDGWHjgbX54izZf/TrQ3gU8+xdtKnEBl80GLp4BnFkD1FQBVkt2yk1ERDSAjIeXefPmoaOjAw8++CBaWlowZcoUrFmzBmPGjAEAtLS0JI35ctNNN8Hr9eLXv/41fvCDH6CoqAhf+cpX8J//+Z+ZLqpxWfNPHF6sFuDeRVqIee9j4LW3gaYNQKcb+NP/aRMAmE1agBk/GhhfA5SXArIEyLLWCFiStPeSpD1DCQAEer3uqTkTSTMIMfTtkep132V9j9Nrmx75eVpIKy3S5iVFgCtfuwYiIhqWJGGwezEejwculwtutxuFhYW5Ls7w0PoJ4B5iQ+ZoTGvIu/ZdYPseYO8hIBDMTPmGG7MJOPdM4JpLga9+ESjmnyMiogTZAky8LO2HHcr3N8PLSNC5Dzj22ekdQwjgaAew9yDw+UFt3u3RlqvxmhNV1Wo2VPV4TQwASOj1uk+NRt/l0gDLcZL1fZdjgPW9lwkB+AJAR7c2dboBrz+5fBYz8KULtHFxLvkCb5sREQ2D8JLx20Y0DKQa62WoJAmoLNOm2dNP/3jDVSQKtHUAr28EXv4bsONzoGm9NhUVAl+Yot1WyncCBU6gIE977bABkhy/jdbrVpocX4Zet9R63qfr1pTVok02K2CLv7Za4+fJATUeZBUVUBRtrqrJt++ybaDPeqgf0UDHKXYBTscQD0ZEp4rhZSQ4UXdpSma1AKMrgQXXatPOfcD/vA688ro2oF9T6pGhaYSzWoBLL9RuNV40Q6uxI6KM4W2jkUAIYPdaQPAp0qcspgDvfQTsOwz4/NrtJW/g+OtQRPt81Z7baPHXigpAJN9e691A+XQJobVPCkeAcFSrOYpEhkc3d5Os1TyZTL1qoHJgoM96yD+CAXZQBRDq1SC+tEgbL+maS4FJ49n4m4xnGNw2YngZKfa9DUR8uS4FZYuinMKXc5pIOH7bbCQQAvh0L/A/fwP+902t/VSP6nKgtkqb954qSrXRrZ0O7ZYjB4YkPRkG4YV1myOF1cnwMpLwyzB7JEnrnXbumcC/3Ay8+6EWZP62ETjSpk0nY7MCDjuQZwNstngbpnjbJZv1eLum3pPFrE1ymsYaLS0CaquBsdXarVM2TqdhjOFlpLDmAzia61IQGVtP77QvXaDdTvxs7/EAc6QNOByfH+sEgmGtITMQv+0XAbpzWvrjJAmoGgWMqQYK09Dgf6SSoP0iYTZrQzCYTIBZBmTT0BuLDyeSDJT9H/CLX+SsCAwvIwUb7RJlV4ET+MLUgdcLobVRCgSBQEibgiFtWaINU6RPe6YoEO31OhJNT1mF0HrZHTgCHGjRyjTYWiMamWw2hhfKAoYXouFFkuLd261aV+vhQgit3U5zPMgEQ7kukX6pQmt/FlPi8xgQUwF1GDSoPx2SCSifmNMiMLyMFAwvRDQYkgSUFWvTjMm5Lg0NRxlqsDukIuT07JQ9Jgtgsua6FERERKeN4WUkScdIu0RERDnG8DKS8NYREREZAMPLSMLwQkREBsDwMpIwvBARkQEwvIwkDC9ERGQADC8jiSVPGxmRiIhIx/hNNpJIkhZgiIiIdIzhZaSxMrwQEZG+MbyMNBzrhYiIdI7hZaRho10iItI5hpeRxl4EfT+LnYiIRjqGl5HGlg8UVue6FERERKeM4WUkKjtLe6Q5ERGRDjG8jEQWO1AyPtelICIiOiUMLyNVyTjAbMt1KYiIiIaM4WWkkk1A2dm5LgUREdGQMbyMZK4zALsr16UgIiIaEoaXkW7UObkuARER0ZAwvIx0eSVAfkWuS0FERDRoDC8EjDqbT5smIiLd4DcWaY8MKBqT61IQERENijnXBaBhonSC1gOpuxlQIrkuDRER0YAYXkhjMgNlE4GSMwHPYaBrPxDx5bpURERE/TC8UDJZBopqtMnfroUYfzsAkeuSERERAWB4oRNxlmmTqgDBbiDUrc2DXYAazXHhiIhopGJ4oZOTTYCzVJt6hH1ANAhAACJeKyNU9K+hkfq87fM+1TYptxvMNifYdqDtlSgQCwLRkDaPhbXrioUGODYREeUawwudGlu+NhlVsAvo+BzwH8t1SYiIqA+GF6JUHMXA6Hog5AY69gC+tlyXiIiI4hheiE7E7gLOqANCHqDzc8DfwfY+REQ5xvBCNBj2QqB6uvZaCK2tjBKJT1FAjSGp/U/idZ9lPfunk1C1RtVC6TVP1f4oW6R4syNJG7lZkuKvB2qjlCHp/py1g6ZeHHIDEX8GzkdEqWQlvCxbtgw/+9nP0NLSgsmTJ+ORRx7BnDlzBtw+HA7jwQcfxDPPPIPW1laMHj0a9957L26++eZsFJfoxCQJMFu1iQjQgpK3VaudC3tzXRoiw8t4eFm1ahUWL16MZcuW4aKLLsLjjz+OuXPnYseOHaitrU25z3XXXYejR4/iqaeewoQJE9DW1oZYLJbpohIRnRpJAgqrtMnXpjX2DnXnulREhiUJkZG61YSZM2dixowZWL58eWLZpEmTcO2116KxsbHf9q+++iquv/567N27FyUlJUM+n8fjgcvlgtvtRmFh4WmVnYjolPk7AO8RbViBiJ9tpcg4ZAsw8bK0H3Yo398ZrXmJRCLYvHkz7r777qTlDQ0NWL9+fcp9Vq9ejfr6evz0pz/FH/7wBzidTnzta1/Dv//7v8PhcPTbPhwOIxwOJ957PJ70XgQR0anoOzZSLKyFmIhPG1dIjcbbSynaazUWn3raLuWy3RLR8JbR8NLe3g5FUVBRUZG0vKKiAq2trSn32bt3L9555x3Y7Xa89NJLaG9vx/e+9z10dnbid7/7Xb/tGxsb8cADD2Sk/EREaWO2aVPeEGqUVbVPQ+x4qOmZVCVNhRPa4IxhrzZFfPHwRDQ8ZaXBrtSnl4EQot+yHqqqQpIkrFy5Ei6XCwDw8MMP45vf/CZ+85vf9Kt9Wbp0KZYsWZJ47/F4UFNTk+YrICLKAVkGIAMmS3bPK4QWYMJerXaITl1P0Oz9OrOtNTJPNuW6BJkNL2VlZTCZTP1qWdra2vrVxvSoqqrCGWeckQgugNZGRgiBQ4cOYeLEiUnb22w22Gy29BeeiGikkiTAVqBNRMOQnMmDW61W1NXVoampKWl5U1MTZs+enXKfiy66CEeOHIHP50ss27VrF2RZxujRozNZXCIiItKBjIYXAFiyZAl++9vf4ne/+x0+/fRT3HnnnWhubsaiRYsAaLd9FixYkNj+hhtuQGlpKb797W9jx44deOutt/DDH/4QN998c8oGu0RERDSyZLzNy7x589DR0YEHH3wQLS0tmDJlCtasWYMxY8YAAFpaWtDc3JzYPj8/H01NTfj+97+P+vp6lJaW4rrrrsOPf/zjTBeViIiIdCDj47xkG8d5ISIi0p+hfH9n/LYRERERUToxvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka4wvBAREZGuMLwQERGRrjC8EBERka5kJbwsW7YM48aNg91uR11dHd5+++1B7ffuu+/CbDZj2rRpmS0gERER6UbGw8uqVauwePFi3HvvvdiyZQvmzJmDuXPnorm5+YT7ud1uLFiwAJdeemmmi0hEREQ6IgkhRCZPMHPmTMyYMQPLly9PLJs0aRKuvfZaNDY2Drjf9ddfj4kTJ8JkMuHll1/G1q1bB3U+j8cDl8sFt9uNwsLC0y0+ERERZcFQvr8zWvMSiUSwefNmNDQ0JC1vaGjA+vXrB9zv6aefxueff4777rvvpOcIh8PweDxJExERERlXRsNLe3s7FEVBRUVF0vKKigq0tram3Gf37t24++67sXLlSpjN5pOeo7GxES6XKzHV1NSkpexEREQ0PGWlwa4kSUnvhRD9lgGAoii44YYb8MADD+Css84a1LGXLl0Kt9udmA4ePJiWMhMREdHwdPKqjdNQVlYGk8nUr5alra2tX20MAHi9XmzatAlbtmzBbbfdBgBQVRVCCJjNZqxduxZf+cpXkvax2Wyw2WyZuwgiIiIaVjJa82K1WlFXV4empqak5U1NTZg9e3a/7QsLC7Ft2zZs3bo1MS1atAhnn302tm7dipkzZ2ayuERERKQDGa15AYAlS5Zg/vz5qK+vx6xZs/DEE0+gubkZixYtAqDd9jl8+DBWrFgBWZYxZcqUpP3Ly8tht9v7LSciIqKRKePhZd68eejo6MCDDz6IlpYWTJkyBWvWrMGYMWMAAC0tLScd84WIiIioR8bHeck2jvNCRESkP8NmnBciIiKidGN4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl0x57oAlDlCCISUEALRAAKxAALRAIKxIJwWJ0odpSiyFUGWmF+JKDUhBITIdSn0refjE/EP0ggfpwTAbMrtdwfDiwEJIbCzayfaAm1QhdpvfXe4G4d9h2GWzSi2F6PMXoYSRwkssiUHpSUyFiEEIoqKSExFTBFQhICqavOYIqAKAVUAaiIYaHMBxOfx971eHz928rJ0fhEqqoCiCsRUVZsr2nuivswmCV86uzy3Zcjp2SkjdnbtRKu/9aTbxdQYjgWO4VjgGCRIsJgssMjxqddrWZKhCjX+j2Z86vkv8dtEfB5flw4SpLQcZ8DjSxJMkinpes2yGVbZCofZAZNsAqBdkz+iwBOMIhRVoAptmSq0f/DVPr+a9v1Nte/nke7fZAV6PnckfRnmkpT40Ul93huTqiYHFiLKLIYXg9nZObjg0peAQESJIKJEMlAqfVBVgVj8N89ITEBRLIBih6rYYZWdsJucMEmmXBeTiGjEY3gxkD1de9DibzmlfSMxFb5wDNH4b489v0VGFRWqAGQJgATIkCBJgBz/VVqKvwe036711oJGEUhUk5+shlyCBFmSIWmfQvy1BEmS0b+eKHVVQyZqk6ReP4vEGXJd1dGr6ifVK6ORJBlmyQKzbI3PLTBLFpgkMyRJhgwZsiRDZvglSguGF4PY696LQ75DQ95PUVW0ecI45guf8MtbFQAEoB5vfnZK5dQzAQFFKACUngVEQ9Y7+Mbjv7ZckpIDaK/lybG39y8O6QupWrlM2gQ58TrTt29HknT+vHLJYjIBGAFtXpYtW4af/exnaGlpweTJk/HII49gzpw5Kbd98cUXsXz5cmzduhXhcBiTJ0/G/fffj8svvzwbRdWl/e79aPY0D2kfIQQ6fBG0ekKIsVEeUdYcD8H9VhDpgtVkBjA1p2XIeC3/qlWrsHjxYtx7773YsmUL5syZg7lz56K5OfWX7VtvvYW/+7u/w5o1a7B582Z8+ctfxtVXX40tW7Zkuqi61OJrwX7P/iHt4w5G8VmrF4e6gwwuRESkO5IQme2XMHPmTMyYMQPLly9PLJs0aRKuvfZaNDY2DuoYkydPxrx58/CjH/3opNt6PB64XC643W4UFhaecrn1QBUqNrZsHFIj2y5/BAc6AxksFRERGZnVZMb3Zl6V9uMO5fs7ozUvkUgEmzdvRkNDQ9LyhoYGrF+/flDHUFUVXq8XJSUlKdeHw2F4PJ6kaaQ44jsy5N5BXYGR25uIiIiMIaPhpb29HYqioKKiIml5RUUFWlsH1533F7/4Bfx+P6677rqU6xsbG+FyuRJTTU3NaZdbD1Shotk7tHYuMUXrUURERKRnWenZ2reFtRBiUK2un332Wdx///1YtWoVystTt2xeunQp3G53Yjp48GBayjzctfhbhlzr0h2MnrQ7MBER0XCX0d5GZWVlMJlM/WpZ2tra+tXG9LVq1SrccssteP7553HZZZcNuJ3NZoPNZktLefVCFeqQexcBQHcgmoHSEBERZVdGa16sVivq6urQ1NSUtLypqQmzZ88ecL9nn30WN910E/74xz/iyiuvzGQRdanV34qwEh7SPlFFhZ+3jIiIyAAyPs7LkiVLMH/+fNTX12PWrFl44okn0NzcjEWLFgHQbvscPnwYK1asAKAFlwULFuDRRx/FhRdemKi1cTgccLlcmS7usCeEGHJbFwBwB6IcRoKIiAwh4+Fl3rx56OjowIMPPoiWlhZMmTIFa9aswZgxYwAALS0tSWO+PP7444jFYrj11ltx6623JpbfeOON+P3vf5/p4g57rf5WhGKhIe/HXkZERGQUGR/nJduMPM6LEALvt76PYCw4pP0iMRU7WkZOF3IiIsocw4/zQul1NHB0yMEFALpZ60JERAbC8KITQggc8Bw4pX272MuIiIgMhOFFJ0611iUcUxCMpngIHBERkU5l5anSdOqiahT73PvQ4ms5pf27/LxlRERExsLwMoy1+Fqw170XUfXUb/vwlhERERkNw8sw5I14sbtrNzyR0+shFIzEEI6paSoVERHR8MDwMkyoQkV3uBttgTYc9R+FSMOQcnwcABERGRHDSw4FogF0hjrRGepEd7gbqkhvLUl3kOGFiIiMh+ElB7wRLz7r/Az+qD9j5/CHecuIiIiMieEly1r9rdjVtSvttSy9xVQVBzuH3q2aiIhIDxheskQVKvZ078ER35HMnkcV2N/uRyjGsV2IiMiYGF6yIKJEsL1jO9xhd8bPdbArAF+YwYWIiIyL4SXD3GE3tndsR0TJ/GBxLd1BjutCRESGx/CSAapQ0RHsQKu/FZ2hzrR0ez6ZDn8ER73hjJ+HiIgo1xhehiiqRGGWzZAkqd86T8SDVn8r2gJtiKmxrJXJG4ziUGcga+cjIiLKJYaXIVp/ZD0EBEySCWbZDLNshkW2IKJEEIhlP0AEIzHs6/RnoW6HiIhoeGB4OUWKUKAoCsJK7m7VdAcjaO4MQOVwLkRENIIwvOhUizuIox62cSEiopGH4UVnYqqK5s4gPBz6n4iIRiiGFx0JRRXsa/dz2H8iIhrRGF4yKBxTcKgriPICOwrsp/ZRx1QV0ZiKQFTF4W62byEiImJ4yRBtmP4AglEF3pAP+TYTKgsdyB8gxMRUFb5QDN5QDBFFRSSmIqqoUNmNiIiIKAnDS4a0eEIIRo8P0+8LK9hzzId8mxlVLjucNjOCES2seIJR+CMKuzsTERENAsNLBriDURwbYLRbXziG3W0+mGUJMVarEBERDZmc6wIYTSSm4uAgRrtlcCEiIjo1DC9pJIRAc2eAwYSIiCiDGF7SqM0bhi+cvWcaERERjUQML2niD8fQ6g7luhhERESGxwa7Q+QNRRETKlRV6w6tCAFVFegKRNlbiIiIKAsYXoZo7zE/VMYUIiKinOFtIyIiItIVhhciIiLSFYYXIiIi0hWGFyIiItIVhhciIiLSFYYXIiIi0hWGFyIiItIVhhciIiLSFQ5SZwBhJYSOQCvaAy04FmxFe+AIukMduPCMv8P5FbNyXTwiIqK0YnjRKSEENrWsw6ufP4vOUFvKbXZ3bkOe5UeYWDI1y6UjIiLKnKzcNlq2bBnGjRsHu92Ouro6vP322yfcft26dairq4Pdbsf48ePx2GOPZaOYaRXwWPDqU+fgmQfqcGiXK63HDkb9eOaTX+KP2x9NBJc8Sz5qCydiRuUlaBh/Hc4tq4ciYnj6o//EUd/BtJ6fiIgolzJe87Jq1SosXrwYy5Ytw0UXXYTHH38cc+fOxY4dO1BbW9tv+3379uGrX/0qFi5ciGeeeQbvvvsuvve972HUqFH4xje+keninraAx4J1fzoTb/3pTIT8FgDA5rU1mH7ZIXx14Q6UjQ6c1vH3dn+KZ7b9El2hY5AlGZePvx4X1VwBp6UgabuoEsGyzfdhv/szPLH1J1h8wUMosBad1rmJiIiGA0kIkdGnDM6cORMzZszA8uXLE8smTZqEa6+9Fo2Njf22v+uuu7B69Wp8+umniWWLFi3CRx99hA0bNpz0fB6PBy6XC263G4WFhem5iF5+tf5/Uj6YMei14M1VyaGl6kw3ymt9+OiNMwAAsknFrGv2o+GmnSgsDQ/pvIqqoGnf81i793kIqChxVGD+lDsxtujsAffxRdx49P270R5sxRjXWfhe3YOwmmxDOi8REVFvVpMZ35t5VdqPO5Tv74zWvEQiEWzevBl333130vKGhgasX78+5T4bNmxAQ0ND0rLLL78cTz31FKLRKCwWS8bKeyKRCHDL8sfx2aFuSOESSIESiGAZhK8MUV8hdr5fiZAvHlrGu9Fw806c98UjkGXg0K5d+Mtj5+Kz9yrw7ovj8cGaWlxwZTOsjhgiQTOiYRMiQRPCITOEArjKQyipDKC4yg/TqD3wO7dic/dq7Hd/BgCor/oSvnHOQtjNeScsc77VhYXT/xWPvn83Drh34Y+fPIoF5/0LZImdzIiISL8yGl7a29uhKAoqKiqSlldUVKC1tTXlPq2trSm3j8ViaG9vR1VVVdK6cDiMcPh4LYbH40lT6ZMFIxE8070IyIc2lfZaqViACeehZOtDuPrqQpz3JS209Bh9lhvffXgDdm8uwyuPn4WDBSvxTt4aIFQEBMsBfzkQGKXNFQtg/xAoeB8Ivg+0dwLt2nHkWAEuK74TV0yeDkkaXLnLnWfg5ml3Y/nm+/FR2wb8Zc8zuHrigvR8KERERDmQld5GUp9vWiFEv2Un2z7VcgBobGzEAw88kIZSnphkDqOq6xvwSkegWDuhWLoRM3VBlSOAKQpUb0ZXdQMOnHE1zhU3wIr+t2fsE9+DevONgHfv4E+sWIGW6cChC6FuXIy13WPx0VgPZl2zH1+44iDyCqNJmwc8FrQfdqKzJQ9+txVBnwVB37kYZ87DnrFL8Pr+l7Dr7bMw3fEtVI/3oWq8F4VloUGHISIiolzLaHgpKyuDyWTqV8vS1tbWr3alR2VlZcrtzWYzSktL+22/dOlSLFmyJPHe4/GgpqYmDaVPVmgvwJFH/pzU5kUIgYgahjfchbV7n8cHLW/gzebV+OTYB7h+8m04s/hcANo4LK9+/hzWHXgFAiryzPm4bNw3YJat8EW64Y244Yu64Yu4EVEiqM4fg1rXRNQWTkR1wRhIqhWHdhZhowX4sCmGo/sL8fKj5+EvyydjyiUtgADaDzvRcdiJgNc6wBWcBXzRC3z5Phyq+U8cOvw34OfLgCNXIK8ggsrxHhSVB+HIj8LujMHujMKeH4PDGYWzKIyi8hBco4KwO2MMOkRElFMZDS9WqxV1dXVoamrC17/+9cTypqYmXHPNNSn3mTVrFl555ZWkZWvXrkV9fX3K9i42mw02W24aoUqSBJvJDlteFW6YcjvOr5iN5z99DO3BFvxm07/i4pqv4qyS8/DSzqcSXZqnV1yMr599CwpsRYM/kSwwZnIXxkzuwjXf/wSb147G+pfH4cgeF7b8dXS/zQtKQig9w4+C4rAWRvKjcBREYc+/BoeiAXxk+g1iZ2wCFs4ENv8/BP7aiL0flQ2qKDZHFK5RIbhGhZDnisBkUmEyC8jxucmsQpIHaAMujqeepC0G2rzX9r23GWjfpO2TjjPw8dWYhGjEhFhE1uZhba7EJAhVgqpKUBVtEqqUOIckxQ8qoVeYS32irIU9qVe5snnePjLbBeDUnM5n0fszxQDHsdoUFJaF4CoL9ZoH4XRFEn8v5PjcZDr+90SK//nRXovjf54kAQm9Xku9ytKzD3+JoBEs472NVq1ahfnz5+Oxxx7DrFmz8MQTT+DJJ5/E9u3bMWbMGCxduhSHDx/GihUrAGhdpadMmYLvfve7WLhwITZs2IBFixbh2WefHVRX6Vz1NuoRjPrxP7uexntH/pa0vNg+Ct8857s4d1RdWsohBNC8oxjb362EIz+K0jP8KDvDj9JqP2x5ygn39YS7sHr3f2NzyzoAgEMuwvmx21He9Q2EfFaEfBaE/GYEfRaE/Bb4uqxwH3OcoFaHiHIlKVydznFkAZNJ+0VENgvIJgGTSQXb96fPYIKwHkiQUFrgwO7d6T3uUL6/Mx5eAG2Qup/+9KdoaWnBlClT8Mtf/hKXXHIJAOCmm27C/v378eabbya2X7duHe68805s374d1dXVuOuuu7Bo0aJBnSvX4aXHp+0f4k87lsEd7sKc2ivx1TO/BZvZkfbynI49nZ/gz589jqP+QwCAAmsRyvIqUerQprK8CpQ6KpFnyddukYUleDst8HRZ4e2yIBSQEVNUKKqAqoj4axWqmvz38vhfWDHwX9gBlksDvBnwt84B/iGXknY+vo1NzofTVIx8czFsVjMsVhVmqwrZrEKWtX/Ae+aSrL0GjtfyiF6XlspAtUHpdrwcfcqVI7muFcjE9Z/omOGABe52Ozzt9qR50GuBEpPjkwQ1JkNRJKgKEwHpm80GhELpPeawCy/ZNFzCC6ANFBeI+eCylaS9HOkSU6NY1/y/WLv3T4goaf6TqDN2cx4KrcUosLpQmV+LCSVTMKF4CvKt6R0hmUiI+KRKveYShNonGIv4egAQ8XWJ91JaQ1riFmlMC1eKooUtQ31B5EjfX3Ky9UtNplhMJlw3dQ7q0nMjIYHhZZiEFz0Jx4I46j+MjuBRdARb0R5oRUewFR3BowgrQUiQ4/fgZUiQtLkkQZZMkCFDlmRIUnwO6YS9yYYLIQQCMR+84W4oIjbgdlX5tZhQPBUTSqagylkLi8kKi2yFxWSDWbZw3BwiGlEMP0gd6YfN7ECtawJqXRNyXZSsE0IgGPPDG+6CN+KGO9yJA+5d2NP1CVp8B9Dia0aLrxlvH/xLyv3NsgUmyZwIbFL8P+1/EhI3rSQp6X26Ap4kyTBJJi1ISqbE61wFSCEEBASEUHvNgQHvraXrvOk6/ikcxmZ2oMRRjlJHBUodFShxVKDUXg6XvRQW2aqLME+kJwwvNOJJkoQ8Sz7yLPmogNbNvq5Ka5Pli7ixp2s79nRuw56u7egOtSOqRqCK442iY2oUMURTHptGjoOePSmXy5IJdpMDNrMDdrMDNlMerCYbTLIZpnjYNMnmePA0Q47XYGphtPc8XruZoqYzHawmOxxmJxwWJxxmJ/Is+XCYnTDLuRnV3Ch6aqklSL1+XlLO24WdDlVIUFQFJtmUszIwvBCdQL7VhWkVszGtYnbSckVVEFMjiKoRRJUIYiIGxGscACRqG0TP//c0WkAaawig1XKoQoUqFKhCgdIzV0/c4yzT5MQ/2LLWrTc7D7AfUg3Hie6YD/U4wZgfHcGj6Ay2xW+3tqEzeDQRdAMxHwIx36CPSTTc/ctfbQj9a+7aSTK8EJ0Ck2yCSXbAhuHVg4yGDyEEQrEAwkoQ4VgQISWIUCyIcCyAsBLuFTZjUFQFiohBEQqEUBMhVAumauK1SLxXIaDN01JWCESUEIJRP4IxP4JRPwIxH0KxQFqOT5RuDC9ERBkgSZJ2C8bizHVRTllPaKJT178NmIAKfX+mVtmMW+ovz2kZGF6IiCilnkbgRL1ZTWaUOHI7BAj7eBIREZGuMLwQERGRrvC20RBVuuzx3hOALEuQtV5viMYEOv0RBKO57eVBRERkdAwvQ1RRaB+wq+uoAhuCUQXd/gg6AxFEFeONxEtERJRrDC9p5rCY4ChyoNJlhy8UwzF/BJ4gBzAjIiJKF4aXDJEkCQUOCwocFnhDMRzpDvKWEhERURqwwW4WFNjNOKsiHzUlebCYdDwmNBER0TDAmpcskSQJpU4rih0WHPWGcMwbhsomMUREREPG8JJlsiyhyuVARYEdoZiCQERBMKLNQ1Elw8/dJSIi0j+GlxyRZQl5VjPyrMd/BKoq4A/H0BWMoDsYharvEaSJiIgyguFlGJHl4418R6sC7mAUnYEIfKEYa2SIiIjiGF6GKVmWUOy0othpRSSm4qgnhA5/JNfFIiIiyjn2NtIBq1nGGUUO2Mz8cREREfHbUCdkWUJNcV6ui0FERJRzDC86km83o8RpyXUxiIiIcorhRWeqixwwyxzojoiIRi422B2isa6xiCpRRNQIomoUUSWKqBpFRIkM+MDGdDLLMqqLHGjuDGT8XERERMMRw8sQjSkck3L5nq49OOQ7lJUylDit6PRH4AvHsnI+IiKi4YS3jdJkrGssrLI1a+erKXGAd4+IiGgkYnhJE7Nsxvii8Vk7n81sQnmBLWvnIyIiGi4YXtKo0lmJQmth1s5XXmDn2C9ERDTi8JsvzSYUT4CE7NzPkWUJY0vzYDHx/hEREY0cDC9pVmgtRKWzMmvnc1jNmFheALvZlLVzEhER5RLDSwaMd42HWc5eRy6rWcaECifybew8RkRExsfwkgEWkwVjC8cOuL7YXoxxrnGwmdLX4NYsyxhf5kRxHkfgJSIiY+Ov6hlyRv4ZaPG3wB/1J5aVOcpQW1ibaNRbW1CLtkAbDnoPwhf1nfY5ZVnCmFInrKYgjnrDp308IiKi4YjhJUMkScLEoon46NhHGJU3CmMKx8BpcfbbpsJZgQpnBbpCXTjoPYjOUOdpn7uqyAGTScKR7tBpH4uIiGi4YXjJoCJ7EWZVz4LVdPLB64rtxSi2FyMUC8EX9cEX8cEf88Mf8SMYCw750QNlThtaPSGo6qmWnoiIaHhieMmwwQSX3uxmO+xmO8ocZYllqlDhCXvwuftzeCPeQR1HliUUOSzo9EeHdH4iIqLhjg12dUCWZBTZizCjfAbOLj4bFnlwjXKLHdl7XAEREVG2MLzoiCRJqMqvwgVVF6DKWXXSwfDy7WYOYEdERIbD8KJDFtmCs0vOxoyKGSd8HIEkSSjOY+0LEREZC8OLjhVYCzC9fDpqC2sH3IbjvhARkdEwvOicJEkY7xqPqWVTU7aFcVjNfHQAEREZSkbDS1dXF+bPnw+XywWXy4X58+eju7t7wO2j0SjuuusuTJ06FU6nE9XV1ViwYAGOHDmSyWIaQqmjFPWV9XDZXP3WFTtZ+0JERMaR0fByww03YOvWrXj11Vfx6quvYuvWrZg/f/6A2wcCAXz44Yf4t3/7N3z44Yd48cUXsWvXLnzta1/LZDENw2ayYdqoaagtSL6NxHYvRERkJJIQYmijnw3Sp59+inPPPRcbN27EzJkzAQAbN27ErFmz8Nlnn+Hss88e1HE++OADXHDBBThw4ABqawdu29HD4/HA5XLB7XajsHDgxqxG1+pvxWednyXe72nzwhdWclgiIiIyAqvJjO/NvCrtxx3K93fGal42bNgAl8uVCC4AcOGFF8LlcmH9+vWDPo7b7YYkSSgqKkq5PhwOw+PxJE0EVORVJD34sdiZvodAEhER5VLGwktrayvKy8v7LS8vL0dra+ugjhEKhXD33XfjhhtuGDCFNTY2JtrUuFwu1NTUnFa5jUKSJFQ5qxLvXQ4zZA75QkREBjDk8HL//fdDkqQTTps2bQKgfYH2JYRIubyvaDSK66+/HqqqYtmyZQNut3TpUrjd7sR08ODBoV6SYVXlHx/IzizLKLSz4S4REenfkJ9tdNttt+H6668/4TZjx47Fxx9/jKNHj/Zbd+zYMVRUVJxw/2g0iuuuuw779u3D66+/fsJ7XzabDTYbb4mkYjPZUOooRXuwHQBQ5LSgO8hnHRERkb4NObyUlZWhrKzspNvNmjULbrcb77//Pi644AIAwHvvvQe3243Zs2cPuF9PcNm9ezfeeOMNlJaWDrWI1Et1fnUivBTaLDDJgMInTRMRkY5lrM3LpEmTcMUVV2DhwoXYuHEjNm7ciIULF+Kqq65K6ml0zjnn4KWXXgIAxGIxfPOb38SmTZuwcuVKKIqC1tZWtLa2IhKJZKqohlZiL4HdbAfQ86RpdpsmIiJ9y+g4LytXrsTUqVPR0NCAhoYGnHfeefjDH/6QtM3OnTvhdrsBAIcOHcLq1atx6NAhTJs2DVVVVYlpKD2UKFnvhrtl+VY23CUiIl0b8m2joSgpKcEzzzxzwm16DzMzduxYZGjYmRGtylmFA54DUIUKh9WMM0flY1+7HzGVnzUREekPn200AlhNVpQ6jrcdctrMmFCeD6uJP34iItIffnuNENXO6qT3dosJE8rz4bDwoY1ERKQvDC8jRLG9GA6zI2mZ1SzjzHIn8m0MMEREpB8MLyNI39oXQBu8bnxZPlwODmBHRET6kNEGuzS8VDorsc+zD6pIHuhFliWMK3MiGFXgD8fgD8cQiCgIxzggDBERDT8MLyOIxWRBmaMMbYG2lOsdFhMcFhPK8rURiyMxFYGIFmQCEQXBaIwD3BERUc4xvIwwNQU1aA+296t9ScVqlmE1W1GUd3xZJKYiEI0hFFEQVQTUeNd2IQAVGFRX95TDzKR43tXJhqMZ6nA1qR6p1VNcEZ8gBAS064zEVLAzORHR8MPwMsIUWAtw/qjzsa19G2JqbMj79wQaOE6+rd6pqkAwpiAUVRGK1zxFFQEhtJCmQAAC4HA5RETZxfAyArlsLkwvn46Pjn2EiMLHLgxEliU4rWY4rQCcA28nhEDfCifRq84mZbZJY+AR6F2DJLQwNRwGe5SkRO3YIB4kn1Hp+jgGOkwkpsIbisIbirGtGFEWMLyMUE6LEzPKZ+DjYx8jEAvkuji6JklSii9nPoNhJHFYTIkee6GoAm8oBk8oikCE7cSIMoHhZQSzm+2YXj4dH7d/DG/Em+viEBmC3WKC3WLCqAKt4buqCihCIKaoiKmAoqqIqcdvP6rxmrueNmNCaDVo2nrtmCqQaI8ltA0gkN5blqoQiKoqVIYt0gGGlxHOYrJg2qhp2N6xHZ2hzlwXh8hwZFmCDAkWnTyOQ1UFYqoWZJRejfLp1PXuGNB7rldWOffRIfcloJwzySZMLZsKX9QHX8QHb9QLX8QHX9Q3qF5JRGQcsizBKkuwcgxTGoCZ4YWGC0mSUGAtQIG1AFWoAqBVTwdigaRGvaLP7wwyZEjxBh+SJEE+wT94Uq5bbaYQVaIIK2FElAjCSjgxMbgREQ1fDC80IEmS4LQ44bScoKuN3g3wVARFVdAZ6kR7sB0doY5T6lZORESZwfBClIJJNmFU3iiMyhsFIQS6w93oCHYkamQUoUARClRVRUzEEoPz9a2ZIiKi9GN4IToJSZJQbC9Gsb14SPv1Hm24b6gZzEjEgz4PhNZrBWq894qa8pzZIvXpJi5JUr9lRhNVo/BH/QhEA/BH/fDH/AjFQrkuFpFhMbwQZUjvNj79vryN/V0+4thhR4G1IGlZTI0hFAshpsYQEzHE1BiiahRRNQpFVaAKNT6o4PHQ2RNAASStB+LdqOP/qUKNd5dOX0BVhMJ2XqQbDC9ERBlgls3It+bnuhhDElNjiCiRRMiKKlGoYKA5HT2hE71Gwdb77WVZyn1PNIYXIiICoAWu4dANluhkch+fiIiIiIaA4YWIiIh0heGFiIiIdIXhhYiIiHSF4YWIiIh0heGFiIiIdIXhhYiIiHSF4YWIiIh0heGFiIiIdIXhhYiIiHSF4YWIiIh0heGFiIiIdIXhhYiIiHSF4YWIiIh0xXDPPhdCAAA8Hk+OS0JERESD1fO93fM9fiKGCy9erxcAUFNTk+OSEBER0VB5vV64XK4TbiOJwUQcHVFVFUeOHEFBQQEkSUrrsT0eD2pqanDw4EEUFham9djDFa+Z12xUvGZes1Hp9ZqFEPB6vaiuroYsn7hVi+FqXmRZxujRozN6jsLCQl39gUgHXvPIwGseGXjNI4Mer/lkNS492GCXiIiIdIXhhYiIiHSF4WUIbDYb7rvvPthstlwXJWt4zSMDr3lk4DWPDCPhmg3XYJeIiIiMjTUvREREpCsML0RERKQrDC9ERESkKwwvREREpCsML4O0bNkyjBs3Dna7HXV1dXj77bdzXaS0euutt3D11VejuroakiTh5ZdfTlovhMD999+P6upqOBwOfOlLX8L27dtzU9g0aGxsxBe+8AUUFBSgvLwc1157LXbu3Jm0jdGuefny5TjvvPMSA1fNmjUL//d//5dYb7TrTaWxsRGSJGHx4sWJZUa77vvvvx+SJCVNlZWVifVGu94ehw8fxj/90z+htLQUeXl5mDZtGjZv3pxYb7TrHjt2bL+fsyRJuPXWWwEY73r7EXRSzz33nLBYLOLJJ58UO3bsEHfccYdwOp3iwIEDuS5a2qxZs0bce++94oUXXhAAxEsvvZS0/qGHHhIFBQXihRdeENu2bRPz5s0TVVVVwuPx5KbAp+nyyy8XTz/9tPjkk0/E1q1bxZVXXilqa2uFz+dLbGO0a169erX4y1/+Inbu3Cl27twp7rnnHmGxWMQnn3wihDDe9fb1/vvvi7Fjx4rzzjtP3HHHHYnlRrvu++67T0yePFm0tLQkpra2tsR6o12vEEJ0dnaKMWPGiJtuukm89957Yt++feKvf/2r2LNnT2Ibo113W1tb0s+4qalJABBvvPGGEMJ419sXw8sgXHDBBWLRokVJy8455xxx991356hEmdU3vKiqKiorK8VDDz2UWBYKhYTL5RKPPfZYDkqYfm1tbQKAWLdunRBiZFyzEEIUFxeL3/72t4a/Xq/XKyZOnCiamprEF7/4xUR4MeJ133fffeL8889Puc6I1yuEEHfddZe4+OKLB1xv1Ovu7Y477hBnnnmmUFV1RFwvbxudRCQSwebNm9HQ0JC0vKGhAevXr89RqbJr3759aG1tTfoMbDYbvvjFLxrmM3C73QCAkpISAMa/ZkVR8Nxzz8Hv92PWrFmGv95bb70VV155JS677LKk5Ua97t27d6O6uhrjxo3D9ddfj7179wIw7vWuXr0a9fX1+Id/+AeUl5dj+vTpePLJJxPrjXrdPSKRCJ555hncfPPNkCTJ8NcLsM3LSbW3t0NRFFRUVCQtr6ioQGtra45KlV0912nUz0AIgSVLluDiiy/GlClTABj3mrdt24b8/HzYbDYsWrQIL730Es4991zDXi8APPfcc/jwww/R2NjYb50Rr3vmzJlYsWIFXnvtNTz55JNobW3F7Nmz0dHRYcjrBYC9e/di+fLlmDhxIl577TUsWrQIt99+O1asWAHAmD/n3l5++WV0d3fjpptuAmD86wUM+FTpTJEkKem9EKLfMqMz6mdw22234eOPP8Y777zTb53Rrvnss8/G1q1b0d3djRdeeAE33ngj1q1bl1hvtOs9ePAg7rjjDqxduxZ2u33A7Yx03XPnzk28njp1KmbNmoUzzzwT//3f/40LL7wQgLGuFwBUVUV9fT3+4z/+AwAwffp0bN++HcuXL8eCBQsS2xntuns89dRTmDt3Lqqrq5OWG/V6Ada8nFRZWRlMJlO/tNrW1tYv1RpVT08FI34G3//+97F69Wq88cYbGD16dGK5Ua/ZarViwoQJqK+vR2NjI84//3w8+uijhr3ezZs3o62tDXV1dTCbzTCbzVi3bh1+9atfwWw2J67NaNfdm9PpxNSpU7F7927D/pyrqqpw7rnnJi2bNGkSmpubARj37zMAHDhwAH/961/xne98J7HMyNfbg+HlJKxWK+rq6tDU1JS0vKmpCbNnz85RqbJr3LhxqKysTPoMIpEI1q1bp9vPQAiB2267DS+++CJef/11jBs3Lmm9Ea85FSEEwuGwYa/30ksvxbZt27B169bEVF9fj3/8x3/E1q1bMX78eENed2/hcBiffvopqqqqDPtzvuiii/oNdbBr1y6MGTMGgLH/Pj/99NMoLy/HlVdemVhm5OtNyFFDYV3p6Sr91FNPiR07dojFixcLp9Mp9u/fn+uipY3X6xVbtmwRW7ZsEQDEww8/LLZs2ZLoDv7QQw8Jl8slXnzxRbFt2zbxrW99S9fd7v75n/9ZuFwu8eabbyZ1NwwEAoltjHbNS5cuFW+99ZbYt2+f+Pjjj8U999wjZFkWa9euFUIY73oH0ru3kRDGu+4f/OAH4s033xR79+4VGzduFFdddZUoKChI/HtltOsVQusGbzabxU9+8hOxe/dusXLlSpGXlyeeeeaZxDZGvG5FUURtba246667+q0z4vX2xvAySL/5zW/EmDFjhNVqFTNmzEh0qTWKN954QwDoN914441CCK2r4X333ScqKyuFzWYTl1xyidi2bVtuC30aUl0rAPH0008ntjHaNd98882JP8OjRo0Sl156aSK4CGG86x1I3/BitOvuGc/DYrGI6upq8fd///di+/btifVGu94er7zyipgyZYqw2WzinHPOEU888UTSeiNe92uvvSYAiJ07d/ZbZ8Tr7U0SQoicVPkQERERnQK2eSEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl1heCEiIiJdYXghIiIiXWF4ISIiIl35/wH16CtOkxbjOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Epochs, Layers, derivs\n",
    "layer_num = 1\n",
    "plt.plot(k_mean[:,layer_num,0],'b')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,0] - 0.5*k_std[:,layer_num,0],k_mean[:,layer_num,0] + 0.5*k_std[:,layer_num,0],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,1],'r')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,1] - 0.5*k_std[:,layer_num,1],k_mean[:,layer_num,1] + 0.5*k_std[:,layer_num,1],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,2],'g')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,2] - 0.5*k_std[:,layer_num,2],k_mean[:,layer_num,2] + 0.5*k_std[:,layer_num,2],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"k_mean\":k_mean,\"k_std\":k_std}\n",
    "savemat('k_tanh_failed.mat', mdic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
