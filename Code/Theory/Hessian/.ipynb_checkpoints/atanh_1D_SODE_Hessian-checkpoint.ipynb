{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_atanh\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "        \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0  #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def forward_hess(self,x,Ws,bs,hess_alphas):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        # print(a.shape)\n",
    "        # print(Ws[0].shape)\n",
    "       \n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = torch.matmul(a,torch.transpose(Ws[i],0,1)) + bs[i].reshape(1,-1)\n",
    "            a = self.activation(self.n*hess_alphas[:,i]*z)\n",
    "            \n",
    "        a = torch.matmul(a,torch.transpose(Ws[-1],0,1)) + bs[-1].reshape(1,-1) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def loss_hess(self,a,b,c,d,e,f,g):\n",
    "        \n",
    "        hess_alphas = a\n",
    "        Ws = [b,d,f]\n",
    "        bs = [c,e,g]\n",
    "#         hess_linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "#         hess_linears[0].weight.data = b\n",
    "#         hess_linears[0].bias.data = c\n",
    "#         hess_linears[1].weight.data = d\n",
    "#         hess_linears[1].bias.data = e\n",
    "#         hess_linears[2].weight.data = f\n",
    "#         hess_linears[2].bias.data = g\n",
    "        \n",
    "        loss_val = self.loss_function(self.forward_hess(x_test_tensor,Ws,bs,hess_alphas),y_true_tensor)        \n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def Hess(self):\n",
    "        H = torch.autograd.functional.hessian(self.loss_hess, tuple([_ for _ in self.parameters()]))\n",
    "        \n",
    "        shapes = [100,50,50,2500,50,50,1]\n",
    "        total_hess = np.zeros((2850,2850))\n",
    "\n",
    "        col_ind = 0\n",
    "        for i in range(6):\n",
    "            shape1 = shapes[i]\n",
    "            col_ind = col_ind + shapes[i] \n",
    "            row_ind = 0\n",
    "            for j in range(6):\n",
    "                shape2 = shapes[j]\n",
    "                hess = H[i][j].reshape(shape2,shape1).cpu().detach().numpy()\n",
    "\n",
    "                row_ind = row_ind + shapes[j]\n",
    "\n",
    "                total_hess[row_ind-shapes[j]:row_ind,col_ind-shapes[i]:col_ind] = hess\n",
    "\n",
    "        w,_ = np.linalg.eig(total_hess)\n",
    "        k = np.max(w)/np.min(w)\n",
    "        return k\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    return PINN.Hess()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    k = np.zeros((max_iter,1))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        k[i] = np.abs(train_step(x_coll,f_hat))\n",
    "        \n",
    "        print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "k = [1.49557243]\n",
      "0 Train Loss 3.7797968 Test MSE 390.32738498071694 Test RE 1.0071104296220055\n",
      "k = [1.58844723]\n",
      "1 Train Loss 3.311038 Test MSE 386.9709203743667 Test RE 1.0027709584415343\n",
      "k = [1.77831309]\n",
      "2 Train Loss 3.048594 Test MSE 390.419725250702 Test RE 1.007229549306029\n",
      "k = [1.28166338]\n",
      "3 Train Loss 2.8314896 Test MSE 389.11743218276723 Test RE 1.0055482770187392\n",
      "k = [1.66585532]\n",
      "4 Train Loss 2.7008648 Test MSE 390.70432668373576 Test RE 1.007596598833364\n",
      "k = [1.43651841]\n",
      "5 Train Loss 2.55292 Test MSE 388.52101775517104 Test RE 1.0047773613115236\n",
      "k = [1.54282628]\n",
      "6 Train Loss 2.4175072 Test MSE 386.25880663436664 Test RE 1.0018478712496452\n",
      "k = [1.72092634]\n",
      "7 Train Loss 2.3891015 Test MSE 384.7529504559354 Test RE 0.9998930781980331\n",
      "k = [1.4862584]\n",
      "8 Train Loss 2.3846388 Test MSE 383.9348322278259 Test RE 0.9988294527343602\n",
      "k = [1.2550768]\n",
      "9 Train Loss 2.3838105 Test MSE 383.49617775969836 Test RE 0.9982586967314364\n",
      "k = [1.23283711]\n",
      "10 Train Loss 2.3826656 Test MSE 383.2514354697406 Test RE 0.9979401080128683\n",
      "k = [1.25185035]\n",
      "11 Train Loss 2.3811858 Test MSE 383.33491464344553 Test RE 0.9980487869008636\n",
      "k = [1.27676651]\n",
      "12 Train Loss 2.380459 Test MSE 383.52244377693046 Test RE 0.9982928819889479\n",
      "k = [1.32073787]\n",
      "13 Train Loss 2.378859 Test MSE 383.3233882315286 Test RE 0.9980337817350542\n",
      "k = [1.25355512]\n",
      "14 Train Loss 2.377513 Test MSE 383.11318971062434 Test RE 0.9977601042105092\n",
      "k = [1.1175469]\n",
      "15 Train Loss 2.377113 Test MSE 382.85584087771537 Test RE 0.9974249349443579\n",
      "k = [1.35967637]\n",
      "16 Train Loss 2.3763087 Test MSE 382.75351518987816 Test RE 0.99729163540694\n",
      "k = [1.47865158]\n",
      "17 Train Loss 2.3721778 Test MSE 382.03580321846005 Test RE 0.9963561717579418\n",
      "k = [0.95702818]\n",
      "18 Train Loss 2.3709745 Test MSE 381.5983798689008 Test RE 0.9957856044501083\n",
      "k = [0.76324746]\n",
      "19 Train Loss 2.369748 Test MSE 380.7799147994601 Test RE 0.9947171339527037\n",
      "k = [0.97068359]\n",
      "20 Train Loss 2.3685353 Test MSE 380.73327884334844 Test RE 0.9946562181805272\n",
      "k = [0.86740971]\n",
      "21 Train Loss 2.3679824 Test MSE 380.4165704778716 Test RE 0.994242435750161\n",
      "k = [0.80998766]\n",
      "22 Train Loss 2.364716 Test MSE 380.39522575795183 Test RE 0.9942145424804384\n",
      "k = [0.90568809]\n",
      "23 Train Loss 2.3619056 Test MSE 379.56377521712585 Test RE 0.9931273936868954\n",
      "k = [0.959724]\n",
      "24 Train Loss 2.360726 Test MSE 379.4462176455363 Test RE 0.9929735872733769\n",
      "k = [0.94964257]\n",
      "25 Train Loss 2.3576581 Test MSE 378.4035106852939 Test RE 0.9916083177037877\n",
      "k = [1.04969958]\n",
      "26 Train Loss 2.353518 Test MSE 376.92651688865726 Test RE 0.9896711907302156\n",
      "k = [1.27699307]\n",
      "27 Train Loss 2.3501148 Test MSE 375.8923838009619 Test RE 0.9883126306039627\n",
      "k = [1.04877197]\n",
      "28 Train Loss 2.317322 Test MSE 371.0873623934233 Test RE 0.9819755279141018\n",
      "k = [0.76101415]\n",
      "29 Train Loss 2.3006434 Test MSE 366.7332916514596 Test RE 0.9761976341559\n",
      "k = [1.09811993]\n",
      "30 Train Loss 2.2887208 Test MSE 362.65816095675524 Test RE 0.9707587415174751\n",
      "k = [0.55764342]\n",
      "31 Train Loss 2.2264092 Test MSE 350.07600077525586 Test RE 0.9537702066104401\n",
      "k = [0.42394332]\n",
      "32 Train Loss 2.187096 Test MSE 339.7474715101549 Test RE 0.9395950046125932\n",
      "k = [0.98447685]\n",
      "33 Train Loss 2.085579 Test MSE 323.20741700185624 Test RE 0.9164383226357237\n",
      "k = [1.14366514]\n",
      "34 Train Loss 1.9287823 Test MSE 289.450026429087 Test RE 0.8672601260135079\n",
      "k = [1.64405682]\n",
      "35 Train Loss 1.7382525 Test MSE 253.8026472393494 Test RE 0.8121021426476531\n",
      "k = [1.15008745]\n",
      "36 Train Loss 1.518367 Test MSE 226.4943119033868 Test RE 0.7671693303338961\n",
      "k = [0.83131671]\n",
      "37 Train Loss 1.3953837 Test MSE 206.25205660748153 Test RE 0.7320853673071153\n",
      "k = [0.89968333]\n",
      "38 Train Loss 1.296533 Test MSE 196.4086018702797 Test RE 0.7144022843987715\n",
      "k = [0.73785859]\n",
      "39 Train Loss 1.0334367 Test MSE 157.7150751393896 Test RE 0.6401757016210493\n",
      "k = [0.76089541]\n",
      "40 Train Loss 0.94799817 Test MSE 138.06188985115375 Test RE 0.5989623023153222\n",
      "k = [0.71044153]\n",
      "41 Train Loss 0.82471853 Test MSE 123.56659364375251 Test RE 0.5666476863464377\n",
      "k = [0.83825445]\n",
      "42 Train Loss 0.6396052 Test MSE 92.3428726878859 Test RE 0.4898513090393702\n",
      "k = [0.98597815]\n",
      "43 Train Loss 0.5716917 Test MSE 78.61994042702645 Test RE 0.45199011584713\n",
      "k = [1.03075733]\n",
      "44 Train Loss 0.5139872 Test MSE 69.93079852067147 Test RE 0.42628183461466157\n",
      "k = [1.29831449]\n",
      "45 Train Loss 0.45276508 Test MSE 60.9691598271442 Test RE 0.39803172403128456\n",
      "k = [1.38622763]\n",
      "46 Train Loss 0.44538593 Test MSE 58.862223532242076 Test RE 0.3910937848703758\n",
      "k = [1.62087708]\n",
      "47 Train Loss 0.41876876 Test MSE 53.06826800993694 Test RE 0.3713471023411003\n",
      "k = [1.92748993]\n",
      "48 Train Loss 0.39301786 Test MSE 49.57231686942478 Test RE 0.35890721678282883\n",
      "k = [1.8876746]\n",
      "49 Train Loss 0.36997023 Test MSE 50.27405992326714 Test RE 0.3614386252816342\n",
      "k = [2.07718028]\n",
      "50 Train Loss 0.36472744 Test MSE 47.58484993799423 Test RE 0.351638917606745\n",
      "k = [2.33942402]\n",
      "51 Train Loss 0.31418857 Test MSE 43.10684316835974 Test RE 0.3346845713576855\n",
      "k = [2.51929851]\n",
      "52 Train Loss 0.30539718 Test MSE 40.27884810272106 Test RE 0.32351997739601995\n",
      "k = [2.60022063]\n",
      "53 Train Loss 0.28329712 Test MSE 36.77493223779041 Test RE 0.30912812742149504\n",
      "k = [2.56398769]\n",
      "54 Train Loss 0.2710243 Test MSE 35.65525510171153 Test RE 0.30438577784201115\n",
      "k = [2.85965415]\n",
      "55 Train Loss 0.2657554 Test MSE 32.200201759007676 Test RE 0.2892623292442102\n",
      "k = [3.10189719]\n",
      "56 Train Loss 0.2543568 Test MSE 30.82089155173877 Test RE 0.2829991826439921\n",
      "k = [3.05843291]\n",
      "57 Train Loss 0.24661505 Test MSE 31.733411546464826 Test RE 0.2871580295357086\n",
      "k = [3.48283484]\n",
      "58 Train Loss 0.22519968 Test MSE 28.517846373508238 Test RE 0.2722205738094501\n",
      "k = [4.18899917]\n",
      "59 Train Loss 0.20418912 Test MSE 23.69550812668497 Test RE 0.24813932448506065\n",
      "k = [4.10820493]\n",
      "60 Train Loss 0.19458286 Test MSE 23.521219851219307 Test RE 0.24722506694015872\n",
      "k = [4.09868411]\n",
      "61 Train Loss 0.17712913 Test MSE 22.85906419798108 Test RE 0.2437203573282778\n",
      "k = [4.30199005]\n",
      "62 Train Loss 0.16378435 Test MSE 21.06848758794302 Test RE 0.23398028351176653\n",
      "k = [3.96874066]\n",
      "63 Train Loss 0.15507194 Test MSE 21.190135074768282 Test RE 0.23465480145430295\n",
      "k = [4.9031312]\n",
      "64 Train Loss 0.1435552 Test MSE 17.58429838854978 Test RE 0.21375934246226183\n",
      "k = [5.5380958]\n",
      "65 Train Loss 0.13817461 Test MSE 15.046536794789793 Test RE 0.19773377526353922\n",
      "k = [5.41195737]\n",
      "66 Train Loss 0.13588479 Test MSE 14.986434744424995 Test RE 0.1973384651385882\n",
      "k = [5.02226656]\n",
      "67 Train Loss 0.1343414 Test MSE 15.724647980634044 Test RE 0.202140366359868\n",
      "k = [6.06166655]\n",
      "68 Train Loss 0.1203731 Test MSE 12.830500783167874 Test RE 0.18259311021963215\n",
      "k = [6.11936641]\n",
      "69 Train Loss 0.11933472 Test MSE 12.906071458694761 Test RE 0.18313005052440523\n",
      "k = [7.07968442]\n",
      "70 Train Loss 0.11449085 Test MSE 11.119273743396864 Test RE 0.16998116193959192\n",
      "k = [7.35485219]\n",
      "71 Train Loss 0.10154445 Test MSE 9.417338503686546 Test RE 0.15643238733496434\n",
      "k = [7.59740479]\n",
      "72 Train Loss 0.082880706 Test MSE 6.854550249317525 Test RE 0.1334602901759301\n",
      "k = [7.77827466]\n",
      "73 Train Loss 0.067294754 Test MSE 5.021140628993428 Test RE 0.11422568601812218\n",
      "k = [7.93532045]\n",
      "74 Train Loss 0.056806467 Test MSE 3.750113578842367 Test RE 0.09871537352833504\n",
      "k = [8.09102561]\n",
      "75 Train Loss 0.052752547 Test MSE 2.6444422722068466 Test RE 0.08289524662376162\n",
      "k = [8.15470115]\n",
      "76 Train Loss 0.05221629 Test MSE 2.2707883203555737 Test RE 0.07681586200229597\n",
      "k = [8.16387349]\n",
      "77 Train Loss 0.051273555 Test MSE 2.1738301363121475 Test RE 0.07515802948549949\n",
      "k = [8.27299521]\n",
      "78 Train Loss 0.04850152 Test MSE 1.551212278459224 Test RE 0.06348895800257483\n",
      "k = [8.31006808]\n",
      "79 Train Loss 0.047438264 Test MSE 1.4541963623033702 Test RE 0.061471542289923124\n",
      "k = [8.21351907]\n",
      "80 Train Loss 0.041326646 Test MSE 2.34939875153474 Test RE 0.07813416032734764\n",
      "k = [8.20623966]\n",
      "81 Train Loss 0.038102813 Test MSE 2.596629343585329 Test RE 0.08214243311860436\n",
      "k = [8.14422206]\n",
      "82 Train Loss 0.0368751 Test MSE 3.0829514213382225 Test RE 0.0895047169089615\n",
      "k = [8.100675]\n",
      "83 Train Loss 0.036297366 Test MSE 3.2850897687998857 Test RE 0.09239239038259704\n",
      "k = [8.05159173]\n",
      "84 Train Loss 0.03553649 Test MSE 3.300580276879131 Test RE 0.09260996767284198\n",
      "k = [8.0709423]\n",
      "85 Train Loss 0.034138564 Test MSE 2.6516928312934183 Test RE 0.08300881035542172\n",
      "k = [8.06020422]\n",
      "86 Train Loss 0.031611633 Test MSE 2.3258327905389438 Test RE 0.07774130508177549\n",
      "k = [8.05540855]\n",
      "87 Train Loss 0.02934391 Test MSE 1.840399073871402 Test RE 0.06915420381133115\n",
      "k = [8.02759341]\n",
      "88 Train Loss 0.028238218 Test MSE 1.5328499720658406 Test RE 0.06311206751949892\n",
      "k = [7.9905255]\n",
      "89 Train Loss 0.025153844 Test MSE 1.0669561861580352 Test RE 0.052654546350243935\n",
      "k = [7.94364692]\n",
      "90 Train Loss 0.024368146 Test MSE 1.1918152398138278 Test RE 0.05565024104199463\n",
      "k = [7.856104]\n",
      "91 Train Loss 0.021509372 Test MSE 1.1379187552757473 Test RE 0.054377371441215815\n",
      "k = [7.75830549]\n",
      "92 Train Loss 0.018315833 Test MSE 1.3663310138003046 Test RE 0.059585494196165656\n",
      "k = [7.69337803]\n",
      "93 Train Loss 0.01698914 Test MSE 1.3279755342854398 Test RE 0.05874320253635364\n",
      "k = [7.67462827]\n",
      "94 Train Loss 0.016891412 Test MSE 1.367023215882211 Test RE 0.05960058570041811\n",
      "k = [7.62210927]\n",
      "95 Train Loss 0.016579626 Test MSE 1.3761753922700342 Test RE 0.05979976488252094\n",
      "k = [7.62347163]\n",
      "96 Train Loss 0.015145758 Test MSE 0.9900317612139173 Test RE 0.050720922786376625\n",
      "k = [7.60079769]\n",
      "97 Train Loss 0.014402742 Test MSE 0.8212714009580843 Test RE 0.04619616590660185\n",
      "k = [7.53486453]\n",
      "98 Train Loss 0.012458473 Test MSE 0.8817248638354163 Test RE 0.04786621931739241\n",
      "k = [7.48377069]\n",
      "99 Train Loss 0.011835853 Test MSE 0.8674640905979938 Test RE 0.04747755389607201\n",
      "Training time: 1087.81\n",
      "Training time: 1087.81\n"
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    k = train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold,\"k\":k}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3993057779.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_7923/3993057779.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    hess_linears[0].weight. = a[1]\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = tuple([_ for _ in PINN.parameters()])\n",
    "hess_betas = a[0]\n",
    "hess_linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "hess_linears[0].weight. = a[1]\n",
    "hess_linears[0].bias = c\n",
    "hess_linears[1].weight = d\n",
    "hess_linears[1].bias = e\n",
    "hess_linears[2].weight = f\n",
    "hess_linears[2].bias = g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
