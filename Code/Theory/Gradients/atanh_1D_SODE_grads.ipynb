{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "\n",
    "   \n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def forward_grads(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "\n",
    "        \n",
    "        i =0\n",
    "        z = self.linears[i](a)\n",
    "        a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "        n = self.n.cpu().detach().numpy()\n",
    "        alpha = self.alpha.cpu().detach().numpy()\n",
    "        a_np = a.cpu().detach().numpy()\n",
    "        a1_np = n*alpha[:,i]*(1-np.square(a_np))\n",
    "        a2_np = np.square(n*alpha[:,i])*-2*a_np*(a1_np)\n",
    "        \n",
    "        W2 = self.linears[1].weight.cpu().detach().numpy()\n",
    "        W1 = np.transpose(self.linears[0].weight.cpu().detach().numpy())\n",
    "        val = (W2)*np.square(W1)*a2_np\n",
    "         \n",
    "        return val\n",
    "    \n",
    "    def grad_test(self,x_grad):\n",
    "        g = x_grad.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "    \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_grad.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_x_w1 = autograd.grad(y_x,self.linears[0].weight,torch.ones([y_x.shape[0],1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "    \n",
    "            \n",
    "        \n",
    "        y_x_w1 = np.transpose(y_x_w1.cpu().detach().numpy())\n",
    "        sd_val = self.forward_grads(x_grad)\n",
    "        \n",
    "        # print(np.mean(y_x_w1))\n",
    "        # print(sd_val/y_x_w1)\n",
    "        return np.mean(np.abs(sd_val)/np.abs(y_x_w1+0.001))\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    x_grad = torch.zeros((1,1)).float().to(device)\n",
    "    v = PINN.grad_test(x_grad)\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    v = np.zeros((max_iter,1))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        v[i] = train_step(x_coll,f_hat)\n",
    "       \n",
    "        \n",
    "        # print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.4657817 Test MSE 386.70819081163086 Test RE 1.0024304905993275\n",
      "1 Train Loss 4.318592 Test MSE 388.00288930225724 Test RE 1.004107156354785\n",
      "2 Train Loss 3.4866958 Test MSE 385.8633607561593 Test RE 1.0013349016340625\n",
      "3 Train Loss 3.171901 Test MSE 387.296678013997 Test RE 1.0031929430990532\n",
      "4 Train Loss 2.9647577 Test MSE 391.4208046668215 Test RE 1.0085200466507074\n",
      "5 Train Loss 2.836018 Test MSE 389.6870836367458 Test RE 1.0062840478951627\n",
      "6 Train Loss 2.6103554 Test MSE 386.7986842775983 Test RE 1.0025477729653436\n",
      "7 Train Loss 2.5262527 Test MSE 390.4136084306381 Test RE 1.0072216589951026\n",
      "8 Train Loss 2.5160422 Test MSE 391.30038110322295 Test RE 1.0083648953158946\n",
      "9 Train Loss 2.509689 Test MSE 391.7401298916762 Test RE 1.0089313434334761\n",
      "10 Train Loss 2.472879 Test MSE 389.69819352433507 Test RE 1.0062983922543203\n",
      "11 Train Loss 2.4617438 Test MSE 388.6429204664689 Test RE 1.004934978882705\n",
      "12 Train Loss 2.4397397 Test MSE 386.089329454111 Test RE 1.0016280588104365\n",
      "13 Train Loss 2.419394 Test MSE 385.14342214334005 Test RE 1.00040032697803\n",
      "14 Train Loss 2.3941233 Test MSE 384.50466496469727 Test RE 0.9995704048984189\n",
      "15 Train Loss 2.3790832 Test MSE 382.7518414891313 Test RE 0.9972894549311071\n",
      "16 Train Loss 2.3750923 Test MSE 381.7549880132722 Test RE 0.9959899189164654\n",
      "17 Train Loss 2.3516142 Test MSE 375.6439971576874 Test RE 0.9879860421517761\n",
      "18 Train Loss 2.3314953 Test MSE 373.24823091479726 Test RE 0.9848304350863518\n",
      "19 Train Loss 2.256263 Test MSE 358.6979235601404 Test RE 0.9654438354708813\n",
      "20 Train Loss 2.1861258 Test MSE 341.6457601068744 Test RE 0.9422162720784072\n",
      "21 Train Loss 2.1015346 Test MSE 328.27510525396843 Test RE 0.9235949660002533\n",
      "22 Train Loss 2.0037255 Test MSE 308.4058360783997 Test RE 0.8952078388446709\n",
      "23 Train Loss 1.8455237 Test MSE 262.17962745381607 Test RE 0.8253954177683556\n",
      "24 Train Loss 1.6376741 Test MSE 244.63800690679886 Test RE 0.7973051098408822\n",
      "25 Train Loss 1.5970615 Test MSE 235.29564947446426 Test RE 0.7819329803642938\n",
      "26 Train Loss 1.4955043 Test MSE 220.80434632580324 Test RE 0.7574716645854231\n",
      "27 Train Loss 1.3673723 Test MSE 198.69387919691027 Test RE 0.7185464149962831\n",
      "28 Train Loss 1.2441365 Test MSE 163.25544354828398 Test RE 0.6513230053290143\n",
      "29 Train Loss 0.99495167 Test MSE 133.23181920668165 Test RE 0.5883917320945882\n",
      "30 Train Loss 0.89795643 Test MSE 115.83798060716065 Test RE 0.5486407626618438\n",
      "31 Train Loss 0.8560017 Test MSE 110.64488379223233 Test RE 0.5362017795981935\n",
      "32 Train Loss 0.7983305 Test MSE 112.61348368963937 Test RE 0.5409508148285099\n",
      "33 Train Loss 0.64416534 Test MSE 90.24087022471471 Test RE 0.4842439682953232\n",
      "34 Train Loss 0.4168452 Test MSE 48.80075030083915 Test RE 0.3561031637714798\n",
      "35 Train Loss 0.40676987 Test MSE 45.247246157539124 Test RE 0.34289303109046465\n",
      "36 Train Loss 0.36070818 Test MSE 35.30524725267639 Test RE 0.3028881001933911\n",
      "37 Train Loss 0.34549236 Test MSE 38.325554328436425 Test RE 0.31557806219651924\n",
      "38 Train Loss 0.3199374 Test MSE 38.451570575047704 Test RE 0.3160964542654792\n",
      "39 Train Loss 0.24081431 Test MSE 28.003229883032855 Test RE 0.2697532248734786\n",
      "40 Train Loss 0.20504479 Test MSE 16.786185072100558 Test RE 0.20885197477317458\n",
      "41 Train Loss 0.14785288 Test MSE 10.695107967851424 Test RE 0.16670751166097592\n",
      "42 Train Loss 0.09315594 Test MSE 8.948145518392687 Test RE 0.15248569432649461\n",
      "43 Train Loss 0.063491285 Test MSE 4.926045070838532 Test RE 0.11313885339663714\n",
      "44 Train Loss 0.04862701 Test MSE 2.89271281385237 Test RE 0.08669923036352821\n",
      "45 Train Loss 0.04528815 Test MSE 2.223858394900522 Test RE 0.07601794891751373\n",
      "46 Train Loss 0.044177216 Test MSE 1.8931830753014325 Test RE 0.07013889009908605\n",
      "47 Train Loss 0.04092338 Test MSE 2.192986993492978 Test RE 0.07548846779243613\n",
      "48 Train Loss 0.030706212 Test MSE 2.66393048937651 Test RE 0.0832001341951864\n",
      "49 Train Loss 0.017765772 Test MSE 0.8266522371565652 Test RE 0.04634725370519352\n",
      "50 Train Loss 0.009484822 Test MSE 0.09361542842499705 Test RE 0.015596828512825152\n",
      "51 Train Loss 0.00419836 Test MSE 0.0010343493043779859 Test RE 0.001639442524838497\n",
      "52 Train Loss 0.0034007789 Test MSE 0.003043897799159921 Test RE 0.0028124034100142077\n",
      "53 Train Loss 0.0031122002 Test MSE 0.004775929849420371 Test RE 0.0035228289129595244\n",
      "54 Train Loss 0.0025339662 Test MSE 0.0017198902923224063 Test RE 0.002114038806119365\n",
      "55 Train Loss 0.001919858 Test MSE 0.0012017306899173776 Test RE 0.0017671204741373252\n",
      "56 Train Loss 0.0019129333 Test MSE 0.001435007248443461 Test RE 0.001931032707786973\n",
      "57 Train Loss 0.0019080626 Test MSE 0.0016701302203742069 Test RE 0.002083232536036757\n",
      "58 Train Loss 0.0019053947 Test MSE 0.001870017690337511 Test RE 0.0022043748122236293\n",
      "59 Train Loss 0.0019025707 Test MSE 0.0021173492664199006 Test RE 0.0023456263619412963\n",
      "60 Train Loss 0.0018997208 Test MSE 0.0024578013174044134 Test RE 0.0025271787951377258\n",
      "61 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "62 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "63 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "64 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "65 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "66 Train Loss 0.0018977582 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "67 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "68 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "69 Train Loss 0.0018977582 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "70 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "71 Train Loss 0.0018977585 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "72 Train Loss 0.0018977585 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "73 Train Loss 0.0018977583 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "74 Train Loss 0.0018977582 Test MSE 0.0027518924087211096 Test RE 0.002674104086183143\n",
      "Training time: 8.97\n",
      "Training time: 8.97\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4946/129217413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_loss_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test_mse_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_mse_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_re_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Thresh Time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Thresh epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 75\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    v = train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold,\"k\":k}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "plt.plot(k_mean[:,layer_num,0],'b')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,0] - 0.5*k_std[:,layer_num,0],k_mean[:,layer_num,0] + 0.5*k_std[:,layer_num,0],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,1],'r')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,1] - 0.5*k_std[:,layer_num,1],k_mean[:,layer_num,1] + 0.5*k_std[:,layer_num,1],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,2],'g')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,2] - 0.5*k_std[:,layer_num,2],k_mean[:,layer_num,2] + 0.5*k_std[:,layer_num,2],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"v\":v}\n",
    "savemat('v_atanh.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGdCAYAAAA1/PiZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJ/UlEQVR4nO3deZxcZZU//s+trav3Nb0vSTqdpROydyAhYREnEAFlEAQUBBU10kqYqKPIbwZ0xgnqiOiXBgd0UIQZ0GERJBISCEkgLElIyNLZu5P0vu/dtT+/P6rurep9qVu3qm5/3q9Xv0iqqrue23RyT85znnMkIYQAERERURQwhHsBRERERBPFwIWIiIiiBgMXIiIiihoMXIiIiChqMHAhIiKiqMHAhYiIiKIGAxciIiKKGgxciIiIKGqYwr0AtXk8HtTX1yMxMRGSJIV7OURERDQBQgj09PQgNzcXBsPoeRXdBS719fUoKCgI9zKIiIhoCmpqapCfnz/q87oLXBITEwF4LzwpKSnMqyEiIqKJ6O7uRkFBgXIfH43uAhd5eygpKYmBCxERUZQZr8yDxblEREQUNRi4EBERUdRg4EJERERRQzeBS0VFBUpLS1FWVhbupRAREVGISEIIEe5FqKm7uxvJycno6upicS4REVGUmOj9WzcZFyIiItI/Bi5EREQUNRi4EBERUdRg4EJERERRg4ELERERRQ0GLkRERBQ1GLgQERFR1GDgQkRh4XJ78NTuKlTWd4d7KUQURRi4EFFY7Dndip9uPY5//evRcC+FiKIIAxciCovGbhsA4GRTDyKlgffJxh78bk8VnG5PuJdCRKMwhXsBRDQ9tfc5AAA9Nhdaeu3ITLSGeUXAj187hr1n21CYFof1C7PDvRwiGgEzLkQUFh2+wAUAzjb3hXElfudaves439Yf5pUQ0WgYuBBRWHT0O5VfV7X2hnElXk63R9m+auiyhXk1RDQa3QQuFRUVKC0tRVlZWbiXQkQT0NEfWRmXpm4bPL5Sm4augfAuhohGpZvApby8HJWVldi3b1+4l0JEE9AesFUUCRmXug5/sFLPjAtRxNJN4EJE0WVQxqUl/IFLfUCWpZEZF6KIxcCFiMIisDi3tmMANqc7jKsZnHFp7rHzSDRRhGLgQkSac7o96La5AABmowQhgHNt4a1zqev0bw8J4Q1eiCjyMHAhIs11+k4USRJQmpMEIPwFuvWdg7eHGjq5XUQUiRi4EJHmOn31LSmxZszJTAQAVIWgzuWe5w5gw6/3TGgbqs4XqJgMEgAeiSaKVAxciEhz8omi1HgLijPjAahfoFvT3o+tRxpxvKEbx+q7xnytEELJuCzM9WaAeCSaKDIxcCEizcknilLjLJidkQAAONui7lbR7tMtyq+rW8fuhNvZ70S/w5uVWV6UCgCo72TGhSgSMXAhIs2193lrXFLjLJjjy7hUtfSqOmzxnZOBgcvY2Rx5mygjIQYz073raeRWEVFEYuBCRJqTMy5p8WYUpsXDaJDQ53CjqVudkzwOlwd7z7Qqvz83TsZFDlzyUmORnewd9sitIqLIxMCFiDQn93BJjbPAYjKgMC0OgHoFugfOd6DP4S/IrWodextKrm/JS7EiNzkWAItziSIVAxci0lx7v784FwCKZ6hboPvOqWYAwLLCFADeqc9jbUPJzedyk/0Zl5ZeOxwuNqEjijQMXIhIc3Ifl7Q4b+Aye4a6Bbq7fPUtt19cBKNBwoBz7G0oud1/Xmos0uMtsBgNviZ0zLoQRRoGLkSkucDj0IC6GZembhtONPZAkoAr52ciP9W79VM9xnaRknFJiYXBIAXUuTBwIYo0DFyISHP+49BmAECxL+NSpULGZdcpb7ZlcX4K0uItmJXhDYrGGikgt/vPS/EGOQxciCIXAxci0tzQjIu8VVTXOYABR3DDFuVtoivmzgAA5XjzaBkXm9ON1l7vNpIcuOTKgQvb/hNFHAYuRKQpp9uDHt+ARbnGJS3eomRfqsbpuTIWl9uDPb7Gc5fP8wYus2eMHbjIWZU4ixEpvjVk82QRUcRi4EJEmgocsJgUa1YeV6NA95PaTnTbXEiONWNJfgqA8TMu8lHo3JRYSJLk+zV7uRBFKt0ELhUVFSgtLUVZWVm4l0JEY+gIGLBo9A00BPwFusH0cpG75a4ryVC+tlzjcqGtH27P8CPRcmGuvE0EANlJrHEhilS6CVzKy8tRWVmJffv2hXspRDSGjiH1LbJiFTIucmHuFfMylcdyU2JhMRrgcHuU7EqguoCMS+DnAAxciCKRbgIXIooOgQMWA81WThZNLePS2mvH4VrvFOjLSjKUx40GCYXp3s68I20XyYGLfGwaAHJ8xbmtbEJHFHEYuBCRpgIHLAbybxX1wTPCls545KLc0pwkZPq2emRjHYn217j4Pyct3juKQAhvXxgiihwMXIhIU4EDFgMVpMXB5Oty2zCFYEE5Bu07TRRIDlxG6hOjbBUl+zMukiQpWRduFxFFFgYuRKSpwAGLgcxGA4rSxx62eKGtf8Q6FY9HYPdp7zToy+cOD1zkk0VDMy4ej0CD3HwuYKsICCzQ5ckiokjCwIWINDV0wGIgpUC3eXjgcrSuC59+ZBfW/uxtlD/3MY746lkA4Gh9F9r7HEiIMWF5Ueqwz5UzLkNrXFp77XC4PTBIQNaQ7SUW6BJFJlO4F0BE04uccUmLGx64eAt0m1A1JMBwuDz43l8+gcPtLZR9/UgDXj/SgEvnpGPj5cU4eKETAHDpnHSYjcP/PSYHLrUdA3C6Pcpr5G2i7CTrsM/LYfdcoojEwIWINNXha0A3csZl5GGLj719Gicae5AWb8FjX1yGP++rwWuHG/DemTa8d6ZN6dkSeAw6UFZSDGLNRgw43ahp7x80YgAYfBRaxhoXosjErSIi0tTQAYuBijOHD1s8UtuFinfOAgD+7XOLsKY4A4/eugy7vn8F7lozE7Fmo9JY7rIR6lsAb7HtzBG2i+R6maH1LQCQw7b/RBGJGRci0tTQAYuBijO8gUtDlw29dhfMRgnf+8sncHsErr0oB9cuzlFem58ah4c+uxCbrirBXw7UIDPROqj77VCzM+JxvKF7UOAid80dKePCCdFEkYmBCxFpZqQBi4GS48zISLCgtdeB6pY+bDvWiJNNPUiPt+Ann1s44tdMjbfgG5cVj/veMzOGN6Grk08UjRC4yMFMa68ddpcbMSbjuO9BRKHHrSIi0sxoAxYDzfZlXV45VIcndvm2iG5YhPSEmKDee6Qj0cpW0QiBS2qcGTEm71+Rzd32oN6biNTDwIWINDPagMVAxZneAOP371Z7t4gW5+AzF+WM+NrJmO0r/K1uCcy4jF7jEtiEbqTeMUQUHgxciEgzY9W3yOReLgCQkWDBv31ukSrvLWdc6rtssDnd6LW70DXgzQDJAcpQLNAlijyscSEizXSOMmAxkJwZAYB/v2ER0sYIciYjLd6CJKsJ3TYXzrf1Q/IlfJKsJiRaR9624pFoosjDwIWINDPagMVAl8xOx8Wz0rCsMBXXLAp+i0gmSRJmZcTjk9ouVLf2IsbsLbbNS40b9XNyUtj2nyjSMHAhIs2MNmAxUJzFhBe+uTok7z9TCVz6kWj1/vWXlzLyNhEAZHOriCjiMHAhIs2MNmBRK/6ZRb3I8J1SGqv3S24yMy5EkYaBCxFpZqwBi1qQA5dzrf2wu7xzj0ZqPieTi3MbmXEhihg8VURhI4RAdWsfbE53uJdCGhlrwKIWlIxLW9+Y7f5lcnFua68Ddhd/TokiATMupCmPR+BgTQdeP9yIN442oL7LhusW5+CxLy4P99JIA+1jDFjUgjyvqKXHDqd7/IxLSpwZVrMBNqcHjV02FKXHj/paItIGAxfSxKGaTrxysA5vHG1EY/fgtHtlfXeYVkVa6xxjwKIWkqz+kQJyF9/8MQIXbxO6WFS39qGBgQtRRGDgQiH35rFGfPPZAxDeAb5IiDHhqgWZmJ+dhJ+9cQK9dld4F0iamUgDulCbmR6P1l7vOsxGSSnSHU1OstUXuLBAlygSMHChkHK5PXj4jRMQArhs7gzccUkR1pVkwGo24lxrH372xgn0MXCZFsYbsKiVmRnx2H++A4C3+NYwyugBGadEE0UWBi4UUn89VI+qlj6kxJlR8cVlgzqUxsd4f/z6HG54PGLcGwhFt4kMWNSCXKALjH0UWpYr93LpZOBCFAl0c6qooqICpaWlKCsrC/dSyMfp9uDXb50GAGy8vHhYW3W5ARgA9DmYddG7iQxY1EJg4DJWYa7M3z2XgQtRJNBN4FJeXo7Kykrs27cv3Eshn/87UIsL7f3ISIjBl1cXDXs+xmRQbmB9dh411btIqG8BhmRcxjgKLcthEzqiiKKbwIUii83pxm982ZbyK4sRZxm+KylJEuIt3nkxLNDVv4kMWNTCzPTAraLR2/3L2ISOKLKwxoVC4vmPLqChy4acZCtuW1U46usSYrzTehm46N9EBixqIdZiRG6yFfVdNuSPMWBRJmdc2vocsDndsPqGM1LwjtV3oaXHDrdHwOkWcHsEXB4PXG4BAUDeUJQk3wckSBJgNEgwGSQYJAlGQ8DH0N8bJJiNBljNRsSYvP+1mg2wmoysqYtiDFxIdQMONx7beRYA8O1PzRnzL/oEqwnoAk8WTQMTGbColfs/swDvV7Vh1ay0cV+bHGtGrNmIAacbjV02pYkdBeet40342h/3h+39zUbJFwAZlEBIDnYY0ozve1fPw43L88Py3gxcSHXPvH8Orb12FKTF4uYVBWO+Vj5ZxIyL/kVKjQsAXL8kF9cvyZ3QayVJQk6KFVUt3iZ0DFzU8eTuKgDek10ZCRaYjAZfhsSbSTFIEgS8o0EAQAhAQMDjAdzCm51xewQ8QsDl9v5XfizweafbA5vTA7vLDadbKO/vdAvf7z1huPro1+cIX10iAxdSVa/dhd/u8mZbNl01FxbT2GVUCfKRaAYuutcRITUuU5GTLAcuLNBVw4nGbnxY3Q6jQcL/fWu1UkcUai63B3aX/OFWghtXQJDjYRwzIbkTqA8LFQYupKqn361GR78Ts2fE44al4/+LNt7CjMt0Ee4Bi8HIT4kD0Ibzbf3hXsqUffNP+3G0rhtb712H5DCNXJA98/55AMDVC7M0C1oAwGQ0wGQ0IH7sZskU4XiqiFTT1e/Ek3u86d9/+vRcmIzj/3glWBm4TBfhHrAYjDmZCQCAMy29YV7J1PTaXdh2rAl1nQN451RzWNfS1e/Eyx/XAQC+vHpmWNdC0YmBC6nm5YO16LG5MD87EddelDOhz+FW0fQR7gGLwVACl6boDFyON/gHme4+1RrGlQB/OVCDAacb87IScfEEiqOJhmLgQqqpau0DAFy1IHPCRw3jY7wnjtiATv8iqTh3suTApbq1Dy539BVBHK3rUn797pkWpeBVax6PwJ8+8G4T3blmJiSJ53do8hi4kGpq2r37/xPpjSGTTxXJw/dInyJlwOJU5aXEwmo2wOH2oKYj+gp0j9b5My5N3Xacbg5P5mjXqRacb+tHotWEG5ZN7FQX0VAMXEg1tb6/0PMn0EZdlsitomlBPlEU7gGLU2UwSCie4dsuCtNNPxjH6r0Zl1hfT6U9p8OzXfTH988BAL6wsmDEbtpEE8HAhVQhhFACl4IpZFw4ZFHf5MnQ4R6wGAx5u+h0c0+YVzI5NqdbybDcusrbV+nd0y2ar+Ncax/eOdkCSQLuuGT47DKiiWLgQqpo63NgwOmGJPmn6U4EG9BND9Fc3yKbE6UZl5ONPXB7BNLiLfjCSm/g8kFVO+wubevK5CPQV8ydwSZ+FBQGLqQKOduSlWhFjGnis1zkU0W9rHHRNbmHSzQ2n5PJGZezURa4HKv31rcszE3C/OxEZCTEYMDpxsfnOzVbQ5/dhb8cqAHgLcolCgYDF1JFbYe3MLcgbXLNpHgcenro6I+MAYvBUAKXlr6wncqZiqO++paFucmQJAnrSjIAeE8XaeWVQ3XosbkwMz0Ol5XM0Ox9SZ8YuJAqatrlwtyJ17cA3CqaLiJpwOJUFaXHw2SQ0Gt3obHbFu7lTNgx31HoRXlJAIC1c7yBi1YFukII/HHvOQDAHatnciozBY2BC6lCzrhM5kQREJBxcbij6l+xNDl6qHGxmAwoSvcG5qejpBGd0+3B8UZvMfGi3GQAwFpfxuVIXZeyhRdK755pxammXsRZjLhpRXimCZO+MHAhVUzlRBHgb0Dn9gjYXdHX2IsmJpoHLAZSOuhGSZ3L2ZZeOFweJMSYUJjm/bOZlWTFvKxECAHsPdsW0vfvGnDi/peOAABuWpGP5Cg8Ck+Rh4ELqaJmihmX+IBeDmxCp1/RPGAxULTNLJIbz5XmJg3aopGzLntCeCxaCIEfvXwEtR0DKEiLxfeunhey96LphYELBU0IgTo545I2uYyLwSAh3iK3/WfgolfRPGAxULRlXORW//I2kWxdib/OJVRbtC/sq8HrhxtgMkj4za3LkGRltoXUwcCFgtbSa4fd5YFBArKTJ97DRcYCXf3zH4eO7ptXSWYigOg5El3pOwotF+bKLp6VDovRgLrOAVT7Zoyp6XRTDx567RgA4Lvr52FZYarq70HTFwMXCpp8oignORZm4+R/pHgkWv+UGpcoz7jMnuFtnNbW51AKjiOVxyOUVv8Lh2RcYi1GrCjyBhPvnlH3dJHN6cZ3/vcgbE4P1pVk4JuXzVb16xMxcKGgTfVEkSzByoyLnkX7gMVAcRYT8lK8P+eRvl10rq0PfQ43YkwGFM8Y3ql23dzQHIv+6evHcaKxBxkJFvzyC0t4/JlUx8CFguYfrji5+haZXKDLwEWfon3A4lDRUucid8xdkJME0wiZ0HVzvI3g3j/bBqdbnRN9bxxtxJ8+8Lb2/+UXliIzcfJbx0TjYeBCQQs246IMWrRrOzuFtKGHAYuBoiVw8XfMTRrx+YW5SUiNM6PX7sInNZ1BvVdzjw1/P9KAH7x4GADwjctm4/K57JBLocG54hS02imeKJIlxPBUkZ7poflcoGiZEn2sTi7MTR7xeYNBwqVzMvC3ww3Yc7oVK2emKc/12Jx4+0QzKhu6kRhjQnKcBcmxZqTEmpHiK7D+pLYLH5/vwIHzHbjQ3q987uL8ZHxvPY8+U+gwcJlGKuu7cay+CzetyIckqfcvX/9WUXA1Lj0MXHRJDwMWA5VEwbBFIfyFuUOPQgdaVyIHLi24c81M7Khswt+PNuC9M21wTGL7SJKAuZmJKJuVins/VQKLicl8Ch0GLtPID148jCN1XZiVET/oX1fB8Hj8PVyC3ypi4KJH8lyfNJ1lXOq7bOizu5Sf30hS32VDR78TJoOEudkJo75urW/g4cGaTpT9dAfcHn9Pl+IZ8bh0TgYcLg86+53oHHCga8CFrn4H7C4PSnOTsLwwFSuKUrG0MIV9WkgzkfcnjkKmocsbYByt61ItcGnuscPh9sBkkJCdNLVCvAQLAxc9k9vKLy1ICe9CVJISZ0FGggWtvQ6cbenF4vyUcC9pGLnxXElWImJMxlFfl5cSi3lZiTjZ1AO3EFiYm4RrFmZjw0XZmOPrWUMUaRi4TBNCCKVI8rSKKW651X9OinXEkwsTwQZ0+uV0e/C+L3C5rEQ/xZrFMxLQ2tuOM82RGbjIJ4oWjVKYG6jiS8ux/1w71hRnoDB9anVqRFpi4DJN9DnccPnSwGpOtlVOFKVM/S+8BAYuunWophO9dhdS48yjnm6JRnMyE/Bhdbuq/whQ0zG51f8ohbmB5mQmKNtfRNGAFVTTRGe/v8vnqeYe1eaT1LbLJ4qmVt8C+ItzuVWkP7tPeYf4rS2ZoatGZCURfiRaPgo9tNU/kR4wcJkm5G0i+dctvXZVvq5/KvTUMy7+rSL2cdGb3b6urJf5hvrpxZwInlnU0mNHU7cdkgTMz2bgQvrDwGWaCAxcAPW2i4I9Cg2wj4tedfQ5cLi2EwCwTkf1LYD/ZNH59n44XOp0nVWLfAx6dkZ8RJ54IgoWA5dponNg8EC4003qNM8KtvkcwOJcvXrvbCuEAOZlJU5pangky0qKQUKMCW6PwLk29acrB0MpzJ1AfQtRNGLgMk0MzbicUiHF7fYI1HeqkXFh4KJHcn3LOp1tEwGAJEkoljvoqljsrgb5KLSeiqGJAjFwmSa6BryBS7zFuy2jRsalsdsGl0fAbJSCGqYmBy4Ol0e1YW8UXkIIZerwZTqdWTNnRuQV6Lo9Aodrx++YSxTNGLhME3Lb9eVFqQCAU029QZ8sqvXNJ8lLiQ1qeF7gPjzrXPThTHMvGrpssJgMWDVLnWaHkaYkyxe4tERG4GJzuvGtZw+grnMAMSYDFnKriHQqIgOXv/3tb5g3bx5KSkrwu9/9LtzL0YVOX8ZleWEqDJI3A9PSE9zJohqlMDe4plVmo0GZbdJjY+CiB/JpootnpcFqHr1zazSLpIxLR58DX3zqA7xZ2QSLyYBf37oUybFswU/6FHGBi8vlwubNm/H222/j448/xs9+9jO0t7eHe1lRT65xyUqyoig9HoA36xIMpflcEPUtskR5XpGDgYseyPUteuqWO5R8suhsS++gGT9aq2nvx+d/uxcfX+hEktWE5+6+GNcsygnbeohCLeICl48++ggLFy5EXl4eEhMT8ZnPfAbbtm0L97KiXpfvVFFKnFlpnnW6Obg6FzVOFMk4aFE/bE43Pqz2tvlfN1d/hbmygrQ4WEwGOFweJYjX2tG6Ltz4xF5UtfQhN9mKF7+1BmUqzSEjilSqBy67d+/G9ddfj9zcXEiShFdeeWXYax5//HHMmjULVqsVK1aswJ49e5Tn6uvrkZeXp/w+Pz8fdXV1ai9z2unwZVxSYs2Ym+VtnhVsxqWmXb2MC5vQ6cf+cx2wOT3ITIzBvCz9DuozGiTMzvBmL8OxXbTndAtu+a/30dJjx/zsRLx0z6Uo0fH3m0imeuDS19eHJUuW4LHHHhvx+RdeeAH33XcfHnjgARw8eBDr1q3Dhg0bcOHCBQAYsWBUkvTTKjxc5K2ilDiLUlQY7MkiNZrPyeQmdL2scYl6e07Lx6Bn6P7Prrxd9InvJI9WDpxvx9f+sB99DjdWz07Hnzeu1l2vHKLRqB64bNiwAf/+7/+OG2+8ccTnH3nkEXzta1/D3XffjQULFuDRRx9FQUEBnnjiCQBAXl7eoAxLbW0tcnJG36+12+3o7u4e9EGDCSGGbBXJGZepzyxyuT1o7LYBAAqCLM4F/Eeip8NWUbfNqdqsqEi0S65v0fE2kexy31Hv3+2pUjKQoVbXOYBv/ukAHG4PPr0gC3/4ahmSrCzEpelD0xoXh8OBAwcOYP369YMeX79+Pfbu3QsAWLVqFY4ePYq6ujr09PRg69atuPrqq0f9mlu2bEFycrLyUVBQENJriEb9Djecbu+NMiXOjNkz4mGQgG6bC81TPFnU0GWD2yNgMRmQkRAT9BqnS/fcY/VdWP6T7Xjo1WPhXkpINHfbcKLRm8lbO0f/gcvnl+dj1aw09Dvc+OFLh0MekPY7XPjGM/vR2uvAgpwk/Oa2pYgx6fPUFtFoNA1cWltb4Xa7kZWVNejxrKwsNDY2AgBMJhN++ctf4sorr8SyZcvw/e9/H+np6aN+zfvvvx9dXV3KR01NTUivIRp1+CZDW4wGxJqNsJqNmOk7WTTVrp/KcMWUWFWm/k6XjMu+6na4PAJ7zrSGeykh8a7vuhblJSFdhYA20hkMEn7++cWwmg1470wbnt8Xur9/hBD4/l8O41h9N9LjLXjqyysQZ+EsIpp+wnKqaOi+txBi0GOf/exncerUKZw5cwbf+MY3xvxaMTExSEpKGvRBg/nrW8zK91muczk1xToXpb5FhRNFQEDGRefHoeXv24W2fl12CZ4Ox6CHmpkRj++tnwcA+Onrx5UxGGr7f2+fwetHGmA2SvjtHSuC7p9EFK00DVwyMjJgNBqV7Iqsubl5WBaG1CO3+0+J8++DyyeLpnokulbFE0VAwLwinRfnyoGLyyM0q4nQiscjlIyL3qZBj+crl87CssIU9NpdeODlI6pvGb1xtAGPbD8FAPj3GxbxyDNNa5oGLhaLBStWrMD27dsHPb59+3asWbNGy6VMK0rGJdaiPFYS5JFoNU8UAdNnq6i20x+sVLVE1lThYFU2dKO114E4ixErfKMlpgujQcIvbloMi9GAnSdb8PJB9Vo4VNZ3459e+AQA8JVLZ+KWskLVvjZRNFJ9g7S3txdnzpxRfl9dXY1Dhw4hLS0NhYWF2Lx5M+644w6sXLkSq1evxpNPPokLFy5g48aNai+FfOQal+SAjIvchE4+WTTZY6tK8zmV0tXTpY+L/H0DgKrWXgD6yDR6PAL/tbsKALB6droywmE6mZOZiE2fLsEvtp3Ej1+rxNqSjAkPH/V4BGo7BlDb0Y/azgHUKx82HKnrwoDTjXUlGXjgMwtCfBVEkU/1wGX//v248sorld9v3rwZAHDnnXfiD3/4A2655Ra0tbXhJz/5CRoaGrBo0SJs3boVRUVFai+FfOStotSAwGX2jHgYDRJ6bC40ddsn3QOiRsV2/wAQ7+vjoueMS6/dpWS/AKC6VR8ZFyEE/vXVo3jtk3oYDRK+cumscC8pbL5x2Wz8/WgDjtZ1419eOYrf3r4CkiRBCAG7y4NeuwtdA05UtfThdHMPTjf14lRTD8629MLmHL3mqXhGPB67bTlMxukXEBINpXrgcsUVV4y7v3vPPffgnnvuUfV9KyoqUFFRAbdb3/9in4rOfrmHi3+rKMZkRFF6nPIX6GQCF4fL38NFrQLBRKv+j0PXdQwu2jwboq0iIQQ8AkFN7J7Me235+wk8+8EFSBLwyBeWYG2J/o9Bj8ZsNOAXNy3B9f/vXWw71oS1P9uJfocLvXaX0pJgNBaTAQWpschLjUNeihW5ybHITYlFTooVywtTdTuskmiydHOWrry8HOXl5eju7kZyMse5B5L/lT90WuzczERUtfThVFPvpIopG7oGIARgNRuQkWAZ/xMmIN6i/xoXeZ6NPN8mVDUutz31Aapb+/DLm5eGPIj4zVtn8KRvi2jLP16Ezy3NG+cz9G9BThK+86kS/GrHKdSNcMIo3mJEYXo85mYloCQzASVZiZiblYjCtDhNgk2iaKebwIVG19E//FQRAMzNSsAbxybf+r+mXS7MjVOtpft0aEAn17esmpmGd8+0orXXjm6bU9Wupw1dA/igyjtN/Y7//hDf+VQJNl1VEpIb4lO7q/CrHd6TLv9yXSluXcWiUdm9V83B2pIMSJJ38nmC1YSEGBPiLSZV+h4RTWcMXKYBud1/atzg7MicLH/r/8moVbm+BZgep4rk79v87EScbOpBS48dVS19WFqQotp7HLrQCQCIMRlgd3nwm7dOY/+5djx669IRC0VtTjd2n2pBXecALpmdjvnZiRMKRp/94Dx+uvU4AOB76+fia2unb13LSCRJmnYnq4i0wsBlGugMmAwdaK4ybLF3UieL5MJctU4UAUCCr8alz+GGxyN0+a/SwCPkszPifYFLr6qBy8GaTgDATSvyUTYzDT96+Qj2nm3DZ379Ln5z21KsKc5QgpXXjzTgrePNg7JceSmxuGpBJq5akIVLZqchxmSEEAL1XTYcrevCsbouHKnrwju+RnMbLy9G+ZVzVFs/EdF4GLhMA52+U0XJQ7aKZmX4ThbZJ3ey6FyrN3ApSlcxcInx/yj2OVxI1OHQOH/gEofZMxLwYXW76ieLDl7oAAAsK0zFDcvysCgvGeXPfYyTTT24/XcfYl3JDBw43zEoWMlJtmJOZgI+qm5HXecAnnn/PJ55/zziLEaU5iShqrUP7X2OYe915+oi/OCaebqfAE1EkYWBi84JIUY8VQR4TxbNTI/D2ZY+nGqa+MmiKt/NdvaMeNXWGWMywGiQ4PYI9NndOg1cfFtsabEo9n3v1CzQdbo9OFLXBQBYVpgCAJiTmYBXyi/Fg68exZ/31yqTm3OTrdhwUQ4+c1EOlhWkwGCQMOBwY+/ZVuw43oy3TzShqduO/ee9gZDJIKEkKxGLcpOwKC8ZSwtSsDg/mUELEWlON4ELj0OPLHAydGrc8GBgblaiErhcNnf8k0Uej0B1q7fb7qyMBNXWKUkS4i1GdNtcuizQ7bW7lCLpvJRYJeg72zK1zsUjOdnYA5vTg+RYM2al+4PKWIsRP79pCS6bOwOV9d34dGkWluanDNuOi7UYcdWCLFy1IAtCLMKx+m6caurBnMwEzM1K5HFcIooIuglceBx6ZPI2kTwZeqiSzAT8HROfEt3QbYPN6YHZKKFAxeJcAEi0mnUbuMg9XFLizEi0mjHbF/Sda+tTraZH3iZaUjA8KAGA6xbn4rrFuRP6WpIkYVFeMhbl8c8SEUUWtmHUuc6Adv8jpfWVmUUTHLZY7dvaKEyLU72Lp56758rbRHkp3mAvPzUWZqMEm9OD+i51pgkf9J0oWqZisS8RUaRh4KJzXaOcKJLJU6LP+E4WjacqBNtEMj33cpEbkclHyE1GAwrTvMXNatW5yCeK5PoWIiI9YuCic3JdxdAeLrJZGfEw+U4WyW38xyLfZItVLMyV6bmXS+CJItnsGd7gT42TRR19DuXrqHm8mogo0jBw0bnOgeGToQNZTAbMzPAGIacmUOcSihNFsgQdZ1xGato3WzlZFHyB7qHaTu/XzIgfdnqMiEhPGLjo3GjN5wLNy/ZuFx1v6B7368k3WW4VTc5IGZdi3/ewSoWMi1zfspTbRESkcwxcdK5rYOQ5RYFKc5IAAJX1YwcuNqdbqdUIZcZF31tFI2Vc1Ahc/I3niIj0jIGLznX0jdx8LlBprjdwOVbfNebXOt/WDyGAJKsJ6fHqb0f4TxXpqxdPn92ldJ7NGxS4eDMudZ0DGHBM/Zo9HoFDcmEu61uISOd0E7hUVFSgtLQUZWVl4V5KROmcQMZloS9wqWrtQ79j9GyHsk00IyEkHVMTYrxr7LHpK+MiZ6mSY82DJkGnxpmR7NvCC6ZAt6q1Dz02F6xmA+b7tv2IiPRKN4FLeXk5KisrsW/fvnAvJaL4j0OPniHJTLQiIyEGQni7r45GrsUozlB/mwgAEnTax2W0adqSJCnbRcEELvI20eK8FNV76xARRRr+Ladz8qmisTIuQOB20eh1LnItRijqWwB/cW7fGFmfaDRSfYtM7qAbzMki9m8houmEgYvOyX1cxgtc5O2iyjFOFoWy+Ryg31NFI50okikFukFlXDoBMHAhoumBgYuOCSH8W0Xj9PYY72SRECLkGZdEOXDRWY3L0Hb/gYqD7OXSZ3fhZKP3/9nSAp4oIiL9Y+CiYwNONxxuD4Cx+7gA/ozLicZuuD3DW/939DuVo9Uz00O8VaSzjEvdWFtFM+Stor4JjVwY6nBtFzwCyEm2IjvZGtxCiYiiAAMXHZObz5mNEuIswydDBypKj0ecxQib04Pq1uH/+pczAnkpsYgd52tN1XTcKipKj4MkAT12F1p67ZP+2odY30JE0wwDFx3r6Pf3cBnv+LLRIClHaUcq0A31NhEQ0IDO4Z5S9iES9TtcaBuhh4ssxmRUMjHVU2hEpzSe4zYREU0TDFx0bLzJ0EMtzE0GMHKdi1w8OitER6EBIMHqDVzcHgG7yxOy99GSvE2UZDUpPVuGmj3F1v9CCJ4oIqJph4GLjk2k+Vyg0jFOFslbRbNDGLjEmf1bUHppQjfWNpFsqsMW6zoH0NJjh8kgYVFe8tQXSUQURXQTuLBz7nByjUvyGM3nAskni47Vdw/bqvFPhQ7NUWgAMBgkxFv01YRutOZzgQILdCdDPga9ICcJVnNo6o6IiCKNbgIXds4dTq5xSZ1gxmVediKMBgntfQ40dfsLRd0egfNtod8qAvRXoDuRjIvciXiyW0UszCWi6Ug3gQsNN5HJ0IGsZqPSVyRw4GJtRz+cboEYk2HEXiRqkutc9JNxGf0otEzOuFxo74djErU9/onQKVNfIBFRlGHgomOd/eNPhh5qpAJdeQtjVkY8DAb1hysGStBdxmX8raKspBjEWYxwewRqfK8fz5nmXhytY+M5Ipp+GLjomL/GZWIZFyCgg25Aga4WJ4pk8Ra9BS7jbxVJkqR8bydS53KmuRe3PfUBHG4PluQnY2b66F+biEhvGLjomBy4pE4i4zLSsEXlRFEIe7jI/N1z3SF/r1Abr4dLIH+B7tgni+SgpaXHjvnZiXj6K6vG7dFDRKQnDFx0bKKToQPJGZcL7f3otnkDH6X5XIiGKwZKiNHPqSK5h0viGD1cZLMnkHE509yLW5/0By3/8/VLkBY/8aCUiEgPGLjo2FS2ilLjLcj1zbw57su6VMtbRRpkXOTi3B4dBC61neNvE8nkbNZbJ5rw2Nuncaa5Z9DzZ5p7cOuTH6C1144FOUkMWoho2jKFewEUGkKISTegk5XmJqO+y4bKhm4syktGY7cNAFCsQcZFT4MWJ3KiSLZyZhriLEa09jrwn2+ewn++eQrFM+JxzaJsLC1Ixf0vHVGClufuvphBCxFNWwxcdMrm9ChHaydT4wJ461x2HG9CZX23km1Jj7cgeZIB0FQkWPQUuIx/okiWlxKLPf98JXYcb8IbRxvx7plWnG3pQ8XOs8prFuQk4X/uvhipDFqIaBpj4KJTcvO5iUyGHiqwg66WJ4oAfTWgm8iJokDpCTG4pawQt5QVotvmxM4Tzdh2rBE7T7RgblYC/vCVVQxaiGjaY+CiU4Ht/id76mSh72TR6eYenGz01rlocaII8Ne46CtwmXzTviSrGZ9bmofPLc2D2yMgASHvoUNEFA10U5zLWUWDTeVEkSw/NRaJVhOcboE3jzUBCO2MokAJOqpxqZvEVtFYjAaJQQsRkY9uAhfOKhqsS+nhMvnARZIkZbvodLO3r4j2W0XR3cdlwOFGa683eJzoVhEREY1PN4ELDdYxycnQQ8mt/2XFWm0V6aSPS12nN9sykR4uREQ0cQxcdCqYrSLA30EXAAwSUJimVeDiXW+017jUTLIwl4iIJoaBi07JW0UpU/zX/sKAwKUgLQ4WkzY/KvG+jEu0By4fVLUBQMinaRMRTTcMXHRKmVM0xeOzxTMSYDF6fzxma1TfAviLcx0uD5xuj2bvq6b3zrTiyd1VAIAbluWGeTVERPrCwEWn5D4uU62vsJgMKMnyniTS6kQR4C/OBaKzzqW524ZNzx+EEMCtZQW4bjEDFyIiNTFw0amptvsPdOmcDADAiqJUVdY0EWajATG+bakeW3QFLi63B9/534No7XVgfnYiHvrswnAviYhId9iATqf8NS5T77T6/avn4eYV+ZiTqV3GBfBuF9ldDvQ5oitweXTHaXxY3Y54ixGPf2k5rObJdSwmIqLxMeOiU8GeKgK82Y+SrMRJd94NVjQOWtx5shmP7TwDAHj484s13V4jIppOGLjokBBC6eMSTOASLtHWhK6+cwCbXzgEALj9kkJcv4R1LUREocLARYcCJ0OnTHIydCRIlAOXSdS4dPQ50NJjD9WSRuX01bV09DuxKC8J/9+1pZqvgYhoOmHgojG7y42fv3ECH/r6fISCvE1kMkiIn+Rk6EgQP8nuuTXt/fiHX+3Cp375Duo6B0K5tEF6bE5869mPceB8BxJjTKj4IutaiIhCjYGLxvaebcPj75zFg68eC9l7dCrbRJOfDB0J/FtF4wcu/Q4XvvGnA2jtdaDH5sJPX6+c0Huca+3DXw/VwTXFXjFnW3pxQ8V72HG8CRajAb+6ZSmK0rXrd0NENF0xcNGYfMT3VFMP+kN0akbu4RKN9S3AxCdECyHwz/93GMcbupEaZ4bRIGHrkUa8e7p1zM9r7rHh80/sxabnD+Ge5z6GzTm5WpodlU244bH3cLalD9lJVvx542p8ujRrUl+DiIimRjeBS0VFBUpLS1FWVhbupYzJ7rtJegRwtK47JO8RbLv/cJMDl95xArsndp3F3w43wGSQ8F93rMQdlxQBAB589ahS4zOUxyPw3T9/grY+b3D3ZmUTvvz7j9Dl63szFo9H4NEdp3D3M/vRY3dh1cw0vPadtVhakDKJqyMiomDoJnApLy9HZWUl9u3bF+6ljMkWcEM9XNsZkvfwN5+LvsJcIGCraIzi3J0nmvGLbScBAA99diFWzUrDP/3DXGQkWHC2pQ9Pv1c94uf993vV2HO6FVazAQ/feBESrSZ8dK4dX/jt+2jqto36fm29dnzjTwfw6I7TAIA7Vxfhua9fjBmJMVO9TCIimgLdBC7Rwh6wLXG4tisk79EZxUehgfG3iqpaenGvr63+basKcbsv05Ica8YPrpkPAPjNW6fR2DU4EDla14WfvXECAPAv15Xi1lWF+PM3V2NGYgxONvXgxsf34mxLr/J6j0dg96kWlD/3MS7Z8pa3nsVkwC9uWowff24RzEb+8SEi0hr/5tWYXZOMi6/GJUq3isbq49Jjc+Lrz+xHj82FlUWp+PGQtvqfX56P5YUp6HO48R9bjyuP9ztc2PT8QTjdAutLs/DFVYUAgAU5SXjpW2swKyMedZ0DuOmJvXjreBN+veM01v18J7783x/h9SMNcLoFluQn4y/fXI2bVxaE8OqJiGgsDFw0FphxOdfWr9SjqKmzL8ozLtbhGRchBPafa8c3/3RAKYp9/PblsJgG/wgbDBJ+8rlFkCTg1U/q8f5Z77Hzf/tbJc629CErKQY/+/ziQaetCtLi8H8bV2NxfjI6+p342h/341c7TqGucwBJVhPuXF2Erfeuw1+/vRZLWM9CRBRWnFWkMduQotEjdV1YW5Kh6nv42/1HZ41Lgq+PS6/dhfNtfXj5YB1ePliH8239ALyTq5/88gpkJlpH/PxFecn40sWFePaDC3jo1WP49qfm4H8/qoEkAb/6wlKkxg//vqQnxOB/v34J7nnuY+w61YJLZqfh1rJCXLMom71ZiIgiCAMXjQ09evtJbaf6gUuU17jEW7w/lsfqu3D5L94JeNyIaxbl4K41M3FRfvKYX+N76+fh9cMNONnUg03PHwQAbLy8GGvmjP69jo8x4Q9fKUP3gAvJUfq9IyLSOwYuGrM75Vb8ZnT2O3EkBAW68tHeYCZDh5OcEfEIwCABl87JwOeX52P9wizEWSb2I5sSZ8E/XzMf9790BB4BLMlPxuZ/mDvu50mSxKCFiCiCMXDRmM3lzbisLErDjuNNqhboCiHwzskW1LR7t1SiNeNSkpmAf75mHswGAz67NBdZSSNvCY3nCysL8Non9TjV1Itf37qMp4CIiHSAgYvG5IzLypmpeOtEE+q7bGjpsQfdD2TfuXb8/I0T2HeuAwCQkWBBUXpc0OsNB0mScM8Vc4L+OkaDhOfuvhhuj4CJQQsRkS4wcNGYnHFJi7egeEYCzjT34khdJz41f2ot4483dOM/t53EWyeaAQAxJgPuWjMTGy8vRqI1OjMuapIkCSZj9M1rIiKikTFw0ZiccbGajVicl4wzzb34pKZrSoHLlq3H8eSeKgjhzS58YWUBNl1VguzkqW2tEBERRToGLhqTMy5WkwGL85Px0sE6HKmbfIHu+bY+/NfuKgDAdYtzsPkf5mL2jARV10pERBRpGLhoTM64xJiNWOxrZna4thNCiEFN0cbzzskWAMAls9Pw2BeXq75OIiKiSMSKRY0FZlxKc5JgMkho7XWgvmv0AX8j2XnSW9Ny5bxM1ddIREQUqRi4aCww42I1GzE3KxEAcGQSx6IHHG6llf2V8xm4EBHR9MHARWN2OeNi9n7rlxR4O8B+MolGdB9UtcHu8iAvJRYlmaxrISKi6YOBi8ZscsbF5J1/c1FeCoDJTYqWt4mumDdjUnUxRERE0Y6Bi8aGZlwW+2buHK7tghBi3M8XQuDtE6xvISKi6Uk3gUtFRQVKS0tRVlYW7qWMyu0RcLq9wYnVl3GZl50Ii8mAHpsL53zTj8dytqUPtR0DsBgNWDMnPaTrJSIiijS6CVzKy8tRWVmJffv2hXspo5KzLQAQ48u4mI3e00XAxLaL3vFtE108O23CAweJiIj0QjeBSzSQ61sAf40L4J1cDHi3i8bDY9BERDSdMXDRkM3pzbiYjRKMBn9R7UX5KQDGz7j02l34qLodAI9BExHR9MTARUN2l29OUUC2BfBnXI7WdcPl9gz7PNl7Z1rhdAsUpcdhVkZ86BZKREQUoRi4aEjOuMj1LbLZMxIQbzFiwOnG2Za+UT//HW4TERHRNMfARUNyxiVmSMbFaJCwME9uRNc54ucKIZT5RFfMmxG6RRIREUUwBi4akjMuVvPwb7u/QLdzxM892dSDhi4brGYDLpnNY9BERDQ9MXDR0GgZFwBY7CvQffd0K7r6ncOe33nCm21ZU5wBq3n45xMREU0HDFw0NFbGZU1xOpKsJpxr68fnf7sXNe2Dm9H5j0Fzm4iIiKYvBi4aUopzR8i4pCfE4IVvrkZ2khVnmnvxj4/vVbaNugacOHC+AwBwBQtziYhoGmPgoiHlOPQIGRcAWJCThJfL12B+diJae+245b8+wFvHm/Du6Va4PQJzMhNQkBan5ZKJiIgiCgMXDdnHyLjIcpJj8ZeNq7GuJAMDTje+/sx+/HL7SQDcJiIiImLgoqHxMi6yRKsZ/31XGW5ekQ+PAKp8vV3Yv4WIiKY7Bi4a8hfnjn8qyGw04Oc3Lcbmf5gLAEiNM2PlzLSQro+IiCjScbywhvzHoScWL0qShHuvKsGlc9KRZDXDMsHPIyIi0isGLhqaTMYl0IoiZlqIiIgAbhVpyuacXMaFiIiIBuMdVEN2lzxkkZ1viYiIpoKBi4aYcSEiIgoO76AakjMunDVEREQ0NQxcNCRnXBi4EBERTQ0DFw0pNS7cKiIiIpoS3kE1xIwLERFRcBi4aMg/HZrfdiIioqngHVRDDhczLkRERMHQTeBSUVGB0tJSlJWVhXspo2LGhYiIKDi6uYOWl5ejsrIS+/btC/dSRmVnxoWIiCgouglcooF/VhG/7URERFPBO6iG/NOhmXEhIiKaCgYuGnG5PXB5BABmXIiIiKaKd1CNyNkWgBkXIiKiqWLgohG5vgXgqSIiIqKp4h1UIzZfxsViNMBgkMK8GiIioujEwEUjdrmHC+tbiIiIpox3UY3Ic4pY30JERDR1DFw0Ik+G5okiIiKiqeNdVCOcDE1ERBQ8Bi4akTMuPFFEREQ0dbyLaoQZFyIiouAxcNEIMy5ERETB411UI3ZmXIiIiILGwEUjNmZciIiIgsa7qEaYcSEiIgoeAxeNyLOK2MeFiIho6ngX1Yg8HZqdc4mIiKaOgYtGbJxVREREFDTeRTXiL85lxoWIiGiqGLhoxF+cy285ERHRVPEuqhEba1yIiIiCxsBFI3aeKiIiIgoa76IakTMuVmZciIiIpoyBi0bsPFVEREQUNN5FNcKMCxERUfAYuGiEGRciIqLg8S6qEblzLmcVERERTR0DF40onXM5HZqIiGjKeBfVCDMuREREwWPgohFlOjSLc4mIiKaMgYtGlOnQLM4lIiKaMt5FNeB0e+D2CADMuBAREQWDgYsG5GwLwIwLERFRMHgX1YBc3wLwVBEREVEweBfVgBy4WEwGSJIU5tUQERFFLwYuGlCOQjPbQkREFBTeSTWgHIVmDxciIqKgRGTg8o//+I9ITU3FTTfdFO6lqIJHoYmIiNQRkXfSe++9F88880y4l6EaNp8jIiJSR0QGLldeeSUSExPDvQzVMONCRESkjknfSXfv3o3rr78eubm5kCQJr7zyyrDXPP7445g1axasVitWrFiBPXv2qLHWqGVnxoWIiEgVkw5c+vr6sGTJEjz22GMjPv/CCy/gvvvuwwMPPICDBw9i3bp12LBhAy5cuKC8ZsWKFVi0aNGwj/r6+qlfSQSzOZlxISIiUoNpsp+wYcMGbNiwYdTnH3nkEXzta1/D3XffDQB49NFHsW3bNjzxxBPYsmULAODAgQNTXO5wdrsddrtd+X13d7dqX1stdhczLkRERGpQNQXgcDhw4MABrF+/ftDj69evx969e9V8K8WWLVuQnJysfBQUFITkfYIhZ1x4HJqIiCg4qgYura2tcLvdyMrKGvR4VlYWGhsbJ/x1rr76atx8883YunUr8vPzsW/fvlFfe//996Orq0v5qKmpmfL6Q0XOuLDdPxERUXAmvVU0EUPb2gshJtXqftu2bRN+bUxMDGJiYib8+nDw17gw40JERBQMVVMAGRkZMBqNw7Irzc3Nw7Iw0wkzLkREROpQ9U5qsViwYsUKbN++fdDj27dvx5o1a9R8q6jCGhciIiJ1THqrqLe3F2fOnFF+X11djUOHDiEtLQ2FhYXYvHkz7rjjDqxcuRKrV6/Gk08+iQsXLmDjxo2qLjyayJ1zmXEhIiIKzqQDl/379+PKK69Ufr9582YAwJ133ok//OEPuOWWW9DW1oaf/OQnaGhowKJFi7B161YUFRWpt+oRVFRUoKKiAm63O6TvMxXKdGhmXIiIiIIiCSFEuBehpu7ubiQnJ6OrqwtJSUnhXg4A4Nv/8zH+drgB/3pdKb66dla4l0NERBRxJnr/5t6FBphxISIiUgcDFw0o06HZ8p+IiCgovJNqQJkOzZb/REREQWHgogE7My5ERESq4J1UA0rnXGZciIiIgsLARQPKdGhmXIiIiIKimztpRUUFSktLUVZWFu6lDMOMCxERkTp0E7iUl5ejsrJyzEnS4cKMCxERkTp4J9UAZxURERGpg4FLiAkhOB2aiIhIJbyThpjTLeDxDVWIYcaFiIgoKAxcQszm8g99ZMaFiIgoOLyThpjdV98CMHAhIiIKFu+kISbPKYoxGSBJUphXQ0REFN0YuIQYJ0MTERGpRzeBS6Q2oONkaCIiIvXo5m4aqQ3oOBmaiIhIPboJXCIVJ0MTERGph3fTEGPGhYiISD0MXEKMNS5ERETq4d00xGxKu39mXIiIiILFwCXE7MqARX6riYiIgsW7aYgpDejYx4WIiChoDFxCzF+cy281ERFRsHg3DTGbk51ziYiI1KKbwCVSO+faXf5ZRURERBQc3dxNI7VzLjMuRERE6tFN4BKpbMy4EBERqYZ30xCzM+NCRESkGgYuISZnXKzMuBAREQWNd9MQkzMu7ONCREQUPAYuISafKmLnXCIiouDxbhpiSsaFs4qIiIiCxsAlxGzMuBAREamGd9MQU2YVMeNCREQUNAYuISbPKmLGhYiIKHi8m4YYMy5ERETq0U3gErmziphxISIiUotu7qaRO6uIGRciIiK16CZwiURCCCXjEsOMCxERUdB4Nw0hh9sDIby/5qwiIiKi4DFwCSGbr/kcwOnQREREauDdNITkdv+SBFiM/FYTEREFi3fTEJLb/VtNRkiSFObVEBERRT8GLiEkZ1xYmEtERKQO3lFDyBaQcSEiIqLgMXAJIWZciIiI1MU7aggx40JERKQuBi4hpHTNZcaFiIhIFbyjhpAyp4gZFyIiIlUwcAkhZlyIiIjUxTtqCClziphxISIiUoVuApeKigqUlpairKws3EtRyBkXKzMuREREqtDNHbW8vByVlZXYt29fuJeiYMaFiIhIXboJXCIRMy5ERETq4h01hJhxISIiUhcDlxBixoWIiEhdvKOGkNw5lxkXIiIidTBwCSF5VhEzLkREROrgHTWE7PKsIjMzLkRERGpg4BJCynRoE7/NREREauAdNYRszLgQERGpioFLCDHjQkREpC7eUUOIGRciIiJ1MXAJIWU6NDMuREREquAdNYSUzrnMuBAREamCgUsIsXMuERGRunhHDSHOKiIiIlIXA5cQYsaFiIhIXbyjhogQghkXIiIilekmcKmoqEBpaSnKysrCvRQA/m0igBkXIiIitejmjlpeXo7Kykrs27cv3EsB4J9TBDDjQkREpBbdBC7hcqqpB0/uPoumbtugx+WuuQYJMBulcCyNiIhId0zhXkA08ngE3j7RjKf3VuO9M20AgJc+rsPL91yKWIs3uxLYNVeSGLgQERGpgYHLJPTYnPjz/lo88/45nG/rB+DNqFjNRpxo7MG//vUofnHzEgCcU0RERBQKDFwm6JHtp/D7PVXoc3gDkiSrCbetKsTtlxShpqMft//uQ/zlQC3KZqbhC2UFnFNEREQUAgxcJsjp9qDP4caczATctWYmblyehziL99tXkBaH766fh19sO4l/+etRLMxLYsaFiIgoBBi4TNBda2ZiTXE61s7JGLFm5VuXF2P/uXbsPNmCe577GD+8Zj4AZlyIiIjUxHTABGUlWbGuZMaohbYGg4Rf3bIUeSmxON/Wjx+/VgmAGRciIiI18a6qopQ4Cx7/0nKYjRIafcejORmaiIhIPQxcVLakIAX/el2p8ntuFREREamHgUsI3H5JET67JBcAkB5vCfNqiIiI9IPFuSEgSRJ+ftNiXDI7HZfNzQj3coiIiHSDgUuIWM1GfPHiwnAvg4iISFe4VURERERRg4ELERERRQ0GLkRERBQ1GLgQERFR1GDgQkRERFGDgQsRERFFDQYuREREFDUYuBAREVHUYOBCREREUYOBCxEREUUNBi5EREQUNRi4EBERUdRg4EJERERRQzfToSsqKlBRUQGXywUA6O7uDvOKiIiIaKLk+7YQYszXSWK8V0SZ2tpaFBQUhHsZRERENAU1NTXIz88f9XndBS4ejwf19fVITEyEJEmqfd3u7m4UFBSgpqYGSUlJqn3dSMZr5jXrFa+Z16xX0XzNQgj09PQgNzcXBsPolSy62SqSGQyGMSO1YCUlJUXdD0OweM3TA695euA1Tw/Res3JycnjvobFuURERBQ1GLgQERFR1GDgMkExMTF48MEHERMTE+6laIbXPD3wmqcHXvP0MB2uWXfFuURERKRfzLgQERFR1GDgQkRERFGDgQsRERFFDQYuREREFDUYuEzQ448/jlmzZsFqtWLFihXYs2dPuJekmt27d+P6669Hbm4uJEnCK6+8Muh5IQQeeugh5ObmIjY2FldccQWOHTsWnsWqYMuWLSgrK0NiYiIyMzNxww034OTJk4Neo7drfuKJJ7B48WKlKdXq1avx97//XXleb9c7ki1btkCSJNx3333KY3q77oceegiSJA36yM7OVp7X2/XK6urqcPvttyM9PR1xcXFYunQpDhw4oDyvt+ueOXPmsP/PkiShvLwcgP6udxhB43r++eeF2WwWTz31lKisrBSbNm0S8fHx4vz58+Femiq2bt0qHnjgAfHiiy8KAOLll18e9PzDDz8sEhMTxYsvviiOHDkibrnlFpGTkyO6u7vDs+AgXX311eLpp58WR48eFYcOHRLXXnutKCwsFL29vcpr9HbNr776qnj99dfFyZMnxcmTJ8WPfvQjYTabxdGjR4UQ+rveoT766CMxc+ZMsXjxYrFp0yblcb1d94MPPigWLlwoGhoalI/m5mbleb1drxBCtLe3i6KiInHXXXeJDz/8UFRXV4sdO3aIM2fOKK/R23U3NzcP+n+8fft2AUDs3LlTCKG/6x2KgcsErFq1SmzcuHHQY/Pnzxc//OEPw7Si0BkauHg8HpGdnS0efvhh5TGbzSaSk5PFb3/72zCsUH3Nzc0CgNi1a5cQYnpcsxBCpKamit/97ne6v96enh5RUlIitm/fLi6//HIlcNHjdT/44INiyZIlIz6nx+sVQogf/OAHYu3ataM+r9frDrRp0yZRXFwsPB7PtLhebhWNw+Fw4MCBA1i/fv2gx9evX4+9e/eGaVXaqa6uRmNj46Drj4mJweWXX66b6+/q6gIApKWlAdD/Nbvdbjz//PPo6+vD6tWrdX+95eXluPbaa/HpT3960ON6ve7Tp08jNzcXs2bNwq233oqqqioA+r3eV199FStXrsTNN9+MzMxMLFu2DE899ZTyvF6vW+ZwOPDss8/iq1/9KiRJ0v31AqxxGVdrayvcbjeysrIGPZ6VlYXGxsYwrUo78jXq9fqFENi8eTPWrl2LRYsWAdDvNR85cgQJCQmIiYnBxo0b8fLLL6O0tFS31wsAzz//PD7++GNs2bJl2HN6vO6LL74YzzzzDLZt24annnoKjY2NWLNmDdra2nR5vQBQVVWFJ554AiUlJdi2bRs2btyIe++9F8888wwAff5/DvTKK6+gs7MTd911FwD9Xy+gw+nQoSJJ0qDfCyGGPaZner3+b3/72zh8+DDefffdYc/p7ZrnzZuHQ4cOobOzEy+++CLuvPNO7Nq1S3leb9dbU1ODTZs24c0334TVah31dXq67g0bNii/vuiii7B69WoUFxfjj3/8Iy655BIA+rpeAPB4PFi5ciX+4z/+AwCwbNkyHDt2DE888QS+/OUvK6/T23XLfv/732PDhg3Izc0d9LherxdgxmVcGRkZMBqNwyLV5ubmYRGtHsknEvR4/d/5znfw6quvYufOncjPz1ce1+s1WywWzJkzBytXrsSWLVuwZMkS/PrXv9bt9R44cADNzc1YsWIFTCYTTCYTdu3ahd/85jcwmUzKtentugPFx8fjoosuwunTp3X7/zknJwelpaWDHluwYAEuXLgAQL9/ngHg/Pnz2LFjB+6++27lMT1fr4yByzgsFgtWrFiB7du3D3p8+/btWLNmTZhWpZ1Zs2YhOzt70PU7HA7s2rUraq9fCIFvf/vbeOmll/D2229j1qxZg57X4zWPRAgBu92u2+u96qqrcOTIERw6dEj5WLlyJb70pS/h0KFDmD17ti6vO5Ddbsfx48eRk5Oj2//Pl1566bB2BqdOnUJRUREAff95fvrpp5GZmYlrr71WeUzP16sIU1FwVJGPQ//+978XlZWV4r777hPx8fHi3Llz4V6aKnp6esTBgwfFwYMHBQDxyCOPiIMHDyrHvR9++GGRnJwsXnrpJXHkyBFx2223RfXRum9961siOTlZvPPOO4OOFPb39yuv0ds133///WL37t2iurpaHD58WPzoRz8SBoNBvPnmm0II/V3vaAJPFQmhv+v+7ne/K9555x1RVVUlPvjgA3HdddeJxMRE5e8qvV2vEN6j7iaTSfz0pz8Vp0+fFs8995yIi4sTzz77rPIaPV632+0WhYWF4gc/+MGw5/R4vYEYuExQRUWFKCoqEhaLRSxfvlw5OqsHO3fuFACGfdx5551CCO9xwgcffFBkZ2eLmJgYcdlll4kjR46Ed9FBGOlaAYinn35aeY3ervmrX/2q8vM7Y8YMcdVVVylBixD6u97RDA1c9Hbdcr8Os9kscnNzxY033iiOHTumPK+365W99tprYtGiRSImJkbMnz9fPPnkk4Oe1+N1b9u2TQAQJ0+eHPacHq83kCSEEGFJ9RARERFNEmtciIiIKGowcCEiIqKowcCFiIiIogYDFyIiIooaDFyIiIgoajBwISIioqjBwIWIiIiiBgMXIiIiihoMXIiIiChqMHAhIiKiqMHAhYiIiKIGAxciIiKKGv8/ZEa0GaQsCxoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(v)\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
