{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "\n",
    "   \n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def forward_grads(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "\n",
    "        \n",
    "        i =0\n",
    "        z = self.linears[i](a)\n",
    "        a1 = self.activation(self.beta[:,i]*z)\n",
    "        a = z*a1\n",
    "            \n",
    "        z_np = z.cpu().detach().numpy()\n",
    "        beta = self.beta.cpu().detach().numpy()\n",
    "        a_np = a1.cpu().detach().numpy()\n",
    "\n",
    "        a1_np =  beta[:,i]*z_np*a_np*(1-a_np) + a_np\n",
    "        a2_np = a_np*(1-a_np)*(1+np.square(beta[:,i])*z_np + beta[:,i] - beta[:,i]*z_np*a_np)\n",
    "\n",
    "        W2 = self.linears[1].weight.cpu().detach().numpy()\n",
    "        W1 = np.transpose(self.linears[0].weight.cpu().detach().numpy())\n",
    "        val = (W2)*np.square(W1)*a2_np\n",
    "         \n",
    "        return val\n",
    "    \n",
    "    def grad_test(self,x_grad):\n",
    "        g = x_grad.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "    \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_grad.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_x_w1 = autograd.grad(y_x,self.linears[0].weight,torch.ones([y_x.shape[0],1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "    \n",
    "            \n",
    "        \n",
    "        y_x_w1 = np.transpose(y_x_w1.cpu().detach().numpy())\n",
    "        sd_val = self.forward_grads(x_grad)\n",
    "        \n",
    "        # print(sd_val/y_x_w1)\n",
    "        return np.mean(np.abs(sd_val)/np.abs(y_x_w1+0.001))\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    x_grad = torch.zeros((1,1)).float().to(device)\n",
    "    v = PINN.grad_test(x_grad)\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    v = np.zeros((max_iter,1))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        v[i] = train_step(x_coll,f_hat)\n",
    "       \n",
    "        \n",
    "        # print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.6936216 Test MSE 384.1087415133986 Test RE 0.9990556448322482\n",
      "1 Train Loss 4.325003 Test MSE 386.8860045538474 Test RE 1.0026609297725968\n",
      "2 Train Loss 4.1889434 Test MSE 386.8342085617345 Test RE 1.002593809800137\n",
      "3 Train Loss 3.414572 Test MSE 379.55543730912126 Test RE 0.9931164855722888\n",
      "4 Train Loss 3.400576 Test MSE 378.8581291877683 Test RE 0.9922038039463953\n",
      "5 Train Loss 3.3547976 Test MSE 378.1581247163215 Test RE 0.9912867479749706\n",
      "6 Train Loss 3.277972 Test MSE 378.71073633408514 Test RE 0.9920107792157008\n",
      "7 Train Loss 3.2425647 Test MSE 378.45039362330704 Test RE 0.9916697442895548\n",
      "8 Train Loss 2.7899613 Test MSE 383.1790683142611 Test RE 0.997845885930803\n",
      "9 Train Loss 2.717186 Test MSE 384.4867376539678 Test RE 0.9995471024267519\n",
      "10 Train Loss 2.6208344 Test MSE 385.27974479911717 Test RE 1.0005773586445832\n",
      "11 Train Loss 2.4678283 Test MSE 384.52470864410316 Test RE 0.9995964576481926\n",
      "12 Train Loss 2.4151757 Test MSE 384.5582209329803 Test RE 0.9996400153632791\n",
      "13 Train Loss 2.376301 Test MSE 382.1337213583291 Test RE 0.9964838497027454\n",
      "14 Train Loss 2.375113 Test MSE 381.6776423046127 Test RE 0.9958890172244514\n",
      "15 Train Loss 2.3685548 Test MSE 380.677381814391 Test RE 0.9945832007017239\n",
      "16 Train Loss 2.3634987 Test MSE 378.90257579504726 Test RE 0.9922620035679256\n",
      "17 Train Loss 2.359912 Test MSE 376.0395278180975 Test RE 0.9885060504071145\n",
      "18 Train Loss 2.337157 Test MSE 371.89510270036044 Test RE 0.9830436726926107\n",
      "19 Train Loss 2.1380594 Test MSE 340.6547165777354 Test RE 0.9408486925469375\n",
      "20 Train Loss 1.9052697 Test MSE 284.88470936667125 Test RE 0.860393562870916\n",
      "21 Train Loss 1.7964795 Test MSE 259.41986716083665 Test RE 0.8210397786934343\n",
      "22 Train Loss 1.7334822 Test MSE 247.44814462228425 Test RE 0.8018713251659797\n",
      "23 Train Loss 1.5401127 Test MSE 195.76883905291982 Test RE 0.7132378221310973\n",
      "24 Train Loss 1.231523 Test MSE 184.99818566086654 Test RE 0.6933400973437647\n",
      "25 Train Loss 0.84192604 Test MSE 86.26027654816751 Test RE 0.4734433334153843\n",
      "26 Train Loss 0.71812725 Test MSE 66.42152884639906 Test RE 0.4154483288681995\n",
      "27 Train Loss 0.17550102 Test MSE 11.074429768786967 Test RE 0.16963804906499239\n",
      "28 Train Loss 0.16133197 Test MSE 6.6054226693292 Test RE 0.13101254670589282\n",
      "29 Train Loss 0.15512148 Test MSE 4.049053118765763 Test RE 0.10257447897302478\n",
      "30 Train Loss 0.10216616 Test MSE 1.8778634388804294 Test RE 0.06985453174332766\n",
      "31 Train Loss 0.025577322 Test MSE 0.7531069797870483 Test RE 0.04423753495980316\n",
      "32 Train Loss 0.018589873 Test MSE 0.006498536751445521 Test RE 0.0041093238828058464\n",
      "33 Train Loss 0.016327808 Test MSE 0.0001289425037532079 Test RE 0.0005788427951272226\n",
      "34 Train Loss 0.014147388 Test MSE 0.14147684570918614 Test RE 0.01917367080992384\n",
      "35 Train Loss 0.004486205 Test MSE 0.004552124755343195 Test RE 0.0034392968380794423\n",
      "36 Train Loss 0.0031057848 Test MSE 0.0022431231570552938 Test RE 0.002414288367984408\n",
      "37 Train Loss 0.00301057 Test MSE 0.007657931882057615 Test RE 0.004460857313791296\n",
      "38 Train Loss 0.0030030815 Test MSE 0.008167229081405108 Test RE 0.00460680629001618\n",
      "39 Train Loss 0.0029969783 Test MSE 0.008439942501599717 Test RE 0.004683088084146345\n",
      "40 Train Loss 0.0029918966 Test MSE 0.008548017856016398 Test RE 0.004712976701901072\n",
      "41 Train Loss 0.0029879815 Test MSE 0.008441696098253198 Test RE 0.004683574569751142\n",
      "42 Train Loss 0.0029845736 Test MSE 0.008221918795587403 Test RE 0.004622204692107255\n",
      "43 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "44 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "45 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "46 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "47 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "48 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "49 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "50 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "51 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "52 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "53 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "54 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "55 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "56 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "57 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "58 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "59 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "60 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "61 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "62 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "63 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "64 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "65 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "66 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "67 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "68 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "69 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "70 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "71 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "72 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "73 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "74 Train Loss 0.0029823761 Test MSE 0.00787321612108747 Test RE 0.0045231258237118\n",
      "Training time: 9.31\n",
      "Training time: 9.31\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21993/129217413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_loss_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test_mse_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_mse_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_re_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Thresh Time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Thresh epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 75\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    v = train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold,\"k\":k}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "plt.plot(k_mean[:,layer_num,0],'b')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,0] - 0.5*k_std[:,layer_num,0],k_mean[:,layer_num,0] + 0.5*k_std[:,layer_num,0],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,1],'r')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,1] - 0.5*k_std[:,layer_num,1],k_mean[:,layer_num,1] + 0.5*k_std[:,layer_num,1],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,2],'g')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,2] - 0.5*k_std[:,layer_num,2],k_mean[:,layer_num,2] + 0.5*k_std[:,layer_num,2],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"v\":v}\n",
    "savemat('v_swish.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(v)\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
