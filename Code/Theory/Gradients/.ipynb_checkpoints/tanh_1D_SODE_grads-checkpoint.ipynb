{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "\n",
    "   \n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def forward_grads(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "\n",
    "        \n",
    "        i =0\n",
    "        z = self.linears[i](a)\n",
    "        a = self.activation(z) \n",
    "\n",
    "        a_np = a.cpu().detach().numpy()\n",
    "        a1_np = 1-np.square(a_np)\n",
    "        a2_np = -2*a_np*(a1_np)\n",
    "\n",
    "        W2 = self.linears[1].weight.cpu().detach().numpy()\n",
    "        W1 = np.transpose(self.linears[0].weight.cpu().detach().numpy())\n",
    "        val = (W2)*np.square(W1)*a2_np\n",
    "         \n",
    "        return val\n",
    "    \n",
    "    def grad_test(self,x_grad):\n",
    "        g = x_grad.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "    \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_grad.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_x_w1 = autograd.grad(y_x,self.linears[0].weight,torch.ones([y_x.shape[0],1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "    \n",
    "            \n",
    "        \n",
    "        y_x_w1 = np.transpose(y_x_w1.cpu().detach().numpy())\n",
    "        sd_val = self.forward_grads(x_grad)\n",
    "        \n",
    "        print(np.mean(y_x_w1))\n",
    "        # print(sd_val/y_x_w1)\n",
    "        return np.mean(np.abs(sd_val)/np.abs(y_x_w1)+0.001)\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    x_grad = torch.zeros((1,1)).float().to(device)\n",
    "    v = PINN.grad_test(x_grad)\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    v = np.zeros((max_iter,1))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        v[i] = train_step(x_coll,f_hat)\n",
    "       \n",
    "        \n",
    "        # print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "[[ 5.3953723e-08 -2.2657793e-03  2.1625198e-03 -6.3798442e-03\n",
      "  -4.5361325e-05  1.7010367e-02 -3.4301338e-04  3.1074539e-06\n",
      "  -1.2874253e-04  9.7033283e-04  1.0049489e-03  4.7248634e-04\n",
      "   2.1822409e-06 -9.8672877e-05  1.4309008e-08 -4.0402313e-04\n",
      "   1.2256760e-03  7.8941724e-05  1.2958825e-05  1.3405888e-03\n",
      "   5.5239454e-04 -1.5591118e-02 -6.0410667e-03 -3.4693818e-05\n",
      "   1.3002260e-02  6.9059988e-06 -1.2480488e-05  1.2113908e-02\n",
      "   1.8941004e-05  1.0090885e-03  1.7343285e-03  2.9702951e-06\n",
      "  -2.2393126e-04 -1.2819444e-04 -1.8133231e-02 -8.2027004e-04\n",
      "  -2.0934566e-04  5.6073302e-04  4.2448933e-03  4.6866405e-04\n",
      "  -1.5688078e-02  2.6689637e-03 -5.3952704e-04 -4.3340194e-09\n",
      "   1.5204036e-04 -1.5329817e-05 -3.6465642e-03 -5.2719490e-10\n",
      "  -2.3652257e-03  1.0193168e-03]]\n",
      "0 Train Loss 4.4657817 Test MSE 386.70819064471647 Test RE 1.002430490382989\n",
      "[[ 8.1603453e-09 -3.1765760e-03  1.4583655e-03 -7.7149998e-03\n",
      "  -5.7084550e-04  1.5043269e-02 -5.6236272e-04  1.6156034e-06\n",
      "  -2.0646284e-04  4.5401198e-04  7.7062909e-04  3.0158393e-04\n",
      "  -2.6145182e-07 -2.2378769e-04 -6.2076403e-08 -6.2862242e-04\n",
      "   6.1111391e-04  3.2486489e-05  4.9579503e-06 -4.9505528e-04\n",
      "   4.3198536e-04 -1.8358819e-02 -7.4051977e-03 -1.4521938e-04\n",
      "   1.0849026e-02 -2.9820533e-09 -3.1378149e-04  1.0173395e-02\n",
      "   7.2123107e-06  8.0115639e-04  1.3900406e-03  2.0156767e-06\n",
      "  -3.2394822e-04 -3.3276010e-04 -2.0173281e-02 -1.7937984e-03\n",
      "  -3.0166289e-04  4.1449504e-04  3.3840265e-03  3.3278731e-04\n",
      "  -1.7915929e-02  2.1823575e-03 -6.9704326e-04 -1.3105578e-06\n",
      "   5.3003405e-05 -2.1429345e-05 -4.4443067e-03 -7.6440798e-10\n",
      "  -3.1693445e-03  8.0234761e-04]]\n",
      "1 Train Loss 4.318592 Test MSE 388.0028901737834 Test RE 1.00410715748249\n",
      "[[-1.0712699e-07 -2.9875252e-03  1.1714610e-03 -4.2940206e-03\n",
      "  -5.7410775e-03  3.9723616e-02 -1.1173056e-03  4.8716430e-08\n",
      "  -4.5788815e-04  1.5729187e-05  4.4374057e-04  7.9470170e-05\n",
      "  -2.0982421e-04 -4.4419852e-04 -1.0024487e-05 -9.0958236e-04\n",
      "   4.7813228e-05 -1.3056771e-07 -9.2987955e-09  2.4698174e-03\n",
      "   2.5333621e-04 -3.6716643e-03 -3.7140518e-03 -9.8525744e-04\n",
      "   2.4334239e-02 -2.0578929e-04 -7.1440765e-04  2.1675950e-02\n",
      "  -1.9956438e-05  5.4713170e-04  1.1132128e-03 -1.5406886e-07\n",
      "  -4.9694441e-04 -6.4144935e-04 -3.4279961e-03 -3.7154150e-03\n",
      "  -5.3696200e-04  2.0614381e-04  4.0391805e-03  9.2145543e-05\n",
      "  -5.1235943e-03  2.0506671e-03 -9.0543379e-04 -4.7830956e-05\n",
      "  -3.7845707e-10 -4.2705022e-05 -3.2363534e-03 -1.7395193e-09\n",
      "  -2.8338300e-03  5.0849438e-04]]\n",
      "2 Train Loss 3.4866962 Test MSE 385.8633611083951 Test RE 1.0013349020910973\n",
      "[[-3.09769291e-07 -6.64679566e-04  5.04088704e-04  6.61075101e-05\n",
      "  -9.21745747e-02  1.96955979e-01 -5.56377042e-03 -1.08969480e-05\n",
      "  -2.97935959e-03 -3.40717030e-03 -7.64501572e-04 -5.46508381e-05\n",
      "  -1.39882052e-02 -8.27975920e-04 -6.89323409e-04 -1.08258647e-03\n",
      "  -2.32032663e-03 -9.47656983e-04 -2.37022075e-04  2.16566063e-02\n",
      "  -7.69465623e-06 -1.04677260e-04  5.97014005e-05 -1.79534014e-02\n",
      "   1.28310248e-01 -1.60933360e-02 -1.37218728e-03  1.13404959e-01\n",
      "  -6.65226762e-05 -2.62434132e-05  1.13354494e-04 -2.16812032e-06\n",
      "  -8.06472672e-04 -1.09707005e-03  3.06128748e-02 -1.65879261e-02\n",
      "  -2.27051345e-03 -3.91618471e-08  1.12534827e-02 -5.39016852e-04\n",
      "   1.22329599e-04  6.18335675e-04 -1.36487186e-03 -2.26346543e-03\n",
      "  -3.25355562e-03 -2.48525990e-04  1.45341488e-04 -1.35994886e-08\n",
      "  -3.57024546e-04 -5.33696730e-04]]\n",
      "3 Train Loss 3.1719036 Test MSE 387.2966501784964 Test RE 1.0031929070486816\n",
      "[[-1.13658803e-06 -7.92403985e-03 -2.43252376e-03  3.11844226e-04\n",
      "  -1.07627427e-02  4.16745543e-01 -7.86227826e-03 -2.10633771e-05\n",
      "  -4.77279117e-03 -4.21700347e-03 -1.51042896e-03 -1.05623832e-04\n",
      "  -9.87968966e-03 -1.94370421e-03 -1.18541939e-03 -2.60399515e-03\n",
      "  -3.33580328e-03 -1.44188234e-03 -4.22478362e-04  2.27968395e-02\n",
      "  -2.17570760e-05 -1.44566744e-04  9.16955934e-04 -1.14251738e-02\n",
      "   2.23540217e-01 -1.05342381e-02 -2.71958113e-03  1.94840729e-01\n",
      "  -2.09837002e-04 -1.00605146e-04 -1.48193372e-04 -8.90184037e-06\n",
      "  -2.03471654e-03 -2.39648926e-03  2.69944742e-02 -1.42625878e-02\n",
      "  -4.22367686e-03  4.41962129e-08  9.52334795e-03 -1.13220979e-03\n",
      "   5.04507261e-05 -1.26902352e-03 -3.54923820e-03 -3.16689839e-03\n",
      "  -3.87213193e-03 -5.89371950e-04 -9.37364355e-04 -3.64246482e-08\n",
      "  -6.05122698e-03 -1.10890309e-03]]\n",
      "4 Train Loss 2.964757 Test MSE 391.42081314649835 Test RE 1.0085200575749147\n",
      "[[-2.3941755e-06 -1.7346779e-02 -4.9475757e-03  5.3512468e-04\n",
      "  -1.6734580e-03  6.4028168e-01 -1.2535847e-02 -5.3615000e-05\n",
      "  -8.3945505e-03 -8.7567996e-03 -2.4905703e-03 -3.3874580e-04\n",
      "  -1.3160551e-02 -3.2207458e-03 -2.5130999e-03 -4.0862109e-03\n",
      "  -7.6040947e-03 -3.1326909e-03 -9.8404987e-04  3.1519879e-02\n",
      "  -3.5887715e-05 -3.2414701e-03  1.6472044e-03 -1.3386635e-02\n",
      "   3.0991974e-01 -1.3458168e-02 -3.9831721e-03  2.7105322e-01\n",
      "  -4.0728028e-04 -2.0734090e-04 -5.1158253e-04 -1.7664466e-05\n",
      "  -3.5270441e-03 -3.7265248e-03  5.1019192e-02 -1.6725572e-02\n",
      "  -7.5740688e-03 -1.4771030e-10  1.1054606e-02 -1.8676446e-03\n",
      "  -2.3100732e-04 -3.5235258e-03 -6.3324803e-03 -5.9935860e-03\n",
      "  -7.3494203e-03 -1.2445761e-03 -1.5676736e-03 -8.4871409e-08\n",
      "  -1.3309003e-02 -1.8856662e-03]]\n",
      "5 Train Loss 2.8360186 Test MSE 389.6870986426155 Test RE 1.0062840672698967\n",
      "[[-3.22736423e-06 -1.45938685e-02 -5.40333474e-03  4.64745943e-04\n",
      "  -7.21686101e-03  8.09970140e-01 -1.32441595e-02 -8.90343872e-05\n",
      "  -9.35031567e-03 -1.13097969e-02 -2.66251899e-03 -5.80330903e-04\n",
      "  -1.60663053e-02 -3.32703372e-03 -3.39552970e-03 -3.98651091e-03\n",
      "  -1.00472532e-02 -4.32301406e-03 -1.46510138e-03  5.21885194e-02\n",
      "  -3.07333976e-05 -9.29823611e-03  1.89758663e-03 -1.62577052e-02\n",
      "   2.42877394e-01 -1.66277327e-02 -3.82535020e-03  2.30750322e-01\n",
      "  -4.93892760e-04 -2.26816061e-04 -6.60796999e-04 -2.20888742e-05\n",
      "  -3.71419056e-03 -3.68693797e-03  9.14029777e-02 -1.99616849e-02\n",
      "  -8.30122828e-03  4.29217948e-07  1.86467264e-02 -1.98730291e-03\n",
      "  -2.53373152e-03 -4.48853336e-03 -6.53460668e-03 -7.45785516e-03\n",
      "  -9.37114656e-03 -1.63263467e-03 -1.07263576e-03 -1.29906184e-07\n",
      "  -1.14804497e-02 -2.05261656e-03]]\n",
      "6 Train Loss 2.6103559 Test MSE 386.7986830110188 Test RE 1.0025477713239133\n",
      "[[-4.74868648e-06 -8.10048822e-03 -6.60026772e-03  1.10627545e-04\n",
      "  -3.82429846e-02  1.50917804e+00 -1.51464483e-02 -1.87694619e-04\n",
      "  -1.17901079e-02 -1.78081375e-02 -2.43207579e-03 -9.08407790e-04\n",
      "  -2.53223646e-02 -3.02595948e-03 -5.83525095e-03 -3.16529325e-03\n",
      "  -1.60721820e-02 -7.39508262e-03 -2.78175552e-03  1.14625715e-01\n",
      "  -4.31711396e-06 -3.60777862e-02  3.20349936e-03 -2.49835644e-02\n",
      "   1.49480134e-01 -2.67501287e-02 -2.86522694e-03  1.62245393e-01\n",
      "  -5.97660139e-04 -1.31596302e-04 -6.18312450e-04 -2.66829029e-05\n",
      "  -3.62938782e-03 -3.00093996e-03  1.35630012e-01 -2.64826268e-02\n",
      "  -9.93948337e-03  5.81400927e-05  4.04470041e-02 -1.79005857e-03\n",
      "  -1.54692540e-02 -6.33399002e-03 -6.51018275e-03 -1.15896808e-02\n",
      "  -1.47733102e-02 -2.57491297e-03  2.89294461e-04 -2.56286455e-07\n",
      "  -6.80039730e-03 -1.88257149e-03]]\n",
      "7 Train Loss 2.5262516 Test MSE 390.4136235945093 Test RE 1.0072216785556138\n",
      "[[-8.08807999e-06 -3.32355760e-02 -9.07235499e-03  8.62904417e-04\n",
      "  -1.63109464e-04  1.73613274e+00 -1.86628252e-02 -2.31218975e-04\n",
      "  -1.51933935e-02 -2.05884911e-02 -3.37716844e-03 -1.42099848e-03\n",
      "  -1.84321292e-02 -5.37031470e-03 -6.98936777e-03 -6.10273937e-03\n",
      "  -1.97561998e-02 -8.77631269e-03 -3.34613118e-03  1.06788598e-01\n",
      "  -1.85828194e-05 -2.72289980e-02 -2.53984798e-03 -1.50258439e-02\n",
      "   2.86118150e-01 -1.82298478e-02 -4.90868511e-03  2.86719412e-01\n",
      "  -1.05138391e-03 -2.17255336e-04 -7.57735048e-04 -5.19605346e-05\n",
      "  -6.47502346e-03 -5.37033938e-03  1.52993724e-01 -1.60583723e-02\n",
      "  -1.40714915e-02  2.86627874e-05  3.47105823e-02 -2.86774617e-03\n",
      "  -7.51729216e-03 -7.47246202e-03 -1.16935978e-02 -1.31360935e-02\n",
      "  -1.61909033e-02 -3.58403474e-03 -4.13140282e-03 -3.46482040e-07\n",
      "  -2.79843528e-02 -2.64110113e-03]]\n",
      "8 Train Loss 2.5160425 Test MSE 391.3003848528609 Test RE 1.0083649001472255\n",
      "[[-9.2131722e-06 -3.7394755e-02 -9.5970724e-03  9.4457919e-04\n",
      "  -1.9805711e-05  1.9034007e+00 -1.9826215e-02 -2.6082605e-04\n",
      "  -1.6367834e-02 -2.2133397e-02 -3.5664972e-03 -1.5794814e-03\n",
      "  -1.8811101e-02 -5.8495677e-03 -7.6693562e-03 -6.6181556e-03\n",
      "  -2.1384429e-02 -9.5847715e-03 -3.7086634e-03  1.1431659e-01\n",
      "  -1.8733190e-05 -2.9884819e-02 -3.8644425e-03 -1.4922853e-02\n",
      "   3.0173519e-01 -1.8465899e-02 -5.2148532e-03  3.0210483e-01\n",
      "  -1.1781350e-03 -2.2526825e-04 -7.8543765e-04 -5.9058333e-05\n",
      "  -7.0976936e-03 -5.7907673e-03  1.5786785e-01 -1.5430179e-02\n",
      "  -1.5258541e-02  3.6138412e-05  3.6788259e-02 -3.0893742e-03\n",
      "  -8.2534188e-03 -7.9072909e-03 -1.2819554e-02 -1.4176814e-02\n",
      "  -1.7365742e-02 -3.9841714e-03 -4.8584398e-03 -3.9426405e-07\n",
      "  -3.1635989e-02 -2.7973375e-03]]\n",
      "9 Train Loss 2.5096953 Test MSE 391.739840700533 Test RE 1.0089309710258003\n",
      "[[-1.32730165e-05 -4.82207574e-02 -9.98672377e-03  1.07877795e-03\n",
      "  -5.13082232e-05  1.97395825e+00 -1.99175812e-02 -3.14925885e-04\n",
      "  -1.67641416e-02 -2.09221821e-02 -4.17620130e-03 -1.92108680e-03\n",
      "  -1.46859223e-02 -7.42687192e-03 -8.10864009e-03 -8.63733143e-03\n",
      "  -2.08317954e-02 -9.85579100e-03 -4.09563491e-03  1.32257268e-01\n",
      "  -3.94046001e-05 -2.42782384e-02 -1.66318156e-02 -1.13067161e-02\n",
      "   3.39763522e-01 -1.39938565e-02 -6.59566233e-03  3.51081401e-01\n",
      "  -1.61290134e-03 -3.22141364e-04 -9.36467783e-04 -8.86870766e-05\n",
      "  -8.89798626e-03 -7.40118558e-03  1.85934097e-01 -1.39250597e-02\n",
      "  -1.63618755e-02  2.66568877e-05  4.44226675e-02 -3.90874967e-03\n",
      "  -7.31417025e-03 -8.02569650e-03 -1.53738866e-02 -1.38775259e-02\n",
      "  -1.63945686e-02 -4.69801947e-03 -1.06474506e-02 -5.19609216e-07\n",
      "  -4.24608253e-02 -3.34354327e-03]]\n",
      "10 Train Loss 2.472881 Test MSE 389.6986394131236 Test RE 1.0062989679524503\n",
      "[[-4.9272290e-05 -7.1479566e-02 -8.8473475e-03 -6.2959767e-03\n",
      "  -3.3521894e-04  1.9059181e+00 -1.5497244e-02 -5.6728302e-04\n",
      "  -1.3386830e-02 -7.1462025e-03 -8.4220348e-03 -2.7873712e-03\n",
      "  -1.7571554e-03 -1.6690979e-02 -7.5265304e-03 -2.1254301e-02\n",
      "  -8.6358227e-03 -7.0727640e-03 -4.7055213e-03  2.1934620e-01\n",
      "  -4.4495569e-04 -8.9463784e-04 -1.4652407e-01 -2.2785135e-03\n",
      "   3.1005299e-01 -1.2739478e-03 -1.6154569e-02  4.1047320e-01\n",
      "  -4.7767940e-03 -1.5791496e-03 -2.5884751e-03 -3.6138610e-04\n",
      "  -1.8296042e-02 -1.7647855e-02  3.6533514e-01 -2.0349106e-02\n",
      "  -1.6414935e-02  3.6394641e-07  9.7661972e-02 -9.6993735e-03\n",
      "  -2.8635433e-03 -7.1393386e-03 -2.4968883e-02 -8.0003599e-03\n",
      "  -6.4014271e-03 -7.7152532e-03 -6.2499817e-02 -1.4911774e-06\n",
      "  -7.7372909e-02 -7.3992130e-03]]\n",
      "11 Train Loss 2.4617217 Test MSE 388.6407827848522 Test RE 1.004932215119529\n",
      "[[-7.87619210e-05 -6.98735490e-02 -7.97094125e-03 -1.12591144e-02\n",
      "  -8.87996110e-04  2.15623903e+00 -1.47253983e-02 -7.71496096e-04\n",
      "  -1.24461707e-02 -4.53012669e-03 -1.00574745e-02 -3.38945165e-03\n",
      "  -5.56069310e-04 -2.06501521e-02 -7.65860360e-03 -2.66735610e-02\n",
      "  -6.09453674e-03 -6.52479660e-03 -5.29966876e-03  2.61848480e-01\n",
      "  -7.50081497e-04 -5.64024995e-05 -2.14708060e-01 -1.38019805e-03\n",
      "   2.47483075e-01 -3.11391545e-04 -2.01243348e-02  3.53334874e-01\n",
      "  -6.68742787e-03 -2.36520101e-03 -3.40560311e-03 -5.71407320e-04\n",
      "  -2.22538523e-02 -2.19794456e-02  4.81572717e-01 -3.17766257e-02\n",
      "  -1.64716449e-02  8.98372647e-08  1.35717481e-01 -1.23382201e-02\n",
      "  -2.40914547e-03 -6.52930560e-03 -2.86799427e-02 -6.57157553e-03\n",
      "  -4.34874697e-03 -9.37857386e-03 -8.97160918e-02 -2.32831394e-06\n",
      "  -8.58443901e-02 -9.10382625e-03]]\n",
      "12 Train Loss 2.4396636 Test MSE 386.08096675003685 Test RE 1.0016172111078876\n",
      "[[-1.24671773e-04 -1.97183806e-02 -5.62103139e-03 -1.55401910e-02\n",
      "  -1.58284570e-03  2.11367798e+00 -1.53659163e-02 -9.45969194e-04\n",
      "  -9.62247886e-03 -1.87940791e-03 -9.65190027e-03 -3.30711971e-03\n",
      "  -1.83049851e-04 -1.99476741e-02 -5.71386237e-03 -2.67433599e-02\n",
      "  -2.83601950e-03 -4.24381904e-03 -4.61076945e-03  3.39232802e-01\n",
      "  -1.25107833e-03 -4.08823974e-03 -2.37060025e-01 -2.20543845e-03\n",
      "   1.93879873e-01 -7.25230784e-05 -2.05073692e-02  2.06383482e-01\n",
      "  -7.75741693e-03 -3.28961317e-03 -3.95851908e-03 -8.59875290e-04\n",
      "  -2.09592078e-02 -2.18287185e-02  6.01490676e-01 -1.04514390e-01\n",
      "  -1.34233693e-02 -6.54895582e-10  2.02788681e-01 -1.24618607e-02\n",
      "  -1.10913720e-03 -4.48136497e-03 -2.63442360e-02 -3.84531124e-03\n",
      "  -1.93226908e-03 -8.84460937e-03 -1.00927636e-01 -3.65982123e-06\n",
      "  -5.31181321e-02 -9.04190354e-03]]\n",
      "13 Train Loss 2.4209867 Test MSE 385.1885360588378 Test RE 1.0004589163912985\n",
      "[[-1.40660661e-04  9.37902778e-02 -3.85567360e-03 -1.14778541e-02\n",
      "  -3.81268486e-02  1.94764423e+00 -2.05092095e-02 -9.72716836e-04\n",
      "  -8.68845731e-03 -2.11613323e-03 -6.45001326e-03 -3.06524732e-03\n",
      "  -6.99058932e-04 -1.42518161e-02 -4.23699943e-03 -1.98338404e-02\n",
      "  -2.88101076e-03 -3.32419155e-03 -3.76422121e-03  4.39086974e-01\n",
      "  -1.26067968e-03 -1.91995560e-03 -1.18614115e-01 -6.76436955e-03\n",
      "   1.69230893e-01 -4.76120040e-04 -1.51424073e-02  1.18376099e-01\n",
      "  -6.39336370e-03 -2.84544285e-03 -2.89118965e-03 -9.03909036e-04\n",
      "  -1.50621468e-02 -1.58948246e-02  6.20961785e-01 -1.94513276e-01\n",
      "  -1.13210455e-02  9.10499498e-09  2.44717702e-01 -8.74069519e-03\n",
      "  -1.99130015e-03 -2.66245520e-03 -2.05573719e-02 -3.01616755e-03\n",
      "  -1.79327826e-03 -6.67843223e-03 -6.76363111e-02 -4.29600323e-06\n",
      "   3.73685844e-02 -6.12161960e-03]]\n",
      "14 Train Loss 2.3895104 Test MSE 384.2291499806189 Test RE 0.9992122220198921\n",
      "[[-2.08024197e-04  4.51497912e-01 -1.91004016e-03 -4.81322873e-03\n",
      "  -8.72284472e-02  1.11598825e+00 -4.19563428e-02 -1.27155136e-03\n",
      "  -1.16448766e-02 -4.80538188e-03 -3.44989845e-03 -3.82179930e-03\n",
      "  -2.63181166e-03 -1.12143550e-02 -3.88852460e-03 -1.64543558e-02\n",
      "  -5.89916436e-03 -3.60182673e-03 -3.82661819e-03  7.03830957e-01\n",
      "  -1.43842876e-03 -5.89283789e-03  5.45167550e-02 -1.88633129e-02\n",
      "   1.55676261e-01 -2.22133473e-03 -1.15545420e-02  8.15512761e-02\n",
      "  -6.15297584e-03 -2.37990054e-03 -1.45240105e-03 -1.21180073e-03\n",
      "  -1.25048235e-02 -1.25089111e-02  9.01934147e-01 -1.39038086e-01\n",
      "  -1.37246894e-02  3.18949418e-07  2.80183494e-01 -5.89412358e-03\n",
      "  -6.77717663e-03 -7.19433709e-04 -2.06269566e-02 -3.30614811e-03\n",
      "  -2.82486179e-03 -5.94596379e-03 -2.19559558e-02 -6.68639632e-06\n",
      "   2.82699496e-01 -3.34743527e-03]]\n",
      "15 Train Loss 2.3783119 Test MSE 382.54870130123993 Test RE 0.997024771090529\n",
      "[[-2.7578996e-04  7.0712370e-01 -1.1284386e-03 -4.9829157e-04\n",
      "  -8.5222654e-02  3.6952147e-01 -5.5156056e-02 -1.5492706e-03\n",
      "  -1.3506173e-02 -5.5915625e-03 -2.8870094e-03 -4.5391852e-03\n",
      "  -2.4562613e-03 -1.2073029e-02 -3.9157066e-03 -1.8116860e-02\n",
      "  -7.0569878e-03 -3.6606966e-03 -4.1220202e-03  8.8099784e-01\n",
      "  -1.7902636e-03 -1.3013464e-02  9.3632944e-02 -2.2267170e-02\n",
      "   1.4996338e-01 -1.9684427e-03 -1.2080945e-02  7.4668221e-02\n",
      "  -7.1596620e-03 -2.5779095e-03 -1.1276943e-03 -1.5633977e-03\n",
      "  -1.3742026e-02 -1.3408936e-02  1.1116996e+00  9.6723408e-04\n",
      "  -1.5952701e-02  3.5531360e-07  2.8778243e-01 -5.8645634e-03\n",
      "  -7.6979897e-03 -1.9492344e-04 -2.3664769e-02 -3.1977850e-03\n",
      "  -2.8009331e-03 -6.5030302e-03 -1.0989243e-02 -8.9186233e-06\n",
      "   4.4640523e-01 -2.8698253e-03]]\n",
      "16 Train Loss 2.3743057 Test MSE 381.5734978150717 Test RE 0.9957531389101185\n",
      "[[-3.08346120e-04  8.23579073e-01 -8.30161676e-04  1.45199778e-03\n",
      "  -7.31485039e-02  8.47484097e-02 -6.15513027e-02 -1.72131369e-03\n",
      "  -1.46566937e-02 -6.33135578e-03 -2.60872417e-03 -5.20589761e-03\n",
      "  -1.89227436e-03 -1.31388111e-02 -4.24603838e-03 -2.00083256e-02\n",
      "  -8.25606566e-03 -4.04183567e-03 -4.56661917e-03  9.71547127e-01\n",
      "  -1.88826735e-03 -1.59614589e-02  1.03361674e-01 -2.11158264e-02\n",
      "   1.38741627e-01 -1.39197486e-03 -1.26554435e-02  8.30586255e-02\n",
      "  -7.91733153e-03 -2.54558190e-03 -8.86036956e-04 -1.74253737e-03\n",
      "  -1.52377822e-02 -1.44465668e-02  1.22271490e+00  1.01757608e-01\n",
      "  -1.76797640e-02  7.68404220e-07  2.93043405e-01 -5.99816162e-03\n",
      "  -8.29991885e-03 -5.79690095e-05 -2.71276329e-02 -3.29260039e-03\n",
      "  -2.94120796e-03 -7.19225081e-03 -1.46997916e-02 -9.97448569e-06\n",
      "   5.21528900e-01 -2.63146800e-03]]\n",
      "17 Train Loss 2.3480918 Test MSE 374.26714079104903 Test RE 0.986173736257903\n",
      "[[-5.42289286e-04  1.63737285e+00 -3.24215580e-05  2.46954579e-02\n",
      "  -2.73996163e-02 -2.11835003e+00 -1.09870881e-01 -2.25036335e-03\n",
      "  -1.62831955e-02 -2.08303099e-03 -1.99282519e-03 -6.28297310e-03\n",
      "  -3.70768510e-04 -2.14518849e-02 -2.62227096e-03 -3.90455015e-02\n",
      "  -4.37770365e-03 -1.75939966e-03 -3.95530695e-03  1.33431268e+00\n",
      "  -3.50153027e-03 -5.14660031e-04  1.55247003e-01 -1.05191255e-02\n",
      "   7.27516413e-02 -8.07900797e-04 -2.07744129e-02  1.27064973e-01\n",
      "  -1.24776484e-02 -3.82129196e-03 -5.55161969e-04 -3.01202922e-03\n",
      "  -2.58026179e-02 -2.47490164e-02  1.55991673e+00  6.21763289e-01\n",
      "  -2.45173872e-02 -1.66364202e-07  3.36580783e-01 -7.71939615e-03\n",
      "  -2.19591789e-06 -5.14770159e-04 -5.59005588e-02 -7.59448041e-04\n",
      "  -2.25734679e-04 -8.90106987e-03 -8.33769515e-02 -1.70173844e-05\n",
      "   1.04221606e+00 -2.23062211e-03]]\n",
      "18 Train Loss 2.326845 Test MSE 371.77260592271966 Test RE 0.9828817592940984\n",
      "[[-5.9492758e-04  1.8896030e+00 -7.0528349e-06  3.3909038e-02\n",
      "  -1.4737406e-02 -2.5325291e+00 -1.3564910e-01 -2.2247417e-03\n",
      "  -1.6618878e-02 -8.0659206e-04 -1.9811173e-03 -5.8035324e-03\n",
      "  -1.2550962e-03 -2.4771964e-02 -1.7742921e-03 -4.8663460e-02\n",
      "  -2.4592285e-03 -8.8347693e-04 -3.2201894e-03  1.3537990e+00\n",
      "  -4.0693674e-03 -2.9163475e-03  1.7501272e-01 -7.6124961e-03\n",
      "   4.7713671e-02 -1.9059201e-03 -2.5062760e-02  1.4356762e-01\n",
      "  -1.3417463e-02 -4.4316123e-03 -6.5562333e-04 -3.2921042e-03\n",
      "  -3.0024266e-02 -2.9624153e-02  1.5372036e+00  7.8310311e-01\n",
      "  -2.7310126e-02 -5.8647545e-09  3.9100081e-01 -8.2720667e-03\n",
      "  -1.9584082e-03 -7.7508064e-04 -7.1113713e-02 -2.1450715e-04\n",
      "  -7.8105572e-08 -8.7117087e-03 -1.1308165e-01 -1.8499803e-05\n",
      "   1.1942673e+00 -2.2675293e-03]]\n",
      "19 Train Loss 2.252415 Test MSE 357.88228003947705 Test RE 0.9643455489779043\n",
      "[[-7.6182850e-04  3.4745405e+00 -1.9162744e-05  9.5262051e-02\n",
      "  -3.5081645e-03 -5.3044062e+00 -2.6701480e-01 -1.9136984e-03\n",
      "  -1.7849565e-02 -3.0401505e-05 -5.0965481e-04 -3.2972042e-03\n",
      "  -5.4370603e-03 -2.6302921e-02 -6.5193399e-07 -6.5642364e-02\n",
      "  -1.2128852e-04 -2.0444269e-04 -6.3937414e-04  6.3061219e-01\n",
      "  -5.4455218e-03 -4.2081565e-02  2.9976106e-01 -7.1489764e-03\n",
      "   1.1059025e-02 -6.7259697e-03 -3.0710297e-02  9.5623188e-02\n",
      "  -1.3012680e-02 -5.3554531e-03 -3.6776275e-04 -3.9466126e-03\n",
      "  -3.3243582e-02 -3.5470277e-02  7.7836406e-01  1.8062677e+00\n",
      "  -3.3784892e-02  7.9961210e-06  7.4557811e-01 -5.8858232e-03\n",
      "  -3.4689385e-02 -2.5341976e-03 -1.0295147e-01 -6.8239641e-04\n",
      "  -1.1835862e-03 -5.1038726e-03 -1.5111713e-02 -2.4094199e-05\n",
      "   2.1623569e+00 -7.5708976e-04]]\n",
      "20 Train Loss 2.162015 Test MSE 336.3276195080988 Test RE 0.9348541261922337\n",
      "[[-1.17410719e-03  5.96025944e+00  1.59280212e-03  1.88057557e-01\n",
      "  -5.63448062e-04 -8.83426762e+00 -4.81881291e-01 -2.23580073e-03\n",
      "  -2.25917213e-02 -1.99711925e-04 -1.40968323e-05 -3.09096556e-03\n",
      "  -1.25720035e-02 -3.21319923e-02 -6.93151262e-04 -9.10873562e-02\n",
      "  -2.04962433e-07 -1.67618610e-03 -4.72826359e-05 -1.82111382e-01\n",
      "  -7.61487568e-03 -5.38203754e-02  3.31148475e-01 -6.72115618e-03\n",
      "   4.47294442e-03 -1.43915452e-02 -3.80515195e-02  6.63269609e-02\n",
      "  -1.61117110e-02 -6.27655396e-03 -1.42837996e-06 -5.66392299e-03\n",
      "  -4.24711369e-02 -4.52149250e-02  9.80448723e-02  3.81035113e+00\n",
      "  -4.61263694e-02  3.42360909e-05  8.00348401e-01 -4.80908854e-03\n",
      "  -9.55261886e-02 -3.94577254e-03 -1.53986216e-01 -2.80875806e-03\n",
      "  -2.20142747e-03 -4.38473700e-03  1.12484261e-01 -3.83092738e-05\n",
      "   3.61260176e+00 -1.45789609e-05]]\n",
      "21 Train Loss 2.089968 Test MSE 326.8011430790788 Test RE 0.921519152788739\n",
      "[[-1.2397845e-03  5.9924903e+00  2.8367492e-03  2.0813060e-01\n",
      "  -1.9416946e-03 -7.9931941e+00 -5.1968992e-01 -1.9569120e-03\n",
      "  -1.9367794e-02 -4.2904651e-04 -1.8158549e-04 -1.9910913e-03\n",
      "  -1.6637154e-02 -2.7516905e-02 -1.8217309e-03 -8.4632479e-02\n",
      "  -7.3358889e-05 -3.0726811e-03 -5.9486381e-05 -6.4796621e-01\n",
      "  -8.0580739e-03 -6.6206358e-02  2.5889593e-01 -8.2567306e-03\n",
      "   4.3449043e-03 -1.9419199e-02 -3.4622204e-02  6.9156170e-02\n",
      "  -1.4927982e-02 -6.4540766e-03 -2.4611130e-05 -5.8360286e-03\n",
      "  -3.6503728e-02 -4.0491078e-02 -3.2828823e-01  4.2930918e+00\n",
      "  -4.1125070e-02  6.7481014e-05  6.3506436e-01 -3.5678511e-03\n",
      "  -1.2589198e-01 -3.5985191e-03 -1.4390858e-01 -4.3884558e-03\n",
      "  -2.9415754e-03 -2.7875802e-03  2.0663995e-01 -4.0553528e-05\n",
      "   3.5398622e+00 -2.6818978e-05]]\n",
      "22 Train Loss 1.91342 Test MSE 283.1526894779158 Test RE 0.8577740984794502\n",
      "[[-1.9613607e-03  7.1517243e+00  4.0670205e-02  3.7267822e-01\n",
      "  -1.5202747e-04 -1.1620414e+01 -1.1707202e+00 -1.9680900e-03\n",
      "  -1.7741075e-02  1.6744613e-03 -6.6387486e-03 -1.3775810e-03\n",
      "  -3.8093425e-02 -2.9288242e-02 -9.1045853e-03 -1.2627596e-01\n",
      "   2.9095472e-04 -8.5718092e-03 -2.3536398e-03 -2.2221613e+00\n",
      "  -1.0758623e-02 -8.3536379e-02  2.6387921e-01 -2.2036436e-03\n",
      "   9.9011837e-04 -3.9980885e-02 -3.7278764e-02  4.3327250e-02\n",
      "  -1.7538346e-02 -5.7240189e-03 -4.1614845e-03 -8.2748746e-03\n",
      "  -4.3841790e-02 -4.7684118e-02 -9.5057845e-01  8.4964304e+00\n",
      "  -5.2188188e-02  1.6073466e-04  4.2788142e-01 -6.7280745e-04\n",
      "  -2.9963964e-01  3.3327725e-02 -2.8256997e-01 -1.0301926e-02\n",
      "   3.2974684e-03 -8.4649003e-04  4.5160660e-01 -6.8116678e-05\n",
      "   4.5162215e+00 -4.6999911e-03]]\n",
      "23 Train Loss 1.6615032 Test MSE 249.01883410748908 Test RE 0.8044122585579061\n",
      "[[-3.2236285e-03  5.6896925e+00  2.2910962e-01  5.0637990e-01\n",
      "  -2.9623587e-08 -1.2831123e+01 -2.2893159e+00 -3.0381132e-03\n",
      "  -1.5265758e-02  6.8278830e-03 -2.9827593e-02 -3.5174096e-03\n",
      "  -3.2831702e-02 -3.4951478e-02 -1.8852409e-02 -2.0101106e-01\n",
      "   9.2100515e-04 -1.3499428e-02 -5.6799934e-03 -4.4035616e+00\n",
      "  -1.4151846e-02 -1.3134105e-02  3.9665908e-01 -1.0150559e-03\n",
      "   2.6939315e-04 -1.9660583e-02 -3.7247337e-02  2.6694426e-02\n",
      "  -2.4084868e-02 -3.3365672e-03 -2.8187165e-02 -1.2026481e-02\n",
      "  -6.1351627e-02 -5.8520842e-02 -2.0957725e+00  1.5600534e+01\n",
      "  -6.7972071e-02  8.6695240e-05  1.2960945e-01 -2.7867183e-04\n",
      "  -4.2266726e-01  2.7124622e-01 -5.8174163e-01 -8.5644247e-03\n",
      "   2.5790222e-02 -4.7223529e-04  4.9002475e-01 -1.2430872e-04\n",
      "   4.6070251e+00 -2.5025243e-02]]\n",
      "24 Train Loss 1.4903423 Test MSE 213.53006863725565 Test RE 0.7448899285851864\n",
      "[[-4.0108417e-03  6.5314751e+00  3.2636720e-01  5.5880070e-01\n",
      "   2.4768218e-04 -1.6294495e+01 -2.6242788e+00 -3.5622406e-03\n",
      "  -1.7983155e-02  8.8627981e-03 -4.5120236e-02 -4.5489529e-03\n",
      "  -2.3674039e-02 -4.1087139e-02 -2.5749216e-02 -2.5357494e-01\n",
      "   1.0022374e-03 -1.7516611e-02 -8.2265632e-03 -4.9234695e+00\n",
      "  -1.6220262e-02 -6.8733427e-03  4.5010072e-01 -4.5548072e-03\n",
      "   8.8878078e-05  2.3184796e-03 -4.1932598e-02  1.8516541e-02\n",
      "  -2.7588084e-02 -2.7864536e-03 -4.4096779e-02 -1.4346474e-02\n",
      "  -7.5868160e-02 -6.9777146e-02 -2.4047637e+00  1.8769297e+01\n",
      "  -8.6487845e-02  8.0588739e-05  1.1260256e-01 -1.1682748e-03\n",
      "  -5.0996584e-01  4.0858677e-01 -7.1568543e-01 -7.8748502e-03\n",
      "   3.7482105e-02 -3.0504467e-04  7.3535514e-01 -1.5877174e-04\n",
      "   5.5232959e+00 -3.9034538e-02]]\n",
      "25 Train Loss 1.4371336 Test MSE 199.08729495566305 Test RE 0.7192574275639221\n",
      "[[-4.30806773e-03  6.91697454e+00  3.63156438e-01  5.76478839e-01\n",
      "   6.79488585e-04 -1.70439701e+01 -2.86127520e+00 -3.55559867e-03\n",
      "  -1.96419433e-02  1.22545753e-02 -5.23079596e-02 -4.13443241e-03\n",
      "  -2.91620065e-02 -4.34476994e-02 -2.99581196e-02 -2.77959764e-01\n",
      "   1.85695325e-03 -1.84370838e-02 -1.04274042e-02 -4.86925602e+00\n",
      "  -1.69092342e-02 -9.12587997e-03  3.99173856e-01 -5.62981283e-03\n",
      "   6.14079545e-05  3.16014251e-04 -4.53709364e-02  1.45456884e-02\n",
      "  -2.81841662e-02 -2.85029039e-03 -4.94396389e-02 -1.50410496e-02\n",
      "  -8.13230351e-02 -7.53947124e-02 -2.38902998e+00  1.95593204e+01\n",
      "  -9.56714451e-02  1.15909752e-04  1.19726673e-01 -1.72080169e-03\n",
      "  -5.91542900e-01  4.60368782e-01 -7.79523313e-01 -6.35724189e-03\n",
      "   4.83889058e-02 -1.19788645e-04  1.03208244e+00 -1.73279637e-04\n",
      "   5.73836851e+00 -4.52952236e-02]]\n",
      "26 Train Loss 1.1330322 Test MSE 158.87682264477272 Test RE 0.6425291797470756\n",
      "[[-4.9127527e-03  7.4431829e+00  4.5176563e-01  6.4759028e-01\n",
      "   1.7193895e-03 -2.1800978e+01 -3.2180183e+00 -3.0086518e-03\n",
      "  -2.3398226e-02  2.6904372e-02 -8.1792295e-02 -1.8212491e-03\n",
      "  -1.4010485e-01 -4.2059854e-02 -4.7866780e-02 -3.5689208e-01\n",
      "   8.3589889e-03 -1.9970044e-02 -2.1798464e-02 -2.7085671e+00\n",
      "  -1.7361159e-02 -1.6232396e-02  1.6695872e-01 -5.7542850e-03\n",
      "   0.0000000e+00 -1.3097998e-01 -5.1762354e-02  6.2965681e-03\n",
      "  -2.5323583e-02 -2.7150984e-03 -6.4351536e-02 -1.5736468e-02\n",
      "  -8.6717635e-02 -8.5387550e-02 -1.5315703e+00  2.0943157e+01\n",
      "  -1.2867020e-01  3.6683129e-04  1.4653867e-01 -6.3448888e-03\n",
      "  -7.7296633e-01  5.6802142e-01 -9.4435662e-01 -3.7933469e-03\n",
      "   7.7545956e-02 -8.9798961e-04  2.5550268e+00 -2.1140449e-04\n",
      "   5.7382531e+00 -7.1031377e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:141: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Train Loss 1.0449748 Test MSE 153.39905688493852 Test RE 0.6313554406201205\n",
      "[[-4.9252389e-03  7.5978370e+00  4.5596126e-01  6.5190256e-01\n",
      "   1.9519839e-03 -2.3256617e+01 -3.1723595e+00 -2.8925282e-03\n",
      "  -2.3122078e-02  2.6308492e-02 -8.5385166e-02 -1.6532065e-03\n",
      "  -1.5852062e-01 -3.9957549e-02 -5.0489601e-02 -3.5863721e-01\n",
      "   8.4498683e-03 -2.2476982e-02 -2.3115916e-02 -2.4721789e+00\n",
      "  -1.7252292e-02 -1.7311320e-02  1.5598916e-01 -6.4433101e-03\n",
      "   0.0000000e+00 -1.5457857e-01 -5.0237492e-02  5.8728186e-03\n",
      "  -2.4348518e-02 -2.4663180e-03 -6.6174112e-02 -1.5694585e-02\n",
      "  -8.4378541e-02 -8.3403356e-02 -1.4150392e+00  2.1763472e+01\n",
      "  -1.3092247e-01  3.6583186e-04  1.5774089e-01 -7.4861581e-03\n",
      "  -7.3898453e-01  5.6360978e-01 -9.5510083e-01 -6.9076754e-03\n",
      "   7.3072106e-02 -1.3079817e-03  2.6439154e+00 -2.1246519e-04\n",
      "   5.8560796e+00 -7.4438237e-02]]\n",
      "28 Train Loss 1.0291268 Test MSE 144.3002170810635 Test RE 0.6123448571859103\n",
      "[[-5.2407989e-03  7.6328683e+00  5.0678861e-01  6.5156293e-01\n",
      "   2.4312867e-03 -2.3294567e+01 -3.4022546e+00 -2.9472152e-03\n",
      "  -2.2921018e-02  3.0708514e-02 -9.3790621e-02 -1.6441069e-03\n",
      "  -1.6615179e-01 -4.0953882e-02 -5.4677647e-02 -3.8244939e-01\n",
      "   1.0016088e-02 -2.3265094e-02 -2.5519656e-02 -2.4534526e+00\n",
      "  -1.7810347e-02 -1.5863705e-02  1.5574773e-01 -8.6249551e-03\n",
      "   0.0000000e+00 -1.5928183e-01 -5.1248778e-02  5.0071361e-03\n",
      "  -2.5052423e-02 -2.3346727e-03 -7.3155969e-02 -1.6359804e-02\n",
      "  -8.8113233e-02 -8.6513974e-02 -1.3595706e+00  2.2081335e+01\n",
      "  -1.3735005e-01  3.8956702e-04  1.4422688e-01 -8.6608743e-03\n",
      "  -7.7371079e-01  6.3062441e-01 -1.0387697e+00 -4.9151946e-03\n",
      "   8.4093854e-02 -1.5846149e-03  2.8088388e+00 -2.2938522e-04\n",
      "   5.9392395e+00 -8.1940308e-02]]\n",
      "29 Train Loss 0.996274 Test MSE 131.21288335506696 Test RE 0.583916600264851\n",
      "[[-5.7819141e-03  7.6035271e+00  5.9373206e-01  5.5982924e-01\n",
      "   2.9969881e-03 -2.2889362e+01 -3.6111443e+00 -2.7129280e-03\n",
      "  -1.5061450e-02  4.9345616e-02 -1.0738183e-01 -1.2794657e-03\n",
      "  -1.6564712e-01 -3.6687844e-02 -6.2467113e-02 -4.4911522e-01\n",
      "   1.9038456e-02 -2.4882650e-02 -3.0917820e-02 -2.4973977e+00\n",
      "  -1.8800417e-02 -1.8199613e-02  1.7609392e-01 -1.7203636e-02\n",
      "   0.0000000e+00 -1.5298310e-01 -4.7185723e-02  4.1491399e-03\n",
      "  -2.5329106e-02 -1.9185501e-03 -8.4807768e-02 -1.7477261e-02\n",
      "  -8.7290309e-02 -8.5205801e-02 -1.2228587e+00  2.2454794e+01\n",
      "  -1.4140205e-01  4.1044212e-04  1.2906097e-01 -1.1761882e-02\n",
      "  -7.9054880e-01  6.7587453e-01 -1.2990506e+00  1.6537276e-03\n",
      "   1.0669980e-01 -2.7217697e-03  2.8368237e+00 -2.5727067e-04\n",
      "   6.1063328e+00 -9.4418816e-02]]\n",
      "30 Train Loss 0.7746752 Test MSE 109.59534837725509 Test RE 0.5336526167101301\n",
      "[[-7.2478461e-03  7.1271133e+00  8.1906736e-01  2.7031136e-01\n",
      "   3.6357623e-03 -1.9348576e+01 -3.8122237e+00 -1.6183646e-03\n",
      "  -8.8352230e-05  1.7167333e-01 -1.3926119e-01 -2.2726896e-04\n",
      "  -1.1987634e-01 -1.7668141e-02 -8.5801519e-02 -6.9686526e-01\n",
      "   9.7637661e-02 -3.3130530e-02 -4.8648968e-02 -2.8737381e+00\n",
      "  -2.1169972e-02 -4.2699859e-02  3.0495843e-01 -6.8671249e-02\n",
      "   0.0000000e+00 -9.2116065e-02 -2.7600605e-02  3.4024017e-03\n",
      "  -2.3601232e-02 -6.5520679e-04 -1.1284085e-01 -2.0278461e-02\n",
      "  -6.9267601e-02 -6.7019232e-02 -8.3923787e-01  2.3913271e+01\n",
      "  -1.2780420e-01  3.7824619e-04  1.1870620e-01 -2.4224030e-02\n",
      "  -6.9451618e-01  6.4143687e-01 -2.3386600e+00  2.9272433e-02\n",
      "   1.8539891e-01 -9.0998691e-03 -3.2566470e-01 -3.3294788e-04\n",
      "   6.4239202e+00 -1.2445708e-01]]\n",
      "31 Train Loss 0.657937 Test MSE 90.56211395004507 Test RE 0.4851051198947241\n",
      "[[-7.98968505e-03  6.61655855e+00  9.40514743e-01  1.58092022e-01\n",
      "   3.83162149e-03 -1.71086731e+01 -4.04248381e+00 -6.41343649e-04\n",
      "   2.33849896e-05  3.33489686e-01 -1.61209881e-01 -1.15817122e-04\n",
      "  -1.08650208e-01 -6.22400129e-03 -1.05648644e-01 -8.77466917e-01\n",
      "   2.24415883e-01 -3.89086530e-02 -6.66248500e-02 -2.74465871e+00\n",
      "  -2.18976531e-02 -7.33238906e-02  4.02076304e-01 -1.20435812e-01\n",
      "   0.00000000e+00 -8.45081508e-02 -1.40953260e-02  3.54074454e-03\n",
      "  -1.99192427e-02 -7.56074296e-05 -1.30839646e-01 -2.12617423e-02\n",
      "  -4.83226031e-02 -4.70030010e-02 -4.43818718e-01  2.82337646e+01\n",
      "  -1.02343179e-01  3.89960478e-04  1.32637560e-01 -3.78756411e-02\n",
      "  -6.14794731e-01  5.87611973e-01 -3.23305583e+00  5.69386184e-02\n",
      "   2.59484500e-01 -1.85014233e-02 -4.26659107e+00 -3.73965479e-04\n",
      "   6.57968807e+00 -1.45946115e-01]]\n",
      "32 Train Loss 0.5882161 Test MSE 72.46350291045864 Test RE 0.4339325660905112\n",
      "[[-8.6198617e-03  6.1222725e+00  1.0419316e+00  1.2845227e-01\n",
      "   4.3505942e-03 -1.9094362e+01 -4.4271011e+00 -3.2541811e-04\n",
      "   1.4373157e-03  4.2130810e-01 -1.8615994e-01 -3.8362134e-04\n",
      "  -1.2327977e-01 -2.2710336e-03 -1.2382198e-01 -9.6733952e-01\n",
      "   2.9357302e-01 -5.2966323e-02 -7.9027593e-02 -2.7931471e+00\n",
      "  -2.2200003e-02 -6.8347313e-02  4.6814594e-01 -1.5619430e-01\n",
      "   0.0000000e+00 -1.0315041e-01 -7.6005389e-03  3.1035077e-03\n",
      "  -1.7897615e-02 -7.0838032e-05 -1.5622392e-01 -2.2157030e-02\n",
      "  -3.7287612e-02 -3.4910105e-02 -3.4743592e-01  3.2782093e+01\n",
      "  -8.6762629e-02  3.0244997e-04  1.2283844e-01 -5.1311154e-02\n",
      "  -5.1685357e-01  6.5091926e-01 -3.8331301e+00  6.5352343e-02\n",
      "   2.9264727e-01 -2.5686987e-02 -5.5016661e+00 -4.1100185e-04\n",
      "   6.5805917e+00 -1.7131709e-01]]\n",
      "33 Train Loss 0.5409967 Test MSE 65.58049439678187 Test RE 0.4128097305587436\n",
      "[[-9.0884576e-03  5.4478736e+00  1.0873790e+00  1.0812120e-01\n",
      "   4.1089766e-03 -2.0218859e+01 -4.6614647e+00 -1.7465057e-04\n",
      "   5.1968875e-03  4.9547470e-01 -2.0652917e-01 -5.9475529e-04\n",
      "  -1.4793493e-01 -5.7130423e-04 -1.3843435e-01 -1.0620710e+00\n",
      "   3.5511014e-01 -6.6845313e-02 -8.7872989e-02 -2.8454642e+00\n",
      "  -2.2093704e-02 -6.1804730e-02  5.1145291e-01 -1.8219025e-01\n",
      "   0.0000000e+00 -1.3609329e-01 -4.0583429e-03  2.6817704e-03\n",
      "  -1.6192393e-02 -5.3469802e-04 -1.7741019e-01 -2.2642599e-02\n",
      "  -2.9758081e-02 -2.6731215e-02 -3.4598950e-01  3.6060730e+01\n",
      "  -7.6100849e-02  2.2730969e-04  1.0942943e-01 -6.3389048e-02\n",
      "  -4.3171731e-01  6.9066733e-01 -4.3505535e+00  6.7239836e-02\n",
      "   3.1167468e-01 -3.1872723e-02 -6.2650533e+00 -4.4333504e-04\n",
      "   6.2326593e+00 -1.9204517e-01]]\n",
      "34 Train Loss 0.51099086 Test MSE 60.95629701891194 Test RE 0.39798973496817486\n",
      "[[-9.29350033e-03  4.89260578e+00  1.01939571e+00  7.30233938e-02\n",
      "   3.33916582e-03 -2.20527382e+01 -4.30765104e+00 -1.51924712e-06\n",
      "   2.82573625e-02  6.10682726e-01 -2.21020654e-01 -1.42010441e-03\n",
      "  -1.71022266e-01 -3.98219127e-04 -1.53050408e-01 -1.27264667e+00\n",
      "   4.73201603e-01 -8.61436203e-02 -9.90746841e-02 -3.13987660e+00\n",
      "  -2.12128144e-02 -6.87475801e-02  6.74337327e-01 -2.13007942e-01\n",
      "   0.00000000e+00 -1.79977298e-01 -5.16841537e-04  2.68616527e-03\n",
      "  -1.24042453e-02 -2.22177710e-03 -1.96918234e-01 -2.26512179e-02\n",
      "  -1.55905765e-02 -1.29325371e-02 -3.11370075e-01  4.20985641e+01\n",
      "  -5.35367019e-02  1.13602546e-04  1.13780931e-01 -8.21731314e-02\n",
      "  -2.99913675e-01  5.58618069e-01 -4.92948627e+00  6.48578703e-02\n",
      "   3.06965917e-01 -4.38657328e-02 -8.84002018e+00 -4.55596164e-04\n",
      "   6.10880089e+00 -2.09946916e-01]]\n",
      "35 Train Loss 0.42150348 Test MSE 47.7720626211677 Test RE 0.35232996362840874\n",
      "[[-9.34761390e-03  4.28169537e+00  8.43787432e-01  4.21353765e-02\n",
      "   2.26984988e-03 -3.12240601e+01 -3.76615977e+00 -4.70648054e-04\n",
      "   1.40686631e-01  7.50442147e-01 -2.49998495e-01 -3.89494328e-03\n",
      "  -2.54866153e-01 -3.34097561e-03 -1.81189671e-01 -1.71750998e+00\n",
      "   6.50597870e-01 -1.24007247e-01 -1.21207640e-01 -3.57481527e+00\n",
      "  -1.86605901e-02 -7.55273029e-02  9.75146472e-01 -2.33580336e-01\n",
      "   0.00000000e+00 -3.07852089e-01  3.54288204e-04  2.63395160e-03\n",
      "  -6.00211788e-03 -8.85943603e-03 -2.36121118e-01 -2.21404545e-02\n",
      "  -1.90655305e-03 -6.89102046e-04 -2.34871015e-01  5.39046707e+01\n",
      "  -2.08630785e-02 -1.82794756e-05  1.26124665e-01 -1.23859510e-01\n",
      "  -1.36276871e-01  3.27115685e-01 -5.72573328e+00  4.19813059e-02\n",
      "   2.53442734e-01 -7.25204721e-02 -1.13742285e+01 -4.56568087e-04\n",
      "   6.01567030e+00 -2.47861609e-01]]\n",
      "36 Train Loss 0.40467906 Test MSE 42.67389415356102 Test RE 0.3329996065449708\n",
      "[[-9.4682621e-03  4.2955604e+00  8.5176772e-01  4.0514175e-02\n",
      "   2.3082655e-03 -3.1695251e+01 -3.8514237e+00 -5.8357720e-04\n",
      "   1.5030921e-01  7.8321815e-01 -2.5292826e-01 -4.3489337e-03\n",
      "  -2.5780231e-01 -3.3429703e-03 -1.8512915e-01 -1.7181969e+00\n",
      "   6.7893714e-01 -1.2747513e-01 -1.2439900e-01 -3.5665953e+00\n",
      "  -1.8715037e-02 -7.6485857e-02  1.0077362e+00 -2.3439665e-01\n",
      "   0.0000000e+00 -3.1167644e-01  5.1491015e-04  2.8094559e-03\n",
      "  -5.7376297e-03 -9.4366092e-03 -2.3921522e-01 -2.2345074e-02\n",
      "  -1.5199159e-03 -3.9107521e-04 -1.9850484e-01  5.4818817e+01\n",
      "  -1.4484857e-02 -2.2432572e-05  1.3073720e-01 -1.2669513e-01\n",
      "  -1.3294980e-01  3.4016001e-01 -5.8450584e+00  4.4207323e-02\n",
      "   2.6503846e-01 -7.4788064e-02 -1.1760864e+01 -4.6191883e-04\n",
      "   6.1134133e+00 -2.5077498e-01]]\n",
      "37 Train Loss 0.36734283 Test MSE 28.075406273107333 Test RE 0.2701006363092335\n",
      "[[-1.01105319e-02  3.48966813e+00  7.53642797e-01  2.80499682e-02\n",
      "   1.52228703e-03 -3.11748371e+01 -3.93459105e+00 -1.82179245e-03\n",
      "   2.81707108e-01  1.00157809e+00 -2.94007301e-01 -8.80152360e-03\n",
      "  -3.82557184e-01  1.50615873e-03 -2.22239494e-01 -2.03383636e+00\n",
      "   9.14631903e-01 -1.60810277e-01 -1.52714208e-01 -2.68653727e+00\n",
      "  -1.76832024e-02 -6.54772893e-02  8.88813317e-01 -2.85180479e-01\n",
      "   0.00000000e+00 -4.69093859e-01  5.23690740e-03  2.28831870e-03\n",
      "  -2.81768851e-03 -1.52940713e-02 -2.67182708e-01 -2.22954024e-02\n",
      "   2.70262797e-04  5.95059071e-04 -9.98956636e-02  5.87406502e+01\n",
      "   8.09577201e-03 -4.81695752e-05  1.21730439e-01 -1.64112240e-01\n",
      "  -8.02475438e-02  3.36425096e-01 -6.98641634e+00  3.23191918e-02\n",
      "   2.94087976e-01 -1.03110544e-01 -1.22243156e+01 -5.16207539e-04\n",
      "   5.36822271e+00 -2.89254725e-01]]\n",
      "38 Train Loss 0.21376908 Test MSE 25.72351043987534 Test RE 0.2585399737396311\n",
      "[[-9.73015558e-03  2.42383242e+00  5.45885980e-01  1.84620284e-02\n",
      "   2.61717127e-04 -2.60372009e+01 -3.69690967e+00 -4.92508616e-03\n",
      "   5.18717229e-01  1.16424108e+00 -2.97685146e-01 -1.96393635e-02\n",
      "  -4.47325528e-01  3.04234922e-02 -2.42573038e-01 -2.42267847e+00\n",
      "   1.14918149e+00 -1.85059845e-01 -1.76404297e-01 -1.38497388e+00\n",
      "  -1.53111629e-02 -5.38146645e-02  5.46149373e-01 -3.56091708e-01\n",
      "   0.00000000e+00 -5.40851533e-01  2.63884403e-02  2.50557601e-03\n",
      "  -2.62365065e-04 -2.02556439e-02 -2.44261339e-01 -1.98862571e-02\n",
      "   1.41310695e-04  1.07109736e-04 -4.11195755e-02  5.05611076e+01\n",
      "   5.82542643e-03 -2.32956081e-05  1.27626806e-01 -1.92846224e-01\n",
      "  -3.65695953e-02  2.46848702e-01 -8.36356068e+00 -1.34922620e-02\n",
      "   2.69566774e-01 -1.36446178e-01 -5.39872551e+00 -5.21668058e-04\n",
      "   3.87291932e+00 -2.85227060e-01]]\n",
      "39 Train Loss 0.14720497 Test MSE 18.197707184745994 Test RE 0.21745576151579465\n",
      "[[-9.87203885e-03  1.85283434e+00  4.28801328e-01  1.34493671e-02\n",
      "   1.58694329e-05 -2.72720699e+01 -3.86420035e+00 -8.12419970e-03\n",
      "   7.71337926e-01  1.30405366e+00 -3.14122379e-01 -2.79175211e-02\n",
      "  -4.09680516e-01  8.01892504e-02 -2.62651563e-01 -2.77601743e+00\n",
      "   1.33251369e+00 -2.15582028e-01 -1.94811061e-01 -1.00152135e+00\n",
      "  -1.34615637e-02 -3.85609306e-02  4.40607786e-01 -4.08071369e-01\n",
      "   0.00000000e+00 -4.84652400e-01  6.54881373e-02  2.56521907e-03\n",
      "  -1.45988161e-04 -2.87145190e-02 -2.49712825e-01 -1.90169904e-02\n",
      "   4.02757572e-03  3.97973042e-03 -2.56488603e-02  4.76935120e+01\n",
      "   1.13417702e-02 -6.08627270e-05  1.21780932e-01 -2.28228673e-01\n",
      "  -1.44211585e-02  2.30618775e-01 -9.70678711e+00 -5.36157973e-02\n",
      "   2.45337769e-01 -1.66780487e-01  7.89544344e-01 -5.49763034e-04\n",
      "   3.13545847e+00 -2.98918366e-01]]\n",
      "40 Train Loss 0.121510684 Test MSE 9.56800213285463 Test RE 0.15767876664534058\n",
      "[[-1.07219936e-02  1.55860186e+00  3.72653902e-01  1.03586549e-02\n",
      "   3.84804298e-05 -2.81564140e+01 -4.15734148e+00 -1.02639077e-02\n",
      "   9.64280844e-01  1.48507094e+00 -3.61838013e-01 -3.23485211e-02\n",
      "  -4.27110344e-01  1.23381816e-01 -2.96203345e-01 -3.16150689e+00\n",
      "   1.52904403e+00 -2.52124637e-01 -2.18222618e-01 -8.62138867e-01\n",
      "  -1.28188245e-02 -2.81556584e-02  4.08955097e-01 -4.75664645e-01\n",
      "   0.00000000e+00 -5.00946641e-01  1.01967394e-01  1.83453178e-03\n",
      "  -6.49769907e-04 -3.76631431e-02 -2.84556210e-01 -1.95467193e-02\n",
      "   8.75127781e-03  9.27159097e-03 -1.98544264e-02  4.84757347e+01\n",
      "   2.97515187e-02 -1.14398252e-04  1.00886859e-01 -2.73462236e-01\n",
      "  -7.59751583e-03  2.75527984e-01 -1.11647816e+01 -7.68124238e-02\n",
      "   2.61700124e-01 -1.96280956e-01  3.67814636e+00 -6.25914894e-04\n",
      "   2.80619693e+00 -3.42721045e-01]]\n",
      "41 Train Loss 0.1111341 Test MSE 8.878895701957182 Test RE 0.1518945038953573\n",
      "[[-1.10993795e-02  1.57247710e+00  3.79725397e-01  9.83838458e-03\n",
      "   6.92902904e-05 -2.47089577e+01 -4.11722755e+00 -9.88779031e-03\n",
      "   9.42917287e-01  1.53381729e+00 -3.74271989e-01 -3.15498151e-02\n",
      "  -4.70861852e-01  1.12136677e-01 -3.04302067e-01 -3.26467657e+00\n",
      "   1.56572616e+00 -2.55108863e-01 -2.23288432e-01 -9.22079444e-01\n",
      "  -1.34240696e-02 -2.95222383e-02  4.40703213e-01 -4.82490182e-01\n",
      "   0.00000000e+00 -5.57440639e-01  9.48967487e-02  1.68398488e-03\n",
      "  -3.42656160e-04 -3.69488820e-02 -2.96597719e-01 -2.01325342e-02\n",
      "   6.70557283e-03  7.32728420e-03 -1.97592694e-02  4.85390244e+01\n",
      "   3.86543721e-02 -1.10759072e-04  9.46288109e-02 -2.75719494e-01\n",
      "  -1.03904800e-02  3.07752579e-01 -1.12800417e+01 -6.24925755e-02\n",
      "   2.99861193e-01 -1.95420027e-01  9.70680296e-01 -6.53835421e-04\n",
      "   2.87156057e+00 -3.53965759e-01]]\n",
      "42 Train Loss 0.09948005 Test MSE 5.330594524992474 Test RE 0.11769293870096321\n",
      "[[-1.17408596e-02  1.48788488e+00  3.48249078e-01  7.85556249e-03\n",
      "  -3.74982810e-05 -2.29497585e+01 -4.02196026e+00 -1.09453211e-02\n",
      "   1.05303812e+00  1.65346968e+00 -3.99348944e-01 -3.23039368e-02\n",
      "  -5.00310540e-01  1.35533437e-01 -3.18107158e-01 -3.60371399e+00\n",
      "   1.68870819e+00 -2.70718217e-01 -2.33080178e-01 -1.07744777e+00\n",
      "  -1.32419653e-02 -2.78243199e-02  5.43237150e-01 -4.90574777e-01\n",
      "   0.00000000e+00 -5.96801341e-01  1.18830584e-01  1.64996507e-03\n",
      "  -4.42820630e-04 -4.35337014e-02 -3.25180441e-01 -2.09323894e-02\n",
      "   8.98343045e-03  1.05529325e-02 -1.81890056e-02  5.12294502e+01\n",
      "   6.67675510e-02 -1.50740452e-04  9.02751237e-02 -2.98233420e-01\n",
      "  -8.78257863e-03  3.32943916e-01 -1.19297113e+01 -5.53654544e-02\n",
      "   3.37071478e-01 -2.07111269e-01 -1.66511285e+00 -7.00576289e-04\n",
      "   2.89583802e+00 -3.78973275e-01]]\n",
      "43 Train Loss 0.08900999 Test MSE 4.855489620056977 Test RE 0.11232569063820345\n",
      "[[-1.2078078e-02  1.4060574e+00  3.1732059e-01  6.7562624e-03\n",
      "  -2.1666905e-04 -2.2122820e+01 -3.9109693e+00 -1.1421042e-02\n",
      "   1.1300249e+00  1.6950388e+00 -4.0876684e-01 -3.1141682e-02\n",
      "  -4.9852672e-01  1.5534756e-01 -3.1698427e-01 -3.8481252e+00\n",
      "   1.7372729e+00 -2.7562448e-01 -2.3144673e-01 -1.2551725e+00\n",
      "  -1.2770551e-02 -2.4384286e-02  6.5141356e-01 -4.7090131e-01\n",
      "   0.0000000e+00 -5.9870768e-01  1.4139602e-01  1.6303819e-03\n",
      "  -5.4587354e-04 -4.9558874e-02 -3.4318066e-01 -2.1337748e-02\n",
      "   1.1421138e-02  1.4184611e-02 -2.0697257e-02  5.1642033e+01\n",
      "   7.7361822e-02 -1.7345959e-04  8.4516391e-02 -3.1060424e-01\n",
      "  -6.2071164e-03  3.4029967e-01 -1.2171569e+01 -5.4974645e-02\n",
      "   3.4169185e-01 -2.1082436e-01 -2.5917556e+00 -7.2650734e-04\n",
      "   2.8820930e+00 -3.8954815e-01]]\n",
      "44 Train Loss 0.08749911 Test MSE 6.060551591217145 Test RE 0.12549275648612523\n",
      "[[-1.19031481e-02  1.42127252e+00  3.19441020e-01  7.01186247e-03\n",
      "  -2.27775410e-04 -2.23001213e+01 -3.87571120e+00 -1.11324815e-02\n",
      "   1.10740256e+00  1.65862525e+00 -3.99884462e-01 -3.04233283e-02\n",
      "  -4.87019837e-01  1.50688410e-01 -3.10173154e-01 -3.79124451e+00\n",
      "   1.70041955e+00 -2.69499302e-01 -2.26675406e-01 -1.27287507e+00\n",
      "  -1.27110612e-02 -2.46178992e-02  6.54312313e-01 -4.62349415e-01\n",
      "   0.00000000e+00 -5.85150778e-01  1.37401611e-01  1.73105986e-03\n",
      "  -5.17220644e-04 -4.85933200e-02 -3.36823046e-01 -2.11666729e-02\n",
      "   1.10427672e-02  1.36722140e-02 -2.23984234e-02  5.09880180e+01\n",
      "   6.97655007e-02 -1.68593382e-04  8.61861110e-02 -3.03981900e-01\n",
      "  -6.15647016e-03  3.28685224e-01 -1.19640741e+01 -5.45250289e-02\n",
      "   3.30579787e-01 -2.06614092e-01 -2.31521106e+00 -7.11968751e-04\n",
      "   2.89415288e+00 -3.81555915e-01]]\n",
      "45 Train Loss 0.08483939 Test MSE 6.440540659017627 Test RE 0.1293670710790577\n",
      "[[-1.17809987e-02  1.42604649e+00  3.12981874e-01  7.21378950e-03\n",
      "  -2.40141933e-04 -2.22347450e+01 -3.89861202e+00 -1.11184865e-02\n",
      "   1.10047436e+00  1.61769736e+00 -3.96494895e-01 -2.99761575e-02\n",
      "  -4.79028910e-01  1.52020246e-01 -3.06171060e-01 -3.81940866e+00\n",
      "   1.66015327e+00 -2.66531140e-01 -2.23765433e-01 -1.28877354e+00\n",
      "  -1.25029720e-02 -2.38247998e-02  6.59199238e-01 -4.57308412e-01\n",
      "   0.00000000e+00 -5.75785458e-01  1.40106753e-01  1.78284675e-03\n",
      "  -5.59914974e-04 -4.89670970e-02 -3.35279316e-01 -2.10263710e-02\n",
      "   1.17833959e-02  1.45578757e-02 -2.36823726e-02  5.02153931e+01\n",
      "   5.62710129e-02 -1.67175152e-04  8.89182612e-02 -3.02578330e-01\n",
      "  -5.61493775e-03  3.17644089e-01 -1.17586508e+01 -5.67214862e-02\n",
      "   3.15250069e-01 -2.05456913e-01 -1.82096350e+00 -7.01714074e-04\n",
      "   2.89909840e+00 -3.79068255e-01]]\n",
      "46 Train Loss 0.05571692 Test MSE 3.153665489747929 Test RE 0.09052538809491975\n",
      "[[-1.2004748e-02  1.1848930e+00  2.1413364e-01  6.4380202e-03\n",
      "  -4.1916498e-04 -1.7986313e+01 -4.5158916e+00 -1.3463957e-02\n",
      "   1.2505654e+00  1.4676927e+00 -4.2654085e-01 -3.0299012e-02\n",
      "  -4.5999238e-01  2.2068286e-01 -3.0670622e-01 -4.6048131e+00\n",
      "   1.5203449e+00 -2.7637249e-01 -2.2444040e-01 -1.3297607e+00\n",
      "  -1.0597551e-02 -1.1066267e-02  6.7154789e-01 -4.8157591e-01\n",
      "   0.0000000e+00 -5.5369133e-01  2.2818768e-01  1.6315290e-03\n",
      "  -1.3979598e-03 -6.3096046e-02 -3.7223038e-01 -2.0765927e-02\n",
      "   3.0127306e-02  3.7799712e-02 -2.6633479e-02  4.3079964e+01\n",
      "  -4.7803544e-02 -1.7373892e-04  8.0322921e-02 -3.3997771e-01\n",
      "  -1.2083052e-03  3.2966959e-01 -1.1228993e+01 -8.7624334e-02\n",
      "   2.4146993e-01 -2.2200954e-01  3.3235738e+00 -7.3058589e-04\n",
      "   2.5618691e+00 -4.1027966e-01]]\n",
      "47 Train Loss 0.028797504 Test MSE 0.7369804476048101 Test RE 0.04376133532735566\n",
      "[[-1.2758262e-02  8.8866794e-01  1.4487839e-01  4.5750677e-03\n",
      "  -5.6486024e-04 -1.3143262e+01 -5.0727534e+00 -1.6729087e-02\n",
      "   1.4786320e+00  1.5003860e+00 -4.6674910e-01 -3.3484567e-02\n",
      "  -4.4940737e-01  3.1552863e-01 -3.1625915e-01 -5.4444318e+00\n",
      "   1.5709722e+00 -2.9429027e-01 -2.3484100e-01 -1.2319889e+00\n",
      "  -9.4299251e-03 -4.4927811e-03  6.3485205e-01 -5.4015368e-01\n",
      "   0.0000000e+00 -5.4056609e-01  3.4387439e-01  1.4396193e-03\n",
      "  -2.3448830e-03 -7.7927150e-02 -4.1301233e-01 -2.0829892e-02\n",
      "   5.7692334e-02  7.2791569e-02 -2.2668466e-02  3.5824329e+01\n",
      "  -1.2617473e-01 -1.8253076e-04  5.8057904e-02 -3.8700491e-01\n",
      "  -1.1306432e-04  4.0385360e-01 -1.1448835e+01 -1.1002446e-01\n",
      "   2.3269579e-01 -2.4366054e-01  8.3442526e+00 -8.1744569e-04\n",
      "   2.0909858e+00 -4.4606772e-01]]\n",
      "48 Train Loss 0.023330625 Test MSE 0.5945406459535302 Test RE 0.03930550295072614\n",
      "[[-1.30284829e-02  7.87400961e-01  1.23130545e-01  3.94549314e-03\n",
      "  -6.36487443e-04 -1.14008102e+01 -5.14667273e+00 -1.77281238e-02\n",
      "   1.55715215e+00  1.50751591e+00 -4.78431135e-01 -3.35341766e-02\n",
      "  -4.39784616e-01  3.54721636e-01 -3.13501090e-01 -5.78513670e+00\n",
      "   1.58682966e+00 -2.95486331e-01 -2.34316662e-01 -1.23690414e+00\n",
      "  -8.93273670e-03 -2.89765862e-03  6.50910020e-01 -5.47488213e-01\n",
      "   0.00000000e+00 -5.30661643e-01  3.93491298e-01  1.41514407e-03\n",
      "  -2.65799649e-03 -8.46914724e-02 -4.29459512e-01 -2.08408646e-02\n",
      "   6.99069202e-02  8.89326110e-02 -2.31750980e-02  3.29445839e+01\n",
      "  -1.64188102e-01 -1.79110575e-04  4.97300960e-02 -4.03202087e-01\n",
      "  -1.51790221e-06  4.29514408e-01 -1.15656357e+01 -1.13970444e-01\n",
      "   2.29808062e-01 -2.48713493e-01  1.01053114e+01 -8.52074299e-04\n",
      "   1.93575406e+00 -4.56826925e-01]]\n",
      "49 Train Loss 0.019678188 Test MSE 0.6022220282516865 Test RE 0.03955859889370053\n",
      "[[-1.3402803e-02  7.4852097e-01  1.1881973e-01  3.6334309e-03\n",
      "  -6.1360572e-04 -1.0282291e+01 -5.1100326e+00 -1.7507296e-02\n",
      "   1.5622590e+00  1.5420082e+00 -4.9897575e-01 -3.2538492e-02\n",
      "  -4.7061303e-01  3.5497859e-01 -3.2150066e-01 -5.9888048e+00\n",
      "   1.6223177e+00 -3.0269730e-01 -2.3845868e-01 -1.2586415e+00\n",
      "  -9.0828678e-03 -2.5437369e-03  6.7045856e-01 -5.4969728e-01\n",
      "   0.0000000e+00 -5.7157612e-01  3.9888990e-01  1.1657148e-03\n",
      "  -2.3276641e-03 -8.6729489e-02 -4.5000783e-01 -2.1122776e-02\n",
      "   6.7892417e-02  8.8078335e-02 -2.4411390e-02  3.3117031e+01\n",
      "  -1.6715869e-01 -1.7866367e-04  4.2451374e-02 -4.1543400e-01\n",
      "  -4.7194294e-06  4.6085179e-01 -1.2064957e+01 -1.1227406e-01\n",
      "   2.4529529e-01 -2.5145561e-01  9.6393089e+00 -8.9469855e-04\n",
      "   1.8852926e+00 -4.7559124e-01]]\n",
      "50 Train Loss 0.015762987 Test MSE 0.27895187537359595 Test RE 0.026923233965212113\n",
      "[[-1.38950655e-02  6.54300630e-01  1.04629435e-01  3.04914545e-03\n",
      "  -5.45547286e-04 -8.05977726e+00 -5.22375870e+00 -1.84105150e-02\n",
      "   1.61224627e+00  1.56858146e+00 -5.31625211e-01 -3.36873308e-02\n",
      "  -5.04510105e-01  3.83008510e-01 -3.35935742e-01 -6.43439722e+00\n",
      "   1.65876245e+00 -3.16124916e-01 -2.49515817e-01 -1.17735827e+00\n",
      "  -9.11390781e-03 -1.70400925e-03  6.30612969e-01 -5.87010562e-01\n",
      "   0.00000000e+00 -6.15931332e-01  4.36919361e-01  8.77139508e-04\n",
      "  -2.19755038e-03 -9.00216848e-02 -4.77758497e-01 -2.11859886e-02\n",
      "   7.55147114e-02  9.86048430e-02 -2.24224161e-02  3.18406334e+01\n",
      "  -2.13747635e-01 -1.84206583e-04  3.27058211e-02 -4.39754099e-01\n",
      "  -7.03788464e-06  5.08096457e-01 -1.23351774e+01 -1.15468279e-01\n",
      "   2.60329783e-01 -2.61119664e-01  1.02412720e+01 -9.63233761e-04\n",
      "   1.70110071e+00 -5.03721595e-01]]\n",
      "51 Train Loss 0.013533174 Test MSE 0.3347713891007635 Test RE 0.02949420875000939\n",
      "[[-1.3881522e-02  6.4758652e-01  1.0663216e-01  3.1370528e-03\n",
      "  -4.5905620e-04 -7.3388348e+00 -5.2692327e+00 -1.8145489e-02\n",
      "   1.5809932e+00  1.5326519e+00 -5.3654224e-01 -3.3781424e-02\n",
      "  -5.1953399e-01  3.7216067e-01 -3.4017098e-01 -6.4605031e+00\n",
      "   1.6197873e+00 -3.1736225e-01 -2.5225368e-01 -1.1179540e+00\n",
      "  -9.3168002e-03 -1.6057359e-03  5.8859670e-01 -6.0698724e-01\n",
      "   0.0000000e+00 -6.3518524e-01  4.2855829e-01  8.1031251e-04\n",
      "  -1.9478956e-03 -8.6910546e-02 -4.7798479e-01 -2.1063887e-02\n",
      "   7.3246486e-02  9.5472865e-02 -2.2265924e-02  3.1456760e+01\n",
      "  -2.5274631e-01 -1.8478051e-04  3.0855160e-02 -4.3866155e-01\n",
      "  -4.4383727e-05  5.1099950e-01 -1.1895043e+01 -1.1509450e-01\n",
      "   2.5805300e-01 -2.6032484e-01  1.0008761e+01 -9.6991257e-04\n",
      "   1.6638777e+00 -5.0718886e-01]]\n",
      "52 Train Loss 0.010496004 Test MSE 0.4015572138717093 Test RE 0.032302512162916665\n",
      "[[-1.36367558e-02  6.81130946e-01  1.18916988e-01  3.62446159e-03\n",
      "  -3.02084518e-04 -7.39185190e+00 -5.31745005e+00 -1.75215323e-02\n",
      "   1.51955962e+00  1.49875569e+00 -5.29998600e-01 -3.46365832e-02\n",
      "  -5.22724330e-01  3.42559248e-01 -3.44964564e-01 -6.28372622e+00\n",
      "   1.57969654e+00 -3.17985833e-01 -2.54834950e-01 -9.64960515e-01\n",
      "  -9.70178284e-03 -1.90880976e-03  5.02334476e-01 -6.41097784e-01\n",
      "   0.00000000e+00 -6.36151850e-01  3.92698437e-01  7.74585526e-04\n",
      "  -1.78260880e-03 -7.96128288e-02 -4.58784461e-01 -2.07782518e-02\n",
      "   6.56442419e-02  8.41919407e-02 -2.11118404e-02  3.01688328e+01\n",
      "  -2.86644787e-01 -1.89370476e-04  3.30751240e-02 -4.28085446e-01\n",
      "  -1.75943947e-04  5.00594497e-01 -1.11967258e+01 -1.17443375e-01\n",
      "   2.52671570e-01 -2.58420378e-01  1.05764799e+01 -9.51001886e-04\n",
      "   1.67416036e+00 -4.98905033e-01]]\n",
      "53 Train Loss 0.00848886 Test MSE 0.07252488364933547 Test RE 0.013727963092222026\n",
      "[[-1.3872025e-02  6.3441068e-01  1.1355009e-01  3.3986394e-03\n",
      "  -2.1662650e-04 -6.7690883e+00 -5.4615769e+00 -1.8353904e-02\n",
      "   1.5599623e+00  1.5343239e+00 -5.5115211e-01 -3.6858059e-02\n",
      "  -5.3511006e-01  3.6163157e-01 -3.5940644e-01 -6.5172048e+00\n",
      "   1.6243336e+00 -3.3153066e-01 -2.6546833e-01 -8.2403743e-01\n",
      "  -9.7154211e-03 -1.5612066e-03  4.3795893e-01 -6.9327831e-01\n",
      "   0.0000000e+00 -6.4935505e-01  4.1183031e-01  6.4039172e-04\n",
      "  -1.9518399e-03 -8.0168694e-02 -4.6648315e-01 -2.0655535e-02\n",
      "   7.0535541e-02  8.9566968e-02 -1.8203450e-02  2.8267054e+01\n",
      "  -3.3171621e-01 -1.9962635e-04  2.9744165e-02 -4.4589278e-01\n",
      "  -1.6300607e-04  5.2947414e-01 -1.1193061e+01 -1.2673903e-01\n",
      "   2.5903839e-01 -2.6920673e-01  1.2423514e+01 -9.9179149e-04\n",
      "   1.5634369e+00 -5.1529652e-01]]\n",
      "54 Train Loss 0.00810015 Test MSE 0.040501271325017754 Test RE 0.010258808204165507\n",
      "[[-1.3926818e-02  6.3313335e-01  1.1322386e-01  3.3939928e-03\n",
      "  -2.1602934e-04 -6.8387575e+00 -5.4865255e+00 -1.8318910e-02\n",
      "   1.5638323e+00  1.5344588e+00 -5.5380243e-01 -3.6486112e-02\n",
      "  -5.3561366e-01  3.6327517e-01 -3.5924569e-01 -6.5421457e+00\n",
      "   1.6236718e+00 -3.3215702e-01 -2.6483536e-01 -8.2738942e-01\n",
      "  -9.6745770e-03 -1.4531028e-03  4.4011194e-01 -6.9577384e-01\n",
      "   0.0000000e+00 -6.5019035e-01  4.1506028e-01  6.0147099e-04\n",
      "  -1.9582312e-03 -8.0881730e-02 -4.6912318e-01 -2.0708261e-02\n",
      "   7.1037538e-02  9.0495698e-02 -1.8434251e-02  2.7954403e+01\n",
      "  -3.3924595e-01 -1.9983765e-04  2.9238449e-02 -4.4799033e-01\n",
      "  -1.3080757e-04  5.3633314e-01 -1.1183311e+01 -1.2850703e-01\n",
      "   2.5714809e-01 -2.6932216e-01  1.2814706e+01 -9.9746743e-04\n",
      "   1.5643941e+00 -5.1761901e-01]]\n",
      "55 Train Loss 0.0056603584 Test MSE 0.05459517536224294 Test RE 0.011910766620976806\n",
      "[[-1.3593474e-02  7.0193183e-01  1.2766840e-01  4.0763295e-03\n",
      "  -2.1414949e-04 -8.3595572e+00 -5.5466104e+00 -1.7208487e-02\n",
      "   1.5151076e+00  1.4535581e+00 -5.2360386e-01 -3.4078527e-02\n",
      "  -4.9255624e-01  3.3854577e-01 -3.3884761e-01 -6.2243147e+00\n",
      "   1.5279801e+00 -3.1596655e-01 -2.4885881e-01 -8.5388660e-01\n",
      "  -9.5929140e-03 -1.2842609e-03  4.3610537e-01 -6.7294735e-01\n",
      "   0.0000000e+00 -5.9607422e-01  3.9247766e-01  7.0993305e-04\n",
      "  -1.9103932e-03 -7.7578865e-02 -4.4408685e-01 -2.0677423e-02\n",
      "   6.7191578e-02  8.5537381e-02 -2.0842463e-02  2.5947578e+01\n",
      "  -3.6849779e-01 -1.9061881e-04  3.2805819e-02 -4.2427471e-01\n",
      "  -5.5638364e-05  5.2211410e-01 -1.0033373e+01 -1.3374925e-01\n",
      "   2.2533579e-01 -2.5511450e-01  1.4446334e+01 -9.5011119e-04\n",
      "   1.6711918e+00 -4.8979318e-01]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28803/129217413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28803/3000037595.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx_coll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolloc_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mf_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28803/1880311016.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_coll, f_hat)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 75\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    v = train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold,\"k\":k}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "plt.plot(k_mean[:,layer_num,0],'b')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,0] - 0.5*k_std[:,layer_num,0],k_mean[:,layer_num,0] + 0.5*k_std[:,layer_num,0],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,1],'r')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,1] - 0.5*k_std[:,layer_num,1],k_mean[:,layer_num,1] + 0.5*k_std[:,layer_num,1],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,2],'g')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,2] - 0.5*k_std[:,layer_num,2],k_mean[:,layer_num,2] + 0.5*k_std[:,layer_num,2],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"k_mean\":k_mean,\"k_std\":k_std}\n",
    "savemat('k_stan.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8bd0784a50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
