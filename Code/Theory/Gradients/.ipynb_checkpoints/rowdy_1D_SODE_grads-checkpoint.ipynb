{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(-3.0*x) + np.exp(2.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_Stan\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_tensor = torch.from_numpy(y_true).float().to(device)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a   \n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def forward_grads(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "\n",
    "        \n",
    "        i =0\n",
    "        z = self.linears[i](a)\n",
    "        a1 = self.activation(self.omega1[i,0]*z)\n",
    "        a = a1\n",
    "        for j in range(rowdy_terms):\n",
    "            a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "\n",
    "        z_np = z.cpu().detach().numpy()\n",
    "        a_np = a1.cpu().detach().numpy()\n",
    "\n",
    "        omega1 = self.omega1.cpu().detach().numpy()\n",
    "        alpha = self.alpha.cpu().detach().numpy()\n",
    "        n = self.n.cpu().detach().numpy()\n",
    "        omega = self.omega.cpu().detach().numpy()\n",
    "\n",
    "        a1_np = omega1[i,0]*(1-np.square(a_np))\n",
    "        for j in range(rowdy_terms):\n",
    "            a1_np = a1_np + alpha[j,i]*n*(j+1)*n*omega[j,i]*np.cos((j+1)*n*omega[j,i]*z_np)\n",
    "\n",
    "        a2_np = -2*np.square(omega1[i,0])*a_np*(1-np.square(a_np))\n",
    "        for j in range(rowdy_terms):\n",
    "            a2_np = a2_np - alpha[j,i]*n*np.square((j+1)*n*omega[j,i])*np.sin((j+1)*n*omega[j,i]*z_np)\n",
    "        \n",
    "        W2 = self.linears[1].weight.cpu().detach().numpy()\n",
    "        W1 = np.transpose(self.linears[0].weight.cpu().detach().numpy())\n",
    "        val = (W2)*np.square(W1)*a2_np\n",
    "         \n",
    "        return val\n",
    "    \n",
    "    def grad_test(self,x_grad):\n",
    "        g = x_grad.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "    \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_grad.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_x_w1 = autograd.grad(y_x,self.linears[0].weight,torch.ones([y_x.shape[0],1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "    \n",
    "            \n",
    "        \n",
    "        y_x_w1 = np.transpose(y_x_w1.cpu().detach().numpy())\n",
    "        sd_val = self.forward_grads(x_grad)\n",
    "        \n",
    "        # print(np.mean(y_x_w1))\n",
    "        # print(sd_val/y_x_w1)\n",
    "        return np.mean(np.abs(sd_val)/np.abs(y_x_w1+0.001))\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    x_grad = torch.zeros((1,1)).float().to(device)\n",
    "    v = PINN.grad_test(x_grad)\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    v = np.zeros((max_iter,1))\n",
    "    \n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        v[i] = train_step(x_coll,f_hat)\n",
    "       \n",
    "        \n",
    "        # print(\"k =\", k[i])\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4.450381 Test MSE 386.97198779002974 Test RE 1.002772341455907\n",
      "1 Train Loss 3.9365206 Test MSE 385.3362547479746 Test RE 1.0006507345525695\n",
      "2 Train Loss 2.736164 Test MSE 391.4160226964159 Test RE 1.008513886109815\n",
      "3 Train Loss 2.672448 Test MSE 392.82158751705606 Test RE 1.0103230370607683\n",
      "4 Train Loss 2.620924 Test MSE 391.83144670143616 Test RE 1.0090489303437788\n",
      "5 Train Loss 2.4301512 Test MSE 387.44851861050586 Test RE 1.003389575934524\n",
      "6 Train Loss 2.4104743 Test MSE 385.12170974489015 Test RE 1.0003721278768651\n",
      "7 Train Loss 2.3985324 Test MSE 383.07688208628736 Test RE 0.9977128242442393\n",
      "8 Train Loss 2.3970964 Test MSE 382.23707944455293 Test RE 0.9966186031862012\n",
      "9 Train Loss 2.3934739 Test MSE 381.83671374948403 Test RE 0.9960965234874638\n",
      "10 Train Loss 2.3911314 Test MSE 382.22940496800794 Test RE 0.9966085981855127\n",
      "11 Train Loss 2.3867774 Test MSE 381.9502095864526 Test RE 0.9962445506479685\n",
      "12 Train Loss 2.3776958 Test MSE 379.67649399177 Test RE 0.9932748468955984\n",
      "13 Train Loss 2.3736086 Test MSE 378.1895153343527 Test RE 0.9913278901003229\n",
      "14 Train Loss 2.3693461 Test MSE 377.13310881179746 Test RE 0.9899423709966874\n",
      "15 Train Loss 2.3684204 Test MSE 377.1834799570706 Test RE 0.9900084787778788\n",
      "16 Train Loss 2.3675199 Test MSE 377.99209261115624 Test RE 0.9910691095265299\n",
      "17 Train Loss 2.367338 Test MSE 378.19138569829136 Test RE 0.9913303414396242\n",
      "18 Train Loss 2.367315 Test MSE 378.1928266501256 Test RE 0.99133222997803\n",
      "19 Train Loss 2.3673143 Test MSE 378.18987329229753 Test RE 0.9913283592481917\n",
      "20 Train Loss 2.3673143 Test MSE 378.18987329229753 Test RE 0.9913283592481917\n",
      "21 Train Loss 2.3673143 Test MSE 378.18987329229753 Test RE 0.9913283592481917\n",
      "22 Train Loss 2.367301 Test MSE 378.1708694141385 Test RE 0.9913034520238647\n",
      "23 Train Loss 2.367301 Test MSE 378.1708694141385 Test RE 0.9913034520238647\n",
      "24 Train Loss 2.3672957 Test MSE 378.17676404556215 Test RE 0.9913111778247935\n",
      "25 Train Loss 2.3672955 Test MSE 378.17676416449444 Test RE 0.9913111779806717\n",
      "26 Train Loss 2.3672955 Test MSE 378.17676416449444 Test RE 0.9913111779806717\n",
      "27 Train Loss 2.3672955 Test MSE 378.17676416449444 Test RE 0.9913111779806717\n",
      "28 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "29 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "30 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "31 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "32 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "33 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "34 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "35 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "36 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "37 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "38 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "39 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "40 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "41 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "42 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "43 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "44 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "45 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "46 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "47 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "48 Train Loss 2.3672948 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "49 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "50 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "51 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "52 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "53 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "54 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "55 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "56 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "57 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "58 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "59 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "60 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "61 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "62 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "63 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "64 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "65 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "66 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "67 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "68 Train Loss 2.3672945 Test MSE 378.1767630747813 Test RE 0.9913111765524442\n",
      "69 Train Loss 2.367294 Test MSE 378.17685613638923 Test RE 0.9913112985231936\n",
      "70 Train Loss 2.367293 Test MSE 378.1813667489546 Test RE 0.9913172103178922\n",
      "71 Train Loss 2.3655019 Test MSE 378.12220496601464 Test RE 0.9912396676457123\n",
      "72 Train Loss 2.3583012 Test MSE 377.27709541628957 Test RE 0.990131329264844\n",
      "73 Train Loss 2.3532536 Test MSE 376.6225635344693 Test RE 0.989272075076383\n",
      "74 Train Loss 2.3435812 Test MSE 373.90786963942065 Test RE 0.9857002925554535\n",
      "Training time: 16.54\n",
      "Training time: 16.54\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6098/708037003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_loss_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test_mse_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_mse_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_re_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"beta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Thresh Time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Thresh epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "N_f = 1000\n",
    "x_coll = torch.from_numpy(colloc_pts(N_f,0)).float().to(device)\n",
    "f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "\n",
    "max_reps = 1\n",
    "max_iter = 75\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    v = train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)    \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold,\"k\":k}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 0\n",
    "plt.plot(k_mean[:,layer_num,0],'b')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,0] - 0.5*k_std[:,layer_num,0],k_mean[:,layer_num,0] + 0.5*k_std[:,layer_num,0],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,1],'r')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,1] - 0.5*k_std[:,layer_num,1],k_mean[:,layer_num,1] + 0.5*k_std[:,layer_num,1],alpha=0.3)\n",
    "plt.plot(k_mean[:,layer_num,2],'g')\n",
    "plt.fill_between(range(0,75),k_mean[:,layer_num,2] - 0.5*k_std[:,layer_num,2],k_mean[:,layer_num,2] + 0.5*k_std[:,layer_num,2],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdic = {\"v\":v}\n",
    "savemat('v_rowdy.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(v)\n",
    "plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
